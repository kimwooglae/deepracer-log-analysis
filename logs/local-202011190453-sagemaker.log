21:C 18 Nov 2020 07:08:23.345 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
21:C 18 Nov 2020 07:08:23.345 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=21, just started
21:C 18 Nov 2020 07:08:23.345 # Configuration loaded
                _._                                                  
           _.-``__ ''-._                                             
      _.-``    `.  `_.  ''-._           Redis 5.0.8 (00000000/0) 64 bit
  .-`` .-```.  ```\/    _.,_ ''-._                                   
 (    '      ,       .-`  | `,    )     Running in standalone mode
 |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
 |    `-._   `._    /     _.-'    |     PID: 21
  `-._    `-._  `-./  _.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |           http://redis.io        
  `-._    `-._`-.__.-'_.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |                                  
  `-._    `-._`-.__.-'_.-'    _.-'                                   
      `-._    `-.__.-'    _.-'                                       
          `-._        _.-'                                           
              `-.__.-'                                               

21:M 18 Nov 2020 07:08:23.347 # WARNING: The TCP backlog setting of 512 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
21:M 18 Nov 2020 07:08:23.347 # Server initialized
21:M 18 Nov 2020 07:08:23.347 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
21:M 18 Nov 2020 07:08:23.347 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
21:M 18 Nov 2020 07:08:23.347 * Ready to accept connections
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2020-11-18 07:08:25,575 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training
2020-11-18 07:08:25,845 sagemaker-containers INFO     Invoking user script

Training Env:

{
    "additional_framework_parameters": {
        "sagemaker_estimator": "RLEstimator"
    },
    "channel_input_dirs": {},
    "current_host": "algo-1-e446d",
    "framework_module": "sagemaker_tensorflow_container.training:main",
    "hosts": [
        "algo-1-e446d"
    ],
    "hyperparameters": {
        "s3_bucket": "bucket",
        "s3_prefix": "current",
        "aws_region": "us-east-1",
        "model_metadata_s3_key": "s3://bucket/custom_files/model_metadata.json",
        "RLCOACH_PRESET": "deepracer",
        "batch_size": 512,
        "beta_entropy": 0.01,
        "discount_factor": 0.9995,
        "e_greedy_value": 0.05,
        "epsilon_steps": 10000,
        "exploration_type": "categorical",
        "loss_type": "huber",
        "lr": 5e-05,
        "num_episodes_between_training": 50,
        "num_epochs": 10,
        "stack_size": 1,
        "term_cond_avg_score": 100000.0,
        "term_cond_max_episodes": 10000,
        "pretrained_s3_bucket": "bucket",
        "pretrained_s3_prefix": "rl-deepracer-pretrained"
    },
    "input_config_dir": "/opt/ml/input/config",
    "input_data_config": {},
    "input_dir": "/opt/ml/input",
    "is_master": true,
    "job_name": "current",
    "log_level": 20,
    "master_hostname": "algo-1-e446d",
    "model_dir": "/opt/ml/model",
    "module_dir": "s3://bucket/current/source/sourcedir.tar.gz",
    "module_name": "training_worker",
    "network_interface_name": "eth0",
    "num_cpus": 16,
    "num_gpus": 3,
    "output_data_dir": "/opt/ml/output/data",
    "output_dir": "/opt/ml/output",
    "output_intermediate_dir": "/opt/ml/output/intermediate",
    "resource_config": {
        "current_host": "algo-1-e446d",
        "hosts": [
            "algo-1-e446d"
        ]
    },
    "user_entry_point": "training_worker.py"
}

Environment variables:

SM_HOSTS=["algo-1-e446d"]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":512,"beta_entropy":0.01,"discount_factor":0.9995,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"huber","lr":5e-05,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":50,"num_epochs":10,"pretrained_s3_bucket":"bucket","pretrained_s3_prefix":"rl-deepracer-pretrained","s3_bucket":"bucket","s3_prefix":"current","stack_size":1,"term_cond_avg_score":100000.0,"term_cond_max_episodes":10000}
SM_USER_ENTRY_POINT=training_worker.py
SM_FRAMEWORK_PARAMS={"sagemaker_estimator":"RLEstimator"}
SM_RESOURCE_CONFIG={"current_host":"algo-1-e446d","hosts":["algo-1-e446d"]}
SM_INPUT_DATA_CONFIG={}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=[]
SM_CURRENT_HOST=algo-1-e446d
SM_MODULE_NAME=training_worker
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=16
SM_NUM_GPUS=3
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://bucket/current/source/sourcedir.tar.gz
SM_TRAINING_ENV={"additional_framework_parameters":{"sagemaker_estimator":"RLEstimator"},"channel_input_dirs":{},"current_host":"algo-1-e446d","framework_module":"sagemaker_tensorflow_container.training:main","hosts":["algo-1-e446d"],"hyperparameters":{"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":512,"beta_entropy":0.01,"discount_factor":0.9995,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"huber","lr":5e-05,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":50,"num_epochs":10,"pretrained_s3_bucket":"bucket","pretrained_s3_prefix":"rl-deepracer-pretrained","s3_bucket":"bucket","s3_prefix":"current","stack_size":1,"term_cond_avg_score":100000.0,"term_cond_max_episodes":10000},"input_config_dir":"/opt/ml/input/config","input_data_config":{},"input_dir":"/opt/ml/input","is_master":true,"job_name":"current","log_level":20,"master_hostname":"algo-1-e446d","model_dir":"/opt/ml/model","module_dir":"s3://bucket/current/source/sourcedir.tar.gz","module_name":"training_worker","network_interface_name":"eth0","num_cpus":16,"num_gpus":3,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1-e446d","hosts":["algo-1-e446d"]},"user_entry_point":"training_worker.py"}
SM_USER_ARGS=["--RLCOACH_PRESET","deepracer","--aws_region","us-east-1","--batch_size","512","--beta_entropy","0.01","--discount_factor","0.9995","--e_greedy_value","0.05","--epsilon_steps","10000","--exploration_type","categorical","--loss_type","huber","--lr","5e-05","--model_metadata_s3_key","s3://bucket/custom_files/model_metadata.json","--num_episodes_between_training","50","--num_epochs","10","--pretrained_s3_bucket","bucket","--pretrained_s3_prefix","rl-deepracer-pretrained","--s3_bucket","bucket","--s3_prefix","current","--stack_size","1","--term_cond_avg_score","100000.0","--term_cond_max_episodes","10000"]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_HP_S3_BUCKET=bucket
SM_HP_S3_PREFIX=current
SM_HP_AWS_REGION=us-east-1
SM_HP_MODEL_METADATA_S3_KEY=s3://bucket/custom_files/model_metadata.json
SM_HP_RLCOACH_PRESET=deepracer
SM_HP_BATCH_SIZE=512
SM_HP_BETA_ENTROPY=0.01
SM_HP_DISCOUNT_FACTOR=0.9995
SM_HP_E_GREEDY_VALUE=0.05
SM_HP_EPSILON_STEPS=10000
SM_HP_EXPLORATION_TYPE=categorical
SM_HP_LOSS_TYPE=huber
SM_HP_LR=5e-05
SM_HP_NUM_EPISODES_BETWEEN_TRAINING=50
SM_HP_NUM_EPOCHS=10
SM_HP_STACK_SIZE=1
SM_HP_TERM_COND_AVG_SCORE=100000.0
SM_HP_TERM_COND_MAX_EPISODES=10000
SM_HP_PRETRAINED_S3_BUCKET=bucket
SM_HP_PRETRAINED_S3_PREFIX=rl-deepracer-pretrained
PYTHONPATH=/usr/local/bin:/opt/amazon:/opt/ml/code:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages

Invoking script with the following command:

/usr/bin/python training_worker.py --RLCOACH_PRESET deepracer --aws_region us-east-1 --batch_size 512 --beta_entropy 0.01 --discount_factor 0.9995 --e_greedy_value 0.05 --epsilon_steps 10000 --exploration_type categorical --loss_type huber --lr 5e-05 --model_metadata_s3_key s3://bucket/custom_files/model_metadata.json --num_episodes_between_training 50 --num_epochs 10 --pretrained_s3_bucket bucket --pretrained_s3_prefix rl-deepracer-pretrained --s3_bucket bucket --s3_prefix current --stack_size 1 --term_cond_avg_score 100000.0 --term_cond_max_episodes 10000


S3 bucket: bucket 
 S3 prefix: current 
 S3 endpoint URL: http://minio:9000
Initializing SageS3Client...
Successfully downloaded model metadata from custom_files/model_metadata.json.
Sensor list ['STEREO_CAMERAS', 'SECTOR_LIDAR'], network DEEP_CONVOLUTIONAL_NETWORK_SHALLOW, simapp_version 2.0
Loaded action space from file: [{'steering_angle': -30.0, 'speed': 2.0, 'index': 0}, {'steering_angle': -11.5842, 'speed': 2.5479, 'index': 1}, {'steering_angle': -5.649, 'speed': 3.8431, 'index': 2}, {'steering_angle': -0.054, 'speed': 4.7192, 'index': 3}, {'steering_angle': 0.7294, 'speed': 5.8319, 'index': 4}, {'steering_angle': 2.3994, 'speed': 2.376, 'index': 5}, {'steering_angle': 4.9649, 'speed': 3.0723, 'index': 6}, {'steering_angle': 8.8131, 'speed': 3.8434, 'index': 7}, {'steering_angle': 16.2503, 'speed': 2.3931, 'index': 8}, {'steering_angle': 30.0, 'speed': 2.0, 'index': 9}]
Using the following hyper-parameters
{
  "batch_size": 512,
  "beta_entropy": 0.01,
  "discount_factor": 0.9995,
  "e_greedy_value": 0.05,
  "epsilon_steps": 10000,
  "exploration_type": "categorical",
  "loss_type": "huber",
  "lr": 5e-05,
  "num_episodes_between_training": 50,
  "num_epochs": 10,
  "stack_size": 1,
  "term_cond_avg_score": 100000.0,
  "term_cond_max_episodes": 10000
}
Uploaded hyperparameters.json to S3
Uploaded IP address information to S3: 172.18.0.5
Sensor list ['STEREO_CAMERAS', 'SECTOR_LIDAR'], network DEEP_CONVOLUTIONAL_NETWORK_SHALLOW, simapp_version 2.0
Unable to find best model data, using last model
## Creating graph - name: MultiAgentGraphManager
## Start physics before creating graph
## Create graph
## Creating agent - name: agent
## Created agent: agent
## Stop physics after creating graph
## Creating session
Checkpoint> Restoring from path=./pretrained_checkpoint/291_Step-716500.ckpt
Checkpoint> Saving in path=['./checkpoint/292_Step-0.ckpt']
Uploaded 3 files for checkpoint 292 in 0.46 seconds
saved intermediate frozen graph: current/model/model_292.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Uploaded 3 files for checkpoint 292 in 0.63 seconds
saved intermediate frozen graph: current/model/model_292.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
DoorMan: installing SIGINT, SIGTERM
Training> Name=main_level/agent, Worker=0, Episode=1, Total reward=22.22, Steps=21, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=2, Total reward=45.25, Steps=63, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=3, Total reward=1.3, Steps=90, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=4, Total reward=11.93, Steps=104, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=5, Total reward=18.48, Steps=128, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=6, Total reward=68.65, Steps=180, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=7, Total reward=48.07, Steps=220, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=8, Total reward=63.1, Steps=276, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=9, Total reward=19.4, Steps=295, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=10, Total reward=57.43, Steps=343, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=11, Total reward=33.46, Steps=378, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=12, Total reward=15.85, Steps=401, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=13, Total reward=52.91, Steps=431, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=14, Total reward=40.9, Steps=453, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=15, Total reward=22.59, Steps=495, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=16, Total reward=19.14, Steps=513, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=17, Total reward=34.91, Steps=550, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=18, Total reward=42.05, Steps=571, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=19, Total reward=16.18, Steps=591, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=20, Total reward=75.84, Steps=625, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=21, Total reward=58.06, Steps=655, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=22, Total reward=28.49, Steps=676, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=23, Total reward=0.02, Steps=691, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=24, Total reward=6.57, Steps=720, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=25, Total reward=45.92, Steps=799, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=26, Total reward=59.25, Steps=872, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=27, Total reward=6.54, Steps=888, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=28, Total reward=53.54, Steps=914, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=29, Total reward=25.15, Steps=943, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=30, Total reward=35.34, Steps=994, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=31, Total reward=25.43, Steps=1023, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=32, Total reward=5.65, Steps=1034, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=33, Total reward=0.02, Steps=1052, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=34, Total reward=35.59, Steps=1071, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=35, Total reward=34.08, Steps=1100, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=36, Total reward=17.97, Steps=1131, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=37, Total reward=39.41, Steps=1163, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=38, Total reward=30.73, Steps=1183, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=39, Total reward=6.5, Steps=1205, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=40, Total reward=85.29, Steps=1244, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=41, Total reward=47.98, Steps=1270, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=42, Total reward=40.96, Steps=1298, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=43, Total reward=41.89, Steps=1359, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=44, Total reward=7.41, Steps=1383, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=45, Total reward=14.86, Steps=1407, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=46, Total reward=50.72, Steps=1447, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=47, Total reward=16.28, Steps=1466, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=48, Total reward=61.86, Steps=1510, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=49, Total reward=22.69, Steps=1531, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=50, Total reward=71.57, Steps=1567, Training iteration=0
Policy training> Surrogate loss=0.0031735326629132032, KL divergence=0.0003150120028294623, Entropy=0.28396400809288025, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04060187190771103, KL divergence=0.010764852166175842, Entropy=0.2795100808143616, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.042236194014549255, KL divergence=0.02359442226588726, Entropy=0.2743164598941803, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04903234541416168, KL divergence=0.0374242402613163, Entropy=0.27153950929641724, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05981403589248657, KL divergence=0.05004400014877319, Entropy=0.26978734135627747, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0648772343993187, KL divergence=0.06073083356022835, Entropy=0.2655197083950043, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06244410201907158, KL divergence=0.07333839684724808, Entropy=0.26671668887138367, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06530485302209854, KL divergence=0.08332286030054092, Entropy=0.2675231695175171, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.057693708688020706, KL divergence=0.09242319315671921, Entropy=0.2660599648952484, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06654481589794159, KL divergence=0.10055341571569443, Entropy=0.26910069584846497, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/293_Step-1567.ckpt']
Uploaded 3 files for checkpoint 293 in 0.57 seconds
saved intermediate frozen graph: current/model/model_293.pb
Best checkpoint number: 292, Last checkpoint number: 292
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=51, Total reward=70.18, Steps=1608, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=52, Total reward=8.76, Steps=1628, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=53, Total reward=44.93, Steps=1658, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=54, Total reward=42.11, Steps=1680, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=55, Total reward=34.16, Steps=1712, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=56, Total reward=21.97, Steps=1736, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=57, Total reward=25.16, Steps=1754, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=58, Total reward=26.21, Steps=1773, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=59, Total reward=50.86, Steps=1819, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=60, Total reward=22.87, Steps=1855, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=61, Total reward=35.77, Steps=1879, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=62, Total reward=30.77, Steps=1903, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=63, Total reward=3.4, Steps=1923, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=64, Total reward=17.97, Steps=1955, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=65, Total reward=41.03, Steps=2037, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=66, Total reward=50.01, Steps=2082, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=67, Total reward=9.37, Steps=2105, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=68, Total reward=50.1, Steps=2132, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=69, Total reward=85.05, Steps=2192, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=70, Total reward=67.46, Steps=2239, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=71, Total reward=36.26, Steps=2266, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=72, Total reward=9.46, Steps=2287, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=73, Total reward=46.2, Steps=2317, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=74, Total reward=27.08, Steps=2328, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=75, Total reward=21.67, Steps=2376, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=76, Total reward=24.94, Steps=2395, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=77, Total reward=39.19, Steps=2429, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=78, Total reward=32.99, Steps=2450, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=79, Total reward=43.33, Steps=2489, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=80, Total reward=23.92, Steps=2519, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=81, Total reward=38.43, Steps=2543, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=82, Total reward=32.77, Steps=2566, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=83, Total reward=5.2, Steps=2604, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=84, Total reward=17.08, Steps=2628, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=85, Total reward=19.8, Steps=2652, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=86, Total reward=7.29, Steps=2675, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=87, Total reward=28.97, Steps=2708, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=88, Total reward=43.62, Steps=2733, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=89, Total reward=19.59, Steps=2762, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=90, Total reward=45.01, Steps=2799, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=91, Total reward=20.96, Steps=2830, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=92, Total reward=3.75, Steps=2841, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=93, Total reward=36.3, Steps=2862, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=94, Total reward=23.06, Steps=2883, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=95, Total reward=28.42, Steps=2913, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=96, Total reward=19.75, Steps=2930, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=97, Total reward=1.87, Steps=2946, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=98, Total reward=34.71, Steps=2972, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=99, Total reward=24.84, Steps=2990, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=100, Total reward=68.89, Steps=3025, Training iteration=1
Policy training> Surrogate loss=-0.032706119120121, KL divergence=2.674377719813492e-05, Entropy=0.28475743532180786, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.010500905103981495, KL divergence=0.0014691652031615376, Entropy=0.28922173380851746, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.030588679015636444, KL divergence=0.006142395548522472, Entropy=0.2907344102859497, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0432087779045105, KL divergence=0.012952066026628017, Entropy=0.29058605432510376, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.02730157971382141, KL divergence=0.02280844748020172, Entropy=0.2961695194244385, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.03637174144387245, KL divergence=0.033950574696063995, Entropy=0.28408974409103394, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04177025705575943, KL divergence=0.043571583926677704, Entropy=0.28539836406707764, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.050457462668418884, KL divergence=0.04909100383520126, Entropy=0.2729465961456299, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06266260892152786, KL divergence=0.06276252865791321, Entropy=0.27951961755752563, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.04947624355554581, KL divergence=0.06912808865308762, Entropy=0.2723492980003357, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/294_Step-3025.ckpt']
Uploaded 3 files for checkpoint 294 in 0.62 seconds
saved intermediate frozen graph: current/model/model_294.pb
Best checkpoint number: 292, Last checkpoint number: 292
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=101, Total reward=44.39, Steps=3051, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=102, Total reward=32.47, Steps=3074, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=103, Total reward=7.23, Steps=3107, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=104, Total reward=12.54, Steps=3123, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=105, Total reward=19.19, Steps=3139, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=106, Total reward=3.6, Steps=3151, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=107, Total reward=48.71, Steps=3190, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=108, Total reward=65.95, Steps=3221, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=109, Total reward=62.48, Steps=3268, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=110, Total reward=58.12, Steps=3313, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=111, Total reward=15.62, Steps=3346, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=112, Total reward=59.22, Steps=3380, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=113, Total reward=46.63, Steps=3411, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=114, Total reward=39.37, Steps=3432, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=115, Total reward=27.84, Steps=3462, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=116, Total reward=22.66, Steps=3491, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=117, Total reward=50.15, Steps=3530, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=118, Total reward=29.14, Steps=3555, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=119, Total reward=12.15, Steps=3583, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=120, Total reward=73.33, Steps=3617, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=121, Total reward=30.19, Steps=3642, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=122, Total reward=38.58, Steps=3666, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=123, Total reward=62.83, Steps=3741, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=124, Total reward=0.02, Steps=3756, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=125, Total reward=8.27, Steps=3773, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=126, Total reward=44.66, Steps=3803, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=127, Total reward=53.5, Steps=3854, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=128, Total reward=57.76, Steps=3884, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=129, Total reward=31.6, Steps=3912, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=130, Total reward=54.02, Steps=3958, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=131, Total reward=30.86, Steps=3987, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=132, Total reward=7.54, Steps=3998, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=133, Total reward=34.35, Steps=4024, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=134, Total reward=29.7, Steps=4035, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=135, Total reward=22.29, Steps=4071, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=136, Total reward=17.42, Steps=4105, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=137, Total reward=42.52, Steps=4139, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=138, Total reward=34.41, Steps=4162, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=139, Total reward=3.81, Steps=4181, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=140, Total reward=47.99, Steps=4223, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=141, Total reward=54.62, Steps=4252, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=142, Total reward=25.65, Steps=4291, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=143, Total reward=3.89, Steps=4304, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=144, Total reward=15.34, Steps=4343, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=145, Total reward=20.53, Steps=4361, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=146, Total reward=61.97, Steps=4405, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=147, Total reward=26.18, Steps=4436, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=148, Total reward=55.1, Steps=4463, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=149, Total reward=16.32, Steps=4489, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=150, Total reward=10.98, Steps=4503, Training iteration=2
Policy training> Surrogate loss=0.012003034353256226, KL divergence=1.924300886457786e-05, Entropy=0.3058459162712097, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.027665603905916214, KL divergence=0.001089209457859397, Entropy=0.30881696939468384, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.040684014558792114, KL divergence=0.0056195384822785854, Entropy=0.28553780913352966, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03723921626806259, KL divergence=0.011956711299717426, Entropy=0.2898433208465576, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05315903201699257, KL divergence=0.022891107946634293, Entropy=0.29650020599365234, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.02103351429104805, KL divergence=0.029886826872825623, Entropy=0.2804988622665405, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0529000386595726, KL divergence=0.04200195148587227, Entropy=0.29477399587631226, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.040765225887298584, KL divergence=0.046622905880212784, Entropy=0.28305840492248535, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04425901919603348, KL divergence=0.060222938656806946, Entropy=0.28724002838134766, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06227943301200867, KL divergence=0.06563892215490341, Entropy=0.2869398593902588, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/295_Step-4503.ckpt']
Uploaded 3 files for checkpoint 295 in 0.64 seconds
saved intermediate frozen graph: current/model/model_295.pb
Best checkpoint number: 292, Last checkpoint number: 293
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'293'}
Training> Name=main_level/agent, Worker=0, Episode=151, Total reward=49.71, Steps=4543, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=152, Total reward=21.52, Steps=4570, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=153, Total reward=46.38, Steps=4599, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=154, Total reward=50.73, Steps=4622, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=155, Total reward=16.21, Steps=4652, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=156, Total reward=23.22, Steps=4672, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=157, Total reward=38.25, Steps=4706, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=158, Total reward=33.56, Steps=4728, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=159, Total reward=65.61, Steps=4775, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=160, Total reward=62.78, Steps=4807, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=161, Total reward=56.8, Steps=4840, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=162, Total reward=35.05, Steps=4862, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=163, Total reward=0.02, Steps=4884, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=164, Total reward=12.85, Steps=4903, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=165, Total reward=52.58, Steps=4956, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=166, Total reward=29.53, Steps=5009, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=167, Total reward=9.8, Steps=5026, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=168, Total reward=51.17, Steps=5054, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=169, Total reward=25.78, Steps=5074, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=170, Total reward=28.75, Steps=5117, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=171, Total reward=35.02, Steps=5144, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=172, Total reward=45.83, Steps=5176, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=173, Total reward=25.33, Steps=5197, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=174, Total reward=19.75, Steps=5220, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=175, Total reward=38.68, Steps=5253, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=176, Total reward=15.61, Steps=5269, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=177, Total reward=38.51, Steps=5306, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=178, Total reward=35.55, Steps=5331, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=179, Total reward=0.01, Steps=5345, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=180, Total reward=64.52, Steps=5386, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=181, Total reward=28.6, Steps=5410, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=182, Total reward=32.05, Steps=5434, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=183, Total reward=3.3, Steps=5458, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=184, Total reward=14.32, Steps=5483, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=185, Total reward=49.5, Steps=5537, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=186, Total reward=15.0, Steps=5554, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=187, Total reward=37.66, Steps=5589, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=188, Total reward=59.62, Steps=5617, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=189, Total reward=35.61, Steps=5641, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=190, Total reward=21.54, Steps=5666, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=191, Total reward=19.94, Steps=5698, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=192, Total reward=55.41, Steps=5732, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=193, Total reward=47.13, Steps=5754, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=194, Total reward=33.41, Steps=5775, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=195, Total reward=16.58, Steps=5821, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=196, Total reward=20.61, Steps=5842, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=197, Total reward=19.17, Steps=5865, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=198, Total reward=39.45, Steps=5889, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=199, Total reward=8.04, Steps=5906, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=200, Total reward=54.84, Steps=5945, Training iteration=3
Policy training> Surrogate loss=0.0020366841927170753, KL divergence=1.6003185010049492e-05, Entropy=0.276117742061615, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0403071753680706, KL divergence=0.0012448860798031092, Entropy=0.2737114727497101, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.028051121160387993, KL divergence=0.005118913017213345, Entropy=0.26566940546035767, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03651542216539383, KL divergence=0.012212889268994331, Entropy=0.2773165702819824, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.01598924957215786, KL divergence=0.019672323018312454, Entropy=0.27785027027130127, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04410892352461815, KL divergence=0.030050130560994148, Entropy=0.2662714421749115, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=0.01105134841054678, KL divergence=0.03558163717389107, Entropy=0.27095162868499756, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07548409700393677, KL divergence=0.05124502629041672, Entropy=0.26339301466941833, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.028877034783363342, KL divergence=0.060411080718040466, Entropy=0.26807570457458496, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.034810297191143036, KL divergence=0.06731459498405457, Entropy=0.2611204981803894, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/296_Step-5945.ckpt']
Uploaded 3 files for checkpoint 296 in 0.56 seconds
saved intermediate frozen graph: current/model/model_296.pb
Best checkpoint number: 292, Last checkpoint number: 294
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'294'}
Training> Name=main_level/agent, Worker=0, Episode=201, Total reward=34.72, Steps=5972, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=202, Total reward=31.49, Steps=5996, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=203, Total reward=20.33, Steps=6026, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=204, Total reward=19.27, Steps=6061, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=205, Total reward=20.18, Steps=6081, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=206, Total reward=55.6, Steps=6129, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=207, Total reward=3.3, Steps=6152, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=208, Total reward=60.43, Steps=6195, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=209, Total reward=27.58, Steps=6224, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=210, Total reward=24.21, Steps=6257, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=211, Total reward=14.25, Steps=6278, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=212, Total reward=26.42, Steps=6306, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=213, Total reward=34.96, Steps=6326, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=214, Total reward=33.03, Steps=6348, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=215, Total reward=38.24, Steps=6379, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=216, Total reward=17.13, Steps=6411, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=217, Total reward=150.71, Steps=6559, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=218, Total reward=48.01, Steps=6581, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=219, Total reward=4.42, Steps=6602, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=220, Total reward=77.41, Steps=6640, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=221, Total reward=55.53, Steps=6673, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=222, Total reward=37.99, Steps=6699, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=223, Total reward=0.02, Steps=6719, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=224, Total reward=12.27, Steps=6753, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=225, Total reward=16.67, Steps=6779, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=226, Total reward=66.09, Steps=6831, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=227, Total reward=56.93, Steps=6868, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=228, Total reward=41.12, Steps=6893, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=229, Total reward=53.82, Steps=6954, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=230, Total reward=62.1, Steps=6991, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=231, Total reward=44.16, Steps=7030, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=232, Total reward=47.77, Steps=7061, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=233, Total reward=38.14, Steps=7090, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=234, Total reward=22.01, Steps=7100, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=235, Total reward=24.38, Steps=7129, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=236, Total reward=16.63, Steps=7153, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=237, Total reward=35.95, Steps=7175, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=238, Total reward=44.01, Steps=7197, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=239, Total reward=82.59, Steps=7278, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=240, Total reward=57.75, Steps=7314, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=241, Total reward=48.09, Steps=7341, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=242, Total reward=39.03, Steps=7385, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=243, Total reward=0.02, Steps=7407, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=244, Total reward=21.91, Steps=7441, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=245, Total reward=11.33, Steps=7458, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=246, Total reward=60.21, Steps=7501, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=247, Total reward=0.01, Steps=7512, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=248, Total reward=50.81, Steps=7538, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=249, Total reward=26.35, Steps=7554, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=250, Total reward=73.8, Steps=7601, Training iteration=4
Policy training> Surrogate loss=-0.009890936315059662, KL divergence=4.5520351704908535e-05, Entropy=0.3111403286457062, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.022290190681815147, KL divergence=0.004367294255644083, Entropy=0.3127310574054718, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04700290784239769, KL divergence=0.016066689044237137, Entropy=0.3019709289073944, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04637088254094124, KL divergence=0.031162044033408165, Entropy=0.29993727803230286, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04098004475235939, KL divergence=0.046854857355356216, Entropy=0.2972118556499481, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.057475995272397995, KL divergence=0.0636904314160347, Entropy=0.2950601875782013, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.055764708667993546, KL divergence=0.07460976392030716, Entropy=0.2901240289211273, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05680355057120323, KL divergence=0.08532338589429855, Entropy=0.28490138053894043, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05178605392575264, KL divergence=0.09575995802879333, Entropy=0.28838613629341125, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05138079449534416, KL divergence=0.10389088839292526, Entropy=0.2881235182285309, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/297_Step-7601.ckpt']
Uploaded 3 files for checkpoint 297 in 0.59 seconds
saved intermediate frozen graph: current/model/model_297.pb
Best checkpoint number: 292, Last checkpoint number: 295
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'295'}
Training> Name=main_level/agent, Worker=0, Episode=251, Total reward=55.98, Steps=7641, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=252, Total reward=51.1, Steps=7683, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=253, Total reward=27.49, Steps=7703, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=254, Total reward=47.69, Steps=7727, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=255, Total reward=20.48, Steps=7756, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=256, Total reward=22.21, Steps=7790, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=257, Total reward=43.09, Steps=7818, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=258, Total reward=23.33, Steps=7830, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=259, Total reward=4.7, Steps=7844, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=260, Total reward=39.14, Steps=7875, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=261, Total reward=41.95, Steps=7900, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=262, Total reward=34.11, Steps=7936, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=263, Total reward=3.26, Steps=7957, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=264, Total reward=12.44, Steps=7979, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=265, Total reward=68.1, Steps=8035, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=266, Total reward=18.23, Steps=8062, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=267, Total reward=50.97, Steps=8099, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=268, Total reward=38.09, Steps=8124, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=269, Total reward=28.6, Steps=8145, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=270, Total reward=11.61, Steps=8175, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=271, Total reward=51.21, Steps=8218, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=272, Total reward=53.26, Steps=8251, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=273, Total reward=29.39, Steps=8272, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=274, Total reward=32.57, Steps=8293, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=275, Total reward=25.69, Steps=8324, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=276, Total reward=17.24, Steps=8356, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=277, Total reward=92.38, Steps=8442, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=278, Total reward=35.67, Steps=8468, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=279, Total reward=19.77, Steps=8500, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=280, Total reward=55.17, Steps=8535, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=281, Total reward=39.24, Steps=8563, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=282, Total reward=39.11, Steps=8596, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=283, Total reward=66.45, Steps=8678, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=284, Total reward=58.85, Steps=8746, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=285, Total reward=14.09, Steps=8768, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=286, Total reward=27.1, Steps=8799, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=287, Total reward=56.87, Steps=8836, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=288, Total reward=38.46, Steps=8860, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=289, Total reward=20.61, Steps=8887, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=290, Total reward=39.87, Steps=8923, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=291, Total reward=25.6, Steps=8962, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=292, Total reward=15.93, Steps=8990, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=293, Total reward=33.46, Steps=9014, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=294, Total reward=25.22, Steps=9024, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=295, Total reward=11.19, Steps=9054, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=296, Total reward=44.12, Steps=9100, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=297, Total reward=53.74, Steps=9136, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=298, Total reward=29.93, Steps=9160, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=299, Total reward=21.55, Steps=9184, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=300, Total reward=21.64, Steps=9203, Training iteration=5
Policy training> Surrogate loss=0.011800989508628845, KL divergence=7.247160829138011e-05, Entropy=0.3052745759487152, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.018830204382538795, KL divergence=0.005232608411461115, Entropy=0.3003368675708771, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03458402678370476, KL divergence=0.017048001289367676, Entropy=0.2967725098133087, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04395603761076927, KL divergence=0.03425710275769234, Entropy=0.2920416593551636, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.049620043486356735, KL divergence=0.050983916968107224, Entropy=0.2891647517681122, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05860162898898125, KL divergence=0.06593554466962814, Entropy=0.28430551290512085, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.053575921803712845, KL divergence=0.0807323232293129, Entropy=0.28447726368904114, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05998006463050842, KL divergence=0.09364240616559982, Entropy=0.2825327515602112, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06328720599412918, KL divergence=0.10096568614244461, Entropy=0.28117993474006653, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.059220362454652786, KL divergence=0.11016645282506943, Entropy=0.28103211522102356, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/298_Step-9203.ckpt']
Uploaded 3 files for checkpoint 298 in 0.52 seconds
saved intermediate frozen graph: current/model/model_298.pb
Best checkpoint number: 292, Last checkpoint number: 296
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'296'}
Training> Name=main_level/agent, Worker=0, Episode=301, Total reward=34.3, Steps=9229, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=302, Total reward=39.48, Steps=9273, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=303, Total reward=40.9, Steps=9347, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=304, Total reward=3.58, Steps=9376, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=305, Total reward=11.06, Steps=9416, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=306, Total reward=49.93, Steps=9473, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=307, Total reward=7.56, Steps=9499, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=308, Total reward=44.31, Steps=9522, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=309, Total reward=74.66, Steps=9574, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=310, Total reward=75.91, Steps=9625, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=311, Total reward=34.29, Steps=9656, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=312, Total reward=47.54, Steps=9688, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=313, Total reward=46.39, Steps=9717, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=314, Total reward=45.33, Steps=9740, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=315, Total reward=19.17, Steps=9773, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=316, Total reward=12.86, Steps=9805, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=317, Total reward=65.0, Steps=9845, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=318, Total reward=38.93, Steps=9868, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=319, Total reward=7.51, Steps=9885, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=320, Total reward=60.83, Steps=9924, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=321, Total reward=50.72, Steps=9954, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=322, Total reward=33.17, Steps=9974, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=323, Total reward=14.66, Steps=10009, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=324, Total reward=1.18, Steps=10036, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=325, Total reward=14.91, Steps=10072, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=326, Total reward=70.04, Steps=10114, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=327, Total reward=3.29, Steps=10142, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=328, Total reward=65.8, Steps=10173, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=329, Total reward=19.9, Steps=10190, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=330, Total reward=41.97, Steps=10226, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=331, Total reward=28.76, Steps=10268, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=332, Total reward=9.35, Steps=10290, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=333, Total reward=47.27, Steps=10321, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=334, Total reward=46.53, Steps=10344, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=335, Total reward=31.16, Steps=10373, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=336, Total reward=24.36, Steps=10394, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=337, Total reward=45.03, Steps=10433, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=338, Total reward=32.9, Steps=10456, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=339, Total reward=70.84, Steps=10523, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=340, Total reward=53.42, Steps=10552, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=341, Total reward=24.13, Steps=10580, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=342, Total reward=31.87, Steps=10600, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=343, Total reward=3.54, Steps=10619, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=344, Total reward=7.66, Steps=10646, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=345, Total reward=32.55, Steps=10698, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=346, Total reward=18.16, Steps=10728, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=347, Total reward=53.84, Steps=10762, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=348, Total reward=43.02, Steps=10787, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=349, Total reward=20.45, Steps=10802, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=350, Total reward=63.41, Steps=10850, Training iteration=6
Policy training> Surrogate loss=0.0017196949338540435, KL divergence=0.00010939787171082571, Entropy=0.3165510594844818, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.016129055991768837, KL divergence=0.00590972974896431, Entropy=0.31328490376472473, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04778946936130524, KL divergence=0.01841125637292862, Entropy=0.3097088038921356, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05774465203285217, KL divergence=0.03557109460234642, Entropy=0.30220237374305725, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05100502446293831, KL divergence=0.05176876485347748, Entropy=0.29751160740852356, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05972588062286377, KL divergence=0.06591083854436874, Entropy=0.2895607650279999, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06117257475852966, KL divergence=0.08472337573766708, Entropy=0.29333314299583435, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.057253655046224594, KL divergence=0.09510939568281174, Entropy=0.28720101714134216, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05742144584655762, KL divergence=0.10453899949789047, Entropy=0.2867804169654846, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05693623796105385, KL divergence=0.11516179889440536, Entropy=0.2860659062862396, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/299_Step-10850.ckpt']
Uploaded 3 files for checkpoint 299 in 0.62 seconds
saved intermediate frozen graph: current/model/model_299.pb
Best checkpoint number: 292, Last checkpoint number: 297
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'297'}
Training> Name=main_level/agent, Worker=0, Episode=351, Total reward=60.02, Steps=10887, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=352, Total reward=10.97, Steps=10910, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=353, Total reward=44.17, Steps=10940, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=354, Total reward=32.29, Steps=10961, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=355, Total reward=32.7, Steps=10990, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=356, Total reward=12.59, Steps=11017, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=357, Total reward=25.74, Steps=11057, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=358, Total reward=32.35, Steps=11080, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=359, Total reward=6.05, Steps=11093, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=360, Total reward=50.46, Steps=11125, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=361, Total reward=32.21, Steps=11149, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=362, Total reward=34.0, Steps=11172, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=363, Total reward=3.43, Steps=11191, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=364, Total reward=19.7, Steps=11215, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=365, Total reward=14.78, Steps=11240, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=366, Total reward=14.78, Steps=11255, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=367, Total reward=47.73, Steps=11295, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=368, Total reward=62.08, Steps=11323, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=369, Total reward=27.51, Steps=11343, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=370, Total reward=17.56, Steps=11368, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=371, Total reward=41.2, Steps=11407, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=372, Total reward=59.65, Steps=11451, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=373, Total reward=25.29, Steps=11472, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=374, Total reward=40.81, Steps=11494, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=375, Total reward=17.66, Steps=11521, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=376, Total reward=11.51, Steps=11554, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=377, Total reward=41.95, Steps=11590, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=378, Total reward=31.17, Steps=11612, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=379, Total reward=71.87, Steps=11663, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=380, Total reward=47.76, Steps=11694, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=381, Total reward=31.85, Steps=11722, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=382, Total reward=30.69, Steps=11746, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=383, Total reward=6.63, Steps=11768, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=384, Total reward=11.52, Steps=11791, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=385, Total reward=6.05, Steps=11808, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=386, Total reward=34.7, Steps=11854, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=387, Total reward=6.65, Steps=11887, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=388, Total reward=65.07, Steps=11933, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=389, Total reward=18.88, Steps=11947, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=390, Total reward=47.1, Steps=11994, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=391, Total reward=35.12, Steps=12029, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=392, Total reward=46.23, Steps=12061, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=393, Total reward=45.71, Steps=12090, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=394, Total reward=43.95, Steps=12113, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=395, Total reward=30.94, Steps=12144, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=396, Total reward=15.76, Steps=12163, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=397, Total reward=12.89, Steps=12183, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=398, Total reward=28.86, Steps=12200, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=399, Total reward=16.2, Steps=12219, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=400, Total reward=83.47, Steps=12255, Training iteration=7
Policy training> Surrogate loss=0.00925234705209732, KL divergence=1.5834095393074676e-05, Entropy=0.3031160235404968, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.016305873170495033, KL divergence=0.0021239861380308867, Entropy=0.3014490306377411, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06088647246360779, KL divergence=0.009373411536216736, Entropy=0.29957228899002075, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04018884897232056, KL divergence=0.021352704614400864, Entropy=0.2937507927417755, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04587842896580696, KL divergence=0.03428874909877777, Entropy=0.28853756189346313, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04924584552645683, KL divergence=0.05066453665494919, Entropy=0.28360897302627563, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.03251189738512039, KL divergence=0.06439661979675293, Entropy=0.280272901058197, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06809228658676147, KL divergence=0.07791313529014587, Entropy=0.27810442447662354, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04588812217116356, KL divergence=0.09206487238407135, Entropy=0.28590691089630127, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.018833409994840622, KL divergence=0.10433395206928253, Entropy=0.28680604696273804, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/300_Step-12255.ckpt']
Uploaded 3 files for checkpoint 300 in 0.54 seconds
saved intermediate frozen graph: current/model/model_300.pb
Best checkpoint number: 292, Last checkpoint number: 298
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'298'}
Training> Name=main_level/agent, Worker=0, Episode=401, Total reward=59.98, Steps=12284, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=402, Total reward=29.29, Steps=12314, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=403, Total reward=14.2, Steps=12349, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=404, Total reward=83.97, Steps=12417, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=405, Total reward=22.59, Steps=12433, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=406, Total reward=9.57, Steps=12458, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=407, Total reward=47.92, Steps=12497, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=408, Total reward=50.84, Steps=12523, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=409, Total reward=28.24, Steps=12550, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=410, Total reward=43.6, Steps=12600, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=411, Total reward=46.07, Steps=12638, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=412, Total reward=56.97, Steps=12670, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=413, Total reward=37.94, Steps=12692, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=414, Total reward=44.41, Steps=12714, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=415, Total reward=11.39, Steps=12739, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=416, Total reward=17.16, Steps=12771, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=417, Total reward=17.59, Steps=12791, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=418, Total reward=34.3, Steps=12815, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=419, Total reward=67.73, Steps=12863, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=420, Total reward=35.42, Steps=12894, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=421, Total reward=34.24, Steps=12921, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=422, Total reward=26.81, Steps=12941, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=423, Total reward=7.49, Steps=12979, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=424, Total reward=2.85, Steps=12993, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=425, Total reward=17.33, Steps=13009, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=426, Total reward=57.94, Steps=13057, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=427, Total reward=54.56, Steps=13095, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=428, Total reward=71.48, Steps=13138, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=429, Total reward=63.93, Steps=13195, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=430, Total reward=44.72, Steps=13240, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=431, Total reward=11.9, Steps=13263, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=432, Total reward=51.89, Steps=13296, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=433, Total reward=0.02, Steps=13311, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=434, Total reward=29.16, Steps=13329, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=435, Total reward=34.88, Steps=13359, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=436, Total reward=22.36, Steps=13389, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=437, Total reward=24.15, Steps=13416, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=438, Total reward=40.17, Steps=13453, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=439, Total reward=60.04, Steps=13519, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=440, Total reward=63.87, Steps=13554, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=441, Total reward=32.88, Steps=13580, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=442, Total reward=24.59, Steps=13599, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=443, Total reward=0.03, Steps=13626, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=444, Total reward=54.11, Steps=13691, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=445, Total reward=41.89, Steps=13748, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=446, Total reward=64.48, Steps=13794, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=447, Total reward=62.87, Steps=13834, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=448, Total reward=20.4, Steps=13848, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=449, Total reward=85.03, Steps=13909, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=450, Total reward=38.48, Steps=13947, Training iteration=8
Policy training> Surrogate loss=-0.0014118632534518838, KL divergence=0.00013425452925730497, Entropy=0.30762913823127747, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.023611219599843025, KL divergence=0.005682843271642923, Entropy=0.30936357378959656, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04253971204161644, KL divergence=0.01955396682024002, Entropy=0.30750396847724915, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05069729685783386, KL divergence=0.034732360392808914, Entropy=0.29911842942237854, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05946063995361328, KL divergence=0.05331679806113243, Entropy=0.2880348861217499, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06680601835250854, KL divergence=0.06947837024927139, Entropy=0.2926619350910187, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06191181018948555, KL divergence=0.07871336489915848, Entropy=0.2827279269695282, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0679122731089592, KL divergence=0.09099764376878738, Entropy=0.28652501106262207, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.058573734015226364, KL divergence=0.10247009247541428, Entropy=0.28871414065361023, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06588398665189743, KL divergence=0.10763087123632431, Entropy=0.27462729811668396, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/301_Step-13947.ckpt']
Uploaded 3 files for checkpoint 301 in 0.60 seconds
saved intermediate frozen graph: current/model/model_301.pb
Best checkpoint number: 292, Last checkpoint number: 299
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'299'}
Training> Name=main_level/agent, Worker=0, Episode=451, Total reward=52.46, Steps=13984, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=452, Total reward=49.3, Steps=14016, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=453, Total reward=45.1, Steps=14043, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=454, Total reward=26.38, Steps=14054, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=455, Total reward=32.33, Steps=14083, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=456, Total reward=17.7, Steps=14115, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=457, Total reward=42.98, Steps=14154, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=458, Total reward=22.9, Steps=14174, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=459, Total reward=60.81, Steps=14238, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=460, Total reward=55.68, Steps=14270, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=461, Total reward=36.64, Steps=14293, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=462, Total reward=33.6, Steps=14314, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=463, Total reward=28.44, Steps=14391, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=464, Total reward=12.73, Steps=14416, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=465, Total reward=51.53, Steps=14470, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=466, Total reward=49.15, Steps=14566, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=467, Total reward=6.56, Steps=14584, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=468, Total reward=55.38, Steps=14611, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=469, Total reward=27.95, Steps=14631, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=470, Total reward=14.76, Steps=14644, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=471, Total reward=42.0, Steps=14671, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=472, Total reward=16.42, Steps=14697, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=473, Total reward=11.71, Steps=14715, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=474, Total reward=31.98, Steps=14737, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=475, Total reward=23.48, Steps=14782, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=476, Total reward=21.68, Steps=14816, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=477, Total reward=42.28, Steps=14850, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=478, Total reward=29.68, Steps=14873, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=479, Total reward=55.02, Steps=14915, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=480, Total reward=36.69, Steps=14948, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=481, Total reward=31.19, Steps=14975, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=482, Total reward=35.19, Steps=14997, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=483, Total reward=9.92, Steps=15030, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=484, Total reward=6.62, Steps=15047, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=485, Total reward=55.76, Steps=15100, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=486, Total reward=29.64, Steps=15133, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=487, Total reward=48.54, Steps=15187, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=488, Total reward=71.07, Steps=15227, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=489, Total reward=62.75, Steps=15284, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=490, Total reward=11.21, Steps=15311, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=491, Total reward=18.47, Steps=15341, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=492, Total reward=3.63, Steps=15352, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=493, Total reward=43.88, Steps=15380, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=494, Total reward=34.38, Steps=15402, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=495, Total reward=22.5, Steps=15433, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=496, Total reward=14.29, Steps=15467, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=497, Total reward=52.87, Steps=15503, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=498, Total reward=82.78, Steps=15573, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=499, Total reward=31.82, Steps=15617, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=500, Total reward=36.04, Steps=15651, Training iteration=9
Policy training> Surrogate loss=-0.004861585330218077, KL divergence=0.00012432811490725726, Entropy=0.31015077233314514, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.021245120093226433, KL divergence=0.007782701402902603, Entropy=0.3044460117816925, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03488064929842949, KL divergence=0.02495230734348297, Entropy=0.29152074456214905, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04362768307328224, KL divergence=0.046097636222839355, Entropy=0.2940821051597595, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06724655628204346, KL divergence=0.06593495607376099, Entropy=0.2818109393119812, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.053803328424692154, KL divergence=0.08365374803543091, Entropy=0.2806358337402344, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06359262019395828, KL divergence=0.10037198662757874, Entropy=0.27831581234931946, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.051902804523706436, KL divergence=0.11076771467924118, Entropy=0.27736523747444153, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06787485629320145, KL divergence=0.12496846914291382, Entropy=0.282527357339859, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.04766270890831947, KL divergence=0.1336912363767624, Entropy=0.28189408779144287, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/302_Step-15651.ckpt']
Uploaded 3 files for checkpoint 302 in 0.55 seconds
saved intermediate frozen graph: current/model/model_302.pb
Best checkpoint number: 292, Last checkpoint number: 300
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'300'}
Training> Name=main_level/agent, Worker=0, Episode=501, Total reward=31.26, Steps=15678, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=502, Total reward=29.71, Steps=15701, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=503, Total reward=0.02, Steps=15718, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=504, Total reward=21.71, Steps=15746, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=505, Total reward=34.89, Steps=15803, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=506, Total reward=21.91, Steps=15828, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=507, Total reward=22.43, Steps=15871, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=508, Total reward=49.38, Steps=15896, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=509, Total reward=28.25, Steps=15916, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=510, Total reward=62.05, Steps=15964, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=511, Total reward=63.11, Steps=16001, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=512, Total reward=41.06, Steps=16033, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=513, Total reward=40.26, Steps=16059, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=514, Total reward=39.56, Steps=16080, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=515, Total reward=17.45, Steps=16112, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=516, Total reward=17.32, Steps=16132, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=517, Total reward=13.87, Steps=16153, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=518, Total reward=40.95, Steps=16177, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=519, Total reward=77.27, Steps=16225, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=520, Total reward=59.69, Steps=16262, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=521, Total reward=34.41, Steps=16286, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=522, Total reward=27.47, Steps=16304, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=523, Total reward=2.92, Steps=16331, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=524, Total reward=18.21, Steps=16376, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=525, Total reward=85.5, Steps=16429, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=526, Total reward=38.56, Steps=16467, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=527, Total reward=9.47, Steps=16500, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=528, Total reward=55.62, Steps=16525, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=529, Total reward=20.89, Steps=16545, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=530, Total reward=50.77, Steps=16593, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=531, Total reward=38.16, Steps=16636, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=532, Total reward=36.71, Steps=16664, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=533, Total reward=5.73, Steps=16682, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=534, Total reward=42.05, Steps=16704, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=535, Total reward=27.63, Steps=16735, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=536, Total reward=23.85, Steps=16760, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=537, Total reward=35.13, Steps=16791, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=538, Total reward=20.21, Steps=16804, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=539, Total reward=10.99, Steps=16829, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=540, Total reward=76.64, Steps=16867, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=541, Total reward=33.38, Steps=16890, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=542, Total reward=28.92, Steps=16912, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=543, Total reward=9.3, Steps=16979, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=544, Total reward=64.58, Steps=17050, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=545, Total reward=19.47, Steps=17112, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=546, Total reward=55.67, Steps=17158, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=547, Total reward=47.02, Steps=17196, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=548, Total reward=51.56, Steps=17224, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=549, Total reward=40.06, Steps=17249, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=550, Total reward=68.15, Steps=17296, Training iteration=10
Policy training> Surrogate loss=-0.013476462103426456, KL divergence=0.0001238988625118509, Entropy=0.3068620264530182, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.033420857042074203, KL divergence=0.006029909942299128, Entropy=0.3012736141681671, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04099961370229721, KL divergence=0.02245922200381756, Entropy=0.29424047470092773, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04644341394305229, KL divergence=0.03979166969656944, Entropy=0.29050031304359436, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05670243874192238, KL divergence=0.06035435572266579, Entropy=0.28413793444633484, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06385068595409393, KL divergence=0.07689680904150009, Entropy=0.2842091917991638, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.056191336363554, KL divergence=0.0862346962094307, Entropy=0.28090164065361023, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05876316502690315, KL divergence=0.10776706784963608, Entropy=0.2804587781429291, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05770564079284668, KL divergence=0.12011528015136719, Entropy=0.27611228823661804, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06853631883859634, KL divergence=0.12590292096138, Entropy=0.27973055839538574, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/303_Step-17296.ckpt']
Uploaded 3 files for checkpoint 303 in 0.57 seconds
saved intermediate frozen graph: current/model/model_303.pb
Best checkpoint number: 292, Last checkpoint number: 301
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'301'}
Training> Name=main_level/agent, Worker=0, Episode=551, Total reward=42.33, Steps=17337, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=552, Total reward=28.13, Steps=17368, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=553, Total reward=0.02, Steps=17384, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=554, Total reward=44.14, Steps=17407, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=555, Total reward=15.36, Steps=17432, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=556, Total reward=14.39, Steps=17465, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=557, Total reward=51.05, Steps=17502, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=558, Total reward=10.73, Steps=17514, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=559, Total reward=65.61, Steps=17569, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=560, Total reward=33.49, Steps=17612, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=561, Total reward=48.48, Steps=17641, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=562, Total reward=61.32, Steps=17702, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=563, Total reward=10.04, Steps=17739, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=564, Total reward=15.47, Steps=17766, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=565, Total reward=12.89, Steps=17824, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=566, Total reward=18.47, Steps=17840, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=567, Total reward=61.18, Steps=17882, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=568, Total reward=65.63, Steps=17938, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=569, Total reward=28.36, Steps=17958, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=570, Total reward=85.26, Steps=18003, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=571, Total reward=52.16, Steps=18038, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=572, Total reward=53.97, Steps=18070, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=573, Total reward=36.07, Steps=18092, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=574, Total reward=20.74, Steps=18115, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=575, Total reward=16.74, Steps=18146, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=576, Total reward=13.07, Steps=18167, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=577, Total reward=22.66, Steps=18213, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=578, Total reward=49.08, Steps=18250, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=579, Total reward=10.92, Steps=18270, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=580, Total reward=67.68, Steps=18315, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=581, Total reward=58.06, Steps=18344, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=582, Total reward=34.79, Steps=18371, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=583, Total reward=13.26, Steps=18406, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=584, Total reward=48.08, Steps=18474, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=585, Total reward=7.78, Steps=18491, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=586, Total reward=11.86, Steps=18523, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=587, Total reward=56.07, Steps=18562, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=588, Total reward=67.41, Steps=18591, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=589, Total reward=24.65, Steps=18606, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=590, Total reward=32.37, Steps=18639, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=591, Total reward=37.21, Steps=18674, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=592, Total reward=13.87, Steps=18692, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=593, Total reward=31.28, Steps=18711, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=594, Total reward=36.9, Steps=18734, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=595, Total reward=40.37, Steps=18763, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=596, Total reward=17.16, Steps=18782, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=597, Total reward=40.38, Steps=18834, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=598, Total reward=38.97, Steps=18859, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=599, Total reward=71.34, Steps=18901, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=600, Total reward=65.07, Steps=18939, Training iteration=11
Policy training> Surrogate loss=0.012183033861219883, KL divergence=0.00012661040818784386, Entropy=0.3116931915283203, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02047101967036724, KL divergence=0.008648736402392387, Entropy=0.3045564889907837, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04328969493508339, KL divergence=0.024192124605178833, Entropy=0.2970148026943207, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05723293498158455, KL divergence=0.042830366641283035, Entropy=0.29289230704307556, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.056619416922330856, KL divergence=0.05895140767097473, Entropy=0.29413965344429016, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05539074167609215, KL divergence=0.0767950564622879, Entropy=0.2841213047504425, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06005774065852165, KL divergence=0.09208086133003235, Entropy=0.28572455048561096, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06118728592991829, KL divergence=0.10379233211278915, Entropy=0.28397753834724426, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06479226797819138, KL divergence=0.11074421554803848, Entropy=0.2813986837863922, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06494105607271194, KL divergence=0.12066233903169632, Entropy=0.28694531321525574, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/304_Step-18939.ckpt']
Uploaded 3 files for checkpoint 304 in 0.57 seconds
saved intermediate frozen graph: current/model/model_304.pb
Best checkpoint number: 292, Last checkpoint number: 302
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'302'}
Training> Name=main_level/agent, Worker=0, Episode=601, Total reward=32.89, Steps=18967, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=602, Total reward=35.22, Steps=19013, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=603, Total reward=6.27, Steps=19037, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=604, Total reward=8.82, Steps=19079, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=605, Total reward=14.99, Steps=19100, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=606, Total reward=32.11, Steps=19132, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=607, Total reward=40.68, Steps=19167, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=608, Total reward=50.3, Steps=19193, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=609, Total reward=98.77, Steps=19257, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=610, Total reward=44.58, Steps=19292, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=611, Total reward=44.85, Steps=19335, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=612, Total reward=2.98, Steps=19355, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=613, Total reward=58.59, Steps=19386, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=614, Total reward=32.46, Steps=19407, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=615, Total reward=29.97, Steps=19438, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=616, Total reward=17.39, Steps=19469, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=617, Total reward=36.48, Steps=19503, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=618, Total reward=42.92, Steps=19528, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=619, Total reward=47.77, Steps=19576, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=620, Total reward=54.75, Steps=19617, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=621, Total reward=54.52, Steps=19649, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=622, Total reward=38.28, Steps=19668, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=623, Total reward=6.61, Steps=19688, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=624, Total reward=24.77, Steps=19729, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=625, Total reward=1.34, Steps=19743, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=626, Total reward=79.46, Steps=19791, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=627, Total reward=12.99, Steps=19809, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=628, Total reward=63.53, Steps=19837, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=629, Total reward=25.22, Steps=19870, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=630, Total reward=40.6, Steps=19910, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=631, Total reward=21.26, Steps=19941, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=632, Total reward=55.57, Steps=19974, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=633, Total reward=44.77, Steps=20004, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=634, Total reward=32.47, Steps=20025, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=635, Total reward=33.18, Steps=20054, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=636, Total reward=23.39, Steps=20082, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=637, Total reward=56.87, Steps=20117, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=638, Total reward=36.11, Steps=20165, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=639, Total reward=58.6, Steps=20215, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=640, Total reward=55.04, Steps=20249, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=641, Total reward=60.72, Steps=20279, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=642, Total reward=34.42, Steps=20303, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=643, Total reward=6.22, Steps=20330, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=644, Total reward=9.52, Steps=20355, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=645, Total reward=66.11, Steps=20413, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=646, Total reward=78.97, Steps=20461, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=647, Total reward=26.92, Steps=20492, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=648, Total reward=59.15, Steps=20520, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=649, Total reward=31.69, Steps=20553, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=650, Total reward=38.18, Steps=20590, Training iteration=12
Policy training> Surrogate loss=0.010497051291167736, KL divergence=0.00020207786292303354, Entropy=0.3170308470726013, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.023965617641806602, KL divergence=0.007458576466888189, Entropy=0.31439751386642456, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.044703159481287, KL divergence=0.024201825261116028, Entropy=0.3013605773448944, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05269423499703407, KL divergence=0.04370101913809776, Entropy=0.29157915711402893, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06344173103570938, KL divergence=0.06211736425757408, Entropy=0.2945672273635864, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06392208486795425, KL divergence=0.07545005530118942, Entropy=0.2816661298274994, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06021382287144661, KL divergence=0.0913207158446312, Entropy=0.2845178544521332, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07063735276460648, KL divergence=0.10440021753311157, Entropy=0.28907644748687744, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0727534219622612, KL divergence=0.1139073371887207, Entropy=0.2761949300765991, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06275110691785812, KL divergence=0.12741278111934662, Entropy=0.2830415666103363, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/305_Step-20590.ckpt']
Uploaded 3 files for checkpoint 305 in 0.57 seconds
saved intermediate frozen graph: current/model/model_305.pb
Best checkpoint number: 292, Last checkpoint number: 303
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'303'}
Training> Name=main_level/agent, Worker=0, Episode=651, Total reward=76.55, Steps=20633, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=652, Total reward=47.99, Steps=20667, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=653, Total reward=0.02, Steps=20684, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=654, Total reward=34.88, Steps=20707, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=655, Total reward=19.39, Steps=20751, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=656, Total reward=22.19, Steps=20789, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=657, Total reward=32.11, Steps=20826, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=658, Total reward=34.29, Steps=20849, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=659, Total reward=70.16, Steps=20906, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=660, Total reward=73.72, Steps=20947, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=661, Total reward=51.92, Steps=20977, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=662, Total reward=31.15, Steps=20997, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=663, Total reward=7.81, Steps=21030, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=664, Total reward=5.92, Steps=21063, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=665, Total reward=54.32, Steps=21114, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=666, Total reward=3.65, Steps=21141, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=667, Total reward=71.45, Steps=21204, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=668, Total reward=74.82, Steps=21255, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=669, Total reward=25.18, Steps=21276, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=670, Total reward=54.45, Steps=21322, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=671, Total reward=44.62, Steps=21362, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=672, Total reward=69.54, Steps=21405, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=673, Total reward=44.29, Steps=21433, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=674, Total reward=31.33, Steps=21456, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=675, Total reward=45.77, Steps=21486, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=676, Total reward=6.09, Steps=21501, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=677, Total reward=12.26, Steps=21513, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=678, Total reward=45.51, Steps=21537, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=679, Total reward=47.94, Steps=21598, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=680, Total reward=76.6, Steps=21633, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=681, Total reward=49.61, Steps=21661, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=682, Total reward=35.84, Steps=21684, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=683, Total reward=36.52, Steps=21746, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=684, Total reward=13.48, Steps=21774, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=685, Total reward=7.93, Steps=21807, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=686, Total reward=40.96, Steps=21851, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=687, Total reward=6.62, Steps=21866, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=688, Total reward=85.61, Steps=21941, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=689, Total reward=71.22, Steps=21999, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=690, Total reward=55.96, Steps=22048, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=691, Total reward=61.47, Steps=22089, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=692, Total reward=41.09, Steps=22119, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=693, Total reward=52.69, Steps=22149, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=694, Total reward=39.44, Steps=22171, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=695, Total reward=12.67, Steps=22194, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=696, Total reward=18.86, Steps=22227, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=697, Total reward=92.5, Steps=22305, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=698, Total reward=42.32, Steps=22330, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=699, Total reward=9.59, Steps=22349, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=700, Total reward=86.65, Steps=22391, Training iteration=13
Policy training> Surrogate loss=0.0023818339686840773, KL divergence=6.155864684842527e-05, Entropy=0.3025766909122467, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02680465765297413, KL divergence=0.005821657832711935, Entropy=0.3031969666481018, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03633471950888634, KL divergence=0.0189231988042593, Entropy=0.28835147619247437, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04410533979535103, KL divergence=0.03235911205410957, Entropy=0.28264859318733215, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.056026339530944824, KL divergence=0.05079849436879158, Entropy=0.2780754268169403, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.056700557470321655, KL divergence=0.06683077663183212, Entropy=0.27855184674263, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.047891005873680115, KL divergence=0.08217654377222061, Entropy=0.26938527822494507, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.044543009251356125, KL divergence=0.09286316484212875, Entropy=0.2708653509616852, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0684657171368599, KL divergence=0.1042383685708046, Entropy=0.27275148034095764, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0692165270447731, KL divergence=0.11064285039901733, Entropy=0.2687515318393707, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/306_Step-22391.ckpt']
Uploaded 3 files for checkpoint 306 in 0.53 seconds
saved intermediate frozen graph: current/model/model_306.pb
Best checkpoint number: 292, Last checkpoint number: 304
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'304'}
Training> Name=main_level/agent, Worker=0, Episode=701, Total reward=42.56, Steps=22417, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=702, Total reward=38.68, Steps=22442, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=703, Total reward=10.67, Steps=22487, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=704, Total reward=7.91, Steps=22515, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=705, Total reward=13.36, Steps=22540, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=706, Total reward=80.58, Steps=22600, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=707, Total reward=40.39, Steps=22636, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=708, Total reward=46.74, Steps=22662, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=709, Total reward=21.85, Steps=22682, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=710, Total reward=122.53, Steps=22730, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=711, Total reward=16.21, Steps=22761, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=712, Total reward=23.37, Steps=22792, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=713, Total reward=14.68, Steps=22811, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=714, Total reward=42.14, Steps=22833, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=715, Total reward=20.91, Steps=22861, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=716, Total reward=18.87, Steps=22881, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=717, Total reward=25.56, Steps=22917, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=718, Total reward=44.02, Steps=22943, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=719, Total reward=86.57, Steps=22988, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=720, Total reward=70.56, Steps=23028, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=721, Total reward=21.07, Steps=23051, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=722, Total reward=25.42, Steps=23068, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=723, Total reward=7.25, Steps=23086, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=724, Total reward=5.4, Steps=23117, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=725, Total reward=79.08, Steps=23179, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=726, Total reward=44.52, Steps=23225, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=727, Total reward=73.59, Steps=23289, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=728, Total reward=65.19, Steps=23344, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=729, Total reward=36.28, Steps=23369, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=730, Total reward=50.67, Steps=23420, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=731, Total reward=59.66, Steps=23462, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=732, Total reward=43.64, Steps=23494, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=733, Total reward=21.11, Steps=23505, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=734, Total reward=43.87, Steps=23527, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=735, Total reward=20.85, Steps=23556, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=736, Total reward=20.78, Steps=23586, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=737, Total reward=32.46, Steps=23613, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=738, Total reward=43.99, Steps=23636, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=739, Total reward=74.75, Steps=23683, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=740, Total reward=51.94, Steps=23718, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=741, Total reward=32.92, Steps=23744, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=742, Total reward=25.25, Steps=23763, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=743, Total reward=2.93, Steps=23801, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=744, Total reward=20.19, Steps=23850, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=745, Total reward=64.52, Steps=23910, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=746, Total reward=18.32, Steps=23935, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=747, Total reward=29.99, Steps=23971, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=748, Total reward=61.23, Steps=24009, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=749, Total reward=27.27, Steps=24034, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=750, Total reward=67.47, Steps=24070, Training iteration=14
Policy training> Surrogate loss=0.0025363813620060682, KL divergence=0.00011769111733883619, Entropy=0.3134498596191406, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0170572642236948, KL divergence=0.007401387672871351, Entropy=0.31359168887138367, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.038010112941265106, KL divergence=0.02072758600115776, Entropy=0.3099667727947235, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05280584096908569, KL divergence=0.0389237143099308, Entropy=0.3028022348880768, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.057486314326524734, KL divergence=0.05519263073801994, Entropy=0.29796355962753296, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06220643222332001, KL divergence=0.06901470571756363, Entropy=0.2968834340572357, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07111439853906631, KL divergence=0.08213051408529282, Entropy=0.2941482961177826, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05971881374716759, KL divergence=0.09688464552164078, Entropy=0.29910293221473694, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08279655873775482, KL divergence=0.10160183906555176, Entropy=0.296344518661499, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07235362380743027, KL divergence=0.11226075142621994, Entropy=0.28996869921684265, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/307_Step-24070.ckpt']
Uploaded 3 files for checkpoint 307 in 0.55 seconds
saved intermediate frozen graph: current/model/model_307.pb
Best checkpoint number: 292, Last checkpoint number: 305
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'305'}
Training> Name=main_level/agent, Worker=0, Episode=751, Total reward=25.49, Steps=24098, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=752, Total reward=9.32, Steps=24118, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=753, Total reward=21.24, Steps=24128, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=754, Total reward=25.58, Steps=24139, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=755, Total reward=7.68, Steps=24183, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=756, Total reward=16.05, Steps=24216, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=757, Total reward=57.55, Steps=24279, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=758, Total reward=31.32, Steps=24299, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=759, Total reward=58.88, Steps=24344, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=760, Total reward=60.06, Steps=24378, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=761, Total reward=70.04, Steps=24407, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=762, Total reward=35.86, Steps=24437, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=763, Total reward=7.91, Steps=24477, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=764, Total reward=25.22, Steps=24511, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=765, Total reward=9.97, Steps=24533, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=766, Total reward=48.42, Steps=24580, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=767, Total reward=3.31, Steps=24593, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=768, Total reward=71.48, Steps=24637, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=769, Total reward=31.6, Steps=24664, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=770, Total reward=41.79, Steps=24711, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=771, Total reward=44.02, Steps=24742, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=772, Total reward=55.82, Steps=24776, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=773, Total reward=53.19, Steps=24806, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=774, Total reward=29.85, Steps=24827, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=775, Total reward=21.95, Steps=24854, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=776, Total reward=16.01, Steps=24872, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=777, Total reward=39.97, Steps=24910, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=778, Total reward=29.58, Steps=24956, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=779, Total reward=47.43, Steps=25000, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=780, Total reward=63.11, Steps=25039, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=781, Total reward=27.6, Steps=25062, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=782, Total reward=49.23, Steps=25136, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=783, Total reward=62.34, Steps=25209, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=784, Total reward=10.07, Steps=25239, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=785, Total reward=6.86, Steps=25272, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=786, Total reward=66.25, Steps=25335, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=787, Total reward=67.4, Steps=25377, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=788, Total reward=67.04, Steps=25431, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=789, Total reward=32.16, Steps=25466, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=790, Total reward=29.31, Steps=25504, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=791, Total reward=46.33, Steps=25531, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=792, Total reward=29.28, Steps=25559, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=793, Total reward=51.52, Steps=25587, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=794, Total reward=40.4, Steps=25619, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=795, Total reward=25.25, Steps=25646, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=796, Total reward=32.34, Steps=25677, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=797, Total reward=83.12, Steps=25776, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=798, Total reward=23.51, Steps=25792, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=799, Total reward=9.53, Steps=25811, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=800, Total reward=94.21, Steps=25859, Training iteration=15
Policy training> Surrogate loss=0.0071409097872674465, KL divergence=0.00015267892740666866, Entropy=0.3095429837703705, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.006823640316724777, KL divergence=0.008017625659704208, Entropy=0.3057035505771637, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.024413714185357094, KL divergence=0.022184131667017937, Entropy=0.2980390787124634, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.031081927940249443, KL divergence=0.03941509500145912, Entropy=0.2934171259403229, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.027926893904805183, KL divergence=0.05808015540242195, Entropy=0.28514066338539124, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06531894952058792, KL divergence=0.07579151540994644, Entropy=0.2832320034503937, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06810677796602249, KL divergence=0.08703551441431046, Entropy=0.2765180170536041, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05936233326792717, KL divergence=0.10160829871892929, Entropy=0.2754248380661011, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06608511507511139, KL divergence=0.11444313079118729, Entropy=0.2747107446193695, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06397654861211777, KL divergence=0.12799648940563202, Entropy=0.27850502729415894, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/308_Step-25859.ckpt']
Uploaded 3 files for checkpoint 308 in 0.60 seconds
saved intermediate frozen graph: current/model/model_308.pb
Best checkpoint number: 292, Last checkpoint number: 306
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'306'}
Training> Name=main_level/agent, Worker=0, Episode=801, Total reward=54.57, Steps=25888, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=802, Total reward=24.89, Steps=25906, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=803, Total reward=12.02, Steps=25939, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=804, Total reward=63.51, Steps=26007, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=805, Total reward=78.69, Steps=26068, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=806, Total reward=35.94, Steps=26104, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=807, Total reward=41.49, Steps=26140, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=808, Total reward=38.08, Steps=26162, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=809, Total reward=22.86, Steps=26181, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=810, Total reward=14.91, Steps=26196, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=811, Total reward=33.92, Steps=26239, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=812, Total reward=19.44, Steps=26266, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=813, Total reward=46.49, Steps=26296, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=814, Total reward=26.09, Steps=26307, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=815, Total reward=28.27, Steps=26336, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=816, Total reward=23.4, Steps=26366, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=817, Total reward=109.96, Steps=26442, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=818, Total reward=35.81, Steps=26465, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=819, Total reward=0.0, Steps=26466, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=820, Total reward=78.42, Steps=26503, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=821, Total reward=28.9, Steps=26529, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=822, Total reward=30.42, Steps=26554, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=823, Total reward=9.73, Steps=26576, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=824, Total reward=7.74, Steps=26603, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=825, Total reward=27.59, Steps=26642, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=826, Total reward=50.69, Steps=26688, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=827, Total reward=46.2, Steps=26724, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=828, Total reward=39.03, Steps=26748, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=829, Total reward=21.49, Steps=26765, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=830, Total reward=23.67, Steps=26799, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=831, Total reward=12.46, Steps=26824, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=832, Total reward=5.72, Steps=26843, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=833, Total reward=32.4, Steps=26867, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=834, Total reward=35.23, Steps=26898, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=835, Total reward=32.33, Steps=26927, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=836, Total reward=19.73, Steps=26955, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=837, Total reward=44.68, Steps=26990, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=838, Total reward=41.61, Steps=27017, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=839, Total reward=48.35, Steps=27057, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=840, Total reward=65.76, Steps=27097, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=841, Total reward=45.92, Steps=27126, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=842, Total reward=25.49, Steps=27147, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=843, Total reward=14.45, Steps=27182, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=844, Total reward=10.82, Steps=27214, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=845, Total reward=3.38, Steps=27230, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=846, Total reward=60.84, Steps=27279, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=847, Total reward=58.51, Steps=27318, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=848, Total reward=58.78, Steps=27346, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=849, Total reward=23.62, Steps=27395, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=850, Total reward=28.98, Steps=27411, Training iteration=16
Policy training> Surrogate loss=0.0018549015512689948, KL divergence=0.0001106326308217831, Entropy=0.32516178488731384, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03385712206363678, KL divergence=0.01002497598528862, Entropy=0.32106658816337585, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.046417903155088425, KL divergence=0.028735384345054626, Entropy=0.31378135085105896, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05613377317786217, KL divergence=0.050500813871622086, Entropy=0.30839648842811584, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06154590845108032, KL divergence=0.07100013643503189, Entropy=0.30292800068855286, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.056976933032274246, KL divergence=0.08959589153528214, Entropy=0.30120691657066345, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06323254853487015, KL divergence=0.10020693391561508, Entropy=0.299093633890152, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.061094019562006, KL divergence=0.11658325046300888, Entropy=0.29914042353630066, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06983228027820587, KL divergence=0.12781645357608795, Entropy=0.30003878474235535, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06882204860448837, KL divergence=0.13545024394989014, Entropy=0.2976500689983368, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/309_Step-27411.ckpt']
Uploaded 3 files for checkpoint 309 in 0.60 seconds
saved intermediate frozen graph: current/model/model_309.pb
Best checkpoint number: 292, Last checkpoint number: 307
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'307'}
Training> Name=main_level/agent, Worker=0, Episode=851, Total reward=35.72, Steps=27450, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=852, Total reward=47.76, Steps=27481, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=853, Total reward=46.97, Steps=27509, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=854, Total reward=40.72, Steps=27531, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=855, Total reward=22.09, Steps=27555, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=856, Total reward=18.17, Steps=27585, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=857, Total reward=22.38, Steps=27612, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=858, Total reward=45.16, Steps=27658, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=859, Total reward=6.75, Steps=27692, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=860, Total reward=88.05, Steps=27733, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=861, Total reward=60.32, Steps=27763, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=862, Total reward=31.4, Steps=27785, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=863, Total reward=3.69, Steps=27801, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=864, Total reward=3.08, Steps=27829, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=865, Total reward=16.34, Steps=27853, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=866, Total reward=48.05, Steps=27898, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=867, Total reward=31.82, Steps=27935, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=868, Total reward=55.48, Steps=27962, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=869, Total reward=66.96, Steps=28009, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=870, Total reward=7.72, Steps=28022, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=871, Total reward=51.22, Steps=28061, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=872, Total reward=29.61, Steps=28089, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=873, Total reward=45.62, Steps=28114, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=874, Total reward=29.02, Steps=28125, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=875, Total reward=17.31, Steps=28171, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=876, Total reward=21.47, Steps=28189, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=877, Total reward=41.7, Steps=28241, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=878, Total reward=36.81, Steps=28264, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=879, Total reward=74.97, Steps=28312, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=880, Total reward=23.85, Steps=28343, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=881, Total reward=62.89, Steps=28376, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=882, Total reward=30.77, Steps=28404, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=883, Total reward=6.76, Steps=28443, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=884, Total reward=5.39, Steps=28492, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=885, Total reward=77.13, Steps=28577, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=886, Total reward=91.44, Steps=28620, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=887, Total reward=60.0, Steps=28663, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=888, Total reward=62.52, Steps=28690, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=889, Total reward=47.06, Steps=28719, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=890, Total reward=39.85, Steps=28754, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=891, Total reward=35.81, Steps=28784, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=892, Total reward=44.62, Steps=28815, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=893, Total reward=21.28, Steps=28826, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=894, Total reward=45.08, Steps=28860, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=895, Total reward=14.61, Steps=28889, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=896, Total reward=15.37, Steps=28921, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=897, Total reward=26.95, Steps=28951, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=898, Total reward=35.39, Steps=28973, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=899, Total reward=19.53, Steps=29004, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=900, Total reward=95.46, Steps=29040, Training iteration=17
Policy training> Surrogate loss=-0.0037347592879086733, KL divergence=0.00014021167589817196, Entropy=0.3053230345249176, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.019810471683740616, KL divergence=0.0071989246644079685, Entropy=0.3070993423461914, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.027389125898480415, KL divergence=0.02172473818063736, Entropy=0.30825304985046387, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.050353050231933594, KL divergence=0.039994221180677414, Entropy=0.3004325330257416, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04740622639656067, KL divergence=0.05819953605532646, Entropy=0.29489174485206604, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04769458994269371, KL divergence=0.07591810822486877, Entropy=0.29625803232192993, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06117190420627594, KL divergence=0.09000403434038162, Entropy=0.29066726565361023, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.058499183505773544, KL divergence=0.1061258539557457, Entropy=0.2923068404197693, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.051277708262205124, KL divergence=0.1130673959851265, Entropy=0.28584107756614685, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07206595689058304, KL divergence=0.12304029613733292, Entropy=0.2899845540523529, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/310_Step-29040.ckpt']
Uploaded 3 files for checkpoint 310 in 0.60 seconds
saved intermediate frozen graph: current/model/model_310.pb
Best checkpoint number: 308, Last checkpoint number: 308
Copying the frozen checkpoint from ./frozen_models/agent/model_308.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'292'}
Training> Name=main_level/agent, Worker=0, Episode=901, Total reward=40.2, Steps=29064, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=902, Total reward=34.01, Steps=29089, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=903, Total reward=7.49, Steps=29123, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=904, Total reward=8.65, Steps=29151, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=905, Total reward=21.93, Steps=29179, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=906, Total reward=46.1, Steps=29226, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=907, Total reward=54.91, Steps=29265, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=908, Total reward=60.36, Steps=29294, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=909, Total reward=34.28, Steps=29323, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=910, Total reward=56.27, Steps=29375, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=911, Total reward=74.18, Steps=29428, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=912, Total reward=5.57, Steps=29439, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=913, Total reward=22.85, Steps=29458, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=914, Total reward=30.79, Steps=29480, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=915, Total reward=18.39, Steps=29507, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=916, Total reward=24.01, Steps=29524, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=917, Total reward=39.96, Steps=29558, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=918, Total reward=30.09, Steps=29583, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=919, Total reward=23.73, Steps=29618, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=920, Total reward=58.03, Steps=29655, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=921, Total reward=35.37, Steps=29684, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=922, Total reward=42.0, Steps=29728, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=923, Total reward=73.43, Steps=29824, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=924, Total reward=10.85, Steps=29879, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=925, Total reward=12.77, Steps=29916, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=926, Total reward=22.53, Steps=29943, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=927, Total reward=59.16, Steps=29981, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=928, Total reward=58.96, Steps=30008, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=929, Total reward=17.53, Steps=30025, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=930, Total reward=59.65, Steps=30072, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=931, Total reward=28.26, Steps=30104, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=932, Total reward=22.64, Steps=30135, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=933, Total reward=30.26, Steps=30156, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=934, Total reward=49.55, Steps=30179, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=935, Total reward=23.28, Steps=30210, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=936, Total reward=22.07, Steps=30230, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=937, Total reward=21.36, Steps=30268, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=938, Total reward=33.21, Steps=30292, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=939, Total reward=60.42, Steps=30345, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=940, Total reward=72.92, Steps=30385, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=941, Total reward=25.04, Steps=30405, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=942, Total reward=33.58, Steps=30441, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=943, Total reward=25.58, Steps=30482, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=944, Total reward=57.48, Steps=30546, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=945, Total reward=36.38, Steps=30585, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=946, Total reward=91.74, Steps=30632, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=947, Total reward=68.92, Steps=30695, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=948, Total reward=106.33, Steps=30772, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=949, Total reward=31.16, Steps=30815, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=950, Total reward=51.83, Steps=30860, Training iteration=18
Policy training> Surrogate loss=0.00384340132586658, KL divergence=0.0001003770375973545, Entropy=0.3330478370189667, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.009336370043456554, KL divergence=0.006153620779514313, Entropy=0.32148414850234985, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.036680761724710464, KL divergence=0.01944214664399624, Entropy=0.32060882449150085, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05297078564763069, KL divergence=0.03813881799578667, Entropy=0.3216002583503723, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.038745149970054626, KL divergence=0.05743276700377464, Entropy=0.31790003180503845, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04530128836631775, KL divergence=0.07196364551782608, Entropy=0.31311455368995667, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04662901163101196, KL divergence=0.0877726599574089, Entropy=0.30595648288726807, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05920172855257988, KL divergence=0.09529147297143936, Entropy=0.3064579665660858, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06356251239776611, KL divergence=0.1066214069724083, Entropy=0.310108482837677, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07835998386144638, KL divergence=0.11896295100450516, Entropy=0.3155241906642914, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/311_Step-30860.ckpt']
Uploaded 3 files for checkpoint 311 in 0.50 seconds
saved intermediate frozen graph: current/model/model_311.pb
Best checkpoint number: 308, Last checkpoint number: 309
Copying the frozen checkpoint from ./frozen_models/agent/model_308.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'292'}
Training> Name=main_level/agent, Worker=0, Episode=951, Total reward=12.9, Steps=30890, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=952, Total reward=1.74, Steps=30901, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=953, Total reward=52.59, Steps=30931, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=954, Total reward=27.63, Steps=30942, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=955, Total reward=17.9, Steps=30973, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=956, Total reward=17.86, Steps=30996, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=957, Total reward=10.62, Steps=31016, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=958, Total reward=42.59, Steps=31041, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=959, Total reward=60.32, Steps=31092, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=960, Total reward=94.63, Steps=31187, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=961, Total reward=52.13, Steps=31216, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=962, Total reward=39.48, Steps=31254, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=963, Total reward=5.16, Steps=31298, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=964, Total reward=10.52, Steps=31325, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=965, Total reward=21.98, Steps=31354, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=966, Total reward=58.37, Steps=31399, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=967, Total reward=60.15, Steps=31438, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=968, Total reward=52.86, Steps=31465, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=969, Total reward=30.05, Steps=31486, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=970, Total reward=26.16, Steps=31525, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=971, Total reward=69.88, Steps=31567, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=972, Total reward=15.9, Steps=31593, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=973, Total reward=21.26, Steps=31603, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=974, Total reward=40.95, Steps=31625, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=975, Total reward=18.78, Steps=31654, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=976, Total reward=19.27, Steps=31688, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=977, Total reward=7.51, Steps=31712, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=978, Total reward=36.42, Steps=31736, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=979, Total reward=82.26, Steps=31782, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=980, Total reward=49.56, Steps=31823, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=981, Total reward=31.03, Steps=31847, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=982, Total reward=30.24, Steps=31870, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=983, Total reward=3.61, Steps=31887, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=984, Total reward=7.59, Steps=31928, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=985, Total reward=63.78, Steps=32004, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=986, Total reward=58.11, Steps=32055, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=987, Total reward=102.62, Steps=32141, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=988, Total reward=58.36, Steps=32172, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=989, Total reward=68.22, Steps=32220, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=990, Total reward=54.43, Steps=32268, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=991, Total reward=32.76, Steps=32305, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=992, Total reward=7.44, Steps=32317, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=993, Total reward=19.55, Steps=32336, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=994, Total reward=40.96, Steps=32359, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=995, Total reward=37.54, Steps=32388, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=996, Total reward=12.79, Steps=32428, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=997, Total reward=44.04, Steps=32467, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=998, Total reward=28.83, Steps=32483, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=999, Total reward=86.55, Steps=32530, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=1000, Total reward=72.8, Steps=32568, Training iteration=19
Policy training> Surrogate loss=0.0025950514245778322, KL divergence=0.00026659792638383806, Entropy=0.3414257764816284, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03416595235466957, KL divergence=0.01000230386853218, Entropy=0.3377681076526642, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.043983787298202515, KL divergence=0.026492854580283165, Entropy=0.33040109276771545, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04672935977578163, KL divergence=0.04427933320403099, Entropy=0.3275040090084076, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0504678376019001, KL divergence=0.05901307985186577, Entropy=0.32709750533103943, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06429022550582886, KL divergence=0.07532796263694763, Entropy=0.3182244300842285, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05747557058930397, KL divergence=0.09005192667245865, Entropy=0.31841281056404114, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0674024224281311, KL divergence=0.10392242670059204, Entropy=0.32053136825561523, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06413407623767853, KL divergence=0.11275630444288254, Entropy=0.32114377617836, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05196612700819969, KL divergence=0.11908372491598129, Entropy=0.3201456367969513, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/312_Step-32568.ckpt']
Uploaded 3 files for checkpoint 312 in 0.56 seconds
saved intermediate frozen graph: current/model/model_312.pb
Best checkpoint number: 308, Last checkpoint number: 310
Copying the frozen checkpoint from ./frozen_models/agent/model_308.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'309'}
Training> Name=main_level/agent, Worker=0, Episode=1001, Total reward=59.63, Steps=32599, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1002, Total reward=43.48, Steps=32623, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1003, Total reward=3.66, Steps=32639, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1004, Total reward=20.62, Steps=32697, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1005, Total reward=67.26, Steps=32756, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1006, Total reward=7.29, Steps=32779, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1007, Total reward=56.65, Steps=32815, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1008, Total reward=50.08, Steps=32843, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1009, Total reward=21.04, Steps=32862, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1010, Total reward=84.21, Steps=32906, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1011, Total reward=28.68, Steps=32937, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1012, Total reward=51.07, Steps=32970, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1013, Total reward=23.72, Steps=32981, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1014, Total reward=25.53, Steps=32992, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1015, Total reward=13.96, Steps=33025, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1016, Total reward=25.29, Steps=33056, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1017, Total reward=44.68, Steps=33094, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1018, Total reward=41.91, Steps=33120, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1019, Total reward=23.49, Steps=33138, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1020, Total reward=74.23, Steps=33175, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1021, Total reward=61.32, Steps=33206, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1022, Total reward=24.83, Steps=33223, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1023, Total reward=13.57, Steps=33265, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1024, Total reward=87.02, Steps=33344, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1025, Total reward=14.66, Steps=33369, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1026, Total reward=53.02, Steps=33418, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1027, Total reward=66.95, Steps=33458, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1028, Total reward=68.83, Steps=33505, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1029, Total reward=24.78, Steps=33528, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1030, Total reward=59.57, Steps=33578, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1031, Total reward=30.92, Steps=33607, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1032, Total reward=60.1, Steps=33650, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1033, Total reward=28.67, Steps=33671, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1034, Total reward=40.98, Steps=33694, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1035, Total reward=18.34, Steps=33721, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1036, Total reward=21.39, Steps=33750, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1037, Total reward=56.19, Steps=33787, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1038, Total reward=47.09, Steps=33834, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1039, Total reward=65.71, Steps=33885, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1040, Total reward=83.91, Steps=33921, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1041, Total reward=57.26, Steps=33949, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1042, Total reward=33.54, Steps=33973, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1043, Total reward=7.76, Steps=34005, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1044, Total reward=15.5, Steps=34033, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1045, Total reward=77.06, Steps=34088, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1046, Total reward=76.7, Steps=34140, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1047, Total reward=117.43, Steps=34227, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1048, Total reward=60.83, Steps=34255, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1049, Total reward=51.37, Steps=34304, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1050, Total reward=25.55, Steps=34330, Training iteration=20
Policy training> Surrogate loss=-0.005640228744596243, KL divergence=8.997946133604273e-05, Entropy=0.34608015418052673, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.028930485248565674, KL divergence=0.005005475599318743, Entropy=0.33789053559303284, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.044752877205610275, KL divergence=0.020193491131067276, Entropy=0.33354055881500244, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.050476253032684326, KL divergence=0.035993486642837524, Entropy=0.324613094329834, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04351905360817909, KL divergence=0.05131234601140022, Entropy=0.32746174931526184, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05267493799328804, KL divergence=0.07079710811376572, Entropy=0.3140918016433716, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06781773269176483, KL divergence=0.08096250891685486, Entropy=0.3204928934574127, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06064988300204277, KL divergence=0.09388396888971329, Entropy=0.31975558400154114, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.060301050543785095, KL divergence=0.10427550226449966, Entropy=0.3221588432788849, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06099855527281761, KL divergence=0.11107257008552551, Entropy=0.3185690939426422, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/313_Step-34330.ckpt']
Uploaded 3 files for checkpoint 313 in 0.53 seconds
saved intermediate frozen graph: current/model/model_313.pb
Best checkpoint number: 308, Last checkpoint number: 311
Copying the frozen checkpoint from ./frozen_models/agent/model_308.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'310'}
Training> Name=main_level/agent, Worker=0, Episode=1051, Total reward=45.23, Steps=34371, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1052, Total reward=45.92, Steps=34402, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1053, Total reward=40.45, Steps=34427, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1054, Total reward=43.66, Steps=34449, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1055, Total reward=14.28, Steps=34493, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1056, Total reward=22.99, Steps=34523, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1057, Total reward=49.29, Steps=34590, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1058, Total reward=32.81, Steps=34614, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1059, Total reward=19.34, Steps=34642, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1060, Total reward=74.09, Steps=34680, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1061, Total reward=26.11, Steps=34696, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1062, Total reward=38.47, Steps=34733, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1063, Total reward=0.01, Steps=34744, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1064, Total reward=20.61, Steps=34774, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1065, Total reward=19.14, Steps=34815, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1066, Total reward=11.11, Steps=34829, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1067, Total reward=67.26, Steps=34871, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1068, Total reward=64.4, Steps=34899, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1069, Total reward=26.6, Steps=34918, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1070, Total reward=52.97, Steps=34966, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1071, Total reward=63.87, Steps=35005, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1072, Total reward=47.96, Steps=35036, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1073, Total reward=21.37, Steps=35047, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1074, Total reward=55.41, Steps=35069, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1075, Total reward=20.34, Steps=35101, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1076, Total reward=18.38, Steps=35117, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1077, Total reward=30.81, Steps=35137, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1078, Total reward=27.76, Steps=35155, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1079, Total reward=17.45, Steps=35173, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1080, Total reward=71.2, Steps=35216, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1081, Total reward=39.12, Steps=35242, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1082, Total reward=24.47, Steps=35260, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1083, Total reward=0.02, Steps=35277, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1084, Total reward=0.02, Steps=35293, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1085, Total reward=25.24, Steps=35315, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1086, Total reward=69.05, Steps=35362, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1087, Total reward=66.71, Steps=35420, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1088, Total reward=60.97, Steps=35448, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1089, Total reward=20.48, Steps=35464, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1090, Total reward=85.65, Steps=35500, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1091, Total reward=44.45, Steps=35532, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1092, Total reward=50.25, Steps=35573, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1093, Total reward=46.64, Steps=35605, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1094, Total reward=28.64, Steps=35616, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1095, Total reward=25.18, Steps=35660, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1096, Total reward=12.07, Steps=35698, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1097, Total reward=26.9, Steps=35718, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1098, Total reward=30.09, Steps=35744, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1099, Total reward=13.66, Steps=35761, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1100, Total reward=95.46, Steps=35797, Training iteration=21
Policy training> Surrogate loss=-0.012542761862277985, KL divergence=2.7774898626375943e-05, Entropy=0.31217241287231445, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0346357636153698, KL divergence=0.0036057799588888884, Entropy=0.3013540506362915, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.02983688749372959, KL divergence=0.012773256748914719, Entropy=0.2937416732311249, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.042937085032463074, KL divergence=0.020202437415719032, Entropy=0.2940928339958191, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04254930466413498, KL divergence=0.03574197739362717, Entropy=0.3070889115333557, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.03364119678735733, KL divergence=0.05455099791288376, Entropy=0.2851319909095764, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04340360313653946, KL divergence=0.06970880925655365, Entropy=0.2816014587879181, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.04209897294640541, KL divergence=0.08412182331085205, Entropy=0.28595131635665894, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.054218728095293045, KL divergence=0.08996625989675522, Entropy=0.2855539321899414, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.03661463409662247, KL divergence=0.09197687357664108, Entropy=0.2768753170967102, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/314_Step-35797.ckpt']
Uploaded 3 files for checkpoint 314 in 0.51 seconds
saved intermediate frozen graph: current/model/model_314.pb
Best checkpoint number: 308, Last checkpoint number: 312
Copying the frozen checkpoint from ./frozen_models/agent/model_308.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'311'}
Training> Name=main_level/agent, Worker=0, Episode=1101, Total reward=19.42, Steps=35810, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1102, Total reward=29.5, Steps=35828, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1103, Total reward=6.41, Steps=35865, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1104, Total reward=63.46, Steps=35938, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1105, Total reward=11.53, Steps=35959, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1106, Total reward=63.62, Steps=36010, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1107, Total reward=54.32, Steps=36050, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1108, Total reward=44.61, Steps=36076, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1109, Total reward=25.22, Steps=36101, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1110, Total reward=52.6, Steps=36149, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1111, Total reward=55.5, Steps=36190, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1112, Total reward=20.24, Steps=36222, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1113, Total reward=36.93, Steps=36248, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1114, Total reward=24.18, Steps=36258, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1115, Total reward=21.61, Steps=36287, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1116, Total reward=25.66, Steps=36329, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1117, Total reward=48.95, Steps=36368, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1118, Total reward=32.72, Steps=36393, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1119, Total reward=69.23, Steps=36442, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1120, Total reward=75.15, Steps=36482, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1121, Total reward=62.15, Steps=36523, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1122, Total reward=33.86, Steps=36542, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1123, Total reward=22.56, Steps=36586, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1124, Total reward=27.95, Steps=36622, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1125, Total reward=19.71, Steps=36647, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1126, Total reward=59.63, Steps=36694, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1127, Total reward=73.84, Steps=36762, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1128, Total reward=61.18, Steps=36792, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1129, Total reward=43.76, Steps=36839, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1130, Total reward=41.3, Steps=36891, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1131, Total reward=45.33, Steps=36930, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1132, Total reward=32.45, Steps=36959, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1133, Total reward=40.01, Steps=36990, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1134, Total reward=36.96, Steps=37012, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1135, Total reward=18.91, Steps=37037, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1136, Total reward=20.07, Steps=37072, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1137, Total reward=52.62, Steps=37122, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1138, Total reward=39.24, Steps=37150, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1139, Total reward=16.16, Steps=37168, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1140, Total reward=53.77, Steps=37203, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1141, Total reward=45.5, Steps=37229, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1142, Total reward=33.62, Steps=37248, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1143, Total reward=3.4, Steps=37267, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1144, Total reward=10.57, Steps=37295, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1145, Total reward=9.88, Steps=37328, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1146, Total reward=87.89, Steps=37373, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1147, Total reward=16.04, Steps=37394, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1148, Total reward=85.28, Steps=37456, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1149, Total reward=61.6, Steps=37498, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1150, Total reward=21.98, Steps=37523, Training iteration=22
Policy training> Surrogate loss=-0.0018026642501354218, KL divergence=0.00013762192975264043, Entropy=0.33168962597846985, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.021296963095664978, KL divergence=0.007536567747592926, Entropy=0.3283732235431671, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03453735262155533, KL divergence=0.027690403163433075, Entropy=0.3196813762187958, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05876191332936287, KL divergence=0.04644974693655968, Entropy=0.31537675857543945, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0473964624106884, KL divergence=0.06206843629479408, Entropy=0.3068356215953827, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04169842600822449, KL divergence=0.07765455543994904, Entropy=0.3077201843261719, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0649300143122673, KL divergence=0.09616035223007202, Entropy=0.30156469345092773, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06125969812273979, KL divergence=0.1059664860367775, Entropy=0.29916876554489136, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07100743055343628, KL divergence=0.12009388208389282, Entropy=0.30561280250549316, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06492677330970764, KL divergence=0.1240980252623558, Entropy=0.307205468416214, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/315_Step-37523.ckpt']
Uploaded 3 files for checkpoint 315 in 0.57 seconds
saved intermediate frozen graph: current/model/model_315.pb
Best checkpoint number: 308, Last checkpoint number: 313
Copying the frozen checkpoint from ./frozen_models/agent/model_308.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'312'}
Training> Name=main_level/agent, Worker=0, Episode=1151, Total reward=73.79, Steps=37564, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1152, Total reward=68.6, Steps=37608, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1153, Total reward=41.44, Steps=37630, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1154, Total reward=35.65, Steps=37642, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1155, Total reward=2.09, Steps=37655, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1156, Total reward=21.63, Steps=37685, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1157, Total reward=41.95, Steps=37719, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1158, Total reward=24.49, Steps=37739, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1159, Total reward=2.85, Steps=37750, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1160, Total reward=88.61, Steps=37789, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1161, Total reward=57.77, Steps=37818, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1162, Total reward=40.08, Steps=37840, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1163, Total reward=0.02, Steps=37858, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1164, Total reward=1.97, Steps=37875, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1165, Total reward=13.32, Steps=37898, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1166, Total reward=80.28, Steps=37966, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1167, Total reward=3.3, Steps=37981, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1168, Total reward=60.8, Steps=38010, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1169, Total reward=94.26, Steps=38070, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1170, Total reward=34.58, Steps=38107, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1171, Total reward=9.66, Steps=38136, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1172, Total reward=58.9, Steps=38169, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1173, Total reward=30.88, Steps=38195, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1174, Total reward=40.49, Steps=38216, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1175, Total reward=22.57, Steps=38239, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1176, Total reward=22.03, Steps=38257, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1177, Total reward=38.37, Steps=38297, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1178, Total reward=29.21, Steps=38320, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1179, Total reward=68.27, Steps=38367, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1180, Total reward=72.1, Steps=38405, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1181, Total reward=56.66, Steps=38445, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1182, Total reward=30.51, Steps=38470, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1183, Total reward=16.23, Steps=38502, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1184, Total reward=3.48, Steps=38521, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1185, Total reward=22.99, Steps=38552, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1186, Total reward=39.67, Steps=38599, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1187, Total reward=40.84, Steps=38637, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1188, Total reward=53.79, Steps=38663, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1189, Total reward=18.25, Steps=38681, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1190, Total reward=70.96, Steps=38717, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1191, Total reward=55.62, Steps=38757, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1192, Total reward=22.96, Steps=38784, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1193, Total reward=54.46, Steps=38814, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1194, Total reward=30.02, Steps=38825, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1195, Total reward=1.4, Steps=38838, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1196, Total reward=17.36, Steps=38872, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1197, Total reward=50.19, Steps=38925, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1198, Total reward=25.31, Steps=38944, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1199, Total reward=86.23, Steps=39003, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1200, Total reward=61.27, Steps=39040, Training iteration=23
Policy training> Surrogate loss=0.023135455325245857, KL divergence=3.744124842341989e-05, Entropy=0.31511449813842773, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.025958966463804245, KL divergence=0.002169207902625203, Entropy=0.3146326541900635, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08138814568519592, KL divergence=0.008647057227790356, Entropy=0.3030679225921631, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.054279349744319916, KL divergence=0.017389042302966118, Entropy=0.3095349669456482, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.03823761269450188, KL divergence=0.028916092589497566, Entropy=0.30281510949134827, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08327488601207733, KL divergence=0.038217298686504364, Entropy=0.3029896914958954, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05829159915447235, KL divergence=0.05124939605593681, Entropy=0.2975020408630371, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.02337716892361641, KL divergence=0.064146026968956, Entropy=0.2944994568824768, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08303998410701752, KL divergence=0.07865560054779053, Entropy=0.2896740436553955, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07638680189847946, KL divergence=0.08882587403059006, Entropy=0.2940080165863037, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/316_Step-39040.ckpt']
Uploaded 3 files for checkpoint 316 in 0.60 seconds
saved intermediate frozen graph: current/model/model_316.pb
Best checkpoint number: 314, Last checkpoint number: 314
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'308'}
Training> Name=main_level/agent, Worker=0, Episode=1201, Total reward=30.95, Steps=39061, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1202, Total reward=29.72, Steps=39080, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1203, Total reward=2.06, Steps=39109, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1204, Total reward=4.24, Steps=39132, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1205, Total reward=69.44, Steps=39190, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1206, Total reward=90.47, Steps=39266, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1207, Total reward=57.85, Steps=39308, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1208, Total reward=68.56, Steps=39348, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1209, Total reward=18.35, Steps=39368, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1210, Total reward=62.91, Steps=39418, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1211, Total reward=78.74, Steps=39457, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1212, Total reward=42.92, Steps=39489, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1213, Total reward=54.15, Steps=39519, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1214, Total reward=31.57, Steps=39540, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1215, Total reward=18.19, Steps=39569, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1216, Total reward=21.04, Steps=39598, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1217, Total reward=29.92, Steps=39633, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1218, Total reward=33.24, Steps=39680, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1219, Total reward=123.77, Steps=39804, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1220, Total reward=72.07, Steps=39846, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1221, Total reward=57.76, Steps=39874, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1222, Total reward=39.48, Steps=39894, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1223, Total reward=0.03, Steps=39921, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1224, Total reward=29.84, Steps=39982, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1225, Total reward=66.36, Steps=40033, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1226, Total reward=72.04, Steps=40082, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1227, Total reward=67.94, Steps=40133, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1228, Total reward=70.49, Steps=40179, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1229, Total reward=22.18, Steps=40195, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1230, Total reward=80.96, Steps=40229, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1231, Total reward=41.99, Steps=40268, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1232, Total reward=3.76, Steps=40279, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1233, Total reward=45.23, Steps=40309, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1234, Total reward=30.21, Steps=40320, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1235, Total reward=21.09, Steps=40368, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1236, Total reward=18.83, Steps=40386, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1237, Total reward=38.4, Steps=40422, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1238, Total reward=29.52, Steps=40461, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1239, Total reward=16.42, Steps=40482, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1240, Total reward=65.08, Steps=40521, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1241, Total reward=55.88, Steps=40575, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1242, Total reward=35.74, Steps=40604, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1243, Total reward=0.02, Steps=40620, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1244, Total reward=11.66, Steps=40645, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1245, Total reward=13.45, Steps=40668, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1246, Total reward=97.17, Steps=40719, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1247, Total reward=61.24, Steps=40760, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1248, Total reward=86.27, Steps=40824, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1249, Total reward=33.32, Steps=40856, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1250, Total reward=55.36, Steps=40893, Training iteration=24
Policy training> Surrogate loss=-0.020553773269057274, KL divergence=9.779475658433512e-05, Entropy=0.33592700958251953, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=0.00028636804199777544, KL divergence=0.006656771060079336, Entropy=0.3347488343715668, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05653074383735657, KL divergence=0.024735035374760628, Entropy=0.3232855498790741, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06011727452278137, KL divergence=0.044287145137786865, Entropy=0.3219739496707916, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.03471854701638222, KL divergence=0.059623684734106064, Entropy=0.31515762209892273, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.051427993923425674, KL divergence=0.07367692142724991, Entropy=0.30279943346977234, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.057624686509370804, KL divergence=0.08808638900518417, Entropy=0.3090040683746338, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.056981995701789856, KL divergence=0.102308489382267, Entropy=0.3080184757709503, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.062063008546829224, KL divergence=0.11065194010734558, Entropy=0.30348721146583557, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.059840455651283264, KL divergence=0.12086784839630127, Entropy=0.3081810474395752, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/317_Step-40893.ckpt']
Uploaded 3 files for checkpoint 317 in 0.64 seconds
saved intermediate frozen graph: current/model/model_317.pb
Best checkpoint number: 314, Last checkpoint number: 315
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'313'}
Training> Name=main_level/agent, Worker=0, Episode=1251, Total reward=19.79, Steps=40921, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1252, Total reward=1.82, Steps=40932, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1253, Total reward=47.83, Steps=40963, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1254, Total reward=29.64, Steps=40975, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1255, Total reward=30.85, Steps=41005, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1256, Total reward=17.54, Steps=41030, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1257, Total reward=42.3, Steps=41069, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1258, Total reward=32.29, Steps=41093, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1259, Total reward=52.79, Steps=41148, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1260, Total reward=79.29, Steps=41190, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1261, Total reward=53.1, Steps=41222, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1262, Total reward=34.88, Steps=41243, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1263, Total reward=3.68, Steps=41260, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1264, Total reward=84.4, Steps=41331, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1265, Total reward=75.5, Steps=41386, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1266, Total reward=49.52, Steps=41433, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1267, Total reward=37.55, Steps=41468, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1268, Total reward=62.33, Steps=41497, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1269, Total reward=75.79, Steps=41552, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1270, Total reward=35.71, Steps=41587, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1271, Total reward=38.68, Steps=41619, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1272, Total reward=3.78, Steps=41630, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1273, Total reward=37.89, Steps=41652, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1274, Total reward=36.06, Steps=41674, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1275, Total reward=24.57, Steps=41705, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1276, Total reward=15.3, Steps=41739, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1277, Total reward=44.61, Steps=41770, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1278, Total reward=30.08, Steps=41793, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1279, Total reward=19.88, Steps=41812, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1280, Total reward=68.28, Steps=41847, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1281, Total reward=28.35, Steps=41874, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1282, Total reward=36.02, Steps=41922, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1283, Total reward=5.84, Steps=41946, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1284, Total reward=2.18, Steps=41978, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1285, Total reward=3.4, Steps=41990, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1286, Total reward=120.55, Steps=42085, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1287, Total reward=25.23, Steps=42124, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1288, Total reward=37.93, Steps=42148, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1289, Total reward=36.75, Steps=42183, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1290, Total reward=74.32, Steps=42217, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1291, Total reward=55.92, Steps=42243, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1292, Total reward=65.46, Steps=42276, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1293, Total reward=58.73, Steps=42305, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1294, Total reward=40.56, Steps=42326, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1295, Total reward=26.61, Steps=42357, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1296, Total reward=22.59, Steps=42374, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1297, Total reward=46.39, Steps=42427, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1298, Total reward=35.18, Steps=42451, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1299, Total reward=15.01, Steps=42492, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1300, Total reward=65.52, Steps=42538, Training iteration=25
Policy training> Surrogate loss=-0.0003236668708268553, KL divergence=0.0001542873797006905, Entropy=0.35012054443359375, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.033264536410570145, KL divergence=0.007596829440444708, Entropy=0.3497726023197174, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03574022278189659, KL divergence=0.02334532141685486, Entropy=0.3466888964176178, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0389125794172287, KL divergence=0.04154267534613609, Entropy=0.3332241475582123, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.050087425857782364, KL divergence=0.05916335806250572, Entropy=0.32862216234207153, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06486990302801132, KL divergence=0.0746140405535698, Entropy=0.323032408952713, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06370190531015396, KL divergence=0.08601215481758118, Entropy=0.320066899061203, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.048542097210884094, KL divergence=0.09709474444389343, Entropy=0.3214964270591736, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05403925105929375, KL divergence=0.10818755626678467, Entropy=0.3191697299480438, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06492485851049423, KL divergence=0.110613614320755, Entropy=0.32124483585357666, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/318_Step-42538.ckpt']
Uploaded 3 files for checkpoint 318 in 0.57 seconds
saved intermediate frozen graph: current/model/model_318.pb
Best checkpoint number: 314, Last checkpoint number: 316
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'315'}
Training> Name=main_level/agent, Worker=0, Episode=1301, Total reward=56.86, Steps=42569, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1302, Total reward=26.82, Steps=42602, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1303, Total reward=3.02, Steps=42626, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1304, Total reward=72.13, Steps=42697, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1305, Total reward=16.26, Steps=42712, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1306, Total reward=39.58, Steps=42743, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1307, Total reward=35.16, Steps=42778, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1308, Total reward=56.01, Steps=42806, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1309, Total reward=19.7, Steps=42827, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1310, Total reward=72.44, Steps=42862, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1311, Total reward=43.39, Steps=42902, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1312, Total reward=38.33, Steps=42933, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1313, Total reward=43.35, Steps=42954, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1314, Total reward=44.41, Steps=42976, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1315, Total reward=31.77, Steps=43008, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1316, Total reward=12.26, Steps=43041, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1317, Total reward=36.06, Steps=43082, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1318, Total reward=32.67, Steps=43122, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1319, Total reward=20.04, Steps=43143, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1320, Total reward=84.59, Steps=43181, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1321, Total reward=57.78, Steps=43211, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1322, Total reward=33.05, Steps=43232, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1323, Total reward=10.29, Steps=43264, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1324, Total reward=0.03, Steps=43291, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1325, Total reward=79.58, Steps=43355, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1326, Total reward=77.85, Steps=43460, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1327, Total reward=6.6, Steps=43475, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1328, Total reward=24.08, Steps=43490, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1329, Total reward=40.3, Steps=43521, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1330, Total reward=66.47, Steps=43557, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1331, Total reward=19.25, Steps=43587, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1332, Total reward=9.41, Steps=43599, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1333, Total reward=32.4, Steps=43620, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1334, Total reward=32.35, Steps=43642, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1335, Total reward=33.27, Steps=43672, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1336, Total reward=23.66, Steps=43704, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1337, Total reward=47.71, Steps=43740, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1338, Total reward=37.85, Steps=43765, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1339, Total reward=74.96, Steps=43813, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1340, Total reward=83.3, Steps=43851, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1341, Total reward=66.03, Steps=43883, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1342, Total reward=36.16, Steps=43905, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1343, Total reward=96.57, Steps=43996, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1344, Total reward=14.49, Steps=44028, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1345, Total reward=6.73, Steps=44040, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1346, Total reward=53.88, Steps=44121, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1347, Total reward=58.25, Steps=44158, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1348, Total reward=51.54, Steps=44181, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1349, Total reward=70.03, Steps=44236, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1350, Total reward=69.19, Steps=44287, Training iteration=26
Policy training> Surrogate loss=-0.004152265843003988, KL divergence=9.255189797841012e-05, Entropy=0.3396775424480438, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02619463950395584, KL divergence=0.007330369204282761, Entropy=0.3340415060520172, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.025510182604193687, KL divergence=0.021070709452033043, Entropy=0.32422778010368347, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.026766164228320122, KL divergence=0.038480259478092194, Entropy=0.3176715075969696, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04704270884394646, KL divergence=0.0579831600189209, Entropy=0.32179391384124756, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.056098029017448425, KL divergence=0.07203671336174011, Entropy=0.3110167682170868, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0642751008272171, KL divergence=0.08640775829553604, Entropy=0.31400132179260254, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05311824008822441, KL divergence=0.10010556131601334, Entropy=0.3114657700061798, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07527080178260803, KL divergence=0.10844974964857101, Entropy=0.31096383929252625, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0620444230735302, KL divergence=0.11165126413106918, Entropy=0.3042052686214447, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/319_Step-44287.ckpt']
Uploaded 3 files for checkpoint 319 in 0.59 seconds
saved intermediate frozen graph: current/model/model_319.pb
Best checkpoint number: 314, Last checkpoint number: 317
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'316'}
Training> Name=main_level/agent, Worker=0, Episode=1351, Total reward=44.34, Steps=44327, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1352, Total reward=43.04, Steps=44358, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1353, Total reward=65.63, Steps=44389, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1354, Total reward=43.79, Steps=44410, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1355, Total reward=20.95, Steps=44441, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1356, Total reward=11.28, Steps=44459, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1357, Total reward=16.17, Steps=44479, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1358, Total reward=31.72, Steps=44532, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1359, Total reward=68.86, Steps=44582, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1360, Total reward=85.03, Steps=44623, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1361, Total reward=42.11, Steps=44650, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1362, Total reward=33.6, Steps=44694, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1363, Total reward=3.46, Steps=44714, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1364, Total reward=14.92, Steps=44748, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1365, Total reward=11.48, Steps=44768, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1366, Total reward=59.08, Steps=44813, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1367, Total reward=62.57, Steps=44852, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1368, Total reward=49.71, Steps=44878, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1369, Total reward=23.64, Steps=44903, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1370, Total reward=60.16, Steps=44948, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1371, Total reward=37.81, Steps=44989, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1372, Total reward=3.75, Steps=45001, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1373, Total reward=48.78, Steps=45031, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1374, Total reward=27.35, Steps=45042, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1375, Total reward=15.5, Steps=45069, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1376, Total reward=26.4, Steps=45095, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1377, Total reward=25.49, Steps=45126, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1378, Total reward=31.43, Steps=45148, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1379, Total reward=56.91, Steps=45196, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1380, Total reward=71.25, Steps=45256, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1381, Total reward=48.36, Steps=45285, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1382, Total reward=28.06, Steps=45307, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1383, Total reward=2.97, Steps=45334, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1384, Total reward=18.8, Steps=45391, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1385, Total reward=23.79, Steps=45425, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1386, Total reward=45.69, Steps=45471, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1387, Total reward=50.3, Steps=45510, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1388, Total reward=49.61, Steps=45537, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1389, Total reward=87.16, Steps=45580, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1390, Total reward=57.82, Steps=45628, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1391, Total reward=28.83, Steps=45660, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1392, Total reward=53.52, Steps=45692, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1393, Total reward=28.4, Steps=45718, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1394, Total reward=39.87, Steps=45752, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1395, Total reward=19.08, Steps=45778, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1396, Total reward=50.3, Steps=45852, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1397, Total reward=36.34, Steps=45883, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1398, Total reward=22.32, Steps=45899, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1399, Total reward=9.54, Steps=45921, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1400, Total reward=96.11, Steps=45960, Training iteration=27
Policy training> Surrogate loss=-0.005455354694277048, KL divergence=0.00010215258225798607, Entropy=0.329823762178421, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.013612146489322186, KL divergence=0.005502635147422552, Entropy=0.3292781412601471, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.053212255239486694, KL divergence=0.021470220759510994, Entropy=0.31747424602508545, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06035204231739044, KL divergence=0.03967295214533806, Entropy=0.3112020790576935, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04126591235399246, KL divergence=0.057121675461530685, Entropy=0.3066360056400299, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05756191536784172, KL divergence=0.07604057341814041, Entropy=0.3092988431453705, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06959662586450577, KL divergence=0.09264111518859863, Entropy=0.29832419753074646, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06419407576322556, KL divergence=0.10225850343704224, Entropy=0.2985149621963501, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06701802462339401, KL divergence=0.11418633908033371, Entropy=0.3005628287792206, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06115427613258362, KL divergence=0.12015002965927124, Entropy=0.30097270011901855, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/320_Step-45960.ckpt']
Uploaded 3 files for checkpoint 320 in 0.57 seconds
saved intermediate frozen graph: current/model/model_320.pb
Best checkpoint number: 314, Last checkpoint number: 318
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'317'}
Training> Name=main_level/agent, Worker=0, Episode=1401, Total reward=56.71, Steps=45999, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1402, Total reward=29.56, Steps=46040, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1403, Total reward=9.37, Steps=46055, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1404, Total reward=63.73, Steps=46127, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1405, Total reward=56.8, Steps=46193, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1406, Total reward=96.77, Steps=46258, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1407, Total reward=44.38, Steps=46297, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1408, Total reward=54.88, Steps=46339, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1409, Total reward=27.76, Steps=46380, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1410, Total reward=54.34, Steps=46417, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1411, Total reward=69.61, Steps=46458, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1412, Total reward=23.92, Steps=46489, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1413, Total reward=19.13, Steps=46508, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1414, Total reward=31.37, Steps=46519, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1415, Total reward=24.12, Steps=46561, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1416, Total reward=16.12, Steps=46580, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1417, Total reward=40.55, Steps=46644, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1418, Total reward=41.16, Steps=46670, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1419, Total reward=57.12, Steps=46715, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1420, Total reward=73.15, Steps=46755, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1421, Total reward=58.93, Steps=46786, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1422, Total reward=42.68, Steps=46827, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1423, Total reward=17.92, Steps=46870, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1424, Total reward=73.73, Steps=46951, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1425, Total reward=21.18, Steps=46986, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1426, Total reward=64.87, Steps=47034, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1427, Total reward=68.5, Steps=47087, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1428, Total reward=72.77, Steps=47145, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1429, Total reward=21.95, Steps=47164, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1430, Total reward=64.48, Steps=47200, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1431, Total reward=44.53, Steps=47241, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1432, Total reward=52.08, Steps=47273, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1433, Total reward=41.43, Steps=47303, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1434, Total reward=38.95, Steps=47325, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1435, Total reward=21.09, Steps=47354, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1436, Total reward=16.79, Steps=47381, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1437, Total reward=51.93, Steps=47435, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1438, Total reward=36.59, Steps=47460, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1439, Total reward=42.12, Steps=47495, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1440, Total reward=87.87, Steps=47540, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1441, Total reward=68.09, Steps=47594, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1442, Total reward=43.72, Steps=47652, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1443, Total reward=20.76, Steps=47679, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1444, Total reward=10.25, Steps=47708, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1445, Total reward=6.27, Steps=47748, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1446, Total reward=102.73, Steps=47804, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1447, Total reward=9.83, Steps=47834, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1448, Total reward=64.04, Steps=47865, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1449, Total reward=34.4, Steps=47899, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1450, Total reward=56.86, Steps=47935, Training iteration=28
Policy training> Surrogate loss=0.018854180350899696, KL divergence=0.00011770093260565773, Entropy=0.3333568274974823, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.01890791766345501, KL divergence=0.005520109552890062, Entropy=0.33786508440971375, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.026753896847367287, KL divergence=0.01787901669740677, Entropy=0.3368915617465973, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03656202182173729, KL divergence=0.033457159996032715, Entropy=0.3331812918186188, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04195873439311981, KL divergence=0.0460224449634552, Entropy=0.31628668308258057, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04097118228673935, KL divergence=0.05794322490692139, Entropy=0.31255272030830383, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.054994720965623856, KL divergence=0.07668274641036987, Entropy=0.31958475708961487, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07131415605545044, KL divergence=0.08487638831138611, Entropy=0.31695011258125305, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07628272473812103, KL divergence=0.09495499730110168, Entropy=0.3121550381183624, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.034783411771059036, KL divergence=0.1065954864025116, Entropy=0.31815263628959656, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/321_Step-47935.ckpt']
Uploaded 3 files for checkpoint 321 in 0.56 seconds
saved intermediate frozen graph: current/model/model_321.pb
Best checkpoint number: 314, Last checkpoint number: 319
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'318'}
Training> Name=main_level/agent, Worker=0, Episode=1451, Total reward=47.94, Steps=47976, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1452, Total reward=67.21, Steps=48017, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1453, Total reward=8.44, Steps=48036, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1454, Total reward=44.83, Steps=48058, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1455, Total reward=20.74, Steps=48087, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1456, Total reward=17.89, Steps=48115, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1457, Total reward=48.25, Steps=48176, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1458, Total reward=31.14, Steps=48199, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1459, Total reward=9.84, Steps=48213, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1460, Total reward=80.85, Steps=48252, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1461, Total reward=33.24, Steps=48280, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1462, Total reward=29.99, Steps=48302, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1463, Total reward=32.19, Steps=48344, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1464, Total reward=5.78, Steps=48371, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1465, Total reward=16.18, Steps=48408, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1466, Total reward=111.49, Steps=48486, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1467, Total reward=60.31, Steps=48526, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1468, Total reward=65.79, Steps=48570, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1469, Total reward=36.59, Steps=48595, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1470, Total reward=59.14, Steps=48634, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1471, Total reward=46.25, Steps=48673, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1472, Total reward=40.77, Steps=48700, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1473, Total reward=47.68, Steps=48729, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1474, Total reward=31.76, Steps=48750, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1475, Total reward=17.82, Steps=48776, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1476, Total reward=12.27, Steps=48794, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1477, Total reward=51.13, Steps=48833, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1478, Total reward=51.82, Steps=48880, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1479, Total reward=9.61, Steps=48896, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1480, Total reward=65.32, Steps=48928, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1481, Total reward=35.54, Steps=48956, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1482, Total reward=44.77, Steps=48980, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1483, Total reward=5.94, Steps=48995, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1484, Total reward=13.61, Steps=49022, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1485, Total reward=27.51, Steps=49059, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1486, Total reward=72.1, Steps=49104, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1487, Total reward=21.72, Steps=49138, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1488, Total reward=59.18, Steps=49166, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1489, Total reward=20.48, Steps=49184, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1490, Total reward=84.03, Steps=49244, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1491, Total reward=45.6, Steps=49281, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1492, Total reward=34.86, Steps=49311, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1493, Total reward=21.31, Steps=49322, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1494, Total reward=35.6, Steps=49344, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1495, Total reward=31.63, Steps=49368, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1496, Total reward=95.94, Steps=49474, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1497, Total reward=32.44, Steps=49505, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1498, Total reward=36.63, Steps=49529, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1499, Total reward=75.61, Steps=49580, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1500, Total reward=87.9, Steps=49617, Training iteration=29
Policy training> Surrogate loss=-0.0030304836109280586, KL divergence=9.975679131457582e-05, Entropy=0.3249981105327606, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.024612732231616974, KL divergence=0.007767044007778168, Entropy=0.3302987515926361, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.026598243042826653, KL divergence=0.02467905730009079, Entropy=0.32337135076522827, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06470244377851486, KL divergence=0.04318447411060333, Entropy=0.3169962167739868, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.057538360357284546, KL divergence=0.05977479740977287, Entropy=0.3086286783218384, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.049869317561388016, KL divergence=0.07817857712507248, Entropy=0.312377005815506, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06518471986055374, KL divergence=0.09129801392555237, Entropy=0.3029541075229645, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.053705524653196335, KL divergence=0.1010715588927269, Entropy=0.30949196219444275, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05343138054013252, KL divergence=0.1107180118560791, Entropy=0.3056342899799347, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.065089650452137, KL divergence=0.1210797131061554, Entropy=0.3078024089336395, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/322_Step-49617.ckpt']
Uploaded 3 files for checkpoint 322 in 0.60 seconds
saved intermediate frozen graph: current/model/model_322.pb
Best checkpoint number: 314, Last checkpoint number: 320
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'319'}
Training> Name=main_level/agent, Worker=0, Episode=1501, Total reward=47.38, Steps=49644, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1502, Total reward=32.68, Steps=49670, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1503, Total reward=18.4, Steps=49705, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1504, Total reward=16.52, Steps=49731, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1505, Total reward=12.95, Steps=49754, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1506, Total reward=64.68, Steps=49804, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1507, Total reward=69.35, Steps=49857, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1508, Total reward=72.48, Steps=49941, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1509, Total reward=18.68, Steps=49957, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1510, Total reward=30.15, Steps=49981, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1511, Total reward=53.76, Steps=50020, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1512, Total reward=64.16, Steps=50063, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1513, Total reward=42.24, Steps=50094, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1514, Total reward=30.76, Steps=50105, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1515, Total reward=7.4, Steps=50129, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1516, Total reward=21.21, Steps=50160, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1517, Total reward=55.21, Steps=50195, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1518, Total reward=34.73, Steps=50222, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1519, Total reward=18.85, Steps=50246, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1520, Total reward=67.5, Steps=50286, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1521, Total reward=54.89, Steps=50320, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1522, Total reward=30.48, Steps=50353, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1523, Total reward=25.0, Steps=50414, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1524, Total reward=0.03, Steps=50442, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1525, Total reward=6.29, Steps=50458, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1526, Total reward=42.04, Steps=50486, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1527, Total reward=30.44, Steps=50525, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1528, Total reward=62.13, Steps=50555, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1529, Total reward=76.6, Steps=50602, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1530, Total reward=63.64, Steps=50655, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1531, Total reward=47.59, Steps=50693, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1532, Total reward=48.41, Steps=50724, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1533, Total reward=42.45, Steps=50752, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1534, Total reward=44.17, Steps=50774, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1535, Total reward=12.47, Steps=50801, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1536, Total reward=20.69, Steps=50830, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1537, Total reward=112.45, Steps=50915, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1538, Total reward=24.79, Steps=50934, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1539, Total reward=14.57, Steps=50954, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1540, Total reward=55.86, Steps=50993, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1541, Total reward=54.01, Steps=51022, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1542, Total reward=31.9, Steps=51041, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1543, Total reward=2.93, Steps=51080, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1544, Total reward=14.4, Steps=51106, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1545, Total reward=3.39, Steps=51122, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1546, Total reward=43.49, Steps=51174, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1547, Total reward=12.28, Steps=51210, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1548, Total reward=67.45, Steps=51269, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1549, Total reward=71.16, Steps=51329, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1550, Total reward=21.99, Steps=51354, Training iteration=30
Policy training> Surrogate loss=0.007624639663845301, KL divergence=0.00012324820272624493, Entropy=0.32619649171829224, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.020457183942198753, KL divergence=0.008518106304109097, Entropy=0.31710729002952576, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.031167373061180115, KL divergence=0.023521557450294495, Entropy=0.3085515797138214, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04943697154521942, KL divergence=0.04420390725135803, Entropy=0.30968359112739563, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.03932527080178261, KL divergence=0.057297978550195694, Entropy=0.3024977445602417, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.033697281032800674, KL divergence=0.07750384509563446, Entropy=0.300848126411438, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05940994992852211, KL divergence=0.09055149555206299, Entropy=0.295734167098999, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05704927071928978, KL divergence=0.09399744868278503, Entropy=0.29552796483039856, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06082598865032196, KL divergence=0.10750362277030945, Entropy=0.2945501208305359, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05653107166290283, KL divergence=0.11451572179794312, Entropy=0.2978433668613434, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/323_Step-51354.ckpt']
Uploaded 3 files for checkpoint 323 in 0.58 seconds
saved intermediate frozen graph: current/model/model_323.pb
Best checkpoint number: 314, Last checkpoint number: 321
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'320'}
Training> Name=main_level/agent, Worker=0, Episode=1551, Total reward=48.48, Steps=51393, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1552, Total reward=48.13, Steps=51422, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1553, Total reward=47.23, Steps=51452, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1554, Total reward=39.15, Steps=51473, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1555, Total reward=24.77, Steps=51502, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1556, Total reward=30.78, Steps=51532, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1557, Total reward=40.54, Steps=51571, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1558, Total reward=50.17, Steps=51635, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1559, Total reward=66.34, Steps=51684, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1560, Total reward=52.37, Steps=51722, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1561, Total reward=51.61, Steps=51750, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1562, Total reward=30.83, Steps=51775, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1563, Total reward=3.43, Steps=51795, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1564, Total reward=79.53, Steps=51858, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1565, Total reward=80.97, Steps=51921, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1566, Total reward=80.57, Steps=51965, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1567, Total reward=3.31, Steps=51978, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1568, Total reward=61.03, Steps=52005, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1569, Total reward=36.83, Steps=52047, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1570, Total reward=19.02, Steps=52080, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1571, Total reward=50.07, Steps=52119, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1572, Total reward=33.24, Steps=52149, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1573, Total reward=43.88, Steps=52179, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1574, Total reward=37.85, Steps=52200, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1575, Total reward=18.88, Steps=52230, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1576, Total reward=10.79, Steps=52246, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1577, Total reward=52.37, Steps=52287, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1578, Total reward=27.74, Steps=52301, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1579, Total reward=7.12, Steps=52321, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1580, Total reward=85.25, Steps=52358, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1581, Total reward=57.19, Steps=52388, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1582, Total reward=33.01, Steps=52443, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1583, Total reward=19.2, Steps=52475, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1584, Total reward=19.01, Steps=52501, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1585, Total reward=97.21, Steps=52554, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1586, Total reward=85.94, Steps=52604, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1587, Total reward=16.68, Steps=52619, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1588, Total reward=75.34, Steps=52660, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1589, Total reward=28.75, Steps=52684, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1590, Total reward=120.83, Steps=52743, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1591, Total reward=18.52, Steps=52769, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1592, Total reward=40.24, Steps=52797, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1593, Total reward=36.71, Steps=52818, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1594, Total reward=38.41, Steps=52841, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1595, Total reward=29.43, Steps=52873, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1596, Total reward=24.05, Steps=52903, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1597, Total reward=49.61, Steps=52937, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1598, Total reward=35.81, Steps=52960, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1599, Total reward=22.71, Steps=52982, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1600, Total reward=96.94, Steps=53020, Training iteration=31
Policy training> Surrogate loss=0.0007592241163365543, KL divergence=0.00014358777843881398, Entropy=0.3228831887245178, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04054230824112892, KL divergence=0.007093256805092096, Entropy=0.3200221359729767, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04023347049951553, KL divergence=0.0244793388992548, Entropy=0.3127196729183197, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04124520719051361, KL divergence=0.04141295328736305, Entropy=0.3098275363445282, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06646700948476791, KL divergence=0.06061851978302002, Entropy=0.30210426449775696, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.054611895233392715, KL divergence=0.07593893259763718, Entropy=0.29601725935935974, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0677015408873558, KL divergence=0.08801516890525818, Entropy=0.29427891969680786, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05170450732111931, KL divergence=0.09677837044000626, Entropy=0.28675660490989685, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04852946102619171, KL divergence=0.11576145142316818, Entropy=0.29336485266685486, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.04746077582240105, KL divergence=0.1252998262643814, Entropy=0.2947392761707306, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/324_Step-53020.ckpt']
Uploaded 3 files for checkpoint 324 in 0.59 seconds
saved intermediate frozen graph: current/model/model_324.pb
Best checkpoint number: 314, Last checkpoint number: 322
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'321'}
Training> Name=main_level/agent, Worker=0, Episode=1601, Total reward=27.84, Steps=53046, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1602, Total reward=35.38, Steps=53071, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1603, Total reward=0.03, Steps=53096, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1604, Total reward=7.29, Steps=53113, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1605, Total reward=11.31, Steps=53139, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1606, Total reward=10.56, Steps=53159, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1607, Total reward=34.77, Steps=53196, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1608, Total reward=50.41, Steps=53222, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1609, Total reward=93.89, Steps=53278, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1610, Total reward=36.94, Steps=53319, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1611, Total reward=44.32, Steps=53347, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1612, Total reward=46.32, Steps=53378, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1613, Total reward=43.6, Steps=53405, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1614, Total reward=41.32, Steps=53426, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1615, Total reward=31.12, Steps=53449, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1616, Total reward=19.21, Steps=53476, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1617, Total reward=47.74, Steps=53513, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1618, Total reward=42.56, Steps=53536, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1619, Total reward=10.27, Steps=53556, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1620, Total reward=75.59, Steps=53590, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1621, Total reward=49.27, Steps=53619, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1622, Total reward=38.48, Steps=53645, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1623, Total reward=7.15, Steps=53662, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1624, Total reward=1.67, Steps=53689, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1625, Total reward=19.68, Steps=53728, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1626, Total reward=69.93, Steps=53775, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1627, Total reward=59.55, Steps=53829, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1628, Total reward=63.84, Steps=53870, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1629, Total reward=25.51, Steps=53896, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1630, Total reward=48.28, Steps=53932, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1631, Total reward=15.37, Steps=53956, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1632, Total reward=49.58, Steps=53987, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1633, Total reward=50.85, Steps=54015, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1634, Total reward=40.72, Steps=54036, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1635, Total reward=16.32, Steps=54083, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1636, Total reward=21.81, Steps=54112, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1637, Total reward=43.1, Steps=54145, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1638, Total reward=34.92, Steps=54168, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1639, Total reward=11.06, Steps=54189, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1640, Total reward=83.93, Steps=54226, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1641, Total reward=37.29, Steps=54254, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1642, Total reward=42.9, Steps=54275, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1643, Total reward=20.3, Steps=54304, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1644, Total reward=4.03, Steps=54336, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1645, Total reward=10.9, Steps=54354, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1646, Total reward=69.88, Steps=54402, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1647, Total reward=42.62, Steps=54444, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1648, Total reward=60.96, Steps=54472, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1649, Total reward=33.71, Steps=54512, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1650, Total reward=58.06, Steps=54558, Training iteration=32
Policy training> Surrogate loss=8.859360968926921e-05, KL divergence=8.49846619530581e-05, Entropy=0.31876447796821594, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.027527114376425743, KL divergence=0.006295122671872377, Entropy=0.3135826289653778, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03768332302570343, KL divergence=0.02070741541683674, Entropy=0.3061228394508362, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.045010458678007126, KL divergence=0.03816048800945282, Entropy=0.3018484115600586, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.047328706830739975, KL divergence=0.055519938468933105, Entropy=0.2974572479724884, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04964987561106682, KL divergence=0.07116452604532242, Entropy=0.29454901814460754, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05216948315501213, KL divergence=0.08447308093309402, Entropy=0.29235783219337463, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05402378365397453, KL divergence=0.0953567624092102, Entropy=0.291592001914978, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05447227135300636, KL divergence=0.10466540604829788, Entropy=0.29049935936927795, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.054488811641931534, KL divergence=0.11312549561262131, Entropy=0.2921622097492218, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/325_Step-54558.ckpt']
Uploaded 3 files for checkpoint 325 in 0.61 seconds
saved intermediate frozen graph: current/model/model_325.pb
Best checkpoint number: 314, Last checkpoint number: 323
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'322'}
Training> Name=main_level/agent, Worker=0, Episode=1651, Total reward=52.8, Steps=54601, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1652, Total reward=49.92, Steps=54631, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1653, Total reward=51.02, Steps=54661, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1654, Total reward=28.05, Steps=54682, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1655, Total reward=23.91, Steps=54703, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1656, Total reward=20.91, Steps=54739, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1657, Total reward=49.58, Steps=54785, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1658, Total reward=36.34, Steps=54810, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1659, Total reward=18.6, Steps=54827, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1660, Total reward=74.48, Steps=54867, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1661, Total reward=43.04, Steps=54894, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1662, Total reward=50.11, Steps=54935, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1663, Total reward=3.07, Steps=54976, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1664, Total reward=62.52, Steps=55043, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1665, Total reward=76.04, Steps=55133, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1666, Total reward=14.92, Steps=55159, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1667, Total reward=6.73, Steps=55172, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1668, Total reward=77.9, Steps=55212, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1669, Total reward=25.66, Steps=55240, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1670, Total reward=64.32, Steps=55275, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1671, Total reward=20.63, Steps=55299, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1672, Total reward=53.74, Steps=55341, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1673, Total reward=0.01, Steps=55355, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1674, Total reward=35.13, Steps=55377, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1675, Total reward=24.85, Steps=55421, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1676, Total reward=17.68, Steps=55436, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1677, Total reward=42.67, Steps=55472, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1678, Total reward=29.43, Steps=55485, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1679, Total reward=17.44, Steps=55506, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1680, Total reward=57.25, Steps=55538, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1681, Total reward=39.73, Steps=55567, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1682, Total reward=37.94, Steps=55593, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1683, Total reward=15.57, Steps=55639, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1684, Total reward=77.6, Steps=55706, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1685, Total reward=21.88, Steps=55737, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1686, Total reward=36.06, Steps=55768, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1687, Total reward=17.93, Steps=55804, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1688, Total reward=40.69, Steps=55828, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1689, Total reward=57.46, Steps=55875, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1690, Total reward=14.0, Steps=55888, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1691, Total reward=41.99, Steps=55917, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1692, Total reward=43.8, Steps=55949, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1693, Total reward=41.35, Steps=55971, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1694, Total reward=42.22, Steps=56001, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1695, Total reward=14.32, Steps=56031, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1696, Total reward=49.36, Steps=56083, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1697, Total reward=20.35, Steps=56103, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1698, Total reward=78.56, Steps=56193, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1699, Total reward=11.65, Steps=56208, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1700, Total reward=94.85, Steps=56248, Training iteration=33
Policy training> Surrogate loss=0.006113964598625898, KL divergence=0.00011553405784070492, Entropy=0.34303703904151917, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02338920347392559, KL divergence=0.007823307998478413, Entropy=0.340588241815567, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04788022115826607, KL divergence=0.024110453203320503, Entropy=0.3348095715045929, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.050500769168138504, KL divergence=0.04305986687541008, Entropy=0.32809245586395264, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0484476201236248, KL divergence=0.06260498613119125, Entropy=0.32400164008140564, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0538802444934845, KL divergence=0.0781295970082283, Entropy=0.3219357430934906, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06758516281843185, KL divergence=0.09237968921661377, Entropy=0.316743940114975, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.060572970658540726, KL divergence=0.10436618328094482, Entropy=0.3112853467464447, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07693418115377426, KL divergence=0.11262781172990799, Entropy=0.3113236129283905, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.059624236077070236, KL divergence=0.1204524114727974, Entropy=0.3073541820049286, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/326_Step-56248.ckpt']
Uploaded 3 files for checkpoint 326 in 0.52 seconds
saved intermediate frozen graph: current/model/model_326.pb
Best checkpoint number: 314, Last checkpoint number: 324
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'323'}
Training> Name=main_level/agent, Worker=0, Episode=1701, Total reward=47.6, Steps=56276, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1702, Total reward=27.85, Steps=56295, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1703, Total reward=0.02, Steps=56319, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1704, Total reward=86.64, Steps=56399, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1705, Total reward=18.02, Steps=56422, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1706, Total reward=26.23, Steps=56446, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1707, Total reward=9.94, Steps=56467, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1708, Total reward=79.72, Steps=56523, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1709, Total reward=54.89, Steps=56569, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1710, Total reward=68.97, Steps=56607, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1711, Total reward=48.38, Steps=56635, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1712, Total reward=69.9, Steps=56675, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1713, Total reward=36.74, Steps=56707, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1714, Total reward=30.72, Steps=56718, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1715, Total reward=30.35, Steps=56749, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1716, Total reward=18.98, Steps=56780, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1717, Total reward=43.52, Steps=56815, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1718, Total reward=32.87, Steps=56840, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1719, Total reward=52.55, Steps=56898, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1720, Total reward=37.44, Steps=56940, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1721, Total reward=22.28, Steps=56963, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1722, Total reward=28.5, Steps=56985, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1723, Total reward=22.23, Steps=57019, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1724, Total reward=95.21, Steps=57089, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1725, Total reward=7.83, Steps=57105, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1726, Total reward=93.99, Steps=57154, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1727, Total reward=77.36, Steps=57220, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1728, Total reward=66.09, Steps=57261, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1729, Total reward=13.06, Steps=57275, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1730, Total reward=73.89, Steps=57318, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1731, Total reward=52.54, Steps=57357, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1732, Total reward=44.12, Steps=57387, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1733, Total reward=19.63, Steps=57397, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1734, Total reward=6.34, Steps=57418, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1735, Total reward=27.54, Steps=57449, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1736, Total reward=25.88, Steps=57479, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1737, Total reward=13.31, Steps=57491, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1738, Total reward=45.69, Steps=57516, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1739, Total reward=5.78, Steps=57531, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1740, Total reward=89.22, Steps=57574, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1741, Total reward=46.98, Steps=57600, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1742, Total reward=35.46, Steps=57629, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1743, Total reward=0.03, Steps=57657, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1744, Total reward=12.16, Steps=57708, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1745, Total reward=40.9, Steps=57744, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1746, Total reward=50.59, Steps=57780, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1747, Total reward=47.44, Steps=57812, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1748, Total reward=25.7, Steps=57835, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1749, Total reward=36.87, Steps=57882, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1750, Total reward=52.18, Steps=57931, Training iteration=34
Policy training> Surrogate loss=0.006147290114313364, KL divergence=5.4601779993390664e-05, Entropy=0.34144720435142517, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.035906657576560974, KL divergence=0.006309951189905405, Entropy=0.3365665376186371, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04255939647555351, KL divergence=0.0249644722789526, Entropy=0.3298650085926056, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.043573636561632156, KL divergence=0.04314550384879112, Entropy=0.3199232518672943, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05025598034262657, KL divergence=0.06439021974802017, Entropy=0.31434813141822815, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05142045393586159, KL divergence=0.07392438501119614, Entropy=0.3119782507419586, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.060259055346250534, KL divergence=0.09347018599510193, Entropy=0.30915531516075134, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06098256632685661, KL divergence=0.10692143440246582, Entropy=0.3080977201461792, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07323264330625534, KL divergence=0.11673877388238907, Entropy=0.3087596893310547, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05662321671843529, KL divergence=0.12783904373645782, Entropy=0.3123854100704193, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/327_Step-57931.ckpt']
Uploaded 3 files for checkpoint 327 in 0.58 seconds
saved intermediate frozen graph: current/model/model_327.pb
Best checkpoint number: 314, Last checkpoint number: 325
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'324'}
Training> Name=main_level/agent, Worker=0, Episode=1751, Total reward=55.53, Steps=57968, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1752, Total reward=41.89, Steps=57998, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1753, Total reward=0.02, Steps=58014, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1754, Total reward=45.42, Steps=58036, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1755, Total reward=28.75, Steps=58067, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1756, Total reward=19.88, Steps=58097, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1757, Total reward=44.75, Steps=58128, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1758, Total reward=24.92, Steps=58150, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1759, Total reward=32.17, Steps=58181, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1760, Total reward=63.63, Steps=58218, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1761, Total reward=55.18, Steps=58246, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1762, Total reward=31.28, Steps=58275, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1763, Total reward=6.92, Steps=58315, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1764, Total reward=11.44, Steps=58342, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1765, Total reward=2.79, Steps=58354, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1766, Total reward=72.4, Steps=58412, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1767, Total reward=60.31, Steps=58451, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1768, Total reward=60.06, Steps=58492, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1769, Total reward=24.88, Steps=58516, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1770, Total reward=48.07, Steps=58569, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1771, Total reward=29.68, Steps=58599, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1772, Total reward=17.47, Steps=58624, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1773, Total reward=50.61, Steps=58653, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1774, Total reward=49.02, Steps=58675, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1775, Total reward=36.08, Steps=58705, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1776, Total reward=25.83, Steps=58734, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1777, Total reward=96.83, Steps=58804, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1778, Total reward=32.39, Steps=58828, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1779, Total reward=78.03, Steps=58890, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1780, Total reward=25.17, Steps=58917, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1781, Total reward=146.56, Steps=59021, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1782, Total reward=29.54, Steps=59043, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1783, Total reward=30.6, Steps=59097, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1784, Total reward=13.78, Steps=59122, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1785, Total reward=15.08, Steps=59139, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1786, Total reward=67.15, Steps=59186, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1787, Total reward=25.68, Steps=59223, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1788, Total reward=65.67, Steps=59264, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1789, Total reward=11.04, Steps=59278, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1790, Total reward=50.82, Steps=59317, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1791, Total reward=30.83, Steps=59344, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1792, Total reward=60.55, Steps=59379, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1793, Total reward=51.66, Steps=59407, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1794, Total reward=39.32, Steps=59429, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1795, Total reward=29.54, Steps=59457, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1796, Total reward=30.45, Steps=59485, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1797, Total reward=46.41, Steps=59533, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1798, Total reward=37.43, Steps=59575, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1799, Total reward=8.1, Steps=59611, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1800, Total reward=80.47, Steps=59652, Training iteration=35
Policy training> Surrogate loss=0.012906539253890514, KL divergence=0.00013403645425569266, Entropy=0.3577454090118408, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03848552331328392, KL divergence=0.0070036775432527065, Entropy=0.3461294174194336, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03996717929840088, KL divergence=0.021520724520087242, Entropy=0.3381107747554779, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0537395216524601, KL divergence=0.039161551743745804, Entropy=0.3381960391998291, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06493262946605682, KL divergence=0.05816129967570305, Entropy=0.3278760612010956, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0697106122970581, KL divergence=0.07004164159297943, Entropy=0.32914838194847107, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.047498177736997604, KL divergence=0.08343357592821121, Entropy=0.32417914271354675, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.053650423884391785, KL divergence=0.0983501672744751, Entropy=0.31913328170776367, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05885469540953636, KL divergence=0.11061066389083862, Entropy=0.32402047514915466, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07728756964206696, KL divergence=0.12172345072031021, Entropy=0.32340121269226074, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/328_Step-59652.ckpt']
Uploaded 3 files for checkpoint 328 in 0.58 seconds
saved intermediate frozen graph: current/model/model_328.pb
Best checkpoint number: 314, Last checkpoint number: 326
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'325'}
Training> Name=main_level/agent, Worker=0, Episode=1801, Total reward=62.8, Steps=59683, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1802, Total reward=32.21, Steps=59707, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1803, Total reward=19.61, Steps=59735, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1804, Total reward=72.58, Steps=59806, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1805, Total reward=20.16, Steps=59845, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1806, Total reward=61.22, Steps=59881, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1807, Total reward=43.63, Steps=59918, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1808, Total reward=55.15, Steps=59946, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1809, Total reward=46.13, Steps=59992, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1810, Total reward=78.52, Steps=60028, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1811, Total reward=71.69, Steps=60067, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1812, Total reward=67.9, Steps=60103, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1813, Total reward=0.02, Steps=60118, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1814, Total reward=40.78, Steps=60140, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1815, Total reward=18.76, Steps=60171, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1816, Total reward=15.05, Steps=60194, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1817, Total reward=63.88, Steps=60234, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1818, Total reward=36.98, Steps=60255, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1819, Total reward=22.92, Steps=60275, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1820, Total reward=86.39, Steps=60311, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1821, Total reward=58.18, Steps=60342, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1822, Total reward=27.34, Steps=60360, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1823, Total reward=6.37, Steps=60384, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1824, Total reward=12.78, Steps=60415, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1825, Total reward=84.4, Steps=60490, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1826, Total reward=139.3, Steps=60585, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1827, Total reward=52.74, Steps=60624, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1828, Total reward=61.82, Steps=60667, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1829, Total reward=31.5, Steps=60694, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1830, Total reward=25.81, Steps=60709, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1831, Total reward=34.86, Steps=60736, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1832, Total reward=38.91, Steps=60768, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1833, Total reward=30.96, Steps=60788, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1834, Total reward=20.75, Steps=60810, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1835, Total reward=33.93, Steps=60835, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1836, Total reward=16.27, Steps=60852, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1837, Total reward=47.9, Steps=60888, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1838, Total reward=19.8, Steps=60915, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1839, Total reward=50.52, Steps=60956, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1840, Total reward=71.56, Steps=60992, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1841, Total reward=40.54, Steps=61020, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1842, Total reward=32.45, Steps=61044, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1843, Total reward=0.02, Steps=61066, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1844, Total reward=14.77, Steps=61090, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1845, Total reward=75.51, Steps=61147, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1846, Total reward=101.42, Steps=61221, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1847, Total reward=19.36, Steps=61237, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1848, Total reward=61.62, Steps=61266, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1849, Total reward=26.61, Steps=61287, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1850, Total reward=61.3, Steps=61323, Training iteration=36
Policy training> Surrogate loss=-0.010467584244906902, KL divergence=0.00010726339678512886, Entropy=0.3278309404850006, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04187802970409393, KL divergence=0.007357984781265259, Entropy=0.3180665969848633, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.027757322415709496, KL divergence=0.023291893303394318, Entropy=0.3132873475551605, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05980181694030762, KL divergence=0.04107287526130676, Entropy=0.3028815984725952, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04939301684498787, KL divergence=0.057203471660614014, Entropy=0.3007737100124359, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05798441171646118, KL divergence=0.07278627902269363, Entropy=0.29807254672050476, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06189187988638878, KL divergence=0.08315317332744598, Entropy=0.29554224014282227, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06163549795746803, KL divergence=0.09083414077758789, Entropy=0.29682520031929016, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.055806536227464676, KL divergence=0.10089651495218277, Entropy=0.2992284297943115, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.054980456829071045, KL divergence=0.10727027803659439, Entropy=0.2994644045829773, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/329_Step-61323.ckpt']
Uploaded 3 files for checkpoint 329 in 0.54 seconds
saved intermediate frozen graph: current/model/model_329.pb
Best checkpoint number: 314, Last checkpoint number: 327
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'326'}
Training> Name=main_level/agent, Worker=0, Episode=1851, Total reward=29.43, Steps=61363, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1852, Total reward=21.41, Steps=61388, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1853, Total reward=30.21, Steps=61408, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1854, Total reward=48.05, Steps=61430, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1855, Total reward=30.98, Steps=61456, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1856, Total reward=16.03, Steps=61487, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1857, Total reward=49.24, Steps=61521, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1858, Total reward=46.18, Steps=61555, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1859, Total reward=74.97, Steps=61604, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1860, Total reward=74.76, Steps=61645, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1861, Total reward=54.06, Steps=61675, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1862, Total reward=30.96, Steps=61718, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1863, Total reward=0.01, Steps=61730, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1864, Total reward=14.83, Steps=61764, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1865, Total reward=69.03, Steps=61832, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1866, Total reward=28.66, Steps=61855, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1867, Total reward=39.59, Steps=61898, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1868, Total reward=65.08, Steps=61939, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1869, Total reward=22.59, Steps=61972, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1870, Total reward=74.51, Steps=62020, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1871, Total reward=47.31, Steps=62058, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1872, Total reward=52.38, Steps=62089, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1873, Total reward=21.55, Steps=62100, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1874, Total reward=32.13, Steps=62121, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1875, Total reward=39.95, Steps=62149, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1876, Total reward=17.02, Steps=62182, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1877, Total reward=17.31, Steps=62198, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1878, Total reward=22.4, Steps=62217, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1879, Total reward=183.97, Steps=62355, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1880, Total reward=49.67, Steps=62401, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1881, Total reward=74.06, Steps=62435, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1882, Total reward=36.9, Steps=62461, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1883, Total reward=8.97, Steps=62488, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1884, Total reward=19.65, Steps=62520, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1885, Total reward=20.31, Steps=62552, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1886, Total reward=105.16, Steps=62600, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1887, Total reward=53.74, Steps=62639, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1888, Total reward=70.59, Steps=62689, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1889, Total reward=70.33, Steps=62750, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1890, Total reward=61.81, Steps=62799, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1891, Total reward=50.36, Steps=62838, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1892, Total reward=62.49, Steps=62881, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1893, Total reward=0.02, Steps=62897, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1894, Total reward=42.59, Steps=62919, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1895, Total reward=24.76, Steps=62954, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1896, Total reward=18.29, Steps=62984, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1897, Total reward=50.59, Steps=63031, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1898, Total reward=34.21, Steps=63054, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1899, Total reward=5.41, Steps=63071, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1900, Total reward=63.93, Steps=63105, Training iteration=37
Policy training> Surrogate loss=-0.0025939394254237413, KL divergence=7.407314114971086e-05, Entropy=0.33021244406700134, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.031225495040416718, KL divergence=0.00472417613491416, Entropy=0.3295491933822632, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0292049590498209, KL divergence=0.01522789429873228, Entropy=0.31618785858154297, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04676678404211998, KL divergence=0.028778478503227234, Entropy=0.30959591269493103, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0372781902551651, KL divergence=0.04710192605853081, Entropy=0.31492355465888977, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0655033215880394, KL divergence=0.05825774371623993, Entropy=0.3061991035938263, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04913316294550896, KL divergence=0.06760318577289581, Entropy=0.3084013760089874, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.053533826023340225, KL divergence=0.08422727137804031, Entropy=0.3073394298553467, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0560142882168293, KL divergence=0.0882200077176094, Entropy=0.3005259335041046, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.04632743075489998, KL divergence=0.10157409310340881, Entropy=0.3112436830997467, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/330_Step-63105.ckpt']
Uploaded 3 files for checkpoint 330 in 0.44 seconds
saved intermediate frozen graph: current/model/model_330.pb
Best checkpoint number: 314, Last checkpoint number: 328
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'327'}
Training> Name=main_level/agent, Worker=0, Episode=1901, Total reward=34.85, Steps=63132, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1902, Total reward=107.59, Steps=63235, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1903, Total reward=3.73, Steps=63260, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1904, Total reward=11.31, Steps=63291, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1905, Total reward=9.63, Steps=63308, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1906, Total reward=65.07, Steps=63354, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1907, Total reward=63.65, Steps=63397, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1908, Total reward=57.81, Steps=63440, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1909, Total reward=31.76, Steps=63482, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1910, Total reward=74.78, Steps=63529, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1911, Total reward=55.03, Steps=63566, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1912, Total reward=14.57, Steps=63593, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1913, Total reward=27.09, Steps=63613, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1914, Total reward=38.7, Steps=63635, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1915, Total reward=19.22, Steps=63660, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1916, Total reward=27.37, Steps=63688, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1917, Total reward=29.46, Steps=63739, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1918, Total reward=34.53, Steps=63777, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1919, Total reward=13.05, Steps=63791, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1920, Total reward=64.1, Steps=63823, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1921, Total reward=57.1, Steps=63855, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1922, Total reward=28.23, Steps=63889, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1923, Total reward=35.98, Steps=63933, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1924, Total reward=48.97, Steps=63976, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1925, Total reward=51.06, Steps=64035, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1926, Total reward=98.41, Steps=64101, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1927, Total reward=69.26, Steps=64144, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1928, Total reward=70.34, Steps=64183, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1929, Total reward=20.1, Steps=64211, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1930, Total reward=70.32, Steps=64258, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1931, Total reward=33.71, Steps=64287, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1932, Total reward=35.91, Steps=64308, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1933, Total reward=58.91, Steps=64339, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1934, Total reward=45.95, Steps=64362, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1935, Total reward=12.14, Steps=64390, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1936, Total reward=21.56, Steps=64424, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1937, Total reward=54.64, Steps=64463, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1938, Total reward=24.82, Steps=64477, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1939, Total reward=93.81, Steps=64552, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1940, Total reward=73.28, Steps=64582, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1941, Total reward=38.02, Steps=64606, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1942, Total reward=42.59, Steps=64631, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1943, Total reward=7.63, Steps=64647, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1944, Total reward=17.15, Steps=64695, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1945, Total reward=79.2, Steps=64768, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1946, Total reward=58.1, Steps=64812, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1947, Total reward=40.15, Steps=64847, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1948, Total reward=40.48, Steps=64872, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1949, Total reward=22.58, Steps=64902, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1950, Total reward=57.11, Steps=64940, Training iteration=38
Policy training> Surrogate loss=-0.015933506190776825, KL divergence=0.00012555629655253142, Entropy=0.3443725109100342, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02722053788602352, KL divergence=0.007297934498637915, Entropy=0.3362698554992676, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03132197633385658, KL divergence=0.022972973063588142, Entropy=0.32487931847572327, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03909459337592125, KL divergence=0.04067857936024666, Entropy=0.33110693097114563, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05970025062561035, KL divergence=0.06155790016055107, Entropy=0.3314887583255768, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.050370678305625916, KL divergence=0.07452365756034851, Entropy=0.3168364465236664, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0657135471701622, KL divergence=0.09187803417444229, Entropy=0.32111260294914246, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.047031205147504807, KL divergence=0.10001768916845322, Entropy=0.3140425682067871, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.056529659777879715, KL divergence=0.10673429816961288, Entropy=0.3210427761077881, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05774654075503349, KL divergence=0.11373110860586166, Entropy=0.3267308175563812, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/331_Step-64940.ckpt']
Uploaded 3 files for checkpoint 331 in 0.60 seconds
saved intermediate frozen graph: current/model/model_331.pb
Best checkpoint number: 314, Last checkpoint number: 329
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'328'}
Training> Name=main_level/agent, Worker=0, Episode=1951, Total reward=27.96, Steps=64967, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1952, Total reward=7.59, Steps=64978, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1953, Total reward=43.81, Steps=64999, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1954, Total reward=49.75, Steps=65020, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1955, Total reward=12.36, Steps=65052, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1956, Total reward=19.61, Steps=65071, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1957, Total reward=6.12, Steps=65089, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1958, Total reward=32.6, Steps=65114, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1959, Total reward=68.33, Steps=65190, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1960, Total reward=65.76, Steps=65235, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1961, Total reward=59.82, Steps=65283, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1962, Total reward=33.46, Steps=65304, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1963, Total reward=6.84, Steps=65342, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1964, Total reward=102.57, Steps=65419, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1965, Total reward=43.42, Steps=65477, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1966, Total reward=110.72, Steps=65551, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1967, Total reward=43.37, Steps=65589, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1968, Total reward=69.74, Steps=65629, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1969, Total reward=82.95, Steps=65687, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1970, Total reward=22.02, Steps=65708, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1971, Total reward=49.83, Steps=65737, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1972, Total reward=0.01, Steps=65748, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1973, Total reward=48.81, Steps=65774, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1974, Total reward=26.44, Steps=65785, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1975, Total reward=29.36, Steps=65816, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1976, Total reward=17.16, Steps=65850, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1977, Total reward=31.65, Steps=65883, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1978, Total reward=31.62, Steps=65909, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1979, Total reward=25.64, Steps=65931, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1980, Total reward=18.89, Steps=65944, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1981, Total reward=43.39, Steps=65970, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1982, Total reward=39.38, Steps=65999, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1983, Total reward=3.09, Steps=66024, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1984, Total reward=12.83, Steps=66051, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1985, Total reward=16.45, Steps=66077, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1986, Total reward=53.75, Steps=66134, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1987, Total reward=98.18, Steps=66232, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1988, Total reward=65.86, Steps=66261, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1989, Total reward=42.71, Steps=66299, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1990, Total reward=94.99, Steps=66333, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1991, Total reward=51.55, Steps=66374, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1992, Total reward=65.57, Steps=66416, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1993, Total reward=37.84, Steps=66437, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1994, Total reward=54.55, Steps=66460, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1995, Total reward=17.27, Steps=66492, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1996, Total reward=16.94, Steps=66511, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1997, Total reward=81.97, Steps=66595, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1998, Total reward=30.65, Steps=66621, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1999, Total reward=52.79, Steps=66672, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=2000, Total reward=80.17, Steps=66715, Training iteration=39
Policy training> Surrogate loss=-0.0008218772709369659, KL divergence=8.713288116268814e-05, Entropy=0.3337130844593048, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.019522814080119133, KL divergence=0.005364326760172844, Entropy=0.3341463804244995, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.052350059151649475, KL divergence=0.020348163321614265, Entropy=0.32253536581993103, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05007913336157799, KL divergence=0.03419046476483345, Entropy=0.31764504313468933, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.044984739273786545, KL divergence=0.05433553457260132, Entropy=0.31316623091697693, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04418747499585152, KL divergence=0.0670706108212471, Entropy=0.3007771074771881, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05178748071193695, KL divergence=0.07435782253742218, Entropy=0.3060876429080963, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.04830464720726013, KL divergence=0.0890171229839325, Entropy=0.30520308017730713, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06419022381305695, KL divergence=0.09762882441282272, Entropy=0.3032960891723633, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.059474509209394455, KL divergence=0.10407481342554092, Entropy=0.3023897111415863, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/332_Step-66715.ckpt']
Uploaded 3 files for checkpoint 332 in 0.52 seconds
saved intermediate frozen graph: current/model/model_332.pb
Best checkpoint number: 314, Last checkpoint number: 330
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'329'}
Training> Name=main_level/agent, Worker=0, Episode=2001, Total reward=64.46, Steps=66771, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2002, Total reward=47.9, Steps=66797, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2003, Total reward=3.4, Steps=66816, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2004, Total reward=15.31, Steps=66848, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2005, Total reward=10.04, Steps=66871, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2006, Total reward=61.37, Steps=66915, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2007, Total reward=77.16, Steps=66957, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2008, Total reward=54.66, Steps=67013, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2009, Total reward=23.26, Steps=67042, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2010, Total reward=77.96, Steps=67080, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2011, Total reward=67.65, Steps=67123, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2012, Total reward=39.29, Steps=67155, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2013, Total reward=21.32, Steps=67166, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2014, Total reward=39.33, Steps=67188, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2015, Total reward=17.08, Steps=67203, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2016, Total reward=23.18, Steps=67229, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2017, Total reward=53.31, Steps=67260, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2018, Total reward=33.0, Steps=67283, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2019, Total reward=16.41, Steps=67305, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2020, Total reward=98.01, Steps=67367, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2021, Total reward=66.52, Steps=67397, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2022, Total reward=33.57, Steps=67440, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2023, Total reward=15.92, Steps=67497, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2024, Total reward=19.32, Steps=67541, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2025, Total reward=76.98, Steps=67632, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2026, Total reward=195.18, Steps=67731, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2027, Total reward=63.18, Steps=67772, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2028, Total reward=60.83, Steps=67814, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2029, Total reward=32.86, Steps=67842, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2030, Total reward=40.68, Steps=67876, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2031, Total reward=34.78, Steps=67904, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2032, Total reward=57.24, Steps=67946, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2033, Total reward=34.75, Steps=67966, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2034, Total reward=34.62, Steps=67988, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2035, Total reward=13.05, Steps=68020, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2036, Total reward=13.21, Steps=68037, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2037, Total reward=28.71, Steps=68076, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2038, Total reward=65.91, Steps=68144, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2039, Total reward=62.24, Steps=68194, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2040, Total reward=89.38, Steps=68229, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2041, Total reward=50.44, Steps=68257, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2042, Total reward=27.07, Steps=68276, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2043, Total reward=0.02, Steps=68299, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2044, Total reward=85.78, Steps=68372, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2045, Total reward=59.09, Steps=68434, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2046, Total reward=47.28, Steps=68458, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2047, Total reward=3.35, Steps=68470, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2048, Total reward=23.83, Steps=68485, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2049, Total reward=38.43, Steps=68535, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2050, Total reward=42.27, Steps=68582, Training iteration=40
Policy training> Surrogate loss=0.003974630031734705, KL divergence=8.295848238049075e-05, Entropy=0.3343857228755951, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.00014250166714191437, KL divergence=0.006867628078907728, Entropy=0.3269250690937042, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.014586411416530609, KL divergence=0.02185366116464138, Entropy=0.32488879561424255, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.014757801778614521, KL divergence=0.03856373950839043, Entropy=0.3326181471347809, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.045962464064359665, KL divergence=0.0530809722840786, Entropy=0.3146098554134369, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04587632790207863, KL divergence=0.06404935568571091, Entropy=0.32473501563072205, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04848337545990944, KL divergence=0.08331233263015747, Entropy=0.31620240211486816, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0388108529150486, KL divergence=0.09083805233240128, Entropy=0.32061102986335754, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05175270140171051, KL divergence=0.102610282599926, Entropy=0.3200511038303375, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.047230690717697144, KL divergence=0.1184159591794014, Entropy=0.3180006742477417, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/333_Step-68582.ckpt']
Uploaded 3 files for checkpoint 333 in 0.79 seconds
saved intermediate frozen graph: current/model/model_333.pb
Best checkpoint number: 314, Last checkpoint number: 331
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'330'}
Training> Name=main_level/agent, Worker=0, Episode=2051, Total reward=58.01, Steps=68621, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2052, Total reward=53.79, Steps=68652, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2053, Total reward=42.26, Steps=68682, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2054, Total reward=42.9, Steps=68703, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2055, Total reward=22.92, Steps=68738, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2056, Total reward=15.78, Steps=68757, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2057, Total reward=23.17, Steps=68785, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2058, Total reward=38.13, Steps=68820, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2059, Total reward=31.05, Steps=68847, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2060, Total reward=81.33, Steps=68886, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2061, Total reward=68.65, Steps=68917, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2062, Total reward=44.69, Steps=68941, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2063, Total reward=18.83, Steps=68976, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2064, Total reward=63.28, Steps=69052, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2065, Total reward=31.92, Steps=69110, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2066, Total reward=79.5, Steps=69158, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2067, Total reward=43.35, Steps=69194, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2068, Total reward=58.78, Steps=69222, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2069, Total reward=23.49, Steps=69243, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2070, Total reward=71.17, Steps=69295, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2071, Total reward=51.5, Steps=69321, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2072, Total reward=64.59, Steps=69354, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2073, Total reward=47.44, Steps=69382, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2074, Total reward=40.04, Steps=69403, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2075, Total reward=12.85, Steps=69428, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2076, Total reward=20.97, Steps=69454, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2077, Total reward=29.04, Steps=69472, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2078, Total reward=29.84, Steps=69496, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2079, Total reward=69.94, Steps=69541, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2080, Total reward=88.0, Steps=69579, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2081, Total reward=64.38, Steps=69610, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2082, Total reward=35.86, Steps=69630, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2083, Total reward=67.48, Steps=69728, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2084, Total reward=3.33, Steps=69757, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2085, Total reward=92.81, Steps=69825, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2086, Total reward=83.81, Steps=69888, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2087, Total reward=72.01, Steps=69944, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2088, Total reward=66.0, Steps=69999, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2089, Total reward=26.4, Steps=70040, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2090, Total reward=65.73, Steps=70075, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2091, Total reward=77.97, Steps=70129, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2092, Total reward=62.94, Steps=70160, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2093, Total reward=52.08, Steps=70189, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2094, Total reward=38.9, Steps=70211, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2095, Total reward=12.6, Steps=70238, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2096, Total reward=19.06, Steps=70273, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2097, Total reward=50.05, Steps=70335, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2098, Total reward=34.22, Steps=70348, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2099, Total reward=63.21, Steps=70397, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2100, Total reward=50.08, Steps=70432, Training iteration=41
Policy training> Surrogate loss=-0.006898572668433189, KL divergence=0.00018146047659683973, Entropy=0.3413297235965729, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.004482624586671591, KL divergence=0.006913271266967058, Entropy=0.3234562575817108, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.013348464854061604, KL divergence=0.020367467775940895, Entropy=0.324039489030838, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04147063568234444, KL divergence=0.03804265335202217, Entropy=0.3189985156059265, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06410057842731476, KL divergence=0.057037126272916794, Entropy=0.3155207335948944, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06608986854553223, KL divergence=0.07118623703718185, Entropy=0.3123209774494171, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05575157329440117, KL divergence=0.07746898382902145, Entropy=0.31282222270965576, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05209017172455788, KL divergence=0.09386258572340012, Entropy=0.31106916069984436, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05914689227938652, KL divergence=0.09640327841043472, Entropy=0.3118791878223419, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.049957096576690674, KL divergence=0.10298440605401993, Entropy=0.30485737323760986, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/334_Step-70432.ckpt']
Uploaded 3 files for checkpoint 334 in 0.59 seconds
saved intermediate frozen graph: current/model/model_334.pb
Best checkpoint number: 314, Last checkpoint number: 332
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'331'}
Training> Name=main_level/agent, Worker=0, Episode=2101, Total reward=57.86, Steps=70478, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2102, Total reward=39.57, Steps=70502, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2103, Total reward=16.47, Steps=70544, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2104, Total reward=24.91, Steps=70590, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2105, Total reward=10.02, Steps=70614, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2106, Total reward=88.44, Steps=70662, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2107, Total reward=74.48, Steps=70710, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2108, Total reward=40.35, Steps=70734, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2109, Total reward=23.62, Steps=70764, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2110, Total reward=67.69, Steps=70809, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2111, Total reward=57.48, Steps=70837, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2112, Total reward=62.7, Steps=70877, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2113, Total reward=44.0, Steps=70908, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2114, Total reward=31.57, Steps=70920, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2115, Total reward=4.4, Steps=70942, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2116, Total reward=11.98, Steps=70962, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2117, Total reward=33.31, Steps=70988, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2118, Total reward=42.32, Steps=71037, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2119, Total reward=67.27, Steps=71101, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2120, Total reward=85.13, Steps=71138, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2121, Total reward=59.11, Steps=71167, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2122, Total reward=32.5, Steps=71187, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2123, Total reward=2.9, Steps=71202, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2124, Total reward=71.72, Steps=71270, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2125, Total reward=71.59, Steps=71338, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2126, Total reward=68.86, Steps=71383, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2127, Total reward=13.39, Steps=71397, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2128, Total reward=68.67, Steps=71437, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2129, Total reward=16.63, Steps=71452, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2130, Total reward=66.93, Steps=71487, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2131, Total reward=60.64, Steps=71524, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2132, Total reward=39.48, Steps=71554, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2133, Total reward=45.12, Steps=71574, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2134, Total reward=50.67, Steps=71595, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2135, Total reward=40.12, Steps=71643, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2136, Total reward=16.85, Steps=71661, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2137, Total reward=48.24, Steps=71696, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2138, Total reward=94.17, Steps=71763, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2139, Total reward=19.4, Steps=71787, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2140, Total reward=82.35, Steps=71827, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2141, Total reward=53.96, Steps=71856, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2142, Total reward=42.83, Steps=71880, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2143, Total reward=12.1, Steps=71924, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2144, Total reward=11.31, Steps=71949, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2145, Total reward=32.94, Steps=72003, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2146, Total reward=80.45, Steps=72053, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2147, Total reward=37.78, Steps=72088, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2148, Total reward=119.67, Steps=72163, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2149, Total reward=25.93, Steps=72191, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2150, Total reward=105.36, Steps=72235, Training iteration=42
Policy training> Surrogate loss=0.007281934842467308, KL divergence=0.00015226457617245615, Entropy=0.36160239577293396, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.026489662006497383, KL divergence=0.007496268022805452, Entropy=0.3622204065322876, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05215750262141228, KL divergence=0.02309313230216503, Entropy=0.3592729866504669, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04791310429573059, KL divergence=0.04107586666941643, Entropy=0.34276604652404785, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05047966167330742, KL divergence=0.05831817165017128, Entropy=0.3454197347164154, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04592758044600487, KL divergence=0.07553521543741226, Entropy=0.3531999886035919, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06990070641040802, KL divergence=0.08941650390625, Entropy=0.34318193793296814, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07267988473176956, KL divergence=0.10012882947921753, Entropy=0.3475430905818939, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04236514866352081, KL divergence=0.10634950548410416, Entropy=0.34606456756591797, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07029455155134201, KL divergence=0.11677403002977371, Entropy=0.3501282036304474, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/335_Step-72235.ckpt']
Uploaded 3 files for checkpoint 335 in 0.56 seconds
saved intermediate frozen graph: current/model/model_335.pb
Best checkpoint number: 314, Last checkpoint number: 333
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'332'}
Training> Name=main_level/agent, Worker=0, Episode=2151, Total reward=22.51, Steps=72268, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2152, Total reward=61.84, Steps=72301, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2153, Total reward=38.54, Steps=72321, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2154, Total reward=20.36, Steps=72341, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2155, Total reward=20.58, Steps=72365, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2156, Total reward=21.23, Steps=72382, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2157, Total reward=54.89, Steps=72436, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2158, Total reward=39.99, Steps=72459, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2159, Total reward=75.85, Steps=72520, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2160, Total reward=78.18, Steps=72584, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2161, Total reward=53.47, Steps=72625, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2162, Total reward=31.03, Steps=72653, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2163, Total reward=13.59, Steps=72693, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2164, Total reward=56.13, Steps=72756, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2165, Total reward=14.68, Steps=72792, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2166, Total reward=13.76, Steps=72813, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2167, Total reward=60.05, Steps=72857, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2168, Total reward=61.17, Steps=72886, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2169, Total reward=29.16, Steps=72906, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2170, Total reward=57.31, Steps=72956, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2171, Total reward=64.13, Steps=72993, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2172, Total reward=51.9, Steps=73026, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2173, Total reward=44.73, Steps=73057, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2174, Total reward=22.65, Steps=73068, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2175, Total reward=24.9, Steps=73099, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2176, Total reward=14.0, Steps=73133, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2177, Total reward=52.68, Steps=73170, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2178, Total reward=92.45, Steps=73251, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2179, Total reward=13.01, Steps=73265, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2180, Total reward=79.89, Steps=73300, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2181, Total reward=54.01, Steps=73330, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2182, Total reward=36.27, Steps=73373, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2183, Total reward=6.35, Steps=73395, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2184, Total reward=22.58, Steps=73417, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2185, Total reward=16.4, Steps=73438, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2186, Total reward=137.28, Steps=73529, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2187, Total reward=60.53, Steps=73571, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2188, Total reward=54.59, Steps=73600, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2189, Total reward=31.39, Steps=73635, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2190, Total reward=88.87, Steps=73671, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2191, Total reward=27.05, Steps=73698, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2192, Total reward=37.8, Steps=73727, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2193, Total reward=40.27, Steps=73755, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2194, Total reward=38.13, Steps=73776, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2195, Total reward=20.04, Steps=73805, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2196, Total reward=15.33, Steps=73825, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2197, Total reward=43.11, Steps=73864, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2198, Total reward=28.23, Steps=73887, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2199, Total reward=88.22, Steps=73937, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2200, Total reward=93.83, Steps=73977, Training iteration=43
Policy training> Surrogate loss=-0.000710057734977454, KL divergence=9.651778964325786e-05, Entropy=0.3567221164703369, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03336768224835396, KL divergence=0.006232126150280237, Entropy=0.3552679717540741, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.044925522059202194, KL divergence=0.02198927104473114, Entropy=0.35027948021888733, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03424215689301491, KL divergence=0.03621266782283783, Entropy=0.33379051089286804, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04741942882537842, KL divergence=0.0517263300716877, Entropy=0.3368666172027588, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.057878654450178146, KL divergence=0.06786421686410904, Entropy=0.32816144824028015, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06969454884529114, KL divergence=0.08225520700216293, Entropy=0.33242592215538025, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.058704718947410583, KL divergence=0.0909670814871788, Entropy=0.3249673545360565, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06740478426218033, KL divergence=0.10037145763635635, Entropy=0.3289203643798828, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07041782885789871, KL divergence=0.10739509016275406, Entropy=0.32390880584716797, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/336_Step-73977.ckpt']
Uploaded 3 files for checkpoint 336 in 0.56 seconds
saved intermediate frozen graph: current/model/model_336.pb
Best checkpoint number: 314, Last checkpoint number: 334
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'333'}
Training> Name=main_level/agent, Worker=0, Episode=2201, Total reward=48.64, Steps=74006, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2202, Total reward=41.16, Steps=74041, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2203, Total reward=5.94, Steps=74067, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2204, Total reward=14.53, Steps=74098, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2205, Total reward=77.12, Steps=74155, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2206, Total reward=112.56, Steps=74253, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2207, Total reward=20.59, Steps=74277, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2208, Total reward=55.34, Steps=74305, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2209, Total reward=24.07, Steps=74338, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2210, Total reward=36.73, Steps=74366, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2211, Total reward=27.99, Steps=74389, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2212, Total reward=9.48, Steps=74399, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2213, Total reward=37.17, Steps=74426, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2214, Total reward=31.66, Steps=74438, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2215, Total reward=34.07, Steps=74466, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2216, Total reward=24.81, Steps=74485, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2217, Total reward=71.69, Steps=74558, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2218, Total reward=89.32, Steps=74619, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2219, Total reward=74.26, Steps=74665, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2220, Total reward=56.28, Steps=74695, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2221, Total reward=60.73, Steps=74729, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2222, Total reward=49.25, Steps=74783, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2223, Total reward=0.02, Steps=74803, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2224, Total reward=8.47, Steps=74832, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2225, Total reward=6.98, Steps=74853, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2226, Total reward=105.64, Steps=74912, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2227, Total reward=15.69, Steps=74945, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2228, Total reward=58.42, Steps=74987, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2229, Total reward=54.55, Steps=75051, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2230, Total reward=58.45, Steps=75090, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2231, Total reward=19.14, Steps=75103, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2232, Total reward=40.79, Steps=75131, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2233, Total reward=44.94, Steps=75160, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2234, Total reward=28.93, Steps=75181, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2235, Total reward=15.39, Steps=75213, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2236, Total reward=27.47, Steps=75244, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2237, Total reward=43.61, Steps=75278, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2238, Total reward=25.88, Steps=75298, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2239, Total reward=72.0, Steps=75349, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2240, Total reward=42.89, Steps=75383, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2241, Total reward=61.39, Steps=75413, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2242, Total reward=27.65, Steps=75431, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2243, Total reward=6.94, Steps=75451, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2244, Total reward=59.98, Steps=75542, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2245, Total reward=16.78, Steps=75566, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2246, Total reward=29.43, Steps=75604, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2247, Total reward=57.72, Steps=75675, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2248, Total reward=55.86, Steps=75704, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2249, Total reward=20.23, Steps=75723, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2250, Total reward=80.73, Steps=75767, Training iteration=44
Policy training> Surrogate loss=-0.011571615934371948, KL divergence=0.00018805247964337468, Entropy=0.38840675354003906, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04403572902083397, KL divergence=0.0059584216214716434, Entropy=0.3850516974925995, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03247185796499252, KL divergence=0.019858187064528465, Entropy=0.37246274948120117, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.045186977833509445, KL divergence=0.03761354088783264, Entropy=0.36444321274757385, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06995094567537308, KL divergence=0.054950978606939316, Entropy=0.35876157879829407, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05905485153198242, KL divergence=0.06688833236694336, Entropy=0.35107216238975525, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06212274730205536, KL divergence=0.07964814454317093, Entropy=0.36353597044944763, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05301604047417641, KL divergence=0.09508480876684189, Entropy=0.35343456268310547, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05694268271327019, KL divergence=0.10247131437063217, Entropy=0.3567543029785156, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0753975510597229, KL divergence=0.10922909528017044, Entropy=0.3538466989994049, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/337_Step-75767.ckpt']
Uploaded 3 files for checkpoint 337 in 0.60 seconds
saved intermediate frozen graph: current/model/model_337.pb
Best checkpoint number: 314, Last checkpoint number: 335
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'334'}
Training> Name=main_level/agent, Worker=0, Episode=2251, Total reward=12.28, Steps=75794, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2252, Total reward=61.41, Steps=75837, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2253, Total reward=48.7, Steps=75867, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2254, Total reward=38.85, Steps=75889, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2255, Total reward=21.69, Steps=75917, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2256, Total reward=23.6, Steps=75941, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2257, Total reward=113.4, Steps=76015, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2258, Total reward=34.26, Steps=76057, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2259, Total reward=15.4, Steps=76094, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2260, Total reward=73.01, Steps=76137, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2261, Total reward=33.15, Steps=76160, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2262, Total reward=31.93, Steps=76184, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2263, Total reward=15.77, Steps=76236, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2264, Total reward=83.23, Steps=76312, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2265, Total reward=75.49, Steps=76387, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2266, Total reward=81.09, Steps=76458, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2267, Total reward=33.34, Steps=76497, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2268, Total reward=61.38, Steps=76536, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2269, Total reward=63.44, Steps=76582, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2270, Total reward=63.9, Steps=76616, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2271, Total reward=48.84, Steps=76655, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2272, Total reward=41.61, Steps=76684, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2273, Total reward=42.77, Steps=76715, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2274, Total reward=31.74, Steps=76735, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2275, Total reward=19.05, Steps=76782, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2276, Total reward=18.44, Steps=76800, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2277, Total reward=5.72, Steps=76811, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2278, Total reward=32.45, Steps=76832, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2279, Total reward=24.47, Steps=76866, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2280, Total reward=34.54, Steps=76902, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2281, Total reward=68.41, Steps=76934, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2282, Total reward=54.19, Steps=76960, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2283, Total reward=2.92, Steps=76987, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2284, Total reward=0.02, Steps=77003, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2285, Total reward=80.34, Steps=77064, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2286, Total reward=97.11, Steps=77123, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2287, Total reward=64.06, Steps=77165, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2288, Total reward=48.8, Steps=77190, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2289, Total reward=23.23, Steps=77217, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2290, Total reward=36.73, Steps=77253, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2291, Total reward=40.57, Steps=77279, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2292, Total reward=53.86, Steps=77312, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2293, Total reward=38.56, Steps=77342, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2294, Total reward=33.67, Steps=77364, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2295, Total reward=21.18, Steps=77392, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2296, Total reward=18.8, Steps=77413, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2297, Total reward=50.29, Steps=77444, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2298, Total reward=29.81, Steps=77469, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2299, Total reward=65.08, Steps=77514, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2300, Total reward=91.54, Steps=77551, Training iteration=45
Policy training> Surrogate loss=0.0005931705236434937, KL divergence=0.00018310298037249595, Entropy=0.36685237288475037, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.011560071259737015, KL divergence=0.006996972020715475, Entropy=0.36084306240081787, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05925638601183891, KL divergence=0.018807604908943176, Entropy=0.35705113410949707, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04854211583733559, KL divergence=0.03325639292597771, Entropy=0.35504385828971863, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.03618155047297478, KL divergence=0.04827529191970825, Entropy=0.3508504331111908, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.053663235157728195, KL divergence=0.06336117535829544, Entropy=0.34686920046806335, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04734319821000099, KL divergence=0.07740192860364914, Entropy=0.34161481261253357, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05052779242396355, KL divergence=0.09055586904287338, Entropy=0.3383721113204956, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.044639796018600464, KL divergence=0.10122406482696533, Entropy=0.3430233895778656, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06483122706413269, KL divergence=0.1048811748623848, Entropy=0.3476440906524658, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/338_Step-77551.ckpt']
Uploaded 3 files for checkpoint 338 in 0.54 seconds
saved intermediate frozen graph: current/model/model_338.pb
Best checkpoint number: 314, Last checkpoint number: 336
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'335'}
Training> Name=main_level/agent, Worker=0, Episode=2301, Total reward=59.54, Steps=77581, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2302, Total reward=32.58, Steps=77600, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2303, Total reward=10.26, Steps=77636, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2304, Total reward=66.44, Steps=77734, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2305, Total reward=13.45, Steps=77759, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2306, Total reward=67.9, Steps=77804, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2307, Total reward=72.31, Steps=77844, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2308, Total reward=64.11, Steps=77873, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2309, Total reward=10.35, Steps=77886, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2310, Total reward=104.45, Steps=77941, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2311, Total reward=18.7, Steps=77957, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2312, Total reward=58.35, Steps=77988, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2313, Total reward=55.06, Steps=78019, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2314, Total reward=30.23, Steps=78039, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2315, Total reward=30.69, Steps=78068, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2316, Total reward=20.9, Steps=78102, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2317, Total reward=51.75, Steps=78138, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2318, Total reward=36.15, Steps=78162, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2319, Total reward=80.87, Steps=78224, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2320, Total reward=94.86, Steps=78272, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2321, Total reward=54.71, Steps=78302, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2322, Total reward=31.22, Steps=78320, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2323, Total reward=0.02, Steps=78344, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2324, Total reward=65.7, Steps=78418, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2325, Total reward=13.06, Steps=78443, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2326, Total reward=18.45, Steps=78459, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2327, Total reward=30.57, Steps=78497, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2328, Total reward=60.42, Steps=78526, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2329, Total reward=57.71, Steps=78572, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2330, Total reward=30.82, Steps=78611, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2331, Total reward=20.31, Steps=78638, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2332, Total reward=60.55, Steps=78670, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2333, Total reward=52.48, Steps=78701, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2334, Total reward=39.26, Steps=78722, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2335, Total reward=18.87, Steps=78750, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2336, Total reward=26.19, Steps=78769, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2337, Total reward=61.89, Steps=78817, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2338, Total reward=36.88, Steps=78842, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2339, Total reward=84.32, Steps=78894, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2340, Total reward=51.48, Steps=78929, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2341, Total reward=34.65, Steps=78954, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2342, Total reward=37.4, Steps=78988, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2343, Total reward=21.25, Steps=79024, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2344, Total reward=14.92, Steps=79057, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2345, Total reward=78.03, Steps=79123, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2346, Total reward=74.66, Steps=79191, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2347, Total reward=94.85, Steps=79272, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2348, Total reward=39.78, Steps=79298, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2349, Total reward=28.75, Steps=79318, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2350, Total reward=29.31, Steps=79343, Training iteration=46
Policy training> Surrogate loss=-0.011149667203426361, KL divergence=9.665440302342176e-05, Entropy=0.3713551461696625, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.028559153899550438, KL divergence=0.006687674671411514, Entropy=0.37186750769615173, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05189957097172737, KL divergence=0.021425241604447365, Entropy=0.371356725692749, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.042105454951524734, KL divergence=0.03800521790981293, Entropy=0.3675342798233032, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.055261075496673584, KL divergence=0.05552681162953377, Entropy=0.3617289364337921, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06197280064225197, KL divergence=0.07160308957099915, Entropy=0.35505595803260803, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06007745862007141, KL divergence=0.08154240995645523, Entropy=0.3506488800048828, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06478884071111679, KL divergence=0.09170347452163696, Entropy=0.34945330023765564, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05474547669291496, KL divergence=0.10438430309295654, Entropy=0.3560148775577545, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07458450645208359, KL divergence=0.1046290472149849, Entropy=0.35298582911491394, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/339_Step-79343.ckpt']
Uploaded 3 files for checkpoint 339 in 0.53 seconds
saved intermediate frozen graph: current/model/model_339.pb
Best checkpoint number: 314, Last checkpoint number: 337
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'336'}
Training> Name=main_level/agent, Worker=0, Episode=2351, Total reward=46.91, Steps=79379, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2352, Total reward=33.28, Steps=79408, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2353, Total reward=44.7, Steps=79436, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2354, Total reward=34.8, Steps=79457, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2355, Total reward=19.12, Steps=79486, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2356, Total reward=28.47, Steps=79515, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2357, Total reward=117.38, Steps=79599, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2358, Total reward=91.48, Steps=79677, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2359, Total reward=15.18, Steps=79699, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2360, Total reward=94.26, Steps=79735, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2361, Total reward=47.17, Steps=79760, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2362, Total reward=29.77, Steps=79779, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2363, Total reward=30.51, Steps=79820, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2364, Total reward=9.7, Steps=79849, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2365, Total reward=32.24, Steps=79877, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2366, Total reward=84.26, Steps=79945, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2367, Total reward=44.19, Steps=79984, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2368, Total reward=58.6, Steps=80025, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2369, Total reward=25.06, Steps=80049, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2370, Total reward=70.16, Steps=80094, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2371, Total reward=67.28, Steps=80123, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2372, Total reward=51.31, Steps=80154, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2373, Total reward=46.78, Steps=80183, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2374, Total reward=41.35, Steps=80203, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2375, Total reward=19.97, Steps=80233, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2376, Total reward=20.97, Steps=80262, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2377, Total reward=40.85, Steps=80299, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2378, Total reward=35.58, Steps=80323, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2379, Total reward=20.35, Steps=80343, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2380, Total reward=61.58, Steps=80378, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2381, Total reward=54.63, Steps=80408, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2382, Total reward=32.09, Steps=80426, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2383, Total reward=14.17, Steps=80460, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2384, Total reward=16.76, Steps=80488, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2385, Total reward=80.08, Steps=80542, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2386, Total reward=74.99, Steps=80596, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2387, Total reward=61.08, Steps=80634, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2388, Total reward=42.18, Steps=80660, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2389, Total reward=29.56, Steps=80693, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2390, Total reward=25.01, Steps=80710, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2391, Total reward=51.62, Steps=80748, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2392, Total reward=45.69, Steps=80777, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2393, Total reward=34.5, Steps=80803, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2394, Total reward=37.93, Steps=80825, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2395, Total reward=20.34, Steps=80853, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2396, Total reward=16.48, Steps=80868, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2397, Total reward=54.1, Steps=80906, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2398, Total reward=31.17, Steps=80927, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2399, Total reward=93.95, Steps=81026, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2400, Total reward=58.0, Steps=81069, Training iteration=47
Policy training> Surrogate loss=0.004897352773696184, KL divergence=7.91788988863118e-05, Entropy=0.358349472284317, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.032145339995622635, KL divergence=0.006544454488903284, Entropy=0.35570958256721497, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0353265218436718, KL divergence=0.018432417884469032, Entropy=0.3417871296405792, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05019759014248848, KL divergence=0.036405112594366074, Entropy=0.3393537998199463, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04355473816394806, KL divergence=0.05424878001213074, Entropy=0.33807241916656494, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04825209453701973, KL divergence=0.06788109242916107, Entropy=0.327637642621994, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04535425081849098, KL divergence=0.07818663865327835, Entropy=0.3253314197063446, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06796091794967651, KL divergence=0.09078377485275269, Entropy=0.32641714811325073, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06120524927973747, KL divergence=0.0982949510216713, Entropy=0.32404860854148865, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06693338602781296, KL divergence=0.10302597284317017, Entropy=0.3246161639690399, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/340_Step-81069.ckpt']
Uploaded 3 files for checkpoint 340 in 0.61 seconds
saved intermediate frozen graph: current/model/model_340.pb
Best checkpoint number: 314, Last checkpoint number: 338
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'337'}
Training> Name=main_level/agent, Worker=0, Episode=2401, Total reward=47.05, Steps=81097, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2402, Total reward=31.69, Steps=81117, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2403, Total reward=6.51, Steps=81145, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2404, Total reward=5.86, Steps=81161, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2405, Total reward=16.23, Steps=81203, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2406, Total reward=67.14, Steps=81253, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2407, Total reward=43.6, Steps=81290, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2408, Total reward=52.62, Steps=81331, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2409, Total reward=77.46, Steps=81390, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2410, Total reward=37.21, Steps=81425, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2411, Total reward=75.94, Steps=81465, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2412, Total reward=40.31, Steps=81505, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2413, Total reward=40.89, Steps=81528, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2414, Total reward=41.88, Steps=81550, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2415, Total reward=13.81, Steps=81578, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2416, Total reward=25.59, Steps=81604, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2417, Total reward=45.65, Steps=81665, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2418, Total reward=42.18, Steps=81692, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2419, Total reward=95.91, Steps=81751, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2420, Total reward=82.54, Steps=81791, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2421, Total reward=42.5, Steps=81818, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2422, Total reward=35.53, Steps=81845, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2423, Total reward=3.21, Steps=81869, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2424, Total reward=24.72, Steps=81933, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2425, Total reward=16.85, Steps=81956, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2426, Total reward=14.68, Steps=81986, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2427, Total reward=73.78, Steps=82040, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2428, Total reward=73.71, Steps=82095, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2429, Total reward=65.76, Steps=82141, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2430, Total reward=60.91, Steps=82176, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2431, Total reward=18.29, Steps=82208, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2432, Total reward=61.21, Steps=82248, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2433, Total reward=32.22, Steps=82270, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2434, Total reward=28.43, Steps=82291, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2435, Total reward=24.65, Steps=82306, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2436, Total reward=30.8, Steps=82338, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2437, Total reward=44.34, Steps=82372, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2438, Total reward=23.32, Steps=82385, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2439, Total reward=12.83, Steps=82404, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2440, Total reward=85.42, Steps=82440, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2441, Total reward=56.31, Steps=82473, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2442, Total reward=36.68, Steps=82516, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2443, Total reward=13.0, Steps=82556, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2444, Total reward=9.24, Steps=82587, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2445, Total reward=19.89, Steps=82613, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2446, Total reward=99.04, Steps=82681, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2447, Total reward=56.71, Steps=82721, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2448, Total reward=56.87, Steps=82752, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2449, Total reward=40.4, Steps=82799, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2450, Total reward=42.47, Steps=82841, Training iteration=48
Policy training> Surrogate loss=-0.0007743763853795826, KL divergence=9.883029269985855e-05, Entropy=0.36611318588256836, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.014601551927626133, KL divergence=0.004807729739695787, Entropy=0.37236979603767395, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04624699428677559, KL divergence=0.01734047569334507, Entropy=0.3607460558414459, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03201671317219734, KL divergence=0.034233372658491135, Entropy=0.34838756918907166, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05478239431977272, KL divergence=0.04749341681599617, Entropy=0.35352596640586853, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05270470306277275, KL divergence=0.061274897307157516, Entropy=0.3363257944583893, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05535600706934929, KL divergence=0.0714927539229393, Entropy=0.3475090563297272, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.04482569918036461, KL divergence=0.08242562413215637, Entropy=0.3426654040813446, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.054079752415418625, KL divergence=0.09257658571004868, Entropy=0.33470869064331055, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07722706347703934, KL divergence=0.09768322855234146, Entropy=0.34065183997154236, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/341_Step-82841.ckpt']
Uploaded 3 files for checkpoint 341 in 0.57 seconds
saved intermediate frozen graph: current/model/model_341.pb
Best checkpoint number: 314, Last checkpoint number: 339
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'338'}
Training> Name=main_level/agent, Worker=0, Episode=2451, Total reward=69.29, Steps=82881, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2452, Total reward=55.89, Steps=82913, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2453, Total reward=48.81, Steps=82942, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2454, Total reward=39.53, Steps=82963, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2455, Total reward=19.56, Steps=82989, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2456, Total reward=26.81, Steps=83012, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2457, Total reward=42.38, Steps=83051, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2458, Total reward=28.74, Steps=83072, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2459, Total reward=23.44, Steps=83091, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2460, Total reward=81.93, Steps=83132, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2461, Total reward=62.75, Steps=83166, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2462, Total reward=26.15, Steps=83186, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2463, Total reward=89.02, Steps=83252, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2464, Total reward=12.87, Steps=83287, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2465, Total reward=9.99, Steps=83312, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2466, Total reward=18.34, Steps=83342, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2467, Total reward=50.71, Steps=83385, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2468, Total reward=58.96, Steps=83414, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2469, Total reward=30.63, Steps=83437, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2470, Total reward=107.25, Steps=83485, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2471, Total reward=51.39, Steps=83526, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2472, Total reward=25.41, Steps=83547, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2473, Total reward=21.42, Steps=83558, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2474, Total reward=42.58, Steps=83579, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2475, Total reward=18.91, Steps=83605, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2476, Total reward=24.47, Steps=83636, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2477, Total reward=59.17, Steps=83674, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2478, Total reward=42.29, Steps=83699, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2479, Total reward=79.71, Steps=83755, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2480, Total reward=47.55, Steps=83781, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2481, Total reward=39.03, Steps=83809, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2482, Total reward=37.03, Steps=83832, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2483, Total reward=11.2, Steps=83874, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2484, Total reward=17.65, Steps=83899, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2485, Total reward=66.24, Steps=83961, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2486, Total reward=101.89, Steps=84047, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2487, Total reward=38.24, Steps=84086, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2488, Total reward=59.03, Steps=84144, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2489, Total reward=19.89, Steps=84174, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2490, Total reward=61.58, Steps=84221, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2491, Total reward=43.44, Steps=84250, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2492, Total reward=3.76, Steps=84261, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2493, Total reward=44.51, Steps=84292, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2494, Total reward=38.85, Steps=84313, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2495, Total reward=16.34, Steps=84339, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2496, Total reward=29.18, Steps=84369, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2497, Total reward=94.78, Steps=84449, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2498, Total reward=29.93, Steps=84484, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2499, Total reward=25.73, Steps=84517, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2500, Total reward=90.02, Steps=84565, Training iteration=49
Policy training> Surrogate loss=-0.002307245507836342, KL divergence=7.372159598162398e-05, Entropy=0.35786470770835876, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.018509872257709503, KL divergence=0.004741269629448652, Entropy=0.3414627015590668, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03708511218428612, KL divergence=0.017009802162647247, Entropy=0.3380809724330902, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04068196192383766, KL divergence=0.03150712326169014, Entropy=0.34445762634277344, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.055584866553545, KL divergence=0.04502706229686737, Entropy=0.3404105603694916, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.03381047397851944, KL divergence=0.0568731315433979, Entropy=0.33341026306152344, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04799164831638336, KL divergence=0.06828496605157852, Entropy=0.33262714743614197, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.04967616870999336, KL divergence=0.07871917635202408, Entropy=0.32169094681739807, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0586540512740612, KL divergence=0.08950444310903549, Entropy=0.33217954635620117, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06061699986457825, KL divergence=0.09352550655603409, Entropy=0.3243021070957184, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/342_Step-84565.ckpt']
Uploaded 3 files for checkpoint 342 in 0.56 seconds
saved intermediate frozen graph: current/model/model_342.pb
Best checkpoint number: 314, Last checkpoint number: 340
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'339'}
Training> Name=main_level/agent, Worker=0, Episode=2501, Total reward=65.95, Steps=84594, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2502, Total reward=23.36, Steps=84610, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2503, Total reward=0.02, Steps=84633, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2504, Total reward=15.92, Steps=84676, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2505, Total reward=2.94, Steps=84685, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2506, Total reward=78.47, Steps=84730, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2507, Total reward=67.32, Steps=84783, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2508, Total reward=91.78, Steps=84862, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2509, Total reward=81.41, Steps=84905, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2510, Total reward=90.49, Steps=84950, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2511, Total reward=47.08, Steps=84979, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2512, Total reward=13.0, Steps=84991, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2513, Total reward=44.72, Steps=85021, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2514, Total reward=27.58, Steps=85032, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2515, Total reward=15.64, Steps=85065, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2516, Total reward=17.88, Steps=85082, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2517, Total reward=80.49, Steps=85161, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2518, Total reward=34.43, Steps=85199, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2519, Total reward=68.77, Steps=85256, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2520, Total reward=71.23, Steps=85304, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2521, Total reward=47.3, Steps=85331, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2522, Total reward=41.25, Steps=85356, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2523, Total reward=28.54, Steps=85397, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2524, Total reward=22.67, Steps=85444, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2525, Total reward=92.49, Steps=85503, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2526, Total reward=82.07, Steps=85580, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2527, Total reward=74.23, Steps=85619, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2528, Total reward=73.89, Steps=85660, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2529, Total reward=26.92, Steps=85686, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2530, Total reward=47.58, Steps=85735, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2531, Total reward=39.24, Steps=85763, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2532, Total reward=51.09, Steps=85794, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2533, Total reward=50.19, Steps=85823, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2534, Total reward=26.63, Steps=85833, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2535, Total reward=17.0, Steps=85880, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2536, Total reward=19.92, Steps=85896, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2537, Total reward=47.93, Steps=85932, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2538, Total reward=34.31, Steps=85973, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2539, Total reward=25.28, Steps=85993, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2540, Total reward=76.0, Steps=86042, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2541, Total reward=42.68, Steps=86069, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2542, Total reward=28.93, Steps=86088, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2543, Total reward=3.55, Steps=86106, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2544, Total reward=20.22, Steps=86131, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2545, Total reward=47.3, Steps=86194, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2546, Total reward=44.58, Steps=86241, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2547, Total reward=53.05, Steps=86282, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2548, Total reward=57.38, Steps=86340, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2549, Total reward=26.75, Steps=86360, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2550, Total reward=63.12, Steps=86392, Training iteration=50
Policy training> Surrogate loss=-0.0006671498413197696, KL divergence=0.00011279444879619405, Entropy=0.3544074296951294, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03244134038686752, KL divergence=0.0067660994827747345, Entropy=0.3541069030761719, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.030987927690148354, KL divergence=0.023230336606502533, Entropy=0.342693954706192, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04285655915737152, KL divergence=0.03527427092194557, Entropy=0.3451094925403595, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04836566746234894, KL divergence=0.05056920275092125, Entropy=0.33699676394462585, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06217283010482788, KL divergence=0.060474514961242676, Entropy=0.33922407031059265, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05213313177227974, KL divergence=0.07105907052755356, Entropy=0.3299283981323242, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06576474756002426, KL divergence=0.07338956743478775, Entropy=0.3283112943172455, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0415722019970417, KL divergence=0.0850757583975792, Entropy=0.3350270688533783, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0603724829852581, KL divergence=0.09954991936683655, Entropy=0.3365165889263153, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/343_Step-86392.ckpt']
Uploaded 3 files for checkpoint 343 in 0.65 seconds
saved intermediate frozen graph: current/model/model_343.pb
Best checkpoint number: 314, Last checkpoint number: 341
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'340'}
Training> Name=main_level/agent, Worker=0, Episode=2551, Total reward=23.62, Steps=86422, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2552, Total reward=3.74, Steps=86433, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2553, Total reward=28.48, Steps=86453, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2554, Total reward=43.07, Steps=86476, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2555, Total reward=39.19, Steps=86505, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2556, Total reward=16.01, Steps=86539, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2557, Total reward=42.15, Steps=86573, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2558, Total reward=40.12, Steps=86598, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2559, Total reward=45.21, Steps=86667, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2560, Total reward=75.48, Steps=86706, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2561, Total reward=59.59, Steps=86737, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2562, Total reward=42.74, Steps=86760, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2563, Total reward=60.09, Steps=86836, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2564, Total reward=21.54, Steps=86878, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2565, Total reward=54.29, Steps=86936, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2566, Total reward=71.55, Steps=86980, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2567, Total reward=64.22, Steps=87020, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2568, Total reward=83.78, Steps=87095, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2569, Total reward=59.74, Steps=87154, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2570, Total reward=65.37, Steps=87190, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2571, Total reward=44.13, Steps=87217, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2572, Total reward=30.15, Steps=87243, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2573, Total reward=19.82, Steps=87262, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2574, Total reward=40.02, Steps=87284, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2575, Total reward=26.34, Steps=87315, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2576, Total reward=10.22, Steps=87347, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2577, Total reward=42.54, Steps=87395, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2578, Total reward=42.57, Steps=87420, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2579, Total reward=69.72, Steps=87472, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2580, Total reward=89.11, Steps=87521, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2581, Total reward=65.18, Steps=87551, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2582, Total reward=34.6, Steps=87569, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2583, Total reward=14.78, Steps=87604, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2584, Total reward=6.39, Steps=87620, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2585, Total reward=9.65, Steps=87640, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2586, Total reward=82.03, Steps=87687, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2587, Total reward=74.37, Steps=87752, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2588, Total reward=68.57, Steps=87780, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2589, Total reward=71.09, Steps=87827, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2590, Total reward=69.38, Steps=87864, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2591, Total reward=54.47, Steps=87902, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2592, Total reward=63.88, Steps=87933, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2593, Total reward=51.96, Steps=87962, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2594, Total reward=45.31, Steps=87984, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2595, Total reward=17.86, Steps=88010, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2596, Total reward=21.9, Steps=88036, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2597, Total reward=60.25, Steps=88087, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2598, Total reward=33.73, Steps=88110, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2599, Total reward=22.9, Steps=88145, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2600, Total reward=84.14, Steps=88195, Training iteration=51
Policy training> Surrogate loss=-0.008671213872730732, KL divergence=7.636256486875936e-05, Entropy=0.37374064326286316, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0195635799318552, KL divergence=0.0052610174752771854, Entropy=0.3729077875614166, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04107053950428963, KL divergence=0.01866660825908184, Entropy=0.35746216773986816, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.041441600769758224, KL divergence=0.03076009452342987, Entropy=0.3532527983188629, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.047945354133844376, KL divergence=0.048179518431425095, Entropy=0.3544149696826935, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06636262685060501, KL divergence=0.05847838148474693, Entropy=0.35093531012535095, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05042130872607231, KL divergence=0.07306250929832458, Entropy=0.35422682762145996, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0416128896176815, KL divergence=0.0823904424905777, Entropy=0.3497849404811859, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04417775943875313, KL divergence=0.09246579557657242, Entropy=0.34562948346138, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07520348578691483, KL divergence=0.09904599189758301, Entropy=0.3495781123638153, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/344_Step-88195.ckpt']
Uploaded 3 files for checkpoint 344 in 0.57 seconds
saved intermediate frozen graph: current/model/model_344.pb
Best checkpoint number: 314, Last checkpoint number: 342
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'341'}
Training> Name=main_level/agent, Worker=0, Episode=2601, Total reward=55.9, Steps=88225, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2602, Total reward=30.22, Steps=88244, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2603, Total reward=2.76, Steps=88283, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2604, Total reward=5.3, Steps=88322, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2605, Total reward=32.71, Steps=88360, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2606, Total reward=14.75, Steps=88390, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2607, Total reward=57.5, Steps=88429, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2608, Total reward=59.53, Steps=88458, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2609, Total reward=80.03, Steps=88519, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2610, Total reward=65.18, Steps=88559, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2611, Total reward=10.89, Steps=88586, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2612, Total reward=49.56, Steps=88618, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2613, Total reward=37.39, Steps=88645, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2614, Total reward=45.29, Steps=88667, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2615, Total reward=18.68, Steps=88711, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2616, Total reward=18.93, Steps=88743, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2617, Total reward=44.7, Steps=88798, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2618, Total reward=32.14, Steps=88823, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2619, Total reward=22.74, Steps=88842, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2620, Total reward=77.03, Steps=88880, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2621, Total reward=35.2, Steps=88905, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2622, Total reward=33.5, Steps=88923, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2623, Total reward=9.62, Steps=88968, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2624, Total reward=102.47, Steps=89054, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2625, Total reward=42.43, Steps=89118, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2626, Total reward=14.76, Steps=89133, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2627, Total reward=57.04, Steps=89171, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2628, Total reward=42.18, Steps=89196, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2629, Total reward=14.24, Steps=89212, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2630, Total reward=66.24, Steps=89248, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2631, Total reward=30.2, Steps=89276, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2632, Total reward=49.03, Steps=89308, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2633, Total reward=38.47, Steps=89339, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2634, Total reward=42.85, Steps=89362, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2635, Total reward=23.92, Steps=89390, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2636, Total reward=20.4, Steps=89413, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2637, Total reward=63.35, Steps=89471, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2638, Total reward=35.09, Steps=89492, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2639, Total reward=91.6, Steps=89540, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2640, Total reward=65.65, Steps=89582, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2641, Total reward=57.2, Steps=89623, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2642, Total reward=29.27, Steps=89643, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2643, Total reward=10.29, Steps=89689, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2644, Total reward=10.59, Steps=89730, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2645, Total reward=54.36, Steps=89785, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2646, Total reward=11.12, Steps=89798, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2647, Total reward=59.99, Steps=89838, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2648, Total reward=47.85, Steps=89867, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2649, Total reward=23.25, Steps=89885, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2650, Total reward=84.65, Steps=89921, Training iteration=52
Policy training> Surrogate loss=0.017665982246398926, KL divergence=0.00010937217302853242, Entropy=0.36098363995552063, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.014683082699775696, KL divergence=0.005489595700055361, Entropy=0.35697293281555176, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03766531124711037, KL divergence=0.01732449419796467, Entropy=0.35055527091026306, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04174937680363655, KL divergence=0.03215966001152992, Entropy=0.3390573561191559, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.03766758367419243, KL divergence=0.05071932077407837, Entropy=0.33673593401908875, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.039392586797475815, KL divergence=0.06365029513835907, Entropy=0.3305377960205078, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.036104146391153336, KL divergence=0.0737769827246666, Entropy=0.3331640064716339, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05184491351246834, KL divergence=0.08104557543992996, Entropy=0.32572364807128906, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05020647868514061, KL divergence=0.09221961349248886, Entropy=0.32656964659690857, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0633852556347847, KL divergence=0.09744428843259811, Entropy=0.32801252603530884, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/345_Step-89921.ckpt']
Uploaded 3 files for checkpoint 345 in 0.53 seconds
saved intermediate frozen graph: current/model/model_345.pb
Best checkpoint number: 314, Last checkpoint number: 343
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'342'}
Training> Name=main_level/agent, Worker=0, Episode=2651, Total reward=58.75, Steps=89959, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2652, Total reward=40.99, Steps=89989, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2653, Total reward=48.06, Steps=90018, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2654, Total reward=41.85, Steps=90038, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2655, Total reward=21.51, Steps=90080, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2656, Total reward=19.94, Steps=90099, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2657, Total reward=48.44, Steps=90161, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2658, Total reward=48.62, Steps=90208, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2659, Total reward=21.97, Steps=90228, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2660, Total reward=80.41, Steps=90265, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2661, Total reward=64.61, Steps=90297, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2662, Total reward=30.08, Steps=90315, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2663, Total reward=10.04, Steps=90341, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2664, Total reward=11.63, Steps=90366, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2665, Total reward=23.99, Steps=90405, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2666, Total reward=59.1, Steps=90444, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2667, Total reward=62.22, Steps=90483, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2668, Total reward=66.99, Steps=90524, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2669, Total reward=16.51, Steps=90541, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2670, Total reward=82.82, Steps=90587, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2671, Total reward=65.48, Steps=90614, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2672, Total reward=36.1, Steps=90643, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2673, Total reward=31.75, Steps=90662, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2674, Total reward=37.5, Steps=90683, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2675, Total reward=26.42, Steps=90714, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2676, Total reward=15.84, Steps=90745, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2677, Total reward=52.64, Steps=90780, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2678, Total reward=29.7, Steps=90804, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2679, Total reward=14.71, Steps=90828, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2680, Total reward=67.68, Steps=90870, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2681, Total reward=43.89, Steps=90897, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2682, Total reward=35.89, Steps=90918, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2683, Total reward=0.03, Steps=90946, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2684, Total reward=55.92, Steps=91019, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2685, Total reward=13.2, Steps=91043, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2686, Total reward=27.5, Steps=91073, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2687, Total reward=77.85, Steps=91114, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2688, Total reward=68.39, Steps=91152, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2689, Total reward=59.13, Steps=91181, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2690, Total reward=49.34, Steps=91204, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2691, Total reward=38.48, Steps=91233, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2692, Total reward=56.42, Steps=91264, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2693, Total reward=49.4, Steps=91293, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2694, Total reward=34.68, Steps=91314, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2695, Total reward=20.67, Steps=91341, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2696, Total reward=25.47, Steps=91372, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2697, Total reward=72.96, Steps=91439, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2698, Total reward=33.58, Steps=91464, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2699, Total reward=63.1, Steps=91506, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2700, Total reward=76.42, Steps=91546, Training iteration=53
Policy training> Surrogate loss=0.003541877493262291, KL divergence=8.360330684809014e-05, Entropy=0.3510236442089081, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.026426950469613075, KL divergence=0.004174523055553436, Entropy=0.35137948393821716, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04044722393155098, KL divergence=0.015244799666106701, Entropy=0.3399832248687744, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04632227495312691, KL divergence=0.029438719153404236, Entropy=0.336385577917099, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.046201109886169434, KL divergence=0.04246533289551735, Entropy=0.3334529399871826, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05840732529759407, KL divergence=0.055202316492795944, Entropy=0.32668837904930115, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05152397230267525, KL divergence=0.0665539801120758, Entropy=0.3309744894504547, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07138480991125107, KL divergence=0.07457580417394638, Entropy=0.32208576798439026, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06110386922955513, KL divergence=0.08349593728780746, Entropy=0.3271644115447998, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07154073566198349, KL divergence=0.09022035449743271, Entropy=0.3252657949924469, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/346_Step-91546.ckpt']
Uploaded 3 files for checkpoint 346 in 0.55 seconds
saved intermediate frozen graph: current/model/model_346.pb
Best checkpoint number: 314, Last checkpoint number: 344
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'343'}
Training> Name=main_level/agent, Worker=0, Episode=2701, Total reward=61.35, Steps=91578, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2702, Total reward=32.13, Steps=91602, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2703, Total reward=46.99, Steps=91682, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2704, Total reward=8.64, Steps=91702, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2705, Total reward=9.88, Steps=91724, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2706, Total reward=87.38, Steps=91768, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2707, Total reward=45.92, Steps=91807, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2708, Total reward=60.94, Steps=91837, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2709, Total reward=18.48, Steps=91850, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2710, Total reward=84.0, Steps=91895, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2711, Total reward=76.95, Steps=91932, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2712, Total reward=14.35, Steps=91958, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2713, Total reward=55.43, Steps=91987, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2714, Total reward=43.63, Steps=92008, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2715, Total reward=36.34, Steps=92037, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2716, Total reward=22.75, Steps=92063, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2717, Total reward=44.14, Steps=92110, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2718, Total reward=23.74, Steps=92135, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2719, Total reward=8.08, Steps=92159, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2720, Total reward=83.38, Steps=92197, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2721, Total reward=29.54, Steps=92220, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2722, Total reward=41.54, Steps=92246, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2723, Total reward=9.91, Steps=92284, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2724, Total reward=80.8, Steps=92346, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2725, Total reward=4.89, Steps=92360, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2726, Total reward=69.5, Steps=92406, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2727, Total reward=60.38, Steps=92445, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2728, Total reward=83.63, Steps=92498, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2729, Total reward=48.59, Steps=92530, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2730, Total reward=82.93, Steps=92564, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2731, Total reward=43.99, Steps=92600, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2732, Total reward=58.3, Steps=92632, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2733, Total reward=44.99, Steps=92662, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2734, Total reward=28.69, Steps=92682, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2735, Total reward=19.73, Steps=92731, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2736, Total reward=16.59, Steps=92768, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2737, Total reward=20.94, Steps=92787, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2738, Total reward=29.96, Steps=92810, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2739, Total reward=10.28, Steps=92832, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2740, Total reward=91.89, Steps=92883, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2741, Total reward=53.93, Steps=92910, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2742, Total reward=36.03, Steps=92927, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2743, Total reward=8.78, Steps=92950, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2744, Total reward=11.55, Steps=92990, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2745, Total reward=13.34, Steps=93013, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2746, Total reward=87.9, Steps=93059, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2747, Total reward=107.74, Steps=93154, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2748, Total reward=66.67, Steps=93193, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2749, Total reward=38.92, Steps=93218, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2750, Total reward=82.14, Steps=93265, Training iteration=54
Policy training> Surrogate loss=-0.006745430175215006, KL divergence=0.00011902811093023047, Entropy=0.3738481104373932, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.020301511511206627, KL divergence=0.0066985501907765865, Entropy=0.3733193576335907, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04616844281554222, KL divergence=0.017792455852031708, Entropy=0.3661646842956543, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04685105010867119, KL divergence=0.03571872413158417, Entropy=0.35612964630126953, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06322416663169861, KL divergence=0.054026443511247635, Entropy=0.3577701151371002, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05967644229531288, KL divergence=0.06683524698019028, Entropy=0.346222847700119, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04667080566287041, KL divergence=0.08489087969064713, Entropy=0.3504951000213623, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0678730383515358, KL divergence=0.09152159094810486, Entropy=0.3450232744216919, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06988196820020676, KL divergence=0.09878041595220566, Entropy=0.34893468022346497, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0710798129439354, KL divergence=0.10789506882429123, Entropy=0.34249112010002136, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/347_Step-93265.ckpt']
Uploaded 3 files for checkpoint 347 in 0.60 seconds
saved intermediate frozen graph: current/model/model_347.pb
Best checkpoint number: 314, Last checkpoint number: 345
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'344'}
Training> Name=main_level/agent, Worker=0, Episode=2751, Total reward=57.06, Steps=93293, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2752, Total reward=5.74, Steps=93304, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2753, Total reward=50.61, Steps=93331, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2754, Total reward=43.25, Steps=93353, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2755, Total reward=21.67, Steps=93384, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2756, Total reward=10.0, Steps=93403, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2757, Total reward=38.41, Steps=93437, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2758, Total reward=31.19, Steps=93459, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2759, Total reward=14.84, Steps=93484, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2760, Total reward=77.81, Steps=93527, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2761, Total reward=46.65, Steps=93559, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2762, Total reward=32.17, Steps=93577, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2763, Total reward=17.54, Steps=93613, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2764, Total reward=4.99, Steps=93633, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2765, Total reward=1.4, Steps=93645, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2766, Total reward=89.62, Steps=93706, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2767, Total reward=100.83, Steps=93792, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2768, Total reward=96.98, Steps=93852, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2769, Total reward=62.77, Steps=93911, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2770, Total reward=72.11, Steps=93958, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2771, Total reward=60.82, Steps=93997, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2772, Total reward=61.02, Steps=94040, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2773, Total reward=47.08, Steps=94068, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2774, Total reward=32.18, Steps=94088, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2775, Total reward=29.37, Steps=94116, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2776, Total reward=21.67, Steps=94135, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2777, Total reward=46.34, Steps=94170, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2778, Total reward=38.2, Steps=94195, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2779, Total reward=65.63, Steps=94245, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2780, Total reward=78.84, Steps=94279, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2781, Total reward=66.03, Steps=94329, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2782, Total reward=42.25, Steps=94354, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2783, Total reward=7.05, Steps=94373, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2784, Total reward=1.79, Steps=94393, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2785, Total reward=22.37, Steps=94429, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2786, Total reward=87.39, Steps=94478, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2787, Total reward=63.36, Steps=94520, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2788, Total reward=64.02, Steps=94557, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2789, Total reward=34.48, Steps=94582, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2790, Total reward=58.99, Steps=94619, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2791, Total reward=18.96, Steps=94632, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2792, Total reward=14.66, Steps=94653, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2793, Total reward=58.4, Steps=94683, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2794, Total reward=45.42, Steps=94705, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2795, Total reward=18.54, Steps=94733, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2796, Total reward=26.34, Steps=94764, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2797, Total reward=27.23, Steps=94786, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2798, Total reward=39.83, Steps=94814, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2799, Total reward=91.68, Steps=94863, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2800, Total reward=87.58, Steps=94899, Training iteration=55
Policy training> Surrogate loss=0.006595275830477476, KL divergence=0.00025462298071943223, Entropy=0.3606022596359253, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.026837656274437904, KL divergence=0.007225415203720331, Entropy=0.36440715193748474, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03715534508228302, KL divergence=0.022637411952018738, Entropy=0.3602379262447357, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04398755356669426, KL divergence=0.03797735273838043, Entropy=0.35749897360801697, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.062050629407167435, KL divergence=0.05366216227412224, Entropy=0.3540550768375397, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05123896524310112, KL divergence=0.067502461373806, Entropy=0.3511688709259033, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06077760085463524, KL divergence=0.0777309462428093, Entropy=0.34780725836753845, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.061432380229234695, KL divergence=0.09067799896001816, Entropy=0.35256609320640564, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05345989391207695, KL divergence=0.10190706700086594, Entropy=0.34479522705078125, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06899634748697281, KL divergence=0.11150586605072021, Entropy=0.3515234887599945, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/348_Step-94899.ckpt']
Uploaded 3 files for checkpoint 348 in 0.52 seconds
saved intermediate frozen graph: current/model/model_348.pb
Best checkpoint number: 314, Last checkpoint number: 346
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'345'}
Training> Name=main_level/agent, Worker=0, Episode=2801, Total reward=59.57, Steps=94929, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2802, Total reward=39.9, Steps=94950, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2803, Total reward=23.33, Steps=94997, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2804, Total reward=15.18, Steps=95026, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2805, Total reward=74.76, Steps=95097, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2806, Total reward=28.49, Steps=95125, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2807, Total reward=44.64, Steps=95165, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2808, Total reward=77.31, Steps=95206, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2809, Total reward=95.61, Steps=95263, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2810, Total reward=37.33, Steps=95300, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2811, Total reward=32.36, Steps=95329, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2812, Total reward=53.68, Steps=95367, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2813, Total reward=33.9, Steps=95390, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2814, Total reward=44.78, Steps=95411, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2815, Total reward=28.99, Steps=95439, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2816, Total reward=21.71, Steps=95459, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2817, Total reward=49.5, Steps=95495, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2818, Total reward=31.28, Steps=95514, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2819, Total reward=49.22, Steps=95557, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2820, Total reward=68.62, Steps=95593, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2821, Total reward=54.33, Steps=95622, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2822, Total reward=31.74, Steps=95643, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2823, Total reward=7.52, Steps=95659, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2824, Total reward=88.35, Steps=95742, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2825, Total reward=22.9, Steps=95764, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2826, Total reward=82.73, Steps=95810, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2827, Total reward=58.62, Steps=95849, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2828, Total reward=58.12, Steps=95878, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2829, Total reward=29.02, Steps=95896, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2830, Total reward=90.32, Steps=95942, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2831, Total reward=53.24, Steps=95970, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2832, Total reward=47.38, Steps=96001, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2833, Total reward=21.17, Steps=96013, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2834, Total reward=39.66, Steps=96034, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2835, Total reward=19.95, Steps=96048, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2836, Total reward=13.23, Steps=96075, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2837, Total reward=55.03, Steps=96114, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2838, Total reward=32.74, Steps=96139, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2839, Total reward=7.8, Steps=96154, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2840, Total reward=84.18, Steps=96194, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2841, Total reward=56.98, Steps=96234, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2842, Total reward=29.92, Steps=96253, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2843, Total reward=3.54, Steps=96271, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2844, Total reward=45.86, Steps=96337, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2845, Total reward=66.51, Steps=96392, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2846, Total reward=105.57, Steps=96451, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2847, Total reward=40.49, Steps=96486, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2848, Total reward=100.09, Steps=96564, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2849, Total reward=24.69, Steps=96591, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2850, Total reward=35.53, Steps=96631, Training iteration=56
Policy training> Surrogate loss=0.0019224832067266107, KL divergence=0.00011283867206657305, Entropy=0.3696647584438324, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.01782888174057007, KL divergence=0.006657651159912348, Entropy=0.35885441303253174, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.061136484146118164, KL divergence=0.021110476925969124, Entropy=0.36597681045532227, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.049418166279792786, KL divergence=0.03805391117930412, Entropy=0.3555046617984772, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04591492936015129, KL divergence=0.050965648144483566, Entropy=0.35376378893852234, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.046726103872060776, KL divergence=0.0655289888381958, Entropy=0.355002760887146, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06267020106315613, KL divergence=0.07761839032173157, Entropy=0.3519922196865082, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07043811678886414, KL divergence=0.09102072566747665, Entropy=0.3526245057582855, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06777750700712204, KL divergence=0.0979694128036499, Entropy=0.353829950094223, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05401306971907616, KL divergence=0.10161706060171127, Entropy=0.35273316502571106, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/349_Step-96631.ckpt']
Uploaded 3 files for checkpoint 349 in 0.51 seconds
saved intermediate frozen graph: current/model/model_349.pb
Best checkpoint number: 314, Last checkpoint number: 347
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'346'}
Training> Name=main_level/agent, Worker=0, Episode=2851, Total reward=24.87, Steps=96661, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2852, Total reward=41.37, Steps=96693, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2853, Total reward=17.6, Steps=96712, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2854, Total reward=45.8, Steps=96736, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2855, Total reward=18.75, Steps=96750, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2856, Total reward=22.01, Steps=96778, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2857, Total reward=60.58, Steps=96813, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2858, Total reward=37.94, Steps=96849, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2859, Total reward=24.06, Steps=96875, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2860, Total reward=65.66, Steps=96915, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2861, Total reward=68.64, Steps=96978, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2862, Total reward=46.32, Steps=97003, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2863, Total reward=31.65, Steps=97046, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2864, Total reward=22.92, Steps=97086, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2865, Total reward=76.37, Steps=97153, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2866, Total reward=100.82, Steps=97220, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2867, Total reward=9.93, Steps=97234, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2868, Total reward=65.45, Steps=97263, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2869, Total reward=18.12, Steps=97276, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2870, Total reward=64.28, Steps=97311, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2871, Total reward=46.14, Steps=97352, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2872, Total reward=50.66, Steps=97384, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2873, Total reward=11.14, Steps=97402, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2874, Total reward=42.53, Steps=97424, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2875, Total reward=20.43, Steps=97447, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2876, Total reward=15.51, Steps=97466, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2877, Total reward=47.43, Steps=97501, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2878, Total reward=36.39, Steps=97522, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2879, Total reward=8.24, Steps=97546, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2880, Total reward=28.52, Steps=97566, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2881, Total reward=46.77, Steps=97596, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2882, Total reward=29.18, Steps=97614, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2883, Total reward=71.73, Steps=97686, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2884, Total reward=23.93, Steps=97721, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2885, Total reward=81.03, Steps=97779, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2886, Total reward=76.53, Steps=97833, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2887, Total reward=30.32, Steps=97870, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2888, Total reward=61.64, Steps=97900, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2889, Total reward=28.0, Steps=97918, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2890, Total reward=67.43, Steps=97953, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2891, Total reward=49.79, Steps=97981, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2892, Total reward=44.86, Steps=98013, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2893, Total reward=45.93, Steps=98040, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2894, Total reward=32.47, Steps=98062, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2895, Total reward=29.26, Steps=98091, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2896, Total reward=18.63, Steps=98123, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2897, Total reward=58.1, Steps=98170, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2898, Total reward=12.65, Steps=98184, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2899, Total reward=24.6, Steps=98205, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2900, Total reward=69.36, Steps=98245, Training iteration=57
Policy training> Surrogate loss=-0.005636447574943304, KL divergence=9.080299059860408e-05, Entropy=0.39835676550865173, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03561101853847504, KL divergence=0.006465482991188765, Entropy=0.3910830020904541, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04667259752750397, KL divergence=0.021137967705726624, Entropy=0.38628554344177246, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.046287134289741516, KL divergence=0.03849491477012634, Entropy=0.3829479217529297, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06310201436281204, KL divergence=0.05464152619242668, Entropy=0.37649789452552795, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.058989059180021286, KL divergence=0.07041233032941818, Entropy=0.371255487203598, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06150127574801445, KL divergence=0.08572482317686081, Entropy=0.36887112259864807, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06077353656291962, KL divergence=0.09536600112915039, Entropy=0.3695301115512848, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0691005289554596, KL divergence=0.10611382126808167, Entropy=0.36707472801208496, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07421132177114487, KL divergence=0.11330155283212662, Entropy=0.3637430965900421, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/350_Step-98245.ckpt']
Uploaded 3 files for checkpoint 350 in 0.56 seconds
saved intermediate frozen graph: current/model/model_350.pb
Best checkpoint number: 314, Last checkpoint number: 348
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'347'}
Training> Name=main_level/agent, Worker=0, Episode=2901, Total reward=61.98, Steps=98279, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2902, Total reward=50.77, Steps=98328, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2903, Total reward=18.39, Steps=98362, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2904, Total reward=17.73, Steps=98398, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2905, Total reward=9.81, Steps=98423, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2906, Total reward=64.75, Steps=98478, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2907, Total reward=39.06, Steps=98513, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2908, Total reward=55.47, Steps=98538, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2909, Total reward=24.59, Steps=98565, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2910, Total reward=22.79, Steps=98592, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2911, Total reward=28.0, Steps=98619, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2912, Total reward=70.34, Steps=98660, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2913, Total reward=52.93, Steps=98690, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2914, Total reward=40.71, Steps=98711, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2915, Total reward=16.99, Steps=98740, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2916, Total reward=20.16, Steps=98757, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2917, Total reward=93.42, Steps=98836, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2918, Total reward=23.76, Steps=98859, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2919, Total reward=76.66, Steps=98904, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2920, Total reward=60.41, Steps=98941, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2921, Total reward=52.11, Steps=98970, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2922, Total reward=32.87, Steps=98993, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2923, Total reward=22.54, Steps=99036, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2924, Total reward=6.43, Steps=99052, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2925, Total reward=51.01, Steps=99114, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2926, Total reward=100.49, Steps=99164, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2927, Total reward=59.83, Steps=99203, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2928, Total reward=45.28, Steps=99249, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2929, Total reward=22.89, Steps=99270, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2930, Total reward=68.89, Steps=99316, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2931, Total reward=55.26, Steps=99354, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2932, Total reward=58.84, Steps=99396, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2933, Total reward=46.01, Steps=99423, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2934, Total reward=39.34, Steps=99444, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2935, Total reward=20.07, Steps=99458, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2936, Total reward=15.09, Steps=99481, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2937, Total reward=42.64, Steps=99529, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2938, Total reward=33.22, Steps=99553, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2939, Total reward=15.19, Steps=99575, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2940, Total reward=60.76, Steps=99618, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2941, Total reward=49.33, Steps=99645, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2942, Total reward=30.65, Steps=99666, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2943, Total reward=15.24, Steps=99741, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2944, Total reward=70.46, Steps=99807, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2945, Total reward=16.62, Steps=99824, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2946, Total reward=68.25, Steps=99879, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2947, Total reward=63.1, Steps=99929, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2948, Total reward=61.35, Steps=99961, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2949, Total reward=25.69, Steps=99981, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2950, Total reward=25.55, Steps=100005, Training iteration=58
Policy training> Surrogate loss=-0.0037854716647416353, KL divergence=0.00023005525872576982, Entropy=0.3965781033039093, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04142129048705101, KL divergence=0.008677243255078793, Entropy=0.38596591353416443, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.058841630816459656, KL divergence=0.022183416411280632, Entropy=0.38489243388175964, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04283970594406128, KL divergence=0.04643693566322327, Entropy=0.375456303358078, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06746497005224228, KL divergence=0.061165113002061844, Entropy=0.3641468584537506, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07645685225725174, KL divergence=0.08223148435354233, Entropy=0.3686649799346924, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07848149538040161, KL divergence=0.09398302435874939, Entropy=0.36566224694252014, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06213507056236267, KL divergence=0.10431263595819473, Entropy=0.36236047744750977, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07493893802165985, KL divergence=0.11677820235490799, Entropy=0.364265114068985, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06655020266771317, KL divergence=0.12044492363929749, Entropy=0.3604659140110016, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/351_Step-100005.ckpt']
Uploaded 3 files for checkpoint 351 in 0.59 seconds
saved intermediate frozen graph: current/model/model_351.pb
Best checkpoint number: 314, Last checkpoint number: 349
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'348'}
Training> Name=main_level/agent, Worker=0, Episode=2951, Total reward=46.97, Steps=100033, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2952, Total reward=58.61, Steps=100063, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2953, Total reward=39.11, Steps=100091, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2954, Total reward=43.3, Steps=100112, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2955, Total reward=19.8, Steps=100141, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2956, Total reward=21.67, Steps=100178, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2957, Total reward=24.9, Steps=100211, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2958, Total reward=82.4, Steps=100278, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2959, Total reward=70.74, Steps=100330, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2960, Total reward=78.6, Steps=100368, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2961, Total reward=61.63, Steps=100398, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2962, Total reward=39.4, Steps=100426, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2963, Total reward=27.98, Steps=100469, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2964, Total reward=2.89, Steps=100486, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2965, Total reward=142.73, Steps=100588, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2966, Total reward=31.27, Steps=100635, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2967, Total reward=64.75, Steps=100688, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2968, Total reward=110.49, Steps=100761, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2969, Total reward=23.66, Steps=100776, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2970, Total reward=78.2, Steps=100823, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2971, Total reward=45.54, Steps=100864, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2972, Total reward=44.71, Steps=100896, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2973, Total reward=23.12, Steps=100907, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2974, Total reward=43.44, Steps=100928, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2975, Total reward=20.78, Steps=100958, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2976, Total reward=13.49, Steps=100978, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2977, Total reward=32.75, Steps=101014, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2978, Total reward=28.22, Steps=101037, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2979, Total reward=60.35, Steps=101084, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2980, Total reward=76.07, Steps=101121, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2981, Total reward=70.23, Steps=101154, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2982, Total reward=33.47, Steps=101174, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2983, Total reward=60.42, Steps=101266, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2984, Total reward=9.75, Steps=101294, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2985, Total reward=18.36, Steps=101316, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2986, Total reward=49.59, Steps=101363, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2987, Total reward=82.91, Steps=101411, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2988, Total reward=61.25, Steps=101445, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2989, Total reward=35.99, Steps=101477, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2990, Total reward=49.63, Steps=101516, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2991, Total reward=54.93, Steps=101544, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2992, Total reward=36.82, Steps=101574, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2993, Total reward=52.03, Steps=101603, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2994, Total reward=35.11, Steps=101624, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2995, Total reward=17.65, Steps=101653, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2996, Total reward=28.79, Steps=101673, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2997, Total reward=46.53, Steps=101706, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2998, Total reward=20.31, Steps=101723, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2999, Total reward=63.7, Steps=101768, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=3000, Total reward=90.18, Steps=101815, Training iteration=59
Policy training> Surrogate loss=0.020747078582644463, KL divergence=0.00011626308696577325, Entropy=0.37338197231292725, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03202403709292412, KL divergence=0.00662120059132576, Entropy=0.36783114075660706, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.02921915613114834, KL divergence=0.01927926577627659, Entropy=0.368715763092041, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.045198798179626465, KL divergence=0.035010989755392075, Entropy=0.3594021797180176, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.03854033350944519, KL divergence=0.051973212510347366, Entropy=0.37225350737571716, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.058894623070955276, KL divergence=0.06379516422748566, Entropy=0.35567936301231384, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.058200422674417496, KL divergence=0.07779701799154282, Entropy=0.3631432056427002, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.061835914850234985, KL divergence=0.08120366930961609, Entropy=0.35562244057655334, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06189340353012085, KL divergence=0.0959007740020752, Entropy=0.36049842834472656, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06644453853368759, KL divergence=0.10417541116476059, Entropy=0.36047664284706116, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/352_Step-101815.ckpt']
Uploaded 3 files for checkpoint 352 in 0.59 seconds
saved intermediate frozen graph: current/model/model_352.pb
Best checkpoint number: 314, Last checkpoint number: 350
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'349'}
Training> Name=main_level/agent, Worker=0, Episode=3001, Total reward=69.97, Steps=101845, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3002, Total reward=35.05, Steps=101866, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3003, Total reward=10.17, Steps=101889, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3004, Total reward=5.35, Steps=101908, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3005, Total reward=29.72, Steps=101953, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3006, Total reward=10.84, Steps=101978, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3007, Total reward=59.54, Steps=102019, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3008, Total reward=68.41, Steps=102060, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3009, Total reward=37.49, Steps=102097, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3010, Total reward=7.73, Steps=102110, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3011, Total reward=9.0, Steps=102131, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3012, Total reward=48.02, Steps=102163, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3013, Total reward=35.99, Steps=102184, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3014, Total reward=37.33, Steps=102205, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3015, Total reward=17.46, Steps=102230, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3016, Total reward=13.64, Steps=102249, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3017, Total reward=43.5, Steps=102294, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3018, Total reward=31.02, Steps=102320, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3019, Total reward=64.8, Steps=102372, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3020, Total reward=76.97, Steps=102411, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3021, Total reward=52.17, Steps=102453, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3022, Total reward=33.41, Steps=102472, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3023, Total reward=3.52, Steps=102490, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3024, Total reward=24.88, Steps=102530, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3025, Total reward=22.61, Steps=102585, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3026, Total reward=18.77, Steps=102611, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3027, Total reward=60.62, Steps=102649, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3028, Total reward=63.1, Steps=102679, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3029, Total reward=88.76, Steps=102723, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3030, Total reward=14.62, Steps=102737, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3031, Total reward=49.56, Steps=102773, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3032, Total reward=60.73, Steps=102806, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3033, Total reward=38.64, Steps=102828, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3034, Total reward=36.06, Steps=102849, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3035, Total reward=25.56, Steps=102876, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3036, Total reward=17.96, Steps=102911, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3037, Total reward=33.51, Steps=102944, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3038, Total reward=41.42, Steps=102969, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3039, Total reward=90.66, Steps=103028, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3040, Total reward=72.32, Steps=103068, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3041, Total reward=59.31, Steps=103108, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3042, Total reward=35.11, Steps=103125, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3043, Total reward=6.08, Steps=103141, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3044, Total reward=0.01, Steps=103151, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3045, Total reward=12.84, Steps=103173, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3046, Total reward=100.63, Steps=103220, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3047, Total reward=63.15, Steps=103263, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3048, Total reward=75.62, Steps=103301, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3049, Total reward=36.59, Steps=103329, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3050, Total reward=29.81, Steps=103345, Training iteration=60
Policy training> Surrogate loss=0.0008615255355834961, KL divergence=2.578509520390071e-05, Entropy=0.3975457549095154, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.013186333701014519, KL divergence=0.0015027725603431463, Entropy=0.38200682401657104, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.044130899012088776, KL divergence=0.00777973560616374, Entropy=0.38828057050704956, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.061538051813840866, KL divergence=0.01508981641381979, Entropy=0.379726380109787, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.07148011773824692, KL divergence=0.02377396449446678, Entropy=0.3714130222797394, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.039368484169244766, KL divergence=0.036530058830976486, Entropy=0.3598228096961975, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.047644808888435364, KL divergence=0.0497220978140831, Entropy=0.36855506896972656, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08765088766813278, KL divergence=0.05992550402879715, Entropy=0.3506864309310913, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05488632246851921, KL divergence=0.06551717966794968, Entropy=0.3650062084197998, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06048204004764557, KL divergence=0.07330254465341568, Entropy=0.34391844272613525, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/353_Step-103345.ckpt']
Uploaded 3 files for checkpoint 353 in 0.59 seconds
saved intermediate frozen graph: current/model/model_353.pb
Best checkpoint number: 314, Last checkpoint number: 351
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'350'}
Training> Name=main_level/agent, Worker=0, Episode=3051, Total reward=7.72, Steps=103360, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3052, Total reward=51.68, Steps=103391, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3053, Total reward=45.18, Steps=103412, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3054, Total reward=42.57, Steps=103434, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3055, Total reward=26.02, Steps=103464, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3056, Total reward=18.1, Steps=103482, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3057, Total reward=97.48, Steps=103560, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3058, Total reward=32.71, Steps=103601, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3059, Total reward=49.64, Steps=103654, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3060, Total reward=98.68, Steps=103722, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3061, Total reward=49.85, Steps=103752, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3062, Total reward=97.98, Steps=103840, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3063, Total reward=16.99, Steps=103897, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3064, Total reward=10.82, Steps=103922, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3065, Total reward=30.84, Steps=103953, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3066, Total reward=62.47, Steps=103999, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3067, Total reward=10.1, Steps=104012, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3068, Total reward=59.86, Steps=104041, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3069, Total reward=28.75, Steps=104075, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3070, Total reward=66.96, Steps=104114, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3071, Total reward=54.19, Steps=104156, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3072, Total reward=40.45, Steps=104189, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3073, Total reward=42.73, Steps=104217, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3074, Total reward=29.67, Steps=104238, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3075, Total reward=13.87, Steps=104265, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3076, Total reward=29.72, Steps=104282, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3077, Total reward=39.09, Steps=104320, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3078, Total reward=41.13, Steps=104369, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3079, Total reward=77.2, Steps=104417, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3080, Total reward=72.12, Steps=104454, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3081, Total reward=58.32, Steps=104487, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3082, Total reward=71.83, Steps=104542, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3083, Total reward=0.02, Steps=104558, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3084, Total reward=68.94, Steps=104627, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3085, Total reward=4.93, Steps=104642, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3086, Total reward=80.61, Steps=104687, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3087, Total reward=60.03, Steps=104728, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3088, Total reward=63.2, Steps=104770, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3089, Total reward=24.17, Steps=104787, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3090, Total reward=55.13, Steps=104823, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3091, Total reward=44.72, Steps=104863, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3092, Total reward=30.36, Steps=104891, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3093, Total reward=47.43, Steps=104911, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3094, Total reward=31.03, Steps=104931, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3095, Total reward=15.24, Steps=104959, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3096, Total reward=26.5, Steps=104985, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3097, Total reward=52.05, Steps=105032, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3098, Total reward=37.18, Steps=105057, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3099, Total reward=81.22, Steps=105110, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3100, Total reward=80.95, Steps=105149, Training iteration=61
Policy training> Surrogate loss=-0.013340629637241364, KL divergence=9.817531099542975e-05, Entropy=0.393401175737381, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.039464402943849564, KL divergence=0.006573066581040621, Entropy=0.3898477256298065, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.052327994257211685, KL divergence=0.020031588152050972, Entropy=0.38644862174987793, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0644611194729805, KL divergence=0.03760958090424538, Entropy=0.3818226158618927, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.07035132497549057, KL divergence=0.05340199172496796, Entropy=0.37823930382728577, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06211699917912483, KL divergence=0.06636125594377518, Entropy=0.3688349723815918, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07372176647186279, KL divergence=0.0833728089928627, Entropy=0.3718085289001465, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07869986444711685, KL divergence=0.09370473772287369, Entropy=0.35962891578674316, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05279184505343437, KL divergence=0.10176635533571243, Entropy=0.3682993948459625, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06236105039715767, KL divergence=0.10723205655813217, Entropy=0.3768528401851654, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/354_Step-105149.ckpt']
Uploaded 3 files for checkpoint 354 in 0.60 seconds
saved intermediate frozen graph: current/model/model_354.pb
Best checkpoint number: 314, Last checkpoint number: 352
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'351'}
Training> Name=main_level/agent, Worker=0, Episode=3101, Total reward=39.71, Steps=105177, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3102, Total reward=50.39, Steps=105225, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3103, Total reward=20.66, Steps=105264, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3104, Total reward=1.16, Steps=105278, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3105, Total reward=62.28, Steps=105332, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3106, Total reward=86.58, Steps=105378, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3107, Total reward=63.26, Steps=105420, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3108, Total reward=64.67, Steps=105465, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3109, Total reward=26.19, Steps=105512, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3110, Total reward=96.01, Steps=105561, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3111, Total reward=37.68, Steps=105597, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3112, Total reward=29.63, Steps=105626, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3113, Total reward=32.54, Steps=105646, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3114, Total reward=52.63, Steps=105668, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3115, Total reward=14.4, Steps=105694, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3116, Total reward=18.07, Steps=105713, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3117, Total reward=55.13, Steps=105766, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3118, Total reward=41.98, Steps=105790, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3119, Total reward=18.57, Steps=105809, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3120, Total reward=97.5, Steps=105849, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3121, Total reward=56.15, Steps=105890, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3122, Total reward=42.06, Steps=105913, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3123, Total reward=75.84, Steps=106020, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3124, Total reward=10.62, Steps=106055, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3125, Total reward=33.33, Steps=106113, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3126, Total reward=0.0, Steps=106114, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3127, Total reward=67.61, Steps=106184, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3128, Total reward=68.38, Steps=106235, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3129, Total reward=31.46, Steps=106267, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3130, Total reward=63.17, Steps=106301, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3131, Total reward=35.57, Steps=106322, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3132, Total reward=74.68, Steps=106363, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3133, Total reward=40.86, Steps=106391, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3134, Total reward=39.71, Steps=106413, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3135, Total reward=34.43, Steps=106442, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3136, Total reward=20.18, Steps=106460, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3137, Total reward=27.45, Steps=106500, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3138, Total reward=14.78, Steps=106518, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3139, Total reward=62.76, Steps=106566, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3140, Total reward=89.15, Steps=106609, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3141, Total reward=48.5, Steps=106640, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3142, Total reward=36.54, Steps=106665, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3143, Total reward=4.37, Steps=106692, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3144, Total reward=2.9, Steps=106710, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3145, Total reward=12.05, Steps=106760, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3146, Total reward=95.79, Steps=106820, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3147, Total reward=6.5, Steps=106835, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3148, Total reward=97.25, Steps=106908, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3149, Total reward=72.77, Steps=106962, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3150, Total reward=41.07, Steps=106992, Training iteration=62
Policy training> Surrogate loss=-0.011442928574979305, KL divergence=9.872746159089729e-05, Entropy=0.38652706146240234, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.015886615961790085, KL divergence=0.004513207823038101, Entropy=0.38301539421081543, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04798071086406708, KL divergence=0.014134894125163555, Entropy=0.3759625256061554, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.025546172633767128, KL divergence=0.026426928117871284, Entropy=0.3815811574459076, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06610989570617676, KL divergence=0.039325933903455734, Entropy=0.3701489269733429, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04273253679275513, KL divergence=0.050668757408857346, Entropy=0.36696282029151917, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06638512015342712, KL divergence=0.06138275936245918, Entropy=0.35526564717292786, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06143487989902496, KL divergence=0.07319281995296478, Entropy=0.3728477656841278, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0606507770717144, KL divergence=0.08164865523576736, Entropy=0.3549232482910156, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06342805176973343, KL divergence=0.08927033096551895, Entropy=0.3737337589263916, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/355_Step-106992.ckpt']
Uploaded 3 files for checkpoint 355 in 0.50 seconds
saved intermediate frozen graph: current/model/model_355.pb
Best checkpoint number: 314, Last checkpoint number: 353
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'352'}
Training> Name=main_level/agent, Worker=0, Episode=3151, Total reward=43.75, Steps=107028, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3152, Total reward=61.0, Steps=107059, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3153, Total reward=34.99, Steps=107081, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3154, Total reward=33.61, Steps=107101, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3155, Total reward=31.73, Steps=107129, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3156, Total reward=19.77, Steps=107149, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3157, Total reward=38.14, Steps=107186, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3158, Total reward=23.11, Steps=107238, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3159, Total reward=23.05, Steps=107272, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3160, Total reward=93.27, Steps=107313, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3161, Total reward=68.54, Steps=107345, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3162, Total reward=37.07, Steps=107363, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3163, Total reward=0.02, Steps=107383, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3164, Total reward=3.62, Steps=107403, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3165, Total reward=73.32, Steps=107469, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3166, Total reward=68.17, Steps=107517, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3167, Total reward=68.85, Steps=107558, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3168, Total reward=68.28, Steps=107617, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3169, Total reward=67.39, Steps=107680, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3170, Total reward=26.34, Steps=107727, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3171, Total reward=57.16, Steps=107766, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3172, Total reward=55.85, Steps=107805, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3173, Total reward=34.53, Steps=107825, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3174, Total reward=37.43, Steps=107847, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3175, Total reward=20.61, Steps=107879, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3176, Total reward=23.6, Steps=107908, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3177, Total reward=30.86, Steps=107931, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3178, Total reward=33.63, Steps=107964, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3179, Total reward=64.88, Steps=108017, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3180, Total reward=77.9, Steps=108053, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3181, Total reward=56.65, Steps=108083, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3182, Total reward=35.33, Steps=108109, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3183, Total reward=0.03, Steps=108138, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3184, Total reward=71.37, Steps=108246, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3185, Total reward=113.81, Steps=108353, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3186, Total reward=77.25, Steps=108413, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3187, Total reward=56.19, Steps=108455, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3188, Total reward=61.5, Steps=108482, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3189, Total reward=90.7, Steps=108539, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3190, Total reward=54.66, Steps=108587, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3191, Total reward=40.51, Steps=108614, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3192, Total reward=60.02, Steps=108658, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3193, Total reward=42.98, Steps=108679, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3194, Total reward=50.16, Steps=108702, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3195, Total reward=42.58, Steps=108731, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3196, Total reward=42.84, Steps=108779, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3197, Total reward=28.22, Steps=108799, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3198, Total reward=27.88, Steps=108823, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3199, Total reward=71.46, Steps=108874, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3200, Total reward=72.14, Steps=108912, Training iteration=63
Policy training> Surrogate loss=-0.0023433405440300703, KL divergence=9.896778647089377e-05, Entropy=0.3744102418422699, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04747295752167702, KL divergence=0.006424487102776766, Entropy=0.3706119954586029, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03541068360209465, KL divergence=0.019736656919121742, Entropy=0.3642787039279938, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03416861221194267, KL divergence=0.03191690519452095, Entropy=0.35748234391212463, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.068426214158535, KL divergence=0.050929147750139236, Entropy=0.3526003360748291, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.056575994938611984, KL divergence=0.06464135646820068, Entropy=0.35283589363098145, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06411074101924896, KL divergence=0.0803043469786644, Entropy=0.3560417592525482, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06186000630259514, KL divergence=0.09357104450464249, Entropy=0.3542536497116089, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.059456419199705124, KL divergence=0.0992785394191742, Entropy=0.3511176109313965, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07257484644651413, KL divergence=0.09675966948270798, Entropy=0.3562452793121338, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/356_Step-108912.ckpt']
Uploaded 3 files for checkpoint 356 in 0.54 seconds
saved intermediate frozen graph: current/model/model_356.pb
Best checkpoint number: 314, Last checkpoint number: 354
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'353'}
Training> Name=main_level/agent, Worker=0, Episode=3201, Total reward=60.41, Steps=108953, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3202, Total reward=44.03, Steps=108976, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3203, Total reward=25.01, Steps=109023, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3204, Total reward=12.54, Steps=109054, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3205, Total reward=40.66, Steps=109113, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3206, Total reward=32.84, Steps=109159, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3207, Total reward=100.05, Steps=109236, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3208, Total reward=75.34, Steps=109292, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3209, Total reward=70.08, Steps=109359, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3210, Total reward=21.91, Steps=109381, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3211, Total reward=45.3, Steps=109422, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3212, Total reward=53.39, Steps=109455, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3213, Total reward=0.01, Steps=109469, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3214, Total reward=45.79, Steps=109491, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3215, Total reward=22.76, Steps=109505, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3216, Total reward=21.38, Steps=109544, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3217, Total reward=34.65, Steps=109576, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3218, Total reward=28.19, Steps=109597, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3219, Total reward=21.25, Steps=109617, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3220, Total reward=90.22, Steps=109656, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3221, Total reward=68.41, Steps=109688, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3222, Total reward=37.29, Steps=109708, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3223, Total reward=10.32, Steps=109745, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3224, Total reward=19.56, Steps=109798, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3225, Total reward=75.04, Steps=109873, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3226, Total reward=58.37, Steps=109920, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3227, Total reward=17.34, Steps=109952, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3228, Total reward=56.82, Steps=109980, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3229, Total reward=31.0, Steps=110005, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3230, Total reward=14.78, Steps=110018, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3231, Total reward=49.63, Steps=110045, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3232, Total reward=19.22, Steps=110076, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3233, Total reward=42.01, Steps=110107, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3234, Total reward=42.35, Steps=110128, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3235, Total reward=22.31, Steps=110153, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3236, Total reward=29.32, Steps=110185, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3237, Total reward=28.56, Steps=110220, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3238, Total reward=28.11, Steps=110246, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3239, Total reward=52.58, Steps=110300, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3240, Total reward=50.35, Steps=110331, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3241, Total reward=30.71, Steps=110355, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3242, Total reward=49.47, Steps=110409, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3243, Total reward=0.03, Steps=110435, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3244, Total reward=81.01, Steps=110514, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3245, Total reward=114.82, Steps=110606, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3246, Total reward=73.52, Steps=110655, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3247, Total reward=22.15, Steps=110677, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3248, Total reward=76.53, Steps=110716, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3249, Total reward=27.32, Steps=110757, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3250, Total reward=88.29, Steps=110802, Training iteration=64
Policy training> Surrogate loss=0.013844113796949387, KL divergence=9.219663479598239e-05, Entropy=0.3981415927410126, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0369485504925251, KL divergence=0.00779263861477375, Entropy=0.4013768434524536, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05407745763659477, KL divergence=0.01898164674639702, Entropy=0.38298746943473816, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06148393824696541, KL divergence=0.03737769275903702, Entropy=0.37550273537635803, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06010881066322327, KL divergence=0.052149560302495956, Entropy=0.3627799451351166, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07099127769470215, KL divergence=0.07018519192934036, Entropy=0.3725537061691284, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07375982403755188, KL divergence=0.07356145232915878, Entropy=0.35919809341430664, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06857603788375854, KL divergence=0.09281045198440552, Entropy=0.366126149892807, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06917431950569153, KL divergence=0.09698223322629929, Entropy=0.3629755973815918, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07393817603588104, KL divergence=0.10543740540742874, Entropy=0.3659493923187256, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/357_Step-110802.ckpt']
Uploaded 3 files for checkpoint 357 in 0.53 seconds
saved intermediate frozen graph: current/model/model_357.pb
Best checkpoint number: 355, Last checkpoint number: 355
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'354'}
Training> Name=main_level/agent, Worker=0, Episode=3251, Total reward=39.18, Steps=110839, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3252, Total reward=55.75, Steps=110878, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3253, Total reward=29.37, Steps=110899, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3254, Total reward=38.76, Steps=110921, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3255, Total reward=20.15, Steps=110950, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3256, Total reward=28.99, Steps=110978, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3257, Total reward=43.64, Steps=111013, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3258, Total reward=38.97, Steps=111038, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3259, Total reward=12.44, Steps=111058, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3260, Total reward=19.24, Steps=111082, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3261, Total reward=48.86, Steps=111108, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3262, Total reward=35.79, Steps=111136, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3263, Total reward=0.02, Steps=111155, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3264, Total reward=10.5, Steps=111180, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3265, Total reward=125.49, Steps=111286, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3266, Total reward=27.21, Steps=111316, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3267, Total reward=6.54, Steps=111330, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3268, Total reward=57.56, Steps=111360, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3269, Total reward=34.21, Steps=111387, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3270, Total reward=32.49, Steps=111412, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3271, Total reward=68.92, Steps=111453, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3272, Total reward=39.96, Steps=111483, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3273, Total reward=45.22, Steps=111509, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3274, Total reward=33.98, Steps=111531, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3275, Total reward=17.12, Steps=111560, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3276, Total reward=15.07, Steps=111591, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3277, Total reward=37.29, Steps=111622, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3278, Total reward=29.69, Steps=111645, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3279, Total reward=74.37, Steps=111695, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3280, Total reward=84.42, Steps=111737, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3281, Total reward=67.64, Steps=111767, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3282, Total reward=38.58, Steps=111787, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3283, Total reward=9.39, Steps=111810, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3284, Total reward=7.05, Steps=111829, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3285, Total reward=84.42, Steps=111888, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3286, Total reward=41.8, Steps=111910, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3287, Total reward=91.23, Steps=111975, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3288, Total reward=63.53, Steps=112003, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3289, Total reward=102.93, Steps=112060, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3290, Total reward=59.21, Steps=112105, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3291, Total reward=67.26, Steps=112147, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3292, Total reward=49.49, Steps=112179, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3293, Total reward=21.15, Steps=112189, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3294, Total reward=40.75, Steps=112211, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3295, Total reward=15.34, Steps=112242, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3296, Total reward=30.72, Steps=112274, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3297, Total reward=57.55, Steps=112309, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3298, Total reward=91.93, Steps=112372, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3299, Total reward=54.77, Steps=112409, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3300, Total reward=66.51, Steps=112450, Training iteration=65
Policy training> Surrogate loss=-0.00674910843372345, KL divergence=6.771581684006378e-05, Entropy=0.3987174928188324, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.041601624339818954, KL divergence=0.005970544647425413, Entropy=0.3971250057220459, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0425153523683548, KL divergence=0.02208016812801361, Entropy=0.38120365142822266, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05940517410635948, KL divergence=0.039852648973464966, Entropy=0.3711124360561371, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05974826216697693, KL divergence=0.0561605840921402, Entropy=0.3627287447452545, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07029525190591812, KL divergence=0.07012144476175308, Entropy=0.36432623863220215, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06121894717216492, KL divergence=0.08465968817472458, Entropy=0.3650965690612793, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.058163996785879135, KL divergence=0.09406501054763794, Entropy=0.3649721145629883, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06734523177146912, KL divergence=0.1048436090350151, Entropy=0.3549504578113556, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07334338873624802, KL divergence=0.11243886500597, Entropy=0.3601112365722656, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/358_Step-112450.ckpt']
Uploaded 3 files for checkpoint 358 in 0.53 seconds
saved intermediate frozen graph: current/model/model_358.pb
Best checkpoint number: 355, Last checkpoint number: 356
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'314'}
Training> Name=main_level/agent, Worker=0, Episode=3301, Total reward=71.95, Steps=112489, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3302, Total reward=34.88, Steps=112508, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3303, Total reward=2.91, Steps=112533, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3304, Total reward=107.17, Steps=112639, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3305, Total reward=4.95, Steps=112657, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3306, Total reward=64.92, Steps=112700, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3307, Total reward=52.86, Steps=112739, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3308, Total reward=99.89, Steps=112812, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3309, Total reward=22.73, Steps=112837, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3310, Total reward=84.4, Steps=112884, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3311, Total reward=46.98, Steps=112914, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3312, Total reward=41.92, Steps=112943, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3313, Total reward=35.62, Steps=112965, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3314, Total reward=36.11, Steps=112986, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3315, Total reward=14.12, Steps=113014, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3316, Total reward=31.34, Steps=113049, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3317, Total reward=52.59, Steps=113099, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3318, Total reward=13.34, Steps=113111, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3319, Total reward=23.0, Steps=113134, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3320, Total reward=62.99, Steps=113176, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3321, Total reward=46.3, Steps=113204, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3322, Total reward=37.99, Steps=113229, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3323, Total reward=151.4, Steps=113346, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3324, Total reward=30.74, Steps=113392, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3325, Total reward=10.51, Steps=113422, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3326, Total reward=62.36, Steps=113470, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3327, Total reward=42.87, Steps=113505, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3328, Total reward=62.36, Steps=113549, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3329, Total reward=104.64, Steps=113610, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3330, Total reward=84.56, Steps=113653, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3331, Total reward=53.32, Steps=113692, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3332, Total reward=16.44, Steps=113722, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3333, Total reward=42.71, Steps=113750, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3334, Total reward=31.24, Steps=113762, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3335, Total reward=16.66, Steps=113797, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3336, Total reward=16.15, Steps=113818, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3337, Total reward=45.45, Steps=113862, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3338, Total reward=29.29, Steps=113885, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3339, Total reward=16.39, Steps=113904, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3340, Total reward=61.01, Steps=113938, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3341, Total reward=42.17, Steps=113966, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3342, Total reward=35.54, Steps=113991, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3343, Total reward=16.14, Steps=114035, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3344, Total reward=2.85, Steps=114049, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3345, Total reward=88.35, Steps=114137, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3346, Total reward=69.26, Steps=114183, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3347, Total reward=119.62, Steps=114273, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3348, Total reward=92.64, Steps=114336, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3349, Total reward=10.49, Steps=114349, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3350, Total reward=53.26, Steps=114400, Training iteration=66
Policy training> Surrogate loss=0.005378981586545706, KL divergence=0.00014331383863463998, Entropy=0.3902939260005951, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.052768856287002563, KL divergence=0.006014829035848379, Entropy=0.39436689019203186, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06610815227031708, KL divergence=0.019790420308709145, Entropy=0.38196539878845215, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03454354405403137, KL divergence=0.03198801353573799, Entropy=0.3675922453403473, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05588158592581749, KL divergence=0.04935808852314949, Entropy=0.36529597640037537, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06552928686141968, KL divergence=0.06437714397907257, Entropy=0.3656507432460785, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.062445443123579025, KL divergence=0.07617830485105515, Entropy=0.36673346161842346, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0386943519115448, KL divergence=0.0809929296374321, Entropy=0.3560481369495392, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07673130184412003, KL divergence=0.09438159316778183, Entropy=0.36255693435668945, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08535964041948318, KL divergence=0.09814377874135971, Entropy=0.36332717537879944, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/359_Step-114400.ckpt']
Uploaded 3 files for checkpoint 359 in 0.55 seconds
saved intermediate frozen graph: current/model/model_359.pb
Best checkpoint number: 355, Last checkpoint number: 357
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'356'}
Training> Name=main_level/agent, Worker=0, Episode=3351, Total reward=46.57, Steps=114438, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3352, Total reward=17.34, Steps=114458, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3353, Total reward=41.12, Steps=114479, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3354, Total reward=34.02, Steps=114499, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3355, Total reward=29.63, Steps=114532, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3356, Total reward=23.86, Steps=114549, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3357, Total reward=32.19, Steps=114592, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3358, Total reward=96.64, Steps=114673, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3359, Total reward=11.44, Steps=114702, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3360, Total reward=84.78, Steps=114740, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3361, Total reward=48.94, Steps=114766, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3362, Total reward=40.24, Steps=114792, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3363, Total reward=3.76, Steps=114807, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3364, Total reward=9.65, Steps=114835, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3365, Total reward=20.37, Steps=114857, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3366, Total reward=13.88, Steps=114877, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3367, Total reward=72.71, Steps=114931, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3368, Total reward=71.22, Steps=114969, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3369, Total reward=12.3, Steps=114985, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3370, Total reward=64.33, Steps=115031, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3371, Total reward=47.81, Steps=115067, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3372, Total reward=52.42, Steps=115099, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3373, Total reward=19.82, Steps=115110, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3374, Total reward=12.69, Steps=115131, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3375, Total reward=22.47, Steps=115160, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3376, Total reward=23.32, Steps=115187, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3377, Total reward=58.4, Steps=115226, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3378, Total reward=34.98, Steps=115263, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3379, Total reward=32.28, Steps=115301, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3380, Total reward=108.77, Steps=115354, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3381, Total reward=61.09, Steps=115383, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3382, Total reward=35.63, Steps=115410, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3383, Total reward=7.13, Steps=115429, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3384, Total reward=13.98, Steps=115468, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3385, Total reward=17.84, Steps=115484, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3386, Total reward=95.93, Steps=115534, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3387, Total reward=30.36, Steps=115572, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3388, Total reward=63.35, Steps=115617, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3389, Total reward=22.82, Steps=115653, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3390, Total reward=65.5, Steps=115702, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3391, Total reward=38.92, Steps=115732, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3392, Total reward=54.63, Steps=115764, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3393, Total reward=45.95, Steps=115792, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3394, Total reward=38.29, Steps=115813, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3395, Total reward=23.02, Steps=115842, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3396, Total reward=18.63, Steps=115863, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3397, Total reward=55.81, Steps=115900, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3398, Total reward=31.81, Steps=115924, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3399, Total reward=89.73, Steps=116004, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3400, Total reward=81.24, Steps=116041, Training iteration=67
Policy training> Surrogate loss=0.010092939250171185, KL divergence=0.00017465522978454828, Entropy=0.39160582423210144, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.027679255232214928, KL divergence=0.007882078178226948, Entropy=0.38674262166023254, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.02884998358786106, KL divergence=0.020492682233452797, Entropy=0.37389007210731506, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.050870317965745926, KL divergence=0.03645544871687889, Entropy=0.37102898955345154, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05025901272892952, KL divergence=0.05193323269486427, Entropy=0.3679504096508026, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04767806455492973, KL divergence=0.06635688990354538, Entropy=0.3658345639705658, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.059908200055360794, KL divergence=0.07923229783773422, Entropy=0.3628433644771576, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.058723676949739456, KL divergence=0.08632668107748032, Entropy=0.3693143427371979, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05919060483574867, KL divergence=0.0970836877822876, Entropy=0.3684544265270233, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06921639293432236, KL divergence=0.10156068205833435, Entropy=0.3653503358364105, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/360_Step-116041.ckpt']
Uploaded 3 files for checkpoint 360 in 0.55 seconds
saved intermediate frozen graph: current/model/model_360.pb
Best checkpoint number: 355, Last checkpoint number: 358
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'357'}
Training> Name=main_level/agent, Worker=0, Episode=3401, Total reward=25.29, Steps=116055, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3402, Total reward=34.86, Steps=116076, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3403, Total reward=3.07, Steps=116099, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3404, Total reward=5.77, Steps=116118, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3405, Total reward=46.19, Steps=116177, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3406, Total reward=78.23, Steps=116227, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3407, Total reward=55.65, Steps=116265, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3408, Total reward=55.26, Steps=116290, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3409, Total reward=43.34, Steps=116326, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3410, Total reward=49.83, Steps=116362, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3411, Total reward=60.58, Steps=116401, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3412, Total reward=0.01, Steps=116412, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3413, Total reward=49.44, Steps=116441, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3414, Total reward=30.12, Steps=116452, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3415, Total reward=7.74, Steps=116494, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3416, Total reward=24.67, Steps=116512, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3417, Total reward=32.47, Steps=116566, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3418, Total reward=41.43, Steps=116588, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3419, Total reward=61.1, Steps=116636, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3420, Total reward=70.51, Steps=116674, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3421, Total reward=39.8, Steps=116702, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3422, Total reward=37.51, Steps=116726, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3423, Total reward=6.37, Steps=116750, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3424, Total reward=67.18, Steps=116818, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3425, Total reward=1.42, Steps=116842, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3426, Total reward=3.56, Steps=116854, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3427, Total reward=71.97, Steps=116893, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3428, Total reward=57.13, Steps=116923, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3429, Total reward=56.78, Steps=116969, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3430, Total reward=74.04, Steps=117005, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3431, Total reward=57.91, Steps=117044, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3432, Total reward=37.12, Steps=117077, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3433, Total reward=21.74, Steps=117087, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3434, Total reward=29.96, Steps=117109, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3435, Total reward=20.0, Steps=117140, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3436, Total reward=21.41, Steps=117159, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3437, Total reward=12.47, Steps=117175, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3438, Total reward=35.83, Steps=117199, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3439, Total reward=15.64, Steps=117234, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3440, Total reward=67.54, Steps=117266, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3441, Total reward=51.88, Steps=117295, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3442, Total reward=42.4, Steps=117322, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3443, Total reward=9.92, Steps=117379, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3444, Total reward=7.53, Steps=117393, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3445, Total reward=97.9, Steps=117485, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3446, Total reward=11.25, Steps=117511, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3447, Total reward=61.99, Steps=117583, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3448, Total reward=65.06, Steps=117623, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3449, Total reward=96.14, Steps=117681, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3450, Total reward=38.64, Steps=117709, Training iteration=68
Policy training> Surrogate loss=-0.002377410652115941, KL divergence=0.00010333515092497692, Entropy=0.38651540875434875, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03117196261882782, KL divergence=0.00521288113668561, Entropy=0.37514790892601013, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04299146309494972, KL divergence=0.01940029300749302, Entropy=0.37064898014068604, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03722017630934715, KL divergence=0.03675748035311699, Entropy=0.3665931522846222, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0538344569504261, KL divergence=0.05076660215854645, Entropy=0.3648498058319092, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04819755628705025, KL divergence=0.0677126944065094, Entropy=0.356413871049881, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05200200155377388, KL divergence=0.08017054945230484, Entropy=0.35804834961891174, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06501360982656479, KL divergence=0.08770132809877396, Entropy=0.3567424714565277, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06959173828363419, KL divergence=0.10271509736776352, Entropy=0.35743197798728943, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.053868282586336136, KL divergence=0.10753572732210159, Entropy=0.3561278283596039, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/361_Step-117709.ckpt']
Uploaded 3 files for checkpoint 361 in 0.50 seconds
saved intermediate frozen graph: current/model/model_361.pb
Best checkpoint number: 355, Last checkpoint number: 359
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'358'}
Training> Name=main_level/agent, Worker=0, Episode=3451, Total reward=28.21, Steps=117735, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3452, Total reward=49.8, Steps=117776, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3453, Total reward=46.13, Steps=117807, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3454, Total reward=43.55, Steps=117829, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3455, Total reward=18.79, Steps=117854, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3456, Total reward=45.98, Steps=117891, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3457, Total reward=72.45, Steps=117967, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3458, Total reward=33.79, Steps=117992, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3459, Total reward=73.53, Steps=118040, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3460, Total reward=86.54, Steps=118079, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3461, Total reward=62.84, Steps=118110, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3462, Total reward=47.51, Steps=118149, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3463, Total reward=124.96, Steps=118269, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3464, Total reward=34.37, Steps=118331, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3465, Total reward=72.17, Steps=118396, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3466, Total reward=3.63, Steps=118422, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3467, Total reward=25.3, Steps=118453, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3468, Total reward=61.67, Steps=118481, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3469, Total reward=82.74, Steps=118538, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3470, Total reward=18.56, Steps=118565, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3471, Total reward=38.34, Steps=118595, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3472, Total reward=57.03, Steps=118627, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3473, Total reward=21.29, Steps=118638, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3474, Total reward=36.04, Steps=118658, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3475, Total reward=27.64, Steps=118688, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3476, Total reward=17.73, Steps=118710, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3477, Total reward=5.72, Steps=118727, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3478, Total reward=40.8, Steps=118778, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3479, Total reward=63.02, Steps=118832, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3480, Total reward=92.22, Steps=118872, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3481, Total reward=17.54, Steps=118889, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3482, Total reward=40.4, Steps=118915, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3483, Total reward=3.06, Steps=118942, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3484, Total reward=51.73, Steps=119012, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3485, Total reward=61.08, Steps=119066, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3486, Total reward=14.79, Steps=119095, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3487, Total reward=71.54, Steps=119139, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3488, Total reward=58.13, Steps=119166, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3489, Total reward=28.38, Steps=119190, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3490, Total reward=91.25, Steps=119235, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3491, Total reward=51.49, Steps=119263, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3492, Total reward=65.11, Steps=119293, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3493, Total reward=49.79, Steps=119323, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3494, Total reward=37.82, Steps=119344, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3495, Total reward=50.1, Steps=119372, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3496, Total reward=19.27, Steps=119403, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3497, Total reward=45.04, Steps=119456, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3498, Total reward=44.21, Steps=119483, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3499, Total reward=18.01, Steps=119502, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3500, Total reward=86.98, Steps=119542, Training iteration=69
Policy training> Surrogate loss=0.024237684905529022, KL divergence=0.00010741908772615716, Entropy=0.3861835300922394, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.018437517806887627, KL divergence=0.007947029545903206, Entropy=0.3864094316959381, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.048477500677108765, KL divergence=0.02606102265417576, Entropy=0.377241849899292, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.051418691873550415, KL divergence=0.04559307172894478, Entropy=0.3865804672241211, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04856623336672783, KL divergence=0.059527140110731125, Entropy=0.3723963797092438, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06319676339626312, KL divergence=0.06911931186914444, Entropy=0.377795547246933, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05486718937754631, KL divergence=0.08992262929677963, Entropy=0.3688545227050781, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05143068730831146, KL divergence=0.09396544843912125, Entropy=0.37271642684936523, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07384894043207169, KL divergence=0.11253312975168228, Entropy=0.3713701069355011, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07868239283561707, KL divergence=0.11771035194396973, Entropy=0.3710990846157074, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/362_Step-119542.ckpt']
Uploaded 3 files for checkpoint 362 in 0.59 seconds
saved intermediate frozen graph: current/model/model_362.pb
Best checkpoint number: 355, Last checkpoint number: 360
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'359'}
Training> Name=main_level/agent, Worker=0, Episode=3501, Total reward=52.12, Steps=119568, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3502, Total reward=53.51, Steps=119611, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3503, Total reward=6.44, Steps=119636, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3504, Total reward=7.68, Steps=119668, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3505, Total reward=7.98, Steps=119688, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3506, Total reward=77.83, Steps=119735, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3507, Total reward=85.29, Steps=119795, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3508, Total reward=66.21, Steps=119826, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3509, Total reward=28.59, Steps=119846, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3510, Total reward=95.5, Steps=119892, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3511, Total reward=56.74, Steps=119931, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3512, Total reward=24.77, Steps=119954, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3513, Total reward=35.07, Steps=119973, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3514, Total reward=29.24, Steps=119984, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3515, Total reward=30.94, Steps=120013, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3516, Total reward=17.75, Steps=120047, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3517, Total reward=44.61, Steps=120087, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3518, Total reward=91.97, Steps=120151, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3519, Total reward=12.43, Steps=120173, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3520, Total reward=67.23, Steps=120207, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3521, Total reward=54.26, Steps=120238, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3522, Total reward=39.45, Steps=120267, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3523, Total reward=10.09, Steps=120295, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3524, Total reward=81.47, Steps=120385, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3525, Total reward=63.81, Steps=120491, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3526, Total reward=72.46, Steps=120540, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3527, Total reward=41.73, Steps=120577, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3528, Total reward=66.74, Steps=120625, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3529, Total reward=17.03, Steps=120642, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3530, Total reward=11.19, Steps=120671, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3531, Total reward=40.34, Steps=120707, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3532, Total reward=29.2, Steps=120739, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3533, Total reward=35.13, Steps=120761, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3534, Total reward=34.9, Steps=120781, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3535, Total reward=35.38, Steps=120811, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3536, Total reward=18.94, Steps=120834, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3537, Total reward=51.87, Steps=120890, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3538, Total reward=34.68, Steps=120917, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3539, Total reward=39.05, Steps=120965, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3540, Total reward=68.76, Steps=121002, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3541, Total reward=46.88, Steps=121030, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3542, Total reward=21.32, Steps=121046, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3543, Total reward=22.77, Steps=121091, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3544, Total reward=11.36, Steps=121135, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3545, Total reward=13.35, Steps=121154, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3546, Total reward=11.05, Steps=121183, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3547, Total reward=51.22, Steps=121224, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3548, Total reward=62.76, Steps=121253, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3549, Total reward=39.19, Steps=121275, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3550, Total reward=7.73, Steps=121288, Training iteration=70
Policy training> Surrogate loss=-0.005861894693225622, KL divergence=0.00015888175403233618, Entropy=0.40498772263526917, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02976963482797146, KL divergence=0.00534396804869175, Entropy=0.4112507402896881, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.061146337538957596, KL divergence=0.015594328753650188, Entropy=0.4019857347011566, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05151849985122681, KL divergence=0.030886942520737648, Entropy=0.3937653601169586, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.061408769339323044, KL divergence=0.0425080843269825, Entropy=0.3894660472869873, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06029665470123291, KL divergence=0.05742673948407173, Entropy=0.386741042137146, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05984919145703316, KL divergence=0.06569164246320724, Entropy=0.38953033089637756, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07262799888849258, KL divergence=0.07709183543920517, Entropy=0.3824653625488281, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07070738077163696, KL divergence=0.08503079414367676, Entropy=0.3890538215637207, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06637194007635117, KL divergence=0.09326722472906113, Entropy=0.38465484976768494, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/363_Step-121288.ckpt']
Uploaded 3 files for checkpoint 363 in 0.43 seconds
saved intermediate frozen graph: current/model/model_363.pb
Best checkpoint number: 355, Last checkpoint number: 361
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'360'}
Training> Name=main_level/agent, Worker=0, Episode=3551, Total reward=53.44, Steps=121327, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3552, Total reward=74.82, Steps=121359, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3553, Total reward=32.33, Steps=121381, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3554, Total reward=29.84, Steps=121392, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3555, Total reward=17.55, Steps=121424, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3556, Total reward=24.68, Steps=121445, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3557, Total reward=49.99, Steps=121518, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3558, Total reward=29.64, Steps=121556, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3559, Total reward=15.75, Steps=121580, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3560, Total reward=55.11, Steps=121618, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3561, Total reward=47.6, Steps=121646, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3562, Total reward=28.54, Steps=121665, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3563, Total reward=12.51, Steps=121706, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3564, Total reward=72.44, Steps=121780, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3565, Total reward=70.31, Steps=121841, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3566, Total reward=86.26, Steps=121891, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3567, Total reward=66.47, Steps=121942, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3568, Total reward=62.0, Steps=121988, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3569, Total reward=21.2, Steps=122022, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3570, Total reward=106.27, Steps=122082, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3571, Total reward=48.34, Steps=122108, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3572, Total reward=39.96, Steps=122139, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3573, Total reward=0.01, Steps=122151, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3574, Total reward=27.23, Steps=122172, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3575, Total reward=35.98, Steps=122202, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3576, Total reward=18.89, Steps=122225, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3577, Total reward=37.8, Steps=122256, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3578, Total reward=31.21, Steps=122276, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3579, Total reward=81.74, Steps=122336, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3580, Total reward=101.2, Steps=122375, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3581, Total reward=51.44, Steps=122403, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3582, Total reward=138.23, Steps=122501, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3583, Total reward=8.34, Steps=122538, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3584, Total reward=0.02, Steps=122553, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3585, Total reward=14.43, Steps=122570, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3586, Total reward=64.52, Steps=122624, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3587, Total reward=45.78, Steps=122662, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3588, Total reward=71.84, Steps=122704, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3589, Total reward=90.18, Steps=122746, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3590, Total reward=49.56, Steps=122780, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3591, Total reward=45.16, Steps=122809, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3592, Total reward=3.78, Steps=122821, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3593, Total reward=26.65, Steps=122845, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3594, Total reward=49.23, Steps=122867, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3595, Total reward=19.59, Steps=122895, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3596, Total reward=18.85, Steps=122927, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3597, Total reward=55.1, Steps=122964, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3598, Total reward=26.49, Steps=122982, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3599, Total reward=89.56, Steps=123032, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3600, Total reward=83.65, Steps=123068, Training iteration=71
Policy training> Surrogate loss=-0.014624002389609814, KL divergence=9.25947679206729e-05, Entropy=0.39330971240997314, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.021120399236679077, KL divergence=0.0047129369340837, Entropy=0.3797719478607178, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03974728658795357, KL divergence=0.016423942521214485, Entropy=0.38033270835876465, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04186317324638367, KL divergence=0.030687207356095314, Entropy=0.37987780570983887, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05037315562367439, KL divergence=0.04419471696019173, Entropy=0.3744410276412964, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06200532615184784, KL divergence=0.0585990734398365, Entropy=0.37159276008605957, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05534118413925171, KL divergence=0.06875177472829819, Entropy=0.3701108992099762, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05850366875529289, KL divergence=0.07872439175844193, Entropy=0.3745112121105194, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.049967214465141296, KL divergence=0.09115666151046753, Entropy=0.3726573884487152, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.052745360881090164, KL divergence=0.09280338138341904, Entropy=0.3713846206665039, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/364_Step-123068.ckpt']
Uploaded 3 files for checkpoint 364 in 0.61 seconds
saved intermediate frozen graph: current/model/model_364.pb
Best checkpoint number: 355, Last checkpoint number: 362
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'361'}
Training> Name=main_level/agent, Worker=0, Episode=3601, Total reward=62.69, Steps=123107, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3602, Total reward=29.52, Steps=123125, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3603, Total reward=20.35, Steps=123156, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3604, Total reward=56.99, Steps=123230, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3605, Total reward=64.13, Steps=123293, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3606, Total reward=89.27, Steps=123353, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3607, Total reward=38.31, Steps=123392, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3608, Total reward=67.3, Steps=123423, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3609, Total reward=34.19, Steps=123469, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3610, Total reward=93.09, Steps=123518, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3611, Total reward=67.28, Steps=123557, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3612, Total reward=27.16, Steps=123589, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3613, Total reward=42.44, Steps=123616, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3614, Total reward=41.16, Steps=123638, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3615, Total reward=27.46, Steps=123666, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3616, Total reward=15.29, Steps=123700, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3617, Total reward=51.02, Steps=123734, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3618, Total reward=27.87, Steps=123759, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3619, Total reward=17.15, Steps=123783, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3620, Total reward=88.83, Steps=123823, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3621, Total reward=57.92, Steps=123850, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3622, Total reward=30.03, Steps=123870, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3623, Total reward=95.46, Steps=123950, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3624, Total reward=7.16, Steps=123981, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3625, Total reward=10.28, Steps=124001, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3626, Total reward=76.27, Steps=124048, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3627, Total reward=43.44, Steps=124086, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3628, Total reward=62.14, Steps=124114, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3629, Total reward=21.39, Steps=124130, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3630, Total reward=60.46, Steps=124166, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3631, Total reward=62.3, Steps=124206, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3632, Total reward=40.22, Steps=124236, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3633, Total reward=52.55, Steps=124267, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3634, Total reward=39.35, Steps=124288, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3635, Total reward=32.13, Steps=124316, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3636, Total reward=21.87, Steps=124335, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3637, Total reward=112.34, Steps=124417, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3638, Total reward=30.47, Steps=124437, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3639, Total reward=20.97, Steps=124458, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3640, Total reward=90.1, Steps=124499, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3641, Total reward=57.72, Steps=124530, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3642, Total reward=34.07, Steps=124549, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3643, Total reward=72.87, Steps=124646, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3644, Total reward=23.25, Steps=124679, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3645, Total reward=30.86, Steps=124719, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3646, Total reward=78.96, Steps=124767, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3647, Total reward=68.67, Steps=124809, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3648, Total reward=61.72, Steps=124841, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3649, Total reward=83.69, Steps=124897, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3650, Total reward=49.71, Steps=124949, Training iteration=72
Policy training> Surrogate loss=0.005338404327630997, KL divergence=8.792614244157448e-05, Entropy=0.4216892719268799, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.021518319845199585, KL divergence=0.004696324001997709, Entropy=0.4311753809452057, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.034899238497018814, KL divergence=0.017521468922495842, Entropy=0.4159615933895111, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06147749349474907, KL divergence=0.031191477552056313, Entropy=0.40936270356178284, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04211186245083809, KL divergence=0.043493807315826416, Entropy=0.41339173913002014, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06091516837477684, KL divergence=0.05677109584212303, Entropy=0.406104177236557, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06317397207021713, KL divergence=0.06984702497720718, Entropy=0.3976156413555145, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.053087491542100906, KL divergence=0.07822523266077042, Entropy=0.4010235369205475, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08771255612373352, KL divergence=0.08674613386392593, Entropy=0.40174761414527893, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09412229806184769, KL divergence=0.09306466579437256, Entropy=0.404887318611145, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/365_Step-124949.ckpt']
Uploaded 3 files for checkpoint 365 in 0.58 seconds
saved intermediate frozen graph: current/model/model_365.pb
Best checkpoint number: 355, Last checkpoint number: 363
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'362'}
Training> Name=main_level/agent, Worker=0, Episode=3651, Total reward=62.78, Steps=124975, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3652, Total reward=57.79, Steps=125012, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3653, Total reward=36.47, Steps=125041, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3654, Total reward=46.78, Steps=125062, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3655, Total reward=34.29, Steps=125091, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3656, Total reward=11.95, Steps=125110, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3657, Total reward=15.69, Steps=125122, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3658, Total reward=24.84, Steps=125162, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3659, Total reward=67.41, Steps=125208, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3660, Total reward=79.1, Steps=125248, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3661, Total reward=60.07, Steps=125276, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3662, Total reward=42.92, Steps=125299, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3663, Total reward=3.48, Steps=125319, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3664, Total reward=46.48, Steps=125389, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3665, Total reward=14.74, Steps=125414, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3666, Total reward=35.86, Steps=125464, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3667, Total reward=50.47, Steps=125502, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3668, Total reward=62.28, Steps=125532, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3669, Total reward=31.23, Steps=125554, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3670, Total reward=90.38, Steps=125604, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3671, Total reward=53.3, Steps=125641, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3672, Total reward=49.02, Steps=125672, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3673, Total reward=37.4, Steps=125692, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3674, Total reward=45.14, Steps=125712, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3675, Total reward=40.79, Steps=125740, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3676, Total reward=19.08, Steps=125758, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3677, Total reward=16.55, Steps=125778, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3678, Total reward=39.83, Steps=125798, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3679, Total reward=82.86, Steps=125848, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3680, Total reward=70.64, Steps=125887, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3681, Total reward=46.29, Steps=125914, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3682, Total reward=30.02, Steps=125943, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3683, Total reward=0.02, Steps=125959, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3684, Total reward=8.15, Steps=125981, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3685, Total reward=22.83, Steps=126018, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3686, Total reward=80.75, Steps=126078, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3687, Total reward=68.09, Steps=126118, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3688, Total reward=82.62, Steps=126179, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3689, Total reward=52.9, Steps=126233, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3690, Total reward=81.91, Steps=126280, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3691, Total reward=93.79, Steps=126327, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3692, Total reward=36.62, Steps=126355, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3693, Total reward=28.15, Steps=126376, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3694, Total reward=49.13, Steps=126399, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3695, Total reward=26.7, Steps=126425, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3696, Total reward=19.35, Steps=126440, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3697, Total reward=37.22, Steps=126476, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3698, Total reward=17.86, Steps=126495, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3699, Total reward=17.18, Steps=126532, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3700, Total reward=75.42, Steps=126567, Training iteration=73
Policy training> Surrogate loss=0.0036143381148576736, KL divergence=0.00021629246475640684, Entropy=0.4032205641269684, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.027916336432099342, KL divergence=0.00836766418069601, Entropy=0.39768075942993164, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.043131500482559204, KL divergence=0.021045396104454994, Entropy=0.3986012935638428, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04100310057401657, KL divergence=0.03444882109761238, Entropy=0.3921926021575928, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06370101124048233, KL divergence=0.04759958013892174, Entropy=0.38756999373435974, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.059198055416345596, KL divergence=0.06327465921640396, Entropy=0.3852514922618866, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06444442272186279, KL divergence=0.07630949467420578, Entropy=0.38476109504699707, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06219625473022461, KL divergence=0.08537578582763672, Entropy=0.3797168433666229, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07849951833486557, KL divergence=0.09357502311468124, Entropy=0.3779222071170807, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0610123872756958, KL divergence=0.1015135645866394, Entropy=0.3823273181915283, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/366_Step-126567.ckpt']
Uploaded 3 files for checkpoint 366 in 0.59 seconds
saved intermediate frozen graph: current/model/model_366.pb
Best checkpoint number: 355, Last checkpoint number: 364
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'363'}
Training> Name=main_level/agent, Worker=0, Episode=3701, Total reward=27.64, Steps=126589, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3702, Total reward=148.64, Steps=126729, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3703, Total reward=45.18, Steps=126784, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3704, Total reward=19.48, Steps=126820, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3705, Total reward=82.68, Steps=126879, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3706, Total reward=86.24, Steps=126927, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3707, Total reward=37.73, Steps=126961, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3708, Total reward=58.82, Steps=126990, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3709, Total reward=44.79, Steps=127026, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3710, Total reward=64.22, Steps=127070, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3711, Total reward=54.55, Steps=127108, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3712, Total reward=50.18, Steps=127140, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3713, Total reward=46.82, Steps=127168, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3714, Total reward=43.34, Steps=127189, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3715, Total reward=31.58, Steps=127217, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3716, Total reward=19.75, Steps=127237, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3717, Total reward=57.77, Steps=127274, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3718, Total reward=92.57, Steps=127338, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3719, Total reward=38.08, Steps=127377, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3720, Total reward=83.43, Steps=127415, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3721, Total reward=63.18, Steps=127446, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3722, Total reward=31.64, Steps=127468, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3723, Total reward=12.53, Steps=127495, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3724, Total reward=23.52, Steps=127524, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3725, Total reward=9.28, Steps=127548, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3726, Total reward=46.57, Steps=127595, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3727, Total reward=122.13, Steps=127702, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3728, Total reward=64.23, Steps=127730, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3729, Total reward=28.41, Steps=127759, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3730, Total reward=27.43, Steps=127796, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3731, Total reward=60.19, Steps=127836, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3732, Total reward=53.65, Steps=127868, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3733, Total reward=37.86, Steps=127891, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3734, Total reward=40.31, Steps=127914, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3735, Total reward=9.65, Steps=127926, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3736, Total reward=21.08, Steps=127945, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3737, Total reward=38.66, Steps=127999, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3738, Total reward=33.24, Steps=128020, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3739, Total reward=79.96, Steps=128062, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3740, Total reward=83.82, Steps=128100, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3741, Total reward=48.16, Steps=128131, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3742, Total reward=40.86, Steps=128156, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3743, Total reward=18.49, Steps=128183, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3744, Total reward=27.59, Steps=128215, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3745, Total reward=10.01, Steps=128237, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3746, Total reward=121.39, Steps=128320, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3747, Total reward=65.93, Steps=128358, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3748, Total reward=67.75, Steps=128390, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3749, Total reward=85.38, Steps=128435, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3750, Total reward=58.56, Steps=128470, Training iteration=74
Policy training> Surrogate loss=0.010660749860107899, KL divergence=8.566624455852434e-05, Entropy=0.4191025197505951, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0419866107404232, KL divergence=0.0062372698448598385, Entropy=0.42549368739128113, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03873129561543465, KL divergence=0.020861735567450523, Entropy=0.41130709648132324, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05679932236671448, KL divergence=0.03793566673994064, Entropy=0.4073077142238617, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06661193817853928, KL divergence=0.05239484831690788, Entropy=0.38934555649757385, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07040747255086899, KL divergence=0.06643175333738327, Entropy=0.39617475867271423, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07043804973363876, KL divergence=0.07568889111280441, Entropy=0.3937058746814728, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05571500584483147, KL divergence=0.09057401865720749, Entropy=0.39228716492652893, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06701039522886276, KL divergence=0.09653501957654953, Entropy=0.39472469687461853, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0646716058254242, KL divergence=0.10749015212059021, Entropy=0.38154590129852295, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/367_Step-128470.ckpt']
Uploaded 3 files for checkpoint 367 in 0.61 seconds
saved intermediate frozen graph: current/model/model_367.pb
Best checkpoint number: 355, Last checkpoint number: 365
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'364'}
Training> Name=main_level/agent, Worker=0, Episode=3751, Total reward=22.83, Steps=128483, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3752, Total reward=52.37, Steps=128514, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3753, Total reward=48.05, Steps=128541, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3754, Total reward=37.53, Steps=128563, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3755, Total reward=14.96, Steps=128587, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3756, Total reward=23.41, Steps=128607, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3757, Total reward=52.08, Steps=128677, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3758, Total reward=95.43, Steps=128740, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3759, Total reward=77.41, Steps=128793, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3760, Total reward=70.22, Steps=128828, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3761, Total reward=51.19, Steps=128856, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3762, Total reward=33.37, Steps=128878, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3763, Total reward=9.74, Steps=128900, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3764, Total reward=11.93, Steps=128930, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3765, Total reward=77.59, Steps=128996, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3766, Total reward=83.71, Steps=129044, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3767, Total reward=55.46, Steps=129082, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3768, Total reward=66.79, Steps=129121, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3769, Total reward=36.16, Steps=129147, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3770, Total reward=58.64, Steps=129193, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3771, Total reward=54.77, Steps=129232, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3772, Total reward=28.44, Steps=129257, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3773, Total reward=37.0, Steps=129286, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3774, Total reward=49.11, Steps=129308, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3775, Total reward=18.29, Steps=129337, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3776, Total reward=23.33, Steps=129366, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3777, Total reward=40.44, Steps=129403, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3778, Total reward=35.81, Steps=129429, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3779, Total reward=5.65, Steps=129443, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3780, Total reward=81.94, Steps=129482, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3781, Total reward=36.64, Steps=129509, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3782, Total reward=38.33, Steps=129535, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3783, Total reward=72.91, Steps=129610, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3784, Total reward=46.78, Steps=129682, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3785, Total reward=13.06, Steps=129697, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3786, Total reward=64.09, Steps=129743, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3787, Total reward=62.86, Steps=129786, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3788, Total reward=76.93, Steps=129839, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3789, Total reward=20.75, Steps=129857, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3790, Total reward=97.84, Steps=129907, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3791, Total reward=56.39, Steps=129947, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3792, Total reward=8.66, Steps=129966, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3793, Total reward=47.01, Steps=129996, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3794, Total reward=36.97, Steps=130018, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3795, Total reward=24.1, Steps=130045, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3796, Total reward=30.78, Steps=130076, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3797, Total reward=40.53, Steps=130113, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3798, Total reward=31.12, Steps=130135, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3799, Total reward=15.54, Steps=130156, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3800, Total reward=87.07, Steps=130198, Training iteration=75
Policy training> Surrogate loss=0.0008950997143983841, KL divergence=5.0208789616590366e-05, Entropy=0.4162667691707611, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03646383062005043, KL divergence=0.0038603420834988356, Entropy=0.4095713198184967, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05303652584552765, KL divergence=0.013734069652855396, Entropy=0.4004259407520294, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05636042356491089, KL divergence=0.0265047550201416, Entropy=0.3967820405960083, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05282210186123848, KL divergence=0.03834116831421852, Entropy=0.39651381969451904, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05612535402178764, KL divergence=0.04770462587475777, Entropy=0.3979848325252533, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05547453463077545, KL divergence=0.06178004667162895, Entropy=0.39266371726989746, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06307369470596313, KL divergence=0.06861913949251175, Entropy=0.3974950611591339, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05612850561738014, KL divergence=0.08019360154867172, Entropy=0.40015044808387756, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07635214179754257, KL divergence=0.08398574590682983, Entropy=0.3974328339099884, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/368_Step-130198.ckpt']
Uploaded 3 files for checkpoint 368 in 0.60 seconds
saved intermediate frozen graph: current/model/model_368.pb
Best checkpoint number: 355, Last checkpoint number: 366
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'365'}
Training> Name=main_level/agent, Worker=0, Episode=3801, Total reward=64.8, Steps=130229, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3802, Total reward=32.91, Steps=130247, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3803, Total reward=3.16, Steps=130268, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3804, Total reward=70.72, Steps=130341, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3805, Total reward=50.58, Steps=130395, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3806, Total reward=76.97, Steps=130444, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3807, Total reward=63.49, Steps=130484, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3808, Total reward=73.64, Steps=130533, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3809, Total reward=31.61, Steps=130563, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3810, Total reward=49.39, Steps=130614, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3811, Total reward=71.5, Steps=130649, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3812, Total reward=65.77, Steps=130693, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3813, Total reward=46.32, Steps=130721, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3814, Total reward=39.46, Steps=130742, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3815, Total reward=18.98, Steps=130770, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3816, Total reward=29.51, Steps=130788, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3817, Total reward=41.6, Steps=130823, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3818, Total reward=40.55, Steps=130848, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3819, Total reward=25.45, Steps=130870, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3820, Total reward=60.22, Steps=130903, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3821, Total reward=21.86, Steps=130924, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3822, Total reward=34.07, Steps=130951, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3823, Total reward=5.18, Steps=130987, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3824, Total reward=10.61, Steps=131010, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3825, Total reward=24.67, Steps=131067, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3826, Total reward=10.89, Steps=131090, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3827, Total reward=65.5, Steps=131133, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3828, Total reward=56.0, Steps=131163, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3829, Total reward=28.88, Steps=131187, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3830, Total reward=77.13, Steps=131234, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3831, Total reward=49.04, Steps=131260, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3832, Total reward=50.46, Steps=131291, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3833, Total reward=56.56, Steps=131321, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3834, Total reward=34.55, Steps=131341, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3835, Total reward=51.35, Steps=131369, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3836, Total reward=29.94, Steps=131405, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3837, Total reward=44.91, Steps=131441, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3838, Total reward=85.69, Steps=131504, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3839, Total reward=18.7, Steps=131527, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3840, Total reward=44.95, Steps=131562, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3841, Total reward=59.06, Steps=131596, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3842, Total reward=32.56, Steps=131615, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3843, Total reward=17.48, Steps=131652, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3844, Total reward=2.81, Steps=131667, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3845, Total reward=6.03, Steps=131684, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3846, Total reward=84.35, Steps=131734, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3847, Total reward=33.91, Steps=131768, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3848, Total reward=63.74, Steps=131796, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3849, Total reward=68.18, Steps=131848, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3850, Total reward=81.16, Steps=131899, Training iteration=76
Policy training> Surrogate loss=-0.005919656250625849, KL divergence=8.558952686144039e-05, Entropy=0.3907637298107147, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03928570821881294, KL divergence=0.005645092576742172, Entropy=0.38542309403419495, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.044201355427503586, KL divergence=0.01740310899913311, Entropy=0.37922072410583496, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.039016060531139374, KL divergence=0.03169256076216698, Entropy=0.3737463057041168, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06289844214916229, KL divergence=0.04559744521975517, Entropy=0.36765941977500916, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06223773583769798, KL divergence=0.057619642466306686, Entropy=0.3633381426334381, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06029495596885681, KL divergence=0.06907543540000916, Entropy=0.364896684885025, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.04790569841861725, KL divergence=0.07700294256210327, Entropy=0.3706829845905304, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05467023327946663, KL divergence=0.0868816152215004, Entropy=0.3712058961391449, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06014519929885864, KL divergence=0.09022548049688339, Entropy=0.3679867088794708, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/369_Step-131899.ckpt']
Uploaded 3 files for checkpoint 369 in 0.56 seconds
saved intermediate frozen graph: current/model/model_369.pb
Best checkpoint number: 355, Last checkpoint number: 367
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'366'}
Training> Name=main_level/agent, Worker=0, Episode=3851, Total reward=72.21, Steps=131943, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3852, Total reward=53.08, Steps=131975, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3853, Total reward=47.68, Steps=132004, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3854, Total reward=50.54, Steps=132027, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3855, Total reward=35.68, Steps=132053, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3856, Total reward=21.68, Steps=132083, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3857, Total reward=31.47, Steps=132120, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3858, Total reward=39.94, Steps=132153, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3859, Total reward=54.65, Steps=132208, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3860, Total reward=40.6, Steps=132246, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3861, Total reward=50.86, Steps=132274, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3862, Total reward=29.54, Steps=132292, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3863, Total reward=20.34, Steps=132334, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3864, Total reward=11.8, Steps=132365, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3865, Total reward=49.99, Steps=132427, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3866, Total reward=75.66, Steps=132493, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3867, Total reward=64.08, Steps=132549, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3868, Total reward=59.64, Steps=132580, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3869, Total reward=22.71, Steps=132596, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3870, Total reward=94.84, Steps=132649, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3871, Total reward=38.7, Steps=132679, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3872, Total reward=45.6, Steps=132709, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3873, Total reward=41.56, Steps=132738, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3874, Total reward=30.27, Steps=132759, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3875, Total reward=31.18, Steps=132790, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3876, Total reward=32.98, Steps=132835, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3877, Total reward=7.82, Steps=132847, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3878, Total reward=40.86, Steps=132871, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3879, Total reward=79.18, Steps=132922, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3880, Total reward=70.86, Steps=132956, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3881, Total reward=58.01, Steps=132997, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3882, Total reward=39.62, Steps=133022, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3883, Total reward=56.66, Steps=133109, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3884, Total reward=20.36, Steps=133158, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3885, Total reward=71.93, Steps=133239, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3886, Total reward=94.15, Steps=133289, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3887, Total reward=66.54, Steps=133331, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3888, Total reward=79.22, Steps=133379, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3889, Total reward=33.59, Steps=133405, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3890, Total reward=53.89, Steps=133440, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3891, Total reward=23.4, Steps=133464, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3892, Total reward=47.91, Steps=133495, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3893, Total reward=39.73, Steps=133525, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3894, Total reward=33.45, Steps=133546, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3895, Total reward=30.5, Steps=133589, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3896, Total reward=20.49, Steps=133621, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3897, Total reward=46.21, Steps=133654, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3898, Total reward=35.16, Steps=133680, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3899, Total reward=9.96, Steps=133692, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3900, Total reward=81.76, Steps=133726, Training iteration=77
Policy training> Surrogate loss=-0.004676889628171921, KL divergence=0.00016348833742085844, Entropy=0.4263364374637604, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02340291626751423, KL divergence=0.0062958556227386, Entropy=0.4177300035953522, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04557548835873604, KL divergence=0.018999973312020302, Entropy=0.4123661518096924, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05948038771748543, KL divergence=0.03467676416039467, Entropy=0.4079975187778473, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05223062261939049, KL divergence=0.05071411654353142, Entropy=0.4008070230484009, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05370722711086273, KL divergence=0.06247992441058159, Entropy=0.3892867863178253, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05012953281402588, KL divergence=0.07035966962575912, Entropy=0.39826712012290955, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05191847309470177, KL divergence=0.08499029278755188, Entropy=0.38973185420036316, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.061187755316495895, KL divergence=0.09203534573316574, Entropy=0.3846471607685089, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0821119174361229, KL divergence=0.10028066486120224, Entropy=0.3919776976108551, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/370_Step-133726.ckpt']
Uploaded 3 files for checkpoint 370 in 0.60 seconds
saved intermediate frozen graph: current/model/model_370.pb
Best checkpoint number: 355, Last checkpoint number: 368
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'367'}
Training> Name=main_level/agent, Worker=0, Episode=3901, Total reward=63.16, Steps=133755, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3902, Total reward=44.45, Steps=133806, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3903, Total reward=24.26, Steps=133848, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3904, Total reward=81.56, Steps=133941, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3905, Total reward=75.62, Steps=134028, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3906, Total reward=86.75, Steps=134086, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3907, Total reward=70.9, Steps=134128, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3908, Total reward=69.84, Steps=134157, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3909, Total reward=31.45, Steps=134181, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3910, Total reward=81.75, Steps=134226, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3911, Total reward=34.78, Steps=134253, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3912, Total reward=60.33, Steps=134285, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3913, Total reward=38.73, Steps=134310, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3914, Total reward=37.99, Steps=134331, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3915, Total reward=38.28, Steps=134361, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3916, Total reward=34.94, Steps=134393, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3917, Total reward=51.28, Steps=134427, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3918, Total reward=42.15, Steps=134453, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3919, Total reward=55.92, Steps=134512, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3920, Total reward=91.45, Steps=134559, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3921, Total reward=31.44, Steps=134577, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3922, Total reward=38.1, Steps=134604, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3923, Total reward=0.03, Steps=134632, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3924, Total reward=7.51, Steps=134664, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3925, Total reward=19.74, Steps=134680, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3926, Total reward=84.22, Steps=134728, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3927, Total reward=63.13, Steps=134771, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3928, Total reward=73.12, Steps=134809, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3929, Total reward=81.06, Steps=134868, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3930, Total reward=71.95, Steps=134904, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3931, Total reward=33.36, Steps=134931, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3932, Total reward=39.0, Steps=134960, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3933, Total reward=53.85, Steps=134991, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3934, Total reward=39.72, Steps=135012, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3935, Total reward=31.73, Steps=135041, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3936, Total reward=20.05, Steps=135076, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3937, Total reward=61.1, Steps=135112, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3938, Total reward=25.93, Steps=135136, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3939, Total reward=0.01, Steps=135145, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3940, Total reward=15.21, Steps=135161, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3941, Total reward=48.46, Steps=135189, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3942, Total reward=42.4, Steps=135217, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3943, Total reward=21.75, Steps=135250, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3944, Total reward=83.86, Steps=135326, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3945, Total reward=19.22, Steps=135357, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3946, Total reward=79.08, Steps=135403, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3947, Total reward=68.77, Steps=135444, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3948, Total reward=127.42, Steps=135514, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3949, Total reward=17.29, Steps=135528, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3950, Total reward=25.38, Steps=135551, Training iteration=78
Policy training> Surrogate loss=0.005065412726253271, KL divergence=0.00010539608774706721, Entropy=0.41078758239746094, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.018490463495254517, KL divergence=0.00429714797064662, Entropy=0.41447344422340393, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.030002763494849205, KL divergence=0.013944379985332489, Entropy=0.41521239280700684, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03733653202652931, KL divergence=0.024929940700531006, Entropy=0.41110146045684814, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05486041307449341, KL divergence=0.036570727825164795, Entropy=0.39487025141716003, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05828350409865379, KL divergence=0.04857062175869942, Entropy=0.3979942798614502, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06456432491540909, KL divergence=0.05935405567288399, Entropy=0.3941907584667206, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.045348551124334335, KL divergence=0.06785064190626144, Entropy=0.39218270778656006, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05666736885905266, KL divergence=0.07257513701915741, Entropy=0.4014030992984772, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06026456877589226, KL divergence=0.0829840674996376, Entropy=0.4006270468235016, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/371_Step-135551.ckpt']
Uploaded 3 files for checkpoint 371 in 0.59 seconds
saved intermediate frozen graph: current/model/model_371.pb
Best checkpoint number: 355, Last checkpoint number: 369
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'368'}
Training> Name=main_level/agent, Worker=0, Episode=3951, Total reward=35.02, Steps=135579, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3952, Total reward=50.59, Steps=135610, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3953, Total reward=55.29, Steps=135639, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3954, Total reward=29.03, Steps=135650, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3955, Total reward=31.44, Steps=135682, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3956, Total reward=22.25, Steps=135701, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3957, Total reward=17.55, Steps=135721, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3958, Total reward=41.64, Steps=135744, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3959, Total reward=84.35, Steps=135800, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3960, Total reward=98.35, Steps=135859, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3961, Total reward=59.84, Steps=135890, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3962, Total reward=28.14, Steps=135916, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3963, Total reward=21.23, Steps=135977, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3964, Total reward=7.59, Steps=136012, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3965, Total reward=11.14, Steps=136032, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3966, Total reward=90.95, Steps=136084, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3967, Total reward=52.19, Steps=136124, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3968, Total reward=98.84, Steps=136189, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3969, Total reward=72.48, Steps=136232, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3970, Total reward=30.9, Steps=136272, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3971, Total reward=18.96, Steps=136301, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3972, Total reward=59.51, Steps=136334, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3973, Total reward=36.67, Steps=136354, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3974, Total reward=10.78, Steps=136373, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3975, Total reward=19.24, Steps=136389, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3976, Total reward=16.73, Steps=136410, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3977, Total reward=33.77, Steps=136444, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3978, Total reward=39.37, Steps=136493, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3979, Total reward=60.55, Steps=136549, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3980, Total reward=57.99, Steps=136586, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3981, Total reward=59.76, Steps=136625, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3982, Total reward=45.04, Steps=136650, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3983, Total reward=16.19, Steps=136697, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3984, Total reward=5.84, Steps=136717, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3985, Total reward=17.59, Steps=136749, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3986, Total reward=61.4, Steps=136804, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3987, Total reward=76.52, Steps=136845, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3988, Total reward=53.51, Steps=136871, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3989, Total reward=32.96, Steps=136903, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3990, Total reward=42.11, Steps=136934, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3991, Total reward=26.42, Steps=136959, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3992, Total reward=34.74, Steps=136989, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3993, Total reward=35.0, Steps=137008, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3994, Total reward=36.23, Steps=137029, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3995, Total reward=16.95, Steps=137043, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3996, Total reward=19.87, Steps=137077, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3997, Total reward=44.94, Steps=137112, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3998, Total reward=43.99, Steps=137165, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3999, Total reward=13.67, Steps=137180, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=4000, Total reward=79.68, Steps=137230, Training iteration=79
Policy training> Surrogate loss=-0.0019201996037736535, KL divergence=7.139881927287206e-05, Entropy=0.42427968978881836, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0244356170296669, KL divergence=0.005140363238751888, Entropy=0.42191508412361145, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.052452702075242996, KL divergence=0.017751842737197876, Entropy=0.4076954126358032, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.039267901331186295, KL divergence=0.03213288262486458, Entropy=0.39574241638183594, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.046223413199186325, KL divergence=0.043734028935432434, Entropy=0.4000972509384155, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05269639194011688, KL divergence=0.053476523607969284, Entropy=0.40108874440193176, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07445036619901657, KL divergence=0.06742268800735474, Entropy=0.394924521446228, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.058808837085962296, KL divergence=0.07812991738319397, Entropy=0.39644762873649597, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0677032545208931, KL divergence=0.08677208423614502, Entropy=0.39924106001853943, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06513766199350357, KL divergence=0.09464287012815475, Entropy=0.39990171790122986, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/372_Step-137230.ckpt']
Uploaded 3 files for checkpoint 372 in 0.45 seconds
saved intermediate frozen graph: current/model/model_372.pb
Best checkpoint number: 355, Last checkpoint number: 370
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'369'}
Training> Name=main_level/agent, Worker=0, Episode=4001, Total reward=50.27, Steps=137258, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4002, Total reward=36.85, Steps=137282, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4003, Total reward=8.11, Steps=137321, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4004, Total reward=14.69, Steps=137354, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4005, Total reward=13.15, Steps=137371, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4006, Total reward=55.08, Steps=137420, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4007, Total reward=73.61, Steps=137475, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4008, Total reward=69.85, Steps=137516, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4009, Total reward=33.85, Steps=137541, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4010, Total reward=49.48, Steps=137579, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4011, Total reward=61.71, Steps=137618, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4012, Total reward=54.45, Steps=137650, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4013, Total reward=19.4, Steps=137661, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4014, Total reward=46.26, Steps=137683, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4015, Total reward=33.88, Steps=137708, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4016, Total reward=22.81, Steps=137740, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4017, Total reward=94.82, Steps=137833, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4018, Total reward=34.66, Steps=137854, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4019, Total reward=15.56, Steps=137876, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4020, Total reward=55.77, Steps=137917, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4021, Total reward=60.67, Steps=137947, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4022, Total reward=32.5, Steps=137965, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4023, Total reward=23.62, Steps=138006, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4024, Total reward=11.15, Steps=138038, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4025, Total reward=11.65, Steps=138057, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4026, Total reward=79.75, Steps=138104, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4027, Total reward=55.83, Steps=138143, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4028, Total reward=77.23, Steps=138194, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4029, Total reward=74.14, Steps=138254, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4030, Total reward=75.59, Steps=138293, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4031, Total reward=15.76, Steps=138317, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4032, Total reward=33.73, Steps=138348, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4033, Total reward=57.41, Steps=138377, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4034, Total reward=41.56, Steps=138399, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4035, Total reward=17.9, Steps=138431, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4036, Total reward=17.43, Steps=138464, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4037, Total reward=41.27, Steps=138500, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4038, Total reward=33.34, Steps=138525, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4039, Total reward=82.35, Steps=138575, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4040, Total reward=19.04, Steps=138593, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4041, Total reward=57.95, Steps=138622, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4042, Total reward=41.47, Steps=138647, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4043, Total reward=15.12, Steps=138705, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4044, Total reward=4.5, Steps=138736, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4045, Total reward=23.63, Steps=138757, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4046, Total reward=77.5, Steps=138813, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4047, Total reward=53.84, Steps=138854, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4048, Total reward=60.16, Steps=138898, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4049, Total reward=19.95, Steps=138912, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4050, Total reward=45.7, Steps=138952, Training iteration=80
Policy training> Surrogate loss=-0.017723536118865013, KL divergence=0.00016999890794977546, Entropy=0.41926026344299316, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04147579148411751, KL divergence=0.005855214316397905, Entropy=0.4169154465198517, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.033849120140075684, KL divergence=0.019492683932185173, Entropy=0.4138258993625641, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05853966996073723, KL divergence=0.035344675183296204, Entropy=0.4054394066333771, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.046931590884923935, KL divergence=0.048808276653289795, Entropy=0.3958098590373993, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06006771698594093, KL divergence=0.06272049993276596, Entropy=0.40236568450927734, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06919296085834503, KL divergence=0.0733172595500946, Entropy=0.39731109142303467, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05045950040221214, KL divergence=0.07841850072145462, Entropy=0.3872183859348297, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0643559917807579, KL divergence=0.09033707529306412, Entropy=0.3934539258480072, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07325714081525803, KL divergence=0.09639596939086914, Entropy=0.3963582515716553, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/373_Step-138952.ckpt']
Uploaded 3 files for checkpoint 373 in 0.59 seconds
saved intermediate frozen graph: current/model/model_373.pb
Best checkpoint number: 355, Last checkpoint number: 371
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'370'}
Training> Name=main_level/agent, Worker=0, Episode=4051, Total reward=28.19, Steps=138978, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4052, Total reward=36.77, Steps=139005, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4053, Total reward=49.21, Steps=139033, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4054, Total reward=47.38, Steps=139068, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4055, Total reward=24.72, Steps=139091, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4056, Total reward=18.16, Steps=139117, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4057, Total reward=47.78, Steps=139150, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4058, Total reward=99.76, Steps=139218, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4059, Total reward=1.64, Steps=139231, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4060, Total reward=86.7, Steps=139276, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4061, Total reward=51.94, Steps=139309, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4062, Total reward=20.72, Steps=139326, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4063, Total reward=3.21, Steps=139349, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4064, Total reward=8.19, Steps=139371, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4065, Total reward=13.73, Steps=139392, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4066, Total reward=80.47, Steps=139438, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4067, Total reward=66.4, Steps=139486, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4068, Total reward=82.06, Steps=139538, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4069, Total reward=23.7, Steps=139557, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4070, Total reward=87.8, Steps=139606, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4071, Total reward=45.26, Steps=139636, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4072, Total reward=3.77, Steps=139648, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4073, Total reward=58.17, Steps=139679, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4074, Total reward=39.56, Steps=139701, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4075, Total reward=29.8, Steps=139729, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4076, Total reward=23.41, Steps=139760, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4077, Total reward=48.39, Steps=139801, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4078, Total reward=30.52, Steps=139823, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4079, Total reward=20.54, Steps=139855, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4080, Total reward=79.06, Steps=139893, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4081, Total reward=40.88, Steps=139917, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4082, Total reward=35.3, Steps=139936, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4083, Total reward=3.52, Steps=139956, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4084, Total reward=6.87, Steps=139991, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4085, Total reward=19.9, Steps=140017, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4086, Total reward=73.84, Steps=140083, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4087, Total reward=38.76, Steps=140123, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4088, Total reward=67.86, Steps=140171, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4089, Total reward=35.9, Steps=140206, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4090, Total reward=50.86, Steps=140240, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4091, Total reward=46.85, Steps=140278, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4092, Total reward=64.36, Steps=140319, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4093, Total reward=44.41, Steps=140346, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4094, Total reward=41.5, Steps=140368, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4095, Total reward=13.93, Steps=140394, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4096, Total reward=17.79, Steps=140433, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4097, Total reward=12.18, Steps=140446, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4098, Total reward=36.17, Steps=140467, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4099, Total reward=8.49, Steps=140481, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4100, Total reward=79.92, Steps=140517, Training iteration=81
Policy training> Surrogate loss=0.002637801691889763, KL divergence=0.00011371175787644461, Entropy=0.40661969780921936, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03431732580065727, KL divergence=0.006238250061869621, Entropy=0.4029490053653717, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.042511776089668274, KL divergence=0.018038420006632805, Entropy=0.3906721770763397, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04644867405295372, KL divergence=0.034138984978199005, Entropy=0.38440045714378357, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05883072689175606, KL divergence=0.04947398602962494, Entropy=0.3777535855770111, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05808013305068016, KL divergence=0.06365513801574707, Entropy=0.37585362792015076, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05916616693139076, KL divergence=0.07581520080566406, Entropy=0.3726001977920532, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05940118804574013, KL divergence=0.08516258746385574, Entropy=0.3729006350040436, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06388179212808609, KL divergence=0.09437012672424316, Entropy=0.37198296189308167, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06501040607690811, KL divergence=0.09724190831184387, Entropy=0.37081655859947205, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/374_Step-140517.ckpt']
Uploaded 3 files for checkpoint 374 in 0.56 seconds
saved intermediate frozen graph: current/model/model_374.pb
Best checkpoint number: 355, Last checkpoint number: 372
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'371'}
Training> Name=main_level/agent, Worker=0, Episode=4101, Total reward=62.57, Steps=140548, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4102, Total reward=38.41, Steps=140567, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4103, Total reward=69.57, Steps=140651, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4104, Total reward=18.24, Steps=140678, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4105, Total reward=30.71, Steps=140713, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4106, Total reward=56.96, Steps=140751, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4107, Total reward=66.37, Steps=140790, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4108, Total reward=104.73, Steps=140856, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4109, Total reward=73.51, Steps=140905, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4110, Total reward=50.59, Steps=140945, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4111, Total reward=7.44, Steps=140956, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4112, Total reward=69.37, Steps=140998, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4113, Total reward=37.14, Steps=141018, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4114, Total reward=26.81, Steps=141039, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4115, Total reward=19.96, Steps=141067, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4116, Total reward=21.45, Steps=141102, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4117, Total reward=49.61, Steps=141138, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4118, Total reward=34.83, Steps=141162, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4119, Total reward=19.01, Steps=141180, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4120, Total reward=86.5, Steps=141235, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4121, Total reward=6.68, Steps=141247, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4122, Total reward=43.19, Steps=141273, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4123, Total reward=3.73, Steps=141291, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4124, Total reward=162.15, Steps=141392, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4125, Total reward=105.67, Steps=141473, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4126, Total reward=78.08, Steps=141534, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4127, Total reward=73.24, Steps=141578, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4128, Total reward=70.46, Steps=141618, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4129, Total reward=87.9, Steps=141678, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4130, Total reward=11.02, Steps=141692, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4131, Total reward=28.91, Steps=141720, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4132, Total reward=44.22, Steps=141756, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4133, Total reward=42.25, Steps=141784, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4134, Total reward=38.97, Steps=141804, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4135, Total reward=19.42, Steps=141836, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4136, Total reward=21.52, Steps=141853, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4137, Total reward=69.03, Steps=141890, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4138, Total reward=36.13, Steps=141928, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4139, Total reward=12.41, Steps=141967, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4140, Total reward=77.59, Steps=142010, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4141, Total reward=55.81, Steps=142041, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4142, Total reward=37.24, Steps=142068, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4143, Total reward=0.02, Steps=142090, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4144, Total reward=7.66, Steps=142120, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4145, Total reward=68.36, Steps=142194, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4146, Total reward=129.57, Steps=142297, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4147, Total reward=62.77, Steps=142340, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4148, Total reward=71.11, Steps=142381, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4149, Total reward=23.82, Steps=142409, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4150, Total reward=97.99, Steps=142454, Training iteration=82
Policy training> Surrogate loss=0.0224332083016634, KL divergence=0.00012065404007444158, Entropy=0.41412368416786194, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.012474033050239086, KL divergence=0.005096642300486565, Entropy=0.41734662652015686, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.029704242944717407, KL divergence=0.014867287129163742, Entropy=0.4122854769229889, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04374990239739418, KL divergence=0.028500040993094444, Entropy=0.41076746582984924, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.07235148549079895, KL divergence=0.04021712765097618, Entropy=0.40007326006889343, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0557909719645977, KL divergence=0.04995742812752724, Entropy=0.3984970152378082, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04392293095588684, KL divergence=0.06301012635231018, Entropy=0.3964451551437378, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06568997353315353, KL divergence=0.07938752323389053, Entropy=0.405545711517334, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0571751706302166, KL divergence=0.08799713104963303, Entropy=0.4096924960613251, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.051667094230651855, KL divergence=0.08785131573677063, Entropy=0.3991738557815552, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/375_Step-142454.ckpt']
Uploaded 3 files for checkpoint 375 in 0.61 seconds
saved intermediate frozen graph: current/model/model_375.pb
Best checkpoint number: 355, Last checkpoint number: 373
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'372'}
Training> Name=main_level/agent, Worker=0, Episode=4151, Total reward=96.71, Steps=142494, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4152, Total reward=76.86, Steps=142535, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4153, Total reward=41.53, Steps=142566, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4154, Total reward=32.02, Steps=142588, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4155, Total reward=8.65, Steps=142605, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4156, Total reward=18.88, Steps=142624, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4157, Total reward=54.36, Steps=142663, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4158, Total reward=46.13, Steps=142685, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4159, Total reward=15.59, Steps=142711, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4160, Total reward=64.09, Steps=142745, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4161, Total reward=65.54, Steps=142775, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4162, Total reward=35.56, Steps=142794, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4163, Total reward=0.02, Steps=142813, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4164, Total reward=4.81, Steps=142841, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4165, Total reward=11.65, Steps=142872, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4166, Total reward=87.25, Steps=142923, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4167, Total reward=42.49, Steps=142958, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4168, Total reward=65.32, Steps=143018, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4169, Total reward=40.9, Steps=143067, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4170, Total reward=47.1, Steps=143122, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4171, Total reward=27.83, Steps=143148, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4172, Total reward=58.81, Steps=143188, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4173, Total reward=25.77, Steps=143208, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4174, Total reward=9.15, Steps=143228, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4175, Total reward=17.4, Steps=143258, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4176, Total reward=28.33, Steps=143286, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4177, Total reward=51.35, Steps=143335, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4178, Total reward=39.53, Steps=143360, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4179, Total reward=56.86, Steps=143416, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4180, Total reward=79.95, Steps=143458, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4181, Total reward=58.45, Steps=143492, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4182, Total reward=33.21, Steps=143530, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4183, Total reward=84.84, Steps=143627, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4184, Total reward=93.7, Steps=143744, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4185, Total reward=76.82, Steps=143842, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4186, Total reward=36.77, Steps=143896, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4187, Total reward=46.98, Steps=143934, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4188, Total reward=64.17, Steps=143965, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4189, Total reward=47.02, Steps=143988, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4190, Total reward=60.08, Steps=144024, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4191, Total reward=35.43, Steps=144066, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4192, Total reward=59.85, Steps=144108, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4193, Total reward=20.33, Steps=144128, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4194, Total reward=36.17, Steps=144150, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4195, Total reward=27.8, Steps=144180, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4196, Total reward=16.29, Steps=144200, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4197, Total reward=55.49, Steps=144246, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4198, Total reward=21.76, Steps=144259, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4199, Total reward=21.47, Steps=144280, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4200, Total reward=89.01, Steps=144334, Training iteration=83
Policy training> Surrogate loss=-0.01816415973007679, KL divergence=0.00011329295375617221, Entropy=0.3854862451553345, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.020776765421032906, KL divergence=0.004527326673269272, Entropy=0.37217918038368225, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05876564979553223, KL divergence=0.015442963689565659, Entropy=0.37020406126976013, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04098996892571449, KL divergence=0.026613399386405945, Entropy=0.37505030632019043, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.052496325224637985, KL divergence=0.04176485911011696, Entropy=0.36172643303871155, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07442547380924225, KL divergence=0.05723851919174194, Entropy=0.37019333243370056, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05814065411686897, KL divergence=0.06688326597213745, Entropy=0.36331871151924133, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05069397762417793, KL divergence=0.07646452635526657, Entropy=0.3608008623123169, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07473429292440414, KL divergence=0.08224301785230637, Entropy=0.36177167296409607, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05030839517712593, KL divergence=0.09541682153940201, Entropy=0.3796362578868866, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/376_Step-144334.ckpt']
Uploaded 3 files for checkpoint 376 in 0.53 seconds
saved intermediate frozen graph: current/model/model_376.pb
Best checkpoint number: 355, Last checkpoint number: 374
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'373'}
Training> Name=main_level/agent, Worker=0, Episode=4201, Total reward=59.47, Steps=144365, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4202, Total reward=33.35, Steps=144392, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4203, Total reward=7.47, Steps=144433, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4204, Total reward=4.77, Steps=144470, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4205, Total reward=84.17, Steps=144533, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4206, Total reward=73.22, Steps=144582, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4207, Total reward=58.19, Steps=144623, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4208, Total reward=66.1, Steps=144663, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4209, Total reward=67.06, Steps=144724, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4210, Total reward=71.09, Steps=144769, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4211, Total reward=62.21, Steps=144809, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4212, Total reward=65.11, Steps=144852, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4213, Total reward=37.17, Steps=144872, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4214, Total reward=44.5, Steps=144895, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4215, Total reward=23.14, Steps=144927, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4216, Total reward=19.8, Steps=144945, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4217, Total reward=36.41, Steps=144996, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4218, Total reward=38.3, Steps=145022, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4219, Total reward=11.98, Steps=145037, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4220, Total reward=76.69, Steps=145078, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4221, Total reward=50.49, Steps=145107, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4222, Total reward=33.79, Steps=145139, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4223, Total reward=14.59, Steps=145184, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4224, Total reward=78.17, Steps=145252, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4225, Total reward=8.39, Steps=145272, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4226, Total reward=20.9, Steps=145305, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4227, Total reward=65.02, Steps=145356, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4228, Total reward=59.54, Steps=145386, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4229, Total reward=44.26, Steps=145453, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4230, Total reward=61.05, Steps=145492, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4231, Total reward=29.33, Steps=145522, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4232, Total reward=49.58, Steps=145555, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4233, Total reward=44.42, Steps=145585, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4234, Total reward=28.31, Steps=145596, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4235, Total reward=25.84, Steps=145637, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4236, Total reward=18.83, Steps=145651, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4237, Total reward=61.5, Steps=145691, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4238, Total reward=33.09, Steps=145716, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4239, Total reward=73.01, Steps=145777, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4240, Total reward=73.07, Steps=145842, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4241, Total reward=59.72, Steps=145873, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4242, Total reward=35.73, Steps=145891, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4243, Total reward=3.69, Steps=145908, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4244, Total reward=4.1, Steps=145930, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4245, Total reward=80.46, Steps=145991, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4246, Total reward=42.6, Steps=146037, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4247, Total reward=44.92, Steps=146076, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4248, Total reward=58.65, Steps=146106, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4249, Total reward=28.65, Steps=146129, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4250, Total reward=25.11, Steps=146148, Training iteration=84
Policy training> Surrogate loss=-0.02527749538421631, KL divergence=9.386074816575274e-05, Entropy=0.4332664906978607, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.020488115027546883, KL divergence=0.004089231137186289, Entropy=0.42535242438316345, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06677015870809555, KL divergence=0.013788734562695026, Entropy=0.4266321659088135, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04699423909187317, KL divergence=0.026492280885577202, Entropy=0.42065390944480896, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0593855194747448, KL divergence=0.038630783557891846, Entropy=0.4194793403148651, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05860591307282448, KL divergence=0.05176099017262459, Entropy=0.4223514497280121, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.045912206172943115, KL divergence=0.060591816902160645, Entropy=0.4209212362766266, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06065816804766655, KL divergence=0.07286337018013, Entropy=0.41960421204566956, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.056456129997968674, KL divergence=0.07961034774780273, Entropy=0.42367687821388245, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06652367115020752, KL divergence=0.08401969820261002, Entropy=0.42638978362083435, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/377_Step-146148.ckpt']
Uploaded 3 files for checkpoint 377 in 0.54 seconds
saved intermediate frozen graph: current/model/model_377.pb
Best checkpoint number: 355, Last checkpoint number: 375
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'374'}
Training> Name=main_level/agent, Worker=0, Episode=4251, Total reward=58.74, Steps=146174, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4252, Total reward=11.1, Steps=146185, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4253, Total reward=43.69, Steps=146214, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4254, Total reward=29.07, Steps=146225, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4255, Total reward=22.76, Steps=146254, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4256, Total reward=28.98, Steps=146286, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4257, Total reward=26.33, Steps=146316, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4258, Total reward=95.52, Steps=146392, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4259, Total reward=63.23, Steps=146445, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4260, Total reward=68.94, Steps=146481, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4261, Total reward=64.87, Steps=146520, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4262, Total reward=33.8, Steps=146540, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4263, Total reward=17.78, Steps=146578, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4264, Total reward=0.02, Steps=146597, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4265, Total reward=68.67, Steps=146659, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4266, Total reward=98.67, Steps=146718, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4267, Total reward=110.73, Steps=146793, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4268, Total reward=54.53, Steps=146846, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4269, Total reward=43.38, Steps=146873, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4270, Total reward=71.92, Steps=146922, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4271, Total reward=82.47, Steps=146960, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4272, Total reward=50.39, Steps=146989, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4273, Total reward=50.63, Steps=147020, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4274, Total reward=39.95, Steps=147042, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4275, Total reward=27.14, Steps=147068, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4276, Total reward=15.43, Steps=147095, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4277, Total reward=64.56, Steps=147151, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4278, Total reward=30.76, Steps=147177, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4279, Total reward=12.29, Steps=147199, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4280, Total reward=73.21, Steps=147239, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4281, Total reward=62.82, Steps=147269, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4282, Total reward=31.15, Steps=147287, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4283, Total reward=12.21, Steps=147323, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4284, Total reward=7.28, Steps=147358, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4285, Total reward=18.67, Steps=147379, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4286, Total reward=20.85, Steps=147405, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4287, Total reward=67.3, Steps=147446, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4288, Total reward=107.57, Steps=147508, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4289, Total reward=79.93, Steps=147563, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4290, Total reward=84.05, Steps=147598, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4291, Total reward=65.54, Steps=147634, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4292, Total reward=70.46, Steps=147673, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4293, Total reward=28.62, Steps=147692, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4294, Total reward=44.56, Steps=147711, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4295, Total reward=20.89, Steps=147739, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4296, Total reward=14.35, Steps=147759, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4297, Total reward=63.16, Steps=147796, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4298, Total reward=38.92, Steps=147821, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4299, Total reward=74.46, Steps=147869, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4300, Total reward=61.93, Steps=147903, Training iteration=85
Policy training> Surrogate loss=-0.005309447646141052, KL divergence=0.00012259701907169074, Entropy=0.4335881769657135, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.020816199481487274, KL divergence=0.004423269536346197, Entropy=0.43003425002098083, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06346764415502548, KL divergence=0.01595207490026951, Entropy=0.4227183759212494, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06458436697721481, KL divergence=0.028431350365281105, Entropy=0.4202847480773926, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0615069754421711, KL divergence=0.04192851111292839, Entropy=0.42119932174682617, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05966828390955925, KL divergence=0.0546623170375824, Entropy=0.4176539182662964, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06388615071773529, KL divergence=0.06511760503053665, Entropy=0.41801175475120544, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0699581503868103, KL divergence=0.07752221077680588, Entropy=0.4179268777370453, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08666837215423584, KL divergence=0.08278357982635498, Entropy=0.4192686080932617, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07474824041128159, KL divergence=0.08559423685073853, Entropy=0.41596952080726624, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/378_Step-147903.ckpt']
Uploaded 3 files for checkpoint 378 in 0.53 seconds
saved intermediate frozen graph: current/model/model_378.pb
Best checkpoint number: 355, Last checkpoint number: 376
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'375'}
Training> Name=main_level/agent, Worker=0, Episode=4301, Total reward=67.01, Steps=147965, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4302, Total reward=33.94, Steps=147985, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4303, Total reward=0.02, Steps=148009, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4304, Total reward=79.57, Steps=148105, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4305, Total reward=12.71, Steps=148129, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4306, Total reward=14.57, Steps=148157, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4307, Total reward=54.81, Steps=148202, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4308, Total reward=62.57, Steps=148232, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4309, Total reward=24.36, Steps=148253, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4310, Total reward=71.06, Steps=148286, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4311, Total reward=52.2, Steps=148322, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4312, Total reward=38.47, Steps=148353, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4313, Total reward=41.51, Steps=148382, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4314, Total reward=45.67, Steps=148404, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4315, Total reward=25.67, Steps=148433, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4316, Total reward=18.11, Steps=148466, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4317, Total reward=32.88, Steps=148501, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4318, Total reward=36.47, Steps=148524, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4319, Total reward=49.93, Steps=148569, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4320, Total reward=95.59, Steps=148609, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4321, Total reward=55.23, Steps=148639, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4322, Total reward=42.51, Steps=148661, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4323, Total reward=58.26, Steps=148735, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4324, Total reward=93.43, Steps=148806, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4325, Total reward=35.71, Steps=148856, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4326, Total reward=102.26, Steps=148918, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4327, Total reward=76.19, Steps=148990, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4328, Total reward=102.69, Steps=149051, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4329, Total reward=32.19, Steps=149078, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4330, Total reward=74.73, Steps=149130, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4331, Total reward=51.48, Steps=149164, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4332, Total reward=51.92, Steps=149194, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4333, Total reward=38.06, Steps=149215, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4334, Total reward=44.5, Steps=149237, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4335, Total reward=51.62, Steps=149265, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4336, Total reward=19.36, Steps=149295, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4337, Total reward=31.02, Steps=149332, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4338, Total reward=30.32, Steps=149352, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4339, Total reward=26.15, Steps=149373, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4340, Total reward=85.8, Steps=149412, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4341, Total reward=66.16, Steps=149443, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4342, Total reward=44.5, Steps=149468, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4343, Total reward=20.84, Steps=149508, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4344, Total reward=15.21, Steps=149540, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4345, Total reward=16.63, Steps=149565, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4346, Total reward=101.06, Steps=149652, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4347, Total reward=38.88, Steps=149687, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4348, Total reward=97.15, Steps=149744, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4349, Total reward=108.16, Steps=149788, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4350, Total reward=26.87, Steps=149821, Training iteration=86
Policy training> Surrogate loss=-0.008086365647614002, KL divergence=7.263254519784823e-05, Entropy=0.4229240417480469, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.022509614005684853, KL divergence=0.0062174382619559765, Entropy=0.4169274866580963, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.02863846905529499, KL divergence=0.01749766431748867, Entropy=0.41427743434906006, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.056230902671813965, KL divergence=0.029124299064278603, Entropy=0.4067557156085968, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06255563348531723, KL divergence=0.047059670090675354, Entropy=0.405499666929245, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.049173664301633835, KL divergence=0.05371122434735298, Entropy=0.3986230790615082, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06789178401231766, KL divergence=0.06912064552307129, Entropy=0.40333113074302673, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0575801245868206, KL divergence=0.07830391079187393, Entropy=0.3932530879974365, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07081954926252365, KL divergence=0.08339963108301163, Entropy=0.3920719623565674, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08109311014413834, KL divergence=0.0853259265422821, Entropy=0.3983074128627777, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/379_Step-149821.ckpt']
Uploaded 3 files for checkpoint 379 in 0.56 seconds
saved intermediate frozen graph: current/model/model_379.pb
Best checkpoint number: 355, Last checkpoint number: 377
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'376'}
Training> Name=main_level/agent, Worker=0, Episode=4351, Total reward=80.65, Steps=149860, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4352, Total reward=49.97, Steps=149894, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4353, Total reward=52.27, Steps=149924, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4354, Total reward=30.74, Steps=149935, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4355, Total reward=26.4, Steps=149963, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4356, Total reward=20.98, Steps=149992, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4357, Total reward=40.0, Steps=150023, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4358, Total reward=88.15, Steps=150109, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4359, Total reward=29.85, Steps=150164, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4360, Total reward=58.23, Steps=150206, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4361, Total reward=54.93, Steps=150240, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4362, Total reward=31.24, Steps=150268, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4363, Total reward=8.72, Steps=150312, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4364, Total reward=76.72, Steps=150405, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4365, Total reward=16.27, Steps=150421, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4366, Total reward=17.75, Steps=150461, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4367, Total reward=59.32, Steps=150505, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4368, Total reward=71.28, Steps=150542, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4369, Total reward=80.42, Steps=150600, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4370, Total reward=25.28, Steps=150614, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4371, Total reward=7.19, Steps=150635, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4372, Total reward=61.87, Steps=150668, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4373, Total reward=1.38, Steps=150684, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4374, Total reward=51.64, Steps=150706, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4375, Total reward=28.7, Steps=150733, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4376, Total reward=19.33, Steps=150752, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4377, Total reward=18.34, Steps=150771, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4378, Total reward=53.55, Steps=150821, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4379, Total reward=63.91, Steps=150883, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4380, Total reward=39.82, Steps=150918, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4381, Total reward=25.59, Steps=150941, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4382, Total reward=40.11, Steps=150979, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4383, Total reward=6.55, Steps=151003, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4384, Total reward=16.97, Steps=151039, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4385, Total reward=21.4, Steps=151070, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4386, Total reward=85.77, Steps=151119, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4387, Total reward=50.56, Steps=151161, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4388, Total reward=62.88, Steps=151191, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4389, Total reward=84.93, Steps=151236, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4390, Total reward=80.83, Steps=151273, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4391, Total reward=53.17, Steps=151310, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4392, Total reward=72.4, Steps=151343, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4393, Total reward=41.73, Steps=151375, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4394, Total reward=30.73, Steps=151386, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4395, Total reward=24.73, Steps=151409, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4396, Total reward=26.75, Steps=151445, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4397, Total reward=37.81, Steps=151492, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4398, Total reward=37.42, Steps=151517, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4399, Total reward=89.49, Steps=151596, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4400, Total reward=91.04, Steps=151637, Training iteration=87
Policy training> Surrogate loss=0.009615504182875156, KL divergence=8.753672591410577e-05, Entropy=0.42149293422698975, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.019407792016863823, KL divergence=0.004714136477559805, Entropy=0.41318655014038086, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.009797686710953712, KL divergence=0.013942689634859562, Entropy=0.4107316732406616, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.053404152393341064, KL divergence=0.028616338968276978, Entropy=0.4079528748989105, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.057190462946891785, KL divergence=0.041639458388090134, Entropy=0.40438246726989746, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05778735503554344, KL divergence=0.054605498909950256, Entropy=0.40107178688049316, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08289170265197754, KL divergence=0.0614938884973526, Entropy=0.4004053771495819, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.050058525055646896, KL divergence=0.07464952766895294, Entropy=0.3861469328403473, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06362225860357285, KL divergence=0.07907717674970627, Entropy=0.39195236563682556, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05513473227620125, KL divergence=0.08487582206726074, Entropy=0.3997211158275604, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/380_Step-151637.ckpt']
Uploaded 3 files for checkpoint 380 in 0.60 seconds
saved intermediate frozen graph: current/model/model_380.pb
Best checkpoint number: 355, Last checkpoint number: 378
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'377'}
Training> Name=main_level/agent, Worker=0, Episode=4401, Total reward=53.67, Steps=151673, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4402, Total reward=39.08, Steps=151701, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4403, Total reward=10.33, Steps=151725, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4404, Total reward=114.02, Steps=151829, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4405, Total reward=26.3, Steps=151862, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4406, Total reward=58.25, Steps=151909, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4407, Total reward=65.87, Steps=151965, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4408, Total reward=64.93, Steps=152010, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4409, Total reward=32.28, Steps=152040, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4410, Total reward=75.64, Steps=152088, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4411, Total reward=75.86, Steps=152138, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4412, Total reward=63.14, Steps=152181, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4413, Total reward=0.02, Steps=152198, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4414, Total reward=45.82, Steps=152220, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4415, Total reward=33.99, Steps=152252, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4416, Total reward=15.86, Steps=152282, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4417, Total reward=43.58, Steps=152319, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4418, Total reward=44.94, Steps=152370, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4419, Total reward=8.25, Steps=152385, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4420, Total reward=75.2, Steps=152423, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4421, Total reward=37.06, Steps=152450, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4422, Total reward=32.88, Steps=152475, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4423, Total reward=30.16, Steps=152513, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4424, Total reward=8.88, Steps=152547, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4425, Total reward=17.84, Steps=152564, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4426, Total reward=87.37, Steps=152613, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4427, Total reward=20.89, Steps=152647, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4428, Total reward=61.95, Steps=152677, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4429, Total reward=30.23, Steps=152708, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4430, Total reward=22.1, Steps=152722, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4431, Total reward=52.24, Steps=152750, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4432, Total reward=57.89, Steps=152783, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4433, Total reward=37.8, Steps=152804, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4434, Total reward=26.01, Steps=152814, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4435, Total reward=31.02, Steps=152842, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4436, Total reward=15.07, Steps=152864, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4437, Total reward=47.58, Steps=152902, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4438, Total reward=36.98, Steps=152925, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4439, Total reward=81.62, Steps=152973, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4440, Total reward=89.33, Steps=153034, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4441, Total reward=64.69, Steps=153064, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4442, Total reward=35.25, Steps=153087, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4443, Total reward=3.22, Steps=153112, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4444, Total reward=69.09, Steps=153188, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4445, Total reward=16.07, Steps=153211, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4446, Total reward=14.41, Steps=153231, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4447, Total reward=14.08, Steps=153251, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4448, Total reward=89.96, Steps=153333, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4449, Total reward=69.9, Steps=153391, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4450, Total reward=18.52, Steps=153404, Training iteration=88
Policy training> Surrogate loss=-0.005849513690918684, KL divergence=0.00017882623069453984, Entropy=0.40793123841285706, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.029326723888516426, KL divergence=0.005366743076592684, Entropy=0.40532198548316956, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03438018634915352, KL divergence=0.01580648310482502, Entropy=0.3988420069217682, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05451235547661781, KL divergence=0.025454692542552948, Entropy=0.38634178042411804, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05947690084576607, KL divergence=0.041122231632471085, Entropy=0.38569918274879456, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04168243333697319, KL divergence=0.04903829097747803, Entropy=0.3798733651638031, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0518127866089344, KL divergence=0.06076526641845703, Entropy=0.38876962661743164, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05805962160229683, KL divergence=0.07381394505500793, Entropy=0.39139047265052795, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06331444531679153, KL divergence=0.07783985882997513, Entropy=0.3812950551509857, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05723865330219269, KL divergence=0.08457335084676743, Entropy=0.3883155286312103, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/381_Step-153404.ckpt']
Uploaded 3 files for checkpoint 381 in 0.57 seconds
saved intermediate frozen graph: current/model/model_381.pb
Best checkpoint number: 355, Last checkpoint number: 379
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'378'}
Training> Name=main_level/agent, Worker=0, Episode=4451, Total reward=34.54, Steps=153430, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4452, Total reward=49.58, Steps=153461, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4453, Total reward=53.34, Steps=153489, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4454, Total reward=37.61, Steps=153510, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4455, Total reward=13.92, Steps=153553, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4456, Total reward=16.13, Steps=153589, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4457, Total reward=54.6, Steps=153657, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4458, Total reward=37.9, Steps=153681, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4459, Total reward=72.77, Steps=153746, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4460, Total reward=84.86, Steps=153801, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4461, Total reward=35.25, Steps=153822, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4462, Total reward=58.66, Steps=153876, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4463, Total reward=19.54, Steps=153917, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4464, Total reward=9.64, Steps=153948, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4465, Total reward=17.6, Steps=153972, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4466, Total reward=89.7, Steps=154047, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4467, Total reward=66.44, Steps=154091, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4468, Total reward=122.02, Steps=154166, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4469, Total reward=26.14, Steps=154203, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4470, Total reward=14.78, Steps=154218, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4471, Total reward=33.67, Steps=154248, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4472, Total reward=19.7, Steps=154274, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4473, Total reward=49.51, Steps=154304, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4474, Total reward=42.9, Steps=154326, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4475, Total reward=30.12, Steps=154355, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4476, Total reward=25.89, Steps=154375, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4477, Total reward=58.5, Steps=154410, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4478, Total reward=23.05, Steps=154424, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4479, Total reward=14.59, Steps=154446, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4480, Total reward=77.58, Steps=154488, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4481, Total reward=60.15, Steps=154537, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4482, Total reward=26.48, Steps=154565, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4483, Total reward=10.4, Steps=154587, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4484, Total reward=14.34, Steps=154613, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4485, Total reward=12.75, Steps=154630, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4486, Total reward=94.66, Steps=154683, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4487, Total reward=104.73, Steps=154771, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4488, Total reward=64.93, Steps=154813, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4489, Total reward=36.22, Steps=154849, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4490, Total reward=86.19, Steps=154897, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4491, Total reward=48.12, Steps=154930, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4492, Total reward=16.49, Steps=154957, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4493, Total reward=42.3, Steps=154979, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4494, Total reward=42.1, Steps=155000, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4495, Total reward=29.32, Steps=155047, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4496, Total reward=22.64, Steps=155066, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4497, Total reward=30.6, Steps=155099, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4498, Total reward=31.39, Steps=155150, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4499, Total reward=89.05, Steps=155207, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4500, Total reward=51.32, Steps=155250, Training iteration=89
Policy training> Surrogate loss=0.008271864615380764, KL divergence=0.00011538431135704741, Entropy=0.4322330057621002, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.013075028546154499, KL divergence=0.007809377741068602, Entropy=0.4397066533565521, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.061639461666345596, KL divergence=0.020908841863274574, Entropy=0.4356667995452881, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06487634032964706, KL divergence=0.031358931213617325, Entropy=0.4227735102176666, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06267617642879486, KL divergence=0.04641121253371239, Entropy=0.4164157807826996, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07992807030677795, KL divergence=0.06246034801006317, Entropy=0.41435226798057556, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05053010210394859, KL divergence=0.0740155279636383, Entropy=0.4116109609603882, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06685911864042282, KL divergence=0.07842179387807846, Entropy=0.4182981550693512, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05721685290336609, KL divergence=0.08868923038244247, Entropy=0.418170690536499, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06809873133897781, KL divergence=0.09797481447458267, Entropy=0.4122333824634552, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/382_Step-155250.ckpt']
Uploaded 3 files for checkpoint 382 in 0.54 seconds
saved intermediate frozen graph: current/model/model_382.pb
Best checkpoint number: 355, Last checkpoint number: 380
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'379'}
Training> Name=main_level/agent, Worker=0, Episode=4501, Total reward=14.84, Steps=155274, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4502, Total reward=28.6, Steps=155308, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4503, Total reward=137.5, Steps=155436, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4504, Total reward=6.5, Steps=155473, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4505, Total reward=66.52, Steps=155535, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4506, Total reward=46.82, Steps=155580, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4507, Total reward=63.24, Steps=155620, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4508, Total reward=42.6, Steps=155637, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4509, Total reward=81.74, Steps=155696, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4510, Total reward=73.42, Steps=155729, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4511, Total reward=60.04, Steps=155765, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4512, Total reward=25.16, Steps=155795, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4513, Total reward=50.4, Steps=155823, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4514, Total reward=40.19, Steps=155844, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4515, Total reward=20.39, Steps=155870, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4516, Total reward=35.31, Steps=155913, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4517, Total reward=25.99, Steps=155934, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4518, Total reward=40.83, Steps=155980, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4519, Total reward=68.22, Steps=156045, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4520, Total reward=37.96, Steps=156074, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4521, Total reward=50.56, Steps=156104, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4522, Total reward=60.54, Steps=156157, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4523, Total reward=3.12, Steps=156181, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4524, Total reward=11.18, Steps=156213, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4525, Total reward=20.86, Steps=156241, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4526, Total reward=66.39, Steps=156293, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4527, Total reward=62.8, Steps=156336, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4528, Total reward=72.49, Steps=156390, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4529, Total reward=70.45, Steps=156448, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4530, Total reward=52.05, Steps=156490, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4531, Total reward=15.32, Steps=156518, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4532, Total reward=35.2, Steps=156548, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4533, Total reward=52.99, Steps=156577, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4534, Total reward=47.7, Steps=156600, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4535, Total reward=31.56, Steps=156630, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4536, Total reward=15.3, Steps=156661, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4537, Total reward=121.34, Steps=156746, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4538, Total reward=27.31, Steps=156772, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4539, Total reward=10.29, Steps=156795, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4540, Total reward=33.84, Steps=156813, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4541, Total reward=8.93, Steps=156826, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4542, Total reward=30.04, Steps=156846, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4543, Total reward=10.71, Steps=156883, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4544, Total reward=13.04, Steps=156919, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4545, Total reward=75.22, Steps=157001, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4546, Total reward=7.24, Steps=157023, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4547, Total reward=21.6, Steps=157062, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4548, Total reward=68.6, Steps=157099, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4549, Total reward=30.69, Steps=157121, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4550, Total reward=77.61, Steps=157170, Training iteration=90
Policy training> Surrogate loss=0.009133394807577133, KL divergence=0.0001749557413859293, Entropy=0.4274207055568695, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.024065835401415825, KL divergence=0.004598521161824465, Entropy=0.4286585748195648, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03326414152979851, KL divergence=0.013310198672115803, Entropy=0.41132044792175293, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.036751627922058105, KL divergence=0.02652118168771267, Entropy=0.4213632643222809, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0516512356698513, KL divergence=0.037892796099185944, Entropy=0.4086097180843353, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07607108354568481, KL divergence=0.05116552114486694, Entropy=0.41041699051856995, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06665127724409103, KL divergence=0.059612199664115906, Entropy=0.4140613377094269, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.04956837370991707, KL divergence=0.06772926449775696, Entropy=0.40551257133483887, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0599314384162426, KL divergence=0.0779053345322609, Entropy=0.4021712839603424, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06636769324541092, KL divergence=0.08445780724287033, Entropy=0.4076519310474396, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/383_Step-157170.ckpt']
Uploaded 3 files for checkpoint 383 in 0.53 seconds
saved intermediate frozen graph: current/model/model_383.pb
Best checkpoint number: 355, Last checkpoint number: 381
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'380'}
Training> Name=main_level/agent, Worker=0, Episode=4551, Total reward=60.91, Steps=157197, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4552, Total reward=47.47, Steps=157227, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4553, Total reward=43.46, Steps=157253, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4554, Total reward=54.41, Steps=157274, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4555, Total reward=16.12, Steps=157288, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4556, Total reward=29.05, Steps=157313, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4557, Total reward=40.3, Steps=157340, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4558, Total reward=30.05, Steps=157377, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4559, Total reward=11.99, Steps=157400, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4560, Total reward=34.26, Steps=157431, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4561, Total reward=50.05, Steps=157459, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4562, Total reward=55.74, Steps=157506, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4563, Total reward=7.0, Steps=157550, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4564, Total reward=79.85, Steps=157620, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4565, Total reward=18.21, Steps=157650, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4566, Total reward=106.02, Steps=157759, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4567, Total reward=58.5, Steps=157797, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4568, Total reward=75.34, Steps=157852, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4569, Total reward=24.64, Steps=157868, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4570, Total reward=47.37, Steps=157892, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4571, Total reward=19.11, Steps=157904, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4572, Total reward=50.55, Steps=157936, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4573, Total reward=54.46, Steps=157966, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4574, Total reward=38.56, Steps=158000, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4575, Total reward=29.05, Steps=158027, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4576, Total reward=20.18, Steps=158046, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4577, Total reward=49.4, Steps=158081, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4578, Total reward=29.64, Steps=158095, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4579, Total reward=30.24, Steps=158132, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4580, Total reward=68.87, Steps=158167, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4581, Total reward=57.43, Steps=158197, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4582, Total reward=48.22, Steps=158221, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4583, Total reward=80.77, Steps=158310, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4584, Total reward=16.61, Steps=158353, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4585, Total reward=82.54, Steps=158411, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4586, Total reward=60.35, Steps=158460, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4587, Total reward=59.74, Steps=158520, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4588, Total reward=67.49, Steps=158575, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4589, Total reward=46.88, Steps=158621, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4590, Total reward=95.66, Steps=158680, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4591, Total reward=48.66, Steps=158707, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4592, Total reward=75.65, Steps=158750, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4593, Total reward=41.5, Steps=158780, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4594, Total reward=31.42, Steps=158801, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4595, Total reward=34.09, Steps=158832, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4596, Total reward=20.25, Steps=158850, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4597, Total reward=34.82, Steps=158888, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4598, Total reward=101.08, Steps=158969, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4599, Total reward=19.95, Steps=158987, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4600, Total reward=60.14, Steps=159016, Training iteration=91
Policy training> Surrogate loss=0.0029650970827788115, KL divergence=0.0001272029330721125, Entropy=0.42346230149269104, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.031099217012524605, KL divergence=0.004842208232730627, Entropy=0.4193032681941986, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0536285899579525, KL divergence=0.014712112955749035, Entropy=0.4079543650150299, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05085739120841026, KL divergence=0.027483714744448662, Entropy=0.4037374258041382, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04231372848153114, KL divergence=0.04104751721024513, Entropy=0.4034341871738434, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07739812135696411, KL divergence=0.051434438675642014, Entropy=0.3984622061252594, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06179199740290642, KL divergence=0.06398880481719971, Entropy=0.3964874744415283, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07153034210205078, KL divergence=0.06908337771892548, Entropy=0.3987271785736084, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07501741498708725, KL divergence=0.07585149258375168, Entropy=0.38716545701026917, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05847041681408882, KL divergence=0.08518745750188828, Entropy=0.3981902599334717, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/384_Step-159016.ckpt']
Uploaded 3 files for checkpoint 384 in 0.78 seconds
saved intermediate frozen graph: current/model/model_384.pb
Best checkpoint number: 355, Last checkpoint number: 382
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'381'}
Training> Name=main_level/agent, Worker=0, Episode=4601, Total reward=67.08, Steps=159047, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4602, Total reward=36.42, Steps=159072, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4603, Total reward=60.19, Steps=159151, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4604, Total reward=49.44, Steps=159209, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4605, Total reward=13.02, Steps=159230, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4606, Total reward=64.96, Steps=159280, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4607, Total reward=26.63, Steps=159315, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4608, Total reward=59.52, Steps=159343, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4609, Total reward=29.52, Steps=159366, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4610, Total reward=79.61, Steps=159413, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4611, Total reward=26.87, Steps=159439, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4612, Total reward=54.65, Steps=159481, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4613, Total reward=33.34, Steps=159502, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4614, Total reward=46.15, Steps=159524, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4615, Total reward=9.02, Steps=159549, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4616, Total reward=28.48, Steps=159573, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4617, Total reward=35.94, Steps=159593, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4618, Total reward=62.6, Steps=159644, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4619, Total reward=16.39, Steps=159671, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4620, Total reward=67.12, Steps=159714, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4621, Total reward=44.94, Steps=159743, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4622, Total reward=33.02, Steps=159783, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4623, Total reward=11.12, Steps=159809, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4624, Total reward=9.63, Steps=159850, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4625, Total reward=25.22, Steps=159873, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4626, Total reward=58.01, Steps=159920, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4627, Total reward=75.68, Steps=159964, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4628, Total reward=48.28, Steps=159989, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4629, Total reward=93.59, Steps=160032, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4630, Total reward=18.42, Steps=160076, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4631, Total reward=39.04, Steps=160117, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4632, Total reward=23.58, Steps=160145, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4633, Total reward=27.52, Steps=160167, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4634, Total reward=47.82, Steps=160190, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4635, Total reward=31.26, Steps=160213, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4636, Total reward=22.03, Steps=160251, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4637, Total reward=57.73, Steps=160285, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4638, Total reward=37.51, Steps=160309, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4639, Total reward=84.31, Steps=160361, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4640, Total reward=55.44, Steps=160410, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4641, Total reward=57.55, Steps=160453, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4642, Total reward=41.19, Steps=160490, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4643, Total reward=4.75, Steps=160516, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4644, Total reward=7.82, Steps=160535, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4645, Total reward=16.95, Steps=160567, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4646, Total reward=80.47, Steps=160616, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4647, Total reward=41.75, Steps=160653, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4648, Total reward=93.96, Steps=160733, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4649, Total reward=59.29, Steps=160796, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4650, Total reward=78.35, Steps=160833, Training iteration=92
Policy training> Surrogate loss=-0.0127409053966403, KL divergence=0.00021212802676018327, Entropy=0.44740965962409973, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.035750314593315125, KL divergence=0.006840869784355164, Entropy=0.4483952820301056, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04879393056035042, KL divergence=0.016757825389504433, Entropy=0.4438476860523224, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06669813394546509, KL divergence=0.029760412871837616, Entropy=0.43049535155296326, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0570649616420269, KL divergence=0.044808048754930496, Entropy=0.4267489016056061, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06980045884847641, KL divergence=0.056200768798589706, Entropy=0.4241086542606354, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.049198925495147705, KL divergence=0.06876683235168457, Entropy=0.4149837791919708, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.058527201414108276, KL divergence=0.07547984272241592, Entropy=0.4199061095714569, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06307803094387054, KL divergence=0.08697261661291122, Entropy=0.4239659011363983, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08724456280469894, KL divergence=0.0894060730934143, Entropy=0.4159224033355713, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/385_Step-160833.ckpt']
Uploaded 3 files for checkpoint 385 in 0.58 seconds
saved intermediate frozen graph: current/model/model_385.pb
Best checkpoint number: 355, Last checkpoint number: 383
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'382'}
Training> Name=main_level/agent, Worker=0, Episode=4651, Total reward=48.26, Steps=160872, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4652, Total reward=50.59, Steps=160904, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4653, Total reward=20.95, Steps=160924, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4654, Total reward=35.95, Steps=160944, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4655, Total reward=26.72, Steps=160976, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4656, Total reward=25.23, Steps=161010, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4657, Total reward=52.81, Steps=161048, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4658, Total reward=79.0, Steps=161141, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4659, Total reward=77.5, Steps=161197, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4660, Total reward=57.39, Steps=161247, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4661, Total reward=57.11, Steps=161289, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4662, Total reward=58.04, Steps=161341, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4663, Total reward=84.21, Steps=161446, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4664, Total reward=41.26, Steps=161511, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4665, Total reward=10.05, Steps=161532, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4666, Total reward=124.21, Steps=161610, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4667, Total reward=57.5, Steps=161650, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4668, Total reward=67.03, Steps=161701, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4669, Total reward=19.62, Steps=161718, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4670, Total reward=79.11, Steps=161756, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4671, Total reward=29.06, Steps=161784, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4672, Total reward=66.84, Steps=161825, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4673, Total reward=46.29, Steps=161855, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4674, Total reward=33.0, Steps=161876, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4675, Total reward=29.54, Steps=161908, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4676, Total reward=23.35, Steps=161928, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4677, Total reward=56.32, Steps=161967, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4678, Total reward=38.76, Steps=161989, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4679, Total reward=18.13, Steps=162010, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4680, Total reward=81.9, Steps=162049, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4681, Total reward=48.99, Steps=162080, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4682, Total reward=107.28, Steps=162187, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4683, Total reward=13.64, Steps=162226, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4684, Total reward=4.98, Steps=162249, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4685, Total reward=108.34, Steps=162360, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4686, Total reward=93.58, Steps=162431, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4687, Total reward=84.87, Steps=162492, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4688, Total reward=64.25, Steps=162535, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4689, Total reward=38.0, Steps=162569, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4690, Total reward=82.41, Steps=162603, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4691, Total reward=60.72, Steps=162628, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4692, Total reward=54.38, Steps=162662, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4693, Total reward=38.17, Steps=162683, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4694, Total reward=50.45, Steps=162706, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4695, Total reward=40.24, Steps=162736, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4696, Total reward=23.76, Steps=162755, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4697, Total reward=34.37, Steps=162787, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4698, Total reward=32.68, Steps=162814, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4699, Total reward=11.94, Steps=162832, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4700, Total reward=11.54, Steps=162846, Training iteration=93
Policy training> Surrogate loss=0.0007980267400853336, KL divergence=0.00010128901340067387, Entropy=0.43934929370880127, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.012354972772300243, KL divergence=0.0044032311998307705, Entropy=0.4314292371273041, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.023160889744758606, KL divergence=0.014483948238193989, Entropy=0.4303431510925293, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05443957820534706, KL divergence=0.025489648804068565, Entropy=0.42320525646209717, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06892096251249313, KL divergence=0.037543680518865585, Entropy=0.410491943359375, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05338661000132561, KL divergence=0.049807894974946976, Entropy=0.4083367586135864, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.040671270340681076, KL divergence=0.05882059410214424, Entropy=0.4062483012676239, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06827985495328903, KL divergence=0.06799758225679398, Entropy=0.4083264172077179, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05207538604736328, KL divergence=0.07056042551994324, Entropy=0.41032469272613525, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08022831380367279, KL divergence=0.07600177824497223, Entropy=0.4102555215358734, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/386_Step-162846.ckpt']
Uploaded 3 files for checkpoint 386 in 0.59 seconds
saved intermediate frozen graph: current/model/model_386.pb
Best checkpoint number: 355, Last checkpoint number: 384
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'383'}
Training> Name=main_level/agent, Worker=0, Episode=4701, Total reward=15.68, Steps=162865, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4702, Total reward=43.49, Steps=162894, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4703, Total reward=0.02, Steps=162911, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4704, Total reward=51.65, Steps=162985, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4705, Total reward=59.12, Steps=163047, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4706, Total reward=124.27, Steps=163130, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4707, Total reward=10.86, Steps=163158, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4708, Total reward=66.4, Steps=163206, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4709, Total reward=18.11, Steps=163221, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4710, Total reward=84.2, Steps=163257, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4711, Total reward=47.37, Steps=163284, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4712, Total reward=26.97, Steps=163312, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4713, Total reward=36.09, Steps=163334, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4714, Total reward=47.0, Steps=163357, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4715, Total reward=35.1, Steps=163388, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4716, Total reward=19.02, Steps=163407, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4717, Total reward=50.13, Steps=163443, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4718, Total reward=61.25, Steps=163513, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4719, Total reward=17.82, Steps=163532, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4720, Total reward=94.25, Steps=163584, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4721, Total reward=42.67, Steps=163611, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4722, Total reward=35.4, Steps=163638, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4723, Total reward=0.01, Steps=163650, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4724, Total reward=53.05, Steps=163722, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4725, Total reward=16.12, Steps=163739, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4726, Total reward=34.59, Steps=163787, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4727, Total reward=44.68, Steps=163825, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4728, Total reward=53.39, Steps=163852, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4729, Total reward=15.02, Steps=163874, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4730, Total reward=27.48, Steps=163898, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4731, Total reward=58.73, Steps=163928, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4732, Total reward=37.46, Steps=163956, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4733, Total reward=38.41, Steps=163982, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4734, Total reward=41.61, Steps=164004, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4735, Total reward=37.05, Steps=164035, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4736, Total reward=22.82, Steps=164064, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4737, Total reward=72.33, Steps=164097, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4738, Total reward=33.46, Steps=164119, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4739, Total reward=62.48, Steps=164169, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4740, Total reward=82.92, Steps=164207, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4741, Total reward=49.23, Steps=164248, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4742, Total reward=32.95, Steps=164267, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4743, Total reward=51.15, Steps=164358, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4744, Total reward=17.17, Steps=164394, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4745, Total reward=21.36, Steps=164424, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4746, Total reward=75.93, Steps=164489, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4747, Total reward=61.84, Steps=164532, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4748, Total reward=61.58, Steps=164561, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4749, Total reward=26.32, Steps=164583, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4750, Total reward=75.18, Steps=164620, Training iteration=94
Policy training> Surrogate loss=-0.024794109165668488, KL divergence=0.0001006486636470072, Entropy=0.4495922029018402, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.019677862524986267, KL divergence=0.0049324375577270985, Entropy=0.44511285424232483, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.059750523418188095, KL divergence=0.015803547576069832, Entropy=0.44675877690315247, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04545986279845238, KL divergence=0.027749620378017426, Entropy=0.4376215934753418, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.03806041553616524, KL divergence=0.03937302902340889, Entropy=0.43305209279060364, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06063754856586456, KL divergence=0.05159902572631836, Entropy=0.4187938868999481, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.047195713967084885, KL divergence=0.0635673999786377, Entropy=0.42654967308044434, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07050591707229614, KL divergence=0.06963947415351868, Entropy=0.42483484745025635, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07462054491043091, KL divergence=0.08098353445529938, Entropy=0.4260314404964447, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05471307039260864, KL divergence=0.08297112584114075, Entropy=0.4256550967693329, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/387_Step-164620.ckpt']
Uploaded 3 files for checkpoint 387 in 0.54 seconds
saved intermediate frozen graph: current/model/model_387.pb
Best checkpoint number: 355, Last checkpoint number: 385
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'384'}
Training> Name=main_level/agent, Worker=0, Episode=4751, Total reward=34.65, Steps=164647, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4752, Total reward=44.17, Steps=164678, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4753, Total reward=21.06, Steps=164689, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4754, Total reward=31.68, Steps=164710, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4755, Total reward=32.2, Steps=164741, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4756, Total reward=22.48, Steps=164761, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4757, Total reward=41.18, Steps=164790, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4758, Total reward=43.51, Steps=164817, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4759, Total reward=19.73, Steps=164848, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4760, Total reward=55.4, Steps=164892, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4761, Total reward=49.77, Steps=164927, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4762, Total reward=38.12, Steps=164960, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4763, Total reward=10.56, Steps=165006, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4764, Total reward=7.02, Steps=165049, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4765, Total reward=45.95, Steps=165104, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4766, Total reward=67.21, Steps=165157, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4767, Total reward=64.39, Steps=165215, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4768, Total reward=62.01, Steps=165244, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4769, Total reward=73.25, Steps=165306, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4770, Total reward=28.89, Steps=165323, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4771, Total reward=37.39, Steps=165350, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4772, Total reward=33.72, Steps=165369, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4773, Total reward=33.77, Steps=165394, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4774, Total reward=41.11, Steps=165416, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4775, Total reward=17.54, Steps=165443, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4776, Total reward=16.24, Steps=165459, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4777, Total reward=53.76, Steps=165514, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4778, Total reward=39.03, Steps=165539, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4779, Total reward=69.8, Steps=165591, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4780, Total reward=92.13, Steps=165631, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4781, Total reward=65.84, Steps=165661, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4782, Total reward=21.76, Steps=165677, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4783, Total reward=3.52, Steps=165697, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4784, Total reward=9.22, Steps=165715, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4785, Total reward=28.68, Steps=165754, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4786, Total reward=119.84, Steps=165836, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4787, Total reward=65.7, Steps=165871, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4788, Total reward=62.35, Steps=165926, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4789, Total reward=50.83, Steps=165973, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4790, Total reward=75.58, Steps=166018, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4791, Total reward=22.67, Steps=166044, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4792, Total reward=57.01, Steps=166085, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4793, Total reward=42.78, Steps=166116, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4794, Total reward=42.64, Steps=166151, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4795, Total reward=13.35, Steps=166171, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4796, Total reward=24.63, Steps=166203, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4797, Total reward=54.9, Steps=166241, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4798, Total reward=26.87, Steps=166254, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4799, Total reward=10.69, Steps=166270, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4800, Total reward=71.87, Steps=166307, Training iteration=95
Policy training> Surrogate loss=0.00036371624446474016, KL divergence=8.543919102521613e-05, Entropy=0.41600465774536133, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.015101652592420578, KL divergence=0.004783686716109514, Entropy=0.4115443229675293, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04586970433592796, KL divergence=0.016717150807380676, Entropy=0.4051642417907715, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0606764517724514, KL divergence=0.030547799542546272, Entropy=0.40516749024391174, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05464354529976845, KL divergence=0.03961760178208351, Entropy=0.393084317445755, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06128707528114319, KL divergence=0.0521550290286541, Entropy=0.3968426287174225, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06219768151640892, KL divergence=0.06503768265247345, Entropy=0.38767871260643005, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07016881555318832, KL divergence=0.0738353505730629, Entropy=0.3924565613269806, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04508398845791817, KL divergence=0.08109613507986069, Entropy=0.38880324363708496, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0644519031047821, KL divergence=0.08738842606544495, Entropy=0.38998427987098694, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/388_Step-166307.ckpt']
Uploaded 3 files for checkpoint 388 in 0.62 seconds
saved intermediate frozen graph: current/model/model_388.pb
Best checkpoint number: 355, Last checkpoint number: 386
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'385'}
Training> Name=main_level/agent, Worker=0, Episode=4801, Total reward=63.18, Steps=166337, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4802, Total reward=34.04, Steps=166367, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4803, Total reward=3.58, Steps=166385, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4804, Total reward=0.02, Steps=166405, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4805, Total reward=2.79, Steps=166419, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4806, Total reward=24.41, Steps=166438, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4807, Total reward=66.69, Steps=166478, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4808, Total reward=67.52, Steps=166523, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4809, Total reward=36.38, Steps=166550, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4810, Total reward=36.04, Steps=166566, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4811, Total reward=48.5, Steps=166594, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4812, Total reward=37.76, Steps=166622, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4813, Total reward=65.76, Steps=166651, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4814, Total reward=40.5, Steps=166673, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4815, Total reward=35.51, Steps=166702, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4816, Total reward=29.65, Steps=166740, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4817, Total reward=60.47, Steps=166788, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4818, Total reward=17.86, Steps=166800, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4819, Total reward=71.62, Steps=166852, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4820, Total reward=75.9, Steps=166893, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4821, Total reward=63.15, Steps=166925, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4822, Total reward=38.12, Steps=166951, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4823, Total reward=9.92, Steps=166997, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4824, Total reward=5.98, Steps=167029, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4825, Total reward=16.67, Steps=167065, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4826, Total reward=81.63, Steps=167139, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4827, Total reward=17.97, Steps=167161, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4828, Total reward=68.61, Steps=167189, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4829, Total reward=42.31, Steps=167241, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4830, Total reward=98.3, Steps=167289, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4831, Total reward=11.42, Steps=167302, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4832, Total reward=10.34, Steps=167325, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4833, Total reward=41.36, Steps=167347, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4834, Total reward=30.52, Steps=167367, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4835, Total reward=18.59, Steps=167398, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4836, Total reward=26.94, Steps=167416, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4837, Total reward=50.53, Steps=167465, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4838, Total reward=21.82, Steps=167479, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4839, Total reward=61.3, Steps=167523, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4840, Total reward=56.52, Steps=167567, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4841, Total reward=64.76, Steps=167600, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4842, Total reward=38.81, Steps=167626, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4843, Total reward=3.31, Steps=167649, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4844, Total reward=98.51, Steps=167742, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4845, Total reward=84.64, Steps=167811, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4846, Total reward=7.33, Steps=167828, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4847, Total reward=10.37, Steps=167852, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4848, Total reward=64.19, Steps=167879, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4849, Total reward=68.54, Steps=167941, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4850, Total reward=64.74, Steps=167987, Training iteration=96
Policy training> Surrogate loss=-0.002479695016518235, KL divergence=0.00010541407391428947, Entropy=0.43560686707496643, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.028141362592577934, KL divergence=0.003984426148235798, Entropy=0.438812255859375, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05524859204888344, KL divergence=0.01337865088135004, Entropy=0.4345968961715698, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04692065715789795, KL divergence=0.027633337303996086, Entropy=0.4352445602416992, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05930394306778908, KL divergence=0.040517982095479965, Entropy=0.4283805191516876, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05423932150006294, KL divergence=0.05232514441013336, Entropy=0.4148123562335968, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.053500980138778687, KL divergence=0.058308590203523636, Entropy=0.41887953877449036, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.054500240832567215, KL divergence=0.07093378156423569, Entropy=0.42383965849876404, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07102113962173462, KL divergence=0.07855270057916641, Entropy=0.4225286543369293, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06189470365643501, KL divergence=0.0826820656657219, Entropy=0.4170564115047455, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/389_Step-167987.ckpt']
Uploaded 3 files for checkpoint 389 in 0.60 seconds
saved intermediate frozen graph: current/model/model_389.pb
Best checkpoint number: 355, Last checkpoint number: 387
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'386'}
Training> Name=main_level/agent, Worker=0, Episode=4851, Total reward=57.91, Steps=168026, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4852, Total reward=42.71, Steps=168057, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4853, Total reward=48.36, Steps=168084, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4854, Total reward=43.56, Steps=168105, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4855, Total reward=16.9, Steps=168132, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4856, Total reward=25.34, Steps=168150, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4857, Total reward=37.78, Steps=168189, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4858, Total reward=45.15, Steps=168220, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4859, Total reward=8.17, Steps=168235, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4860, Total reward=74.4, Steps=168278, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4861, Total reward=46.5, Steps=168305, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4862, Total reward=37.41, Steps=168332, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4863, Total reward=106.46, Steps=168420, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4864, Total reward=5.3, Steps=168446, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4865, Total reward=72.4, Steps=168503, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4866, Total reward=18.06, Steps=168524, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4867, Total reward=36.02, Steps=168563, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4868, Total reward=65.67, Steps=168607, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4869, Total reward=21.92, Steps=168643, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4870, Total reward=19.15, Steps=168656, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4871, Total reward=87.64, Steps=168709, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4872, Total reward=50.56, Steps=168740, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4873, Total reward=17.97, Steps=168751, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4874, Total reward=45.99, Steps=168773, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4875, Total reward=29.7, Steps=168806, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4876, Total reward=24.35, Steps=168823, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4877, Total reward=104.32, Steps=168901, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4878, Total reward=30.03, Steps=168929, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4879, Total reward=21.83, Steps=168960, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4880, Total reward=89.69, Steps=169008, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4881, Total reward=60.12, Steps=169036, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4882, Total reward=34.04, Steps=169062, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4883, Total reward=0.01, Steps=169073, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4884, Total reward=92.3, Steps=169177, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4885, Total reward=4.94, Steps=169200, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4886, Total reward=45.94, Steps=169248, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4887, Total reward=72.06, Steps=169314, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4888, Total reward=81.15, Steps=169360, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4889, Total reward=36.36, Steps=169400, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4890, Total reward=22.43, Steps=169427, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4891, Total reward=32.85, Steps=169457, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4892, Total reward=58.27, Steps=169499, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4893, Total reward=56.83, Steps=169529, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4894, Total reward=41.04, Steps=169551, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4895, Total reward=31.73, Steps=169581, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4896, Total reward=26.61, Steps=169601, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4897, Total reward=29.44, Steps=169657, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4898, Total reward=31.73, Steps=169680, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4899, Total reward=97.12, Steps=169732, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4900, Total reward=81.93, Steps=169772, Training iteration=97
Policy training> Surrogate loss=-0.014751292765140533, KL divergence=0.0001458172482671216, Entropy=0.44133198261260986, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.010081445798277855, KL divergence=0.005180399399250746, Entropy=0.4375416934490204, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04642302915453911, KL divergence=0.018153157085180283, Entropy=0.42879462242126465, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04283542558550835, KL divergence=0.02974403090775013, Entropy=0.42916139960289, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06368567794561386, KL divergence=0.042957156896591187, Entropy=0.41667500138282776, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07378688454627991, KL divergence=0.05633868649601936, Entropy=0.4075224697589874, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.060815706849098206, KL divergence=0.06628485769033432, Entropy=0.420223593711853, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.050542980432510376, KL divergence=0.07598138600587845, Entropy=0.41710665822029114, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04526680335402489, KL divergence=0.08288702368736267, Entropy=0.42143797874450684, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.056404706090688705, KL divergence=0.08933041244745255, Entropy=0.4191724359989166, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/390_Step-169772.ckpt']
Uploaded 3 files for checkpoint 390 in 0.53 seconds
saved intermediate frozen graph: current/model/model_390.pb
Best checkpoint number: 355, Last checkpoint number: 388
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'387'}
Training> Name=main_level/agent, Worker=0, Episode=4901, Total reward=61.45, Steps=169800, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4902, Total reward=107.98, Steps=169887, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4903, Total reward=13.83, Steps=169923, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4904, Total reward=83.08, Steps=169992, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4905, Total reward=11.45, Steps=170023, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4906, Total reward=87.48, Steps=170075, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4907, Total reward=70.63, Steps=170119, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4908, Total reward=62.72, Steps=170150, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4909, Total reward=25.69, Steps=170170, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4910, Total reward=91.08, Steps=170215, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4911, Total reward=52.26, Steps=170254, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4912, Total reward=23.81, Steps=170286, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4913, Total reward=58.48, Steps=170316, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4914, Total reward=47.02, Steps=170339, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4915, Total reward=29.83, Steps=170373, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4916, Total reward=22.75, Steps=170400, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4917, Total reward=99.47, Steps=170476, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4918, Total reward=40.03, Steps=170501, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4919, Total reward=18.72, Steps=170535, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4920, Total reward=13.0, Steps=170559, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4921, Total reward=50.64, Steps=170590, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4922, Total reward=32.65, Steps=170609, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4923, Total reward=19.7, Steps=170669, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4924, Total reward=11.49, Steps=170700, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4925, Total reward=16.47, Steps=170740, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4926, Total reward=7.16, Steps=170752, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4927, Total reward=81.04, Steps=170815, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4928, Total reward=59.01, Steps=170858, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4929, Total reward=28.45, Steps=170885, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4930, Total reward=43.46, Steps=170922, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4931, Total reward=70.52, Steps=170961, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4932, Total reward=67.64, Steps=170995, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4933, Total reward=34.89, Steps=171016, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4934, Total reward=41.96, Steps=171038, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4935, Total reward=19.4, Steps=171069, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4936, Total reward=2.87, Steps=171080, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4937, Total reward=44.66, Steps=171127, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4938, Total reward=22.24, Steps=171155, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4939, Total reward=71.27, Steps=171211, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4940, Total reward=66.0, Steps=171249, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4941, Total reward=67.3, Steps=171280, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4942, Total reward=36.81, Steps=171310, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4943, Total reward=3.73, Steps=171323, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4944, Total reward=13.49, Steps=171375, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4945, Total reward=23.36, Steps=171411, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4946, Total reward=75.05, Steps=171489, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4947, Total reward=64.43, Steps=171530, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4948, Total reward=53.33, Steps=171581, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4949, Total reward=70.44, Steps=171642, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4950, Total reward=58.66, Steps=171676, Training iteration=98
Policy training> Surrogate loss=-0.013170596212148666, KL divergence=6.335841317195445e-05, Entropy=0.43122705817222595, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.002021500840783119, KL divergence=0.0034385209437459707, Entropy=0.4314272105693817, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.026837820187211037, KL divergence=0.013431158848106861, Entropy=0.4173118770122528, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04605429992079735, KL divergence=0.02481859177350998, Entropy=0.41367578506469727, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05080493167042732, KL divergence=0.04117125645279884, Entropy=0.4273107349872589, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05754532292485237, KL divergence=0.04960644245147705, Entropy=0.41882288455963135, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.051717162132263184, KL divergence=0.0616392083466053, Entropy=0.42306771874427795, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07359594851732254, KL divergence=0.06730317324399948, Entropy=0.4155462980270386, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.051104143261909485, KL divergence=0.07618191093206406, Entropy=0.41460859775543213, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06781497597694397, KL divergence=0.08734854310750961, Entropy=0.4237700402736664, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/391_Step-171676.ckpt']
Uploaded 3 files for checkpoint 391 in 0.65 seconds
saved intermediate frozen graph: current/model/model_391.pb
Best checkpoint number: 355, Last checkpoint number: 389
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'388'}
Training> Name=main_level/agent, Worker=0, Episode=4951, Total reward=52.25, Steps=171715, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4952, Total reward=52.89, Steps=171746, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4953, Total reward=51.82, Steps=171776, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4954, Total reward=38.36, Steps=171798, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4955, Total reward=39.61, Steps=171824, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4956, Total reward=24.43, Steps=171839, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4957, Total reward=137.34, Steps=171995, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4958, Total reward=31.06, Steps=172017, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4959, Total reward=41.77, Steps=172072, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4960, Total reward=79.3, Steps=172117, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4961, Total reward=72.39, Steps=172174, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4962, Total reward=28.8, Steps=172194, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4963, Total reward=0.02, Steps=172210, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4964, Total reward=57.04, Steps=172249, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4965, Total reward=6.71, Steps=172273, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4966, Total reward=3.6, Steps=172284, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4967, Total reward=18.61, Steps=172300, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4968, Total reward=68.21, Steps=172341, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4969, Total reward=60.94, Steps=172387, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4970, Total reward=51.42, Steps=172419, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4971, Total reward=36.83, Steps=172447, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4972, Total reward=66.9, Steps=172488, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4973, Total reward=9.73, Steps=172507, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4974, Total reward=39.21, Steps=172528, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4975, Total reward=38.59, Steps=172557, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4976, Total reward=23.38, Steps=172591, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4977, Total reward=53.12, Steps=172643, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4978, Total reward=21.5, Steps=172661, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4979, Total reward=12.17, Steps=172678, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4980, Total reward=73.64, Steps=172720, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4981, Total reward=60.78, Steps=172751, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4982, Total reward=52.05, Steps=172791, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4983, Total reward=68.79, Steps=172866, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4984, Total reward=48.8, Steps=172943, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4985, Total reward=60.98, Steps=173019, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4986, Total reward=88.38, Steps=173067, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4987, Total reward=33.14, Steps=173092, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4988, Total reward=71.33, Steps=173135, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4989, Total reward=34.21, Steps=173167, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4990, Total reward=15.36, Steps=173180, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4991, Total reward=79.68, Steps=173222, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4992, Total reward=64.59, Steps=173254, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4993, Total reward=0.01, Steps=173264, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4994, Total reward=44.69, Steps=173286, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4995, Total reward=25.47, Steps=173327, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4996, Total reward=26.52, Steps=173343, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4997, Total reward=90.16, Steps=173426, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4998, Total reward=40.71, Steps=173449, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4999, Total reward=21.25, Steps=173486, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=5000, Total reward=86.96, Steps=173531, Training iteration=99
Policy training> Surrogate loss=-0.012731065042316914, KL divergence=7.961942174006253e-05, Entropy=0.4623859226703644, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.032413508743047714, KL divergence=0.0037156271282583475, Entropy=0.46144723892211914, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04672133922576904, KL divergence=0.013941784389317036, Entropy=0.455235093832016, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04819074273109436, KL divergence=0.025192447006702423, Entropy=0.4493684470653534, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05756945535540581, KL divergence=0.03818618133664131, Entropy=0.452244371175766, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07576245069503784, KL divergence=0.04808719456195831, Entropy=0.44581595063209534, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04962171986699104, KL divergence=0.05959289148449898, Entropy=0.44357573986053467, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08375447243452072, KL divergence=0.0690864622592926, Entropy=0.4510822296142578, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06300213187932968, KL divergence=0.07279730588197708, Entropy=0.4304114878177643, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08063196390867233, KL divergence=0.08262413740158081, Entropy=0.4362826645374298, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/392_Step-173531.ckpt']
Uploaded 3 files for checkpoint 392 in 0.57 seconds
saved intermediate frozen graph: current/model/model_392.pb
Best checkpoint number: 355, Last checkpoint number: 390
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'389'}
Training> Name=main_level/agent, Worker=0, Episode=5001, Total reward=69.28, Steps=173562, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5002, Total reward=136.07, Steps=173676, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5003, Total reward=6.06, Steps=173709, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5004, Total reward=18.86, Steps=173754, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5005, Total reward=15.66, Steps=173779, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5006, Total reward=101.16, Steps=173839, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5007, Total reward=73.87, Steps=173896, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5008, Total reward=81.53, Steps=173947, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5009, Total reward=89.88, Steps=174010, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5010, Total reward=98.76, Steps=174057, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5011, Total reward=76.98, Steps=174112, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5012, Total reward=71.46, Steps=174153, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5013, Total reward=28.01, Steps=174172, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5014, Total reward=48.56, Steps=174194, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5015, Total reward=21.3, Steps=174221, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5016, Total reward=15.61, Steps=174259, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5017, Total reward=60.32, Steps=174316, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5018, Total reward=41.46, Steps=174338, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5019, Total reward=81.09, Steps=174394, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5020, Total reward=35.35, Steps=174429, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5021, Total reward=50.57, Steps=174460, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5022, Total reward=29.54, Steps=174480, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5023, Total reward=87.07, Steps=174563, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5024, Total reward=3.39, Steps=174585, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5025, Total reward=22.73, Steps=174610, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5026, Total reward=85.86, Steps=174673, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5027, Total reward=69.84, Steps=174725, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5028, Total reward=60.03, Steps=174755, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5029, Total reward=17.99, Steps=174773, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5030, Total reward=96.25, Steps=174817, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5031, Total reward=55.87, Steps=174856, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5032, Total reward=46.19, Steps=174886, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5033, Total reward=44.34, Steps=174916, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5034, Total reward=44.07, Steps=174938, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5035, Total reward=15.63, Steps=174968, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5036, Total reward=17.23, Steps=175000, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5037, Total reward=32.95, Steps=175036, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5038, Total reward=33.15, Steps=175084, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5039, Total reward=65.44, Steps=175133, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5040, Total reward=71.71, Steps=175186, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5041, Total reward=60.84, Steps=175214, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5042, Total reward=30.78, Steps=175233, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5043, Total reward=137.37, Steps=175353, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5044, Total reward=2.8, Steps=175374, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5045, Total reward=43.54, Steps=175435, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5046, Total reward=70.5, Steps=175499, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5047, Total reward=56.29, Steps=175543, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5048, Total reward=130.98, Steps=175631, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5049, Total reward=68.64, Steps=175686, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5050, Total reward=76.43, Steps=175722, Training iteration=100
Policy training> Surrogate loss=0.003912998363375664, KL divergence=0.00017499485693406314, Entropy=0.43441787362098694, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.032476916909217834, KL divergence=0.007852126844227314, Entropy=0.4363871216773987, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03888775408267975, KL divergence=0.023272234946489334, Entropy=0.4286203980445862, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05196879804134369, KL divergence=0.04020451754331589, Entropy=0.41737645864486694, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05163492262363434, KL divergence=0.056666579097509384, Entropy=0.4196665585041046, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0718371644616127, KL divergence=0.06839270889759064, Entropy=0.4159095585346222, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06884568929672241, KL divergence=0.07881300151348114, Entropy=0.42231324315071106, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07460044324398041, KL divergence=0.08474600315093994, Entropy=0.418235719203949, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06251958012580872, KL divergence=0.09214089065790176, Entropy=0.4241333603858948, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07995258271694183, KL divergence=0.09641895443201065, Entropy=0.42151859402656555, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/393_Step-175722.ckpt']
Uploaded 3 files for checkpoint 393 in 0.59 seconds
saved intermediate frozen graph: current/model/model_393.pb
Best checkpoint number: 355, Last checkpoint number: 391
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'390'}
Training> Name=main_level/agent, Worker=0, Episode=5051, Total reward=11.38, Steps=175735, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5052, Total reward=57.34, Steps=175766, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5053, Total reward=24.99, Steps=175777, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5054, Total reward=39.34, Steps=175799, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5055, Total reward=51.83, Steps=175847, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5056, Total reward=21.18, Steps=175866, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5057, Total reward=101.68, Steps=175949, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5058, Total reward=41.6, Steps=175973, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5059, Total reward=51.13, Steps=176030, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5060, Total reward=40.17, Steps=176057, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5061, Total reward=69.39, Steps=176088, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5062, Total reward=44.47, Steps=176129, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5063, Total reward=7.7, Steps=176143, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5064, Total reward=12.53, Steps=176162, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5065, Total reward=58.55, Steps=176222, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5066, Total reward=113.21, Steps=176322, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5067, Total reward=80.0, Steps=176383, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5068, Total reward=70.91, Steps=176425, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5069, Total reward=40.22, Steps=176454, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5070, Total reward=45.77, Steps=176508, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5071, Total reward=58.78, Steps=176548, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5072, Total reward=54.0, Steps=176583, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5073, Total reward=21.18, Steps=176601, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5074, Total reward=53.1, Steps=176623, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5075, Total reward=38.85, Steps=176650, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5076, Total reward=16.95, Steps=176671, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5077, Total reward=47.79, Steps=176712, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5078, Total reward=40.66, Steps=176736, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5079, Total reward=20.25, Steps=176755, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5080, Total reward=227.66, Steps=176909, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5081, Total reward=53.38, Steps=176938, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5082, Total reward=40.0, Steps=176963, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5083, Total reward=34.82, Steps=177006, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5084, Total reward=0.02, Steps=177026, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5085, Total reward=79.68, Steps=177101, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5086, Total reward=69.46, Steps=177147, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5087, Total reward=93.86, Steps=177232, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5088, Total reward=111.27, Steps=177296, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5089, Total reward=10.37, Steps=177309, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5090, Total reward=50.82, Steps=177346, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5091, Total reward=87.84, Steps=177395, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5092, Total reward=47.79, Steps=177428, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5093, Total reward=37.84, Steps=177453, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5094, Total reward=39.74, Steps=177474, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5095, Total reward=40.23, Steps=177503, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5096, Total reward=21.92, Steps=177520, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5097, Total reward=103.73, Steps=177600, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5098, Total reward=38.59, Steps=177626, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5099, Total reward=63.4, Steps=177680, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5100, Total reward=77.27, Steps=177717, Training iteration=101
Policy training> Surrogate loss=-0.0006917764549143612, KL divergence=0.0001223623112309724, Entropy=0.4477781355381012, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.011516938917338848, KL divergence=0.006420289631932974, Entropy=0.4437065124511719, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04079313948750496, KL divergence=0.015937937423586845, Entropy=0.43235400319099426, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05752338096499443, KL divergence=0.0291528832167387, Entropy=0.4353444278240204, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0666411817073822, KL divergence=0.0436413399875164, Entropy=0.4358489513397217, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.051659274846315384, KL divergence=0.05119163915514946, Entropy=0.43760356307029724, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.040948934853076935, KL divergence=0.06353621929883957, Entropy=0.4348922669887543, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07157691568136215, KL divergence=0.07929667830467224, Entropy=0.43694815039634705, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08339007943868637, KL divergence=0.08921816200017929, Entropy=0.4309624433517456, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06639523059129715, KL divergence=0.08419787883758545, Entropy=0.4302757978439331, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/394_Step-177717.ckpt']
Uploaded 3 files for checkpoint 394 in 0.49 seconds
saved intermediate frozen graph: current/model/model_394.pb
Best checkpoint number: 355, Last checkpoint number: 392
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'391'}
Training> Name=main_level/agent, Worker=0, Episode=5101, Total reward=66.55, Steps=177751, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5102, Total reward=34.22, Steps=177791, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5103, Total reward=0.01, Steps=177802, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5104, Total reward=59.65, Steps=177883, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5105, Total reward=75.09, Steps=177955, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5106, Total reward=73.51, Steps=178023, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5107, Total reward=78.66, Steps=178110, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5108, Total reward=90.12, Steps=178187, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5109, Total reward=49.19, Steps=178241, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5110, Total reward=39.66, Steps=178281, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5111, Total reward=44.01, Steps=178309, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5112, Total reward=23.83, Steps=178329, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5113, Total reward=37.78, Steps=178355, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5114, Total reward=42.88, Steps=178377, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5115, Total reward=35.4, Steps=178407, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5116, Total reward=22.81, Steps=178426, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5117, Total reward=95.8, Steps=178514, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5118, Total reward=42.37, Steps=178540, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5119, Total reward=68.32, Steps=178600, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5120, Total reward=69.54, Steps=178651, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5121, Total reward=55.52, Steps=178685, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5122, Total reward=104.82, Steps=178802, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5123, Total reward=3.25, Steps=178828, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5124, Total reward=70.63, Steps=178901, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5125, Total reward=94.64, Steps=178979, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5126, Total reward=51.3, Steps=179000, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5127, Total reward=139.62, Steps=179082, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5128, Total reward=70.73, Steps=179122, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5129, Total reward=29.89, Steps=179146, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5130, Total reward=56.98, Steps=179182, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5131, Total reward=46.17, Steps=179216, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5132, Total reward=35.43, Steps=179245, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5133, Total reward=14.61, Steps=179264, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5134, Total reward=42.07, Steps=179286, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5135, Total reward=22.6, Steps=179311, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5136, Total reward=22.07, Steps=179330, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5137, Total reward=61.79, Steps=179397, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5138, Total reward=16.17, Steps=179409, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5139, Total reward=24.46, Steps=179432, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5140, Total reward=85.78, Steps=179476, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5141, Total reward=49.27, Steps=179503, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5142, Total reward=35.76, Steps=179527, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5143, Total reward=94.08, Steps=179610, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5144, Total reward=59.57, Steps=179714, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5145, Total reward=13.64, Steps=179740, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5146, Total reward=74.62, Steps=179794, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5147, Total reward=74.75, Steps=179869, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5148, Total reward=73.3, Steps=179921, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5149, Total reward=34.18, Steps=179971, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5150, Total reward=44.82, Steps=179998, Training iteration=102
Policy training> Surrogate loss=0.0025375522673130035, KL divergence=0.0002584668109193444, Entropy=0.44654086232185364, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.028592534363269806, KL divergence=0.007347705774009228, Entropy=0.4454301595687866, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05319560319185257, KL divergence=0.02118607610464096, Entropy=0.441384494304657, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.057176362723112106, KL divergence=0.03590721637010574, Entropy=0.4257897734642029, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06678653508424759, KL divergence=0.05029536411166191, Entropy=0.43014058470726013, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05636388808488846, KL divergence=0.05999767780303955, Entropy=0.42951688170433044, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06021793186664581, KL divergence=0.07004295289516449, Entropy=0.42436060309410095, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07839536666870117, KL divergence=0.07507383823394775, Entropy=0.425525426864624, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07711595296859741, KL divergence=0.0823739618062973, Entropy=0.4324396550655365, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07443442940711975, KL divergence=0.08670416474342346, Entropy=0.43420565128326416, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/395_Step-179998.ckpt']
Uploaded 3 files for checkpoint 395 in 0.74 seconds
saved intermediate frozen graph: current/model/model_395.pb
Best checkpoint number: 355, Last checkpoint number: 393
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'392'}
Training> Name=main_level/agent, Worker=0, Episode=5151, Total reward=15.28, Steps=180012, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5152, Total reward=32.0, Steps=180044, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5153, Total reward=26.22, Steps=180062, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5154, Total reward=44.29, Steps=180083, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5155, Total reward=21.57, Steps=180111, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5156, Total reward=17.75, Steps=180126, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5157, Total reward=59.47, Steps=180159, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5158, Total reward=31.94, Steps=180182, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5159, Total reward=15.41, Steps=180204, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5160, Total reward=36.51, Steps=180234, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5161, Total reward=58.53, Steps=180264, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5162, Total reward=16.28, Steps=180280, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5163, Total reward=123.83, Steps=180410, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5164, Total reward=1.8, Steps=180438, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5165, Total reward=32.9, Steps=180477, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5166, Total reward=95.2, Steps=180568, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5167, Total reward=56.27, Steps=180612, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5168, Total reward=70.39, Steps=180646, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5169, Total reward=33.06, Steps=180673, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5170, Total reward=76.85, Steps=180723, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5171, Total reward=89.0, Steps=180765, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5172, Total reward=36.31, Steps=180796, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5173, Total reward=38.81, Steps=180821, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5174, Total reward=38.16, Steps=180843, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5175, Total reward=25.62, Steps=180872, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5176, Total reward=19.51, Steps=180889, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5177, Total reward=30.33, Steps=180927, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5178, Total reward=73.21, Steps=181004, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5179, Total reward=51.47, Steps=181056, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5180, Total reward=55.77, Steps=181110, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5181, Total reward=58.29, Steps=181138, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5182, Total reward=50.22, Steps=181204, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5183, Total reward=15.32, Steps=181241, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5184, Total reward=5.49, Steps=181272, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5185, Total reward=88.74, Steps=181349, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5186, Total reward=10.94, Steps=181364, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5187, Total reward=44.12, Steps=181407, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5188, Total reward=40.12, Steps=181452, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5189, Total reward=59.92, Steps=181500, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5190, Total reward=78.86, Steps=181544, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5191, Total reward=57.48, Steps=181571, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5192, Total reward=47.12, Steps=181602, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5193, Total reward=46.72, Steps=181631, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5194, Total reward=31.95, Steps=181643, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5195, Total reward=26.32, Steps=181677, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5196, Total reward=20.21, Steps=181696, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5197, Total reward=37.47, Steps=181725, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5198, Total reward=28.88, Steps=181764, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5199, Total reward=19.82, Steps=181788, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5200, Total reward=57.82, Steps=181824, Training iteration=103
Policy training> Surrogate loss=-0.013784624636173248, KL divergence=0.00010261096758767962, Entropy=0.45494064688682556, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.033282142132520676, KL divergence=0.004545871168375015, Entropy=0.44662773609161377, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07603917270898819, KL divergence=0.018095657229423523, Entropy=0.4537365734577179, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05482887849211693, KL divergence=0.03409256413578987, Entropy=0.4464114010334015, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.051167529076337814, KL divergence=0.04464768245816231, Entropy=0.43553248047828674, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.059112951159477234, KL divergence=0.0585913360118866, Entropy=0.44263461232185364, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.062072355300188065, KL divergence=0.06568577140569687, Entropy=0.4274146556854248, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07467927783727646, KL divergence=0.08063453435897827, Entropy=0.43800655007362366, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08175640553236008, KL divergence=0.08707811683416367, Entropy=0.4343789517879486, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0661749541759491, KL divergence=0.09425883740186691, Entropy=0.42746055126190186, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/396_Step-181824.ckpt']
Uploaded 3 files for checkpoint 396 in 0.54 seconds
saved intermediate frozen graph: current/model/model_396.pb
Best checkpoint number: 355, Last checkpoint number: 394
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'393'}
Training> Name=main_level/agent, Worker=0, Episode=5201, Total reward=48.69, Steps=181860, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5202, Total reward=34.39, Steps=181878, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5203, Total reward=25.34, Steps=181929, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5204, Total reward=52.89, Steps=181989, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5205, Total reward=33.24, Steps=182021, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5206, Total reward=53.87, Steps=182054, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5207, Total reward=51.08, Steps=182108, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5208, Total reward=65.06, Steps=182140, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5209, Total reward=27.02, Steps=182164, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5210, Total reward=42.08, Steps=182209, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5211, Total reward=15.12, Steps=182217, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5212, Total reward=55.93, Steps=182250, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5213, Total reward=50.29, Steps=182280, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5214, Total reward=35.74, Steps=182301, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5215, Total reward=32.92, Steps=182332, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5216, Total reward=19.1, Steps=182351, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5217, Total reward=48.02, Steps=182377, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5218, Total reward=31.82, Steps=182398, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5219, Total reward=8.26, Steps=182434, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5220, Total reward=87.43, Steps=182496, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5221, Total reward=47.16, Steps=182527, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5222, Total reward=29.4, Steps=182544, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5223, Total reward=5.8, Steps=182571, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5224, Total reward=19.91, Steps=182600, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5225, Total reward=89.74, Steps=182667, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5226, Total reward=61.84, Steps=182716, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5227, Total reward=69.14, Steps=182769, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5228, Total reward=96.9, Steps=182819, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5229, Total reward=68.52, Steps=182856, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5230, Total reward=34.79, Steps=182885, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5231, Total reward=63.3, Steps=182927, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5232, Total reward=65.47, Steps=182959, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5233, Total reward=46.06, Steps=182989, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5234, Total reward=43.91, Steps=183011, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5235, Total reward=35.78, Steps=183039, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5236, Total reward=28.3, Steps=183069, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5237, Total reward=90.79, Steps=183150, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5238, Total reward=39.28, Steps=183174, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5239, Total reward=87.12, Steps=183231, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5240, Total reward=77.4, Steps=183277, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5241, Total reward=50.39, Steps=183306, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5242, Total reward=42.6, Steps=183347, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5243, Total reward=0.02, Steps=183371, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5244, Total reward=4.04, Steps=183399, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5245, Total reward=36.12, Steps=183426, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5246, Total reward=90.8, Steps=183475, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5247, Total reward=64.06, Steps=183524, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5248, Total reward=55.55, Steps=183553, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5249, Total reward=44.8, Steps=183578, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5250, Total reward=88.04, Steps=183624, Training iteration=104
Policy training> Surrogate loss=-0.010158538818359375, KL divergence=8.742901991354302e-05, Entropy=0.4455857276916504, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.028968364000320435, KL divergence=0.0034045956563204527, Entropy=0.4397764205932617, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05438463017344475, KL divergence=0.012135584838688374, Entropy=0.4342711865901947, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.046706825494766235, KL divergence=0.022508665919303894, Entropy=0.434249609708786, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05583558604121208, KL divergence=0.03403286635875702, Entropy=0.42449840903282166, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06497576832771301, KL divergence=0.043917637318372726, Entropy=0.42937174439430237, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06464094668626785, KL divergence=0.05398382619023323, Entropy=0.4302828311920166, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08059343695640564, KL divergence=0.06109802797436714, Entropy=0.420156329870224, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08618775010108948, KL divergence=0.07014533132314682, Entropy=0.42241111397743225, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0846986174583435, KL divergence=0.07326854020357132, Entropy=0.4156632125377655, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/397_Step-183624.ckpt']
Uploaded 3 files for checkpoint 397 in 0.57 seconds
saved intermediate frozen graph: current/model/model_397.pb
Best checkpoint number: 355, Last checkpoint number: 395
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'394'}
Training> Name=main_level/agent, Worker=0, Episode=5251, Total reward=34.43, Steps=183665, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5252, Total reward=55.78, Steps=183696, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5253, Total reward=0.01, Steps=183710, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5254, Total reward=43.57, Steps=183733, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5255, Total reward=36.18, Steps=183760, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5256, Total reward=31.12, Steps=183780, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5257, Total reward=57.56, Steps=183835, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5258, Total reward=17.49, Steps=183848, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5259, Total reward=56.67, Steps=183894, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5260, Total reward=67.73, Steps=183932, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5261, Total reward=65.46, Steps=183974, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5262, Total reward=39.2, Steps=184000, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5263, Total reward=2.9, Steps=184015, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5264, Total reward=75.39, Steps=184097, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5265, Total reward=19.11, Steps=184147, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5266, Total reward=125.48, Steps=184259, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5267, Total reward=9.83, Steps=184285, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5268, Total reward=48.62, Steps=184312, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5269, Total reward=26.18, Steps=184328, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5270, Total reward=107.6, Steps=184388, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5271, Total reward=20.65, Steps=184414, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5272, Total reward=0.01, Steps=184424, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5273, Total reward=45.99, Steps=184452, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5274, Total reward=36.84, Steps=184472, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5275, Total reward=31.52, Steps=184503, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5276, Total reward=21.72, Steps=184519, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5277, Total reward=43.32, Steps=184569, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5278, Total reward=95.27, Steps=184643, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5279, Total reward=10.25, Steps=184669, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5280, Total reward=73.9, Steps=184712, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5281, Total reward=120.14, Steps=184817, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5282, Total reward=40.75, Steps=184842, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5283, Total reward=7.23, Steps=184861, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5284, Total reward=0.01, Steps=184874, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5285, Total reward=74.58, Steps=184936, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5286, Total reward=95.35, Steps=184996, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5287, Total reward=44.33, Steps=185033, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5288, Total reward=52.73, Steps=185065, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5289, Total reward=25.06, Steps=185084, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5290, Total reward=68.08, Steps=185129, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5291, Total reward=69.49, Steps=185172, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5292, Total reward=3.75, Steps=185184, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5293, Total reward=47.85, Steps=185215, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5294, Total reward=46.54, Steps=185237, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5295, Total reward=45.9, Steps=185265, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5296, Total reward=28.93, Steps=185282, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5297, Total reward=104.37, Steps=185359, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5298, Total reward=25.35, Steps=185374, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5299, Total reward=35.01, Steps=185406, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5300, Total reward=82.82, Steps=185444, Training iteration=105
Policy training> Surrogate loss=-0.014544118195772171, KL divergence=0.00010247851605527103, Entropy=0.4714997708797455, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03939444199204445, KL divergence=0.006145046558231115, Entropy=0.4653400182723999, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05306771770119667, KL divergence=0.019396811723709106, Entropy=0.4629356563091278, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06859725713729858, KL divergence=0.028997937217354774, Entropy=0.455534964799881, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05454205349087715, KL divergence=0.0425378791987896, Entropy=0.4529320001602173, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05917249992489815, KL divergence=0.055574674159288406, Entropy=0.4415789544582367, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07035776227712631, KL divergence=0.06759068369865417, Entropy=0.4439300000667572, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07467424869537354, KL divergence=0.07747106999158859, Entropy=0.44611144065856934, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.059882331639528275, KL divergence=0.08270037919282913, Entropy=0.43717554211616516, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06494443863630295, KL divergence=0.08660939335823059, Entropy=0.43615099787712097, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/398_Step-185444.ckpt']
Uploaded 3 files for checkpoint 398 in 0.56 seconds
saved intermediate frozen graph: current/model/model_398.pb
Best checkpoint number: 355, Last checkpoint number: 396
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'395'}
Training> Name=main_level/agent, Worker=0, Episode=5301, Total reward=53.18, Steps=185471, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5302, Total reward=41.98, Steps=185497, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5303, Total reward=10.56, Steps=185524, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5304, Total reward=11.21, Steps=185562, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5305, Total reward=16.5, Steps=185588, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5306, Total reward=70.4, Steps=185649, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5307, Total reward=23.45, Steps=185673, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5308, Total reward=61.37, Steps=185716, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5309, Total reward=39.45, Steps=185744, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5310, Total reward=63.23, Steps=185777, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5311, Total reward=44.49, Steps=185817, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5312, Total reward=66.58, Steps=185851, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5313, Total reward=14.8, Steps=185871, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5314, Total reward=29.07, Steps=185882, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5315, Total reward=23.24, Steps=185918, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5316, Total reward=21.66, Steps=185934, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5317, Total reward=103.67, Steps=186010, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5318, Total reward=41.06, Steps=186057, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5319, Total reward=20.41, Steps=186074, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5320, Total reward=68.03, Steps=186111, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5321, Total reward=62.92, Steps=186151, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5322, Total reward=31.88, Steps=186173, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5323, Total reward=19.92, Steps=186208, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5324, Total reward=92.29, Steps=186289, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5325, Total reward=13.41, Steps=186306, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5326, Total reward=144.93, Steps=186409, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5327, Total reward=55.15, Steps=186448, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5328, Total reward=43.17, Steps=186464, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5329, Total reward=17.86, Steps=186482, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5330, Total reward=97.93, Steps=186529, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5331, Total reward=86.79, Steps=186567, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5332, Total reward=40.32, Steps=186598, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5333, Total reward=40.93, Steps=186628, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5334, Total reward=43.62, Steps=186650, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5335, Total reward=36.56, Steps=186678, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5336, Total reward=19.7, Steps=186696, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5337, Total reward=32.72, Steps=186722, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5338, Total reward=16.09, Steps=186735, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5339, Total reward=178.52, Steps=186908, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5340, Total reward=52.36, Steps=186945, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5341, Total reward=64.19, Steps=186976, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5342, Total reward=41.47, Steps=187026, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5343, Total reward=3.31, Steps=187055, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5344, Total reward=6.64, Steps=187080, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5345, Total reward=68.83, Steps=187144, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5346, Total reward=68.27, Steps=187189, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5347, Total reward=116.95, Steps=187278, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5348, Total reward=42.72, Steps=187294, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5349, Total reward=17.9, Steps=187308, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5350, Total reward=59.19, Steps=187343, Training iteration=106
Policy training> Surrogate loss=-0.0004555309715215117, KL divergence=8.586214244132861e-05, Entropy=0.44042328000068665, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.029384369030594826, KL divergence=0.007613936439156532, Entropy=0.4324599504470825, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.037655968219041824, KL divergence=0.015293336473405361, Entropy=0.4429842531681061, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.02609078586101532, KL divergence=0.036232225596904755, Entropy=0.4281183183193207, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05410138890147209, KL divergence=0.04775713384151459, Entropy=0.43458881974220276, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05314314365386963, KL divergence=0.06216120719909668, Entropy=0.4170428514480591, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06615849584341049, KL divergence=0.06747516244649887, Entropy=0.41142717003822327, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05157388374209404, KL divergence=0.07348468154668808, Entropy=0.4136864244937897, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06488668918609619, KL divergence=0.08740580081939697, Entropy=0.4265784025192261, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07117565721273422, KL divergence=0.0970735028386116, Entropy=0.41809675097465515, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/399_Step-187343.ckpt']
Uploaded 3 files for checkpoint 399 in 0.62 seconds
saved intermediate frozen graph: current/model/model_399.pb
Best checkpoint number: 355, Last checkpoint number: 397
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'396'}
Training> Name=main_level/agent, Worker=0, Episode=5351, Total reward=44.47, Steps=187382, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5352, Total reward=57.86, Steps=187414, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5353, Total reward=17.78, Steps=187424, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5354, Total reward=47.02, Steps=187446, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5355, Total reward=18.5, Steps=187477, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5356, Total reward=22.77, Steps=187493, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5357, Total reward=32.34, Steps=187534, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5358, Total reward=29.06, Steps=187572, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5359, Total reward=74.02, Steps=187640, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5360, Total reward=92.8, Steps=187683, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5361, Total reward=60.71, Steps=187714, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5362, Total reward=41.2, Steps=187748, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5363, Total reward=16.11, Steps=187790, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5364, Total reward=81.1, Steps=187878, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5365, Total reward=45.8, Steps=187937, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5366, Total reward=93.65, Steps=187985, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5367, Total reward=51.19, Steps=188026, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5368, Total reward=105.93, Steps=188091, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5369, Total reward=19.98, Steps=188119, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5370, Total reward=15.0, Steps=188132, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5371, Total reward=85.24, Steps=188174, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5372, Total reward=22.9, Steps=188206, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5373, Total reward=39.77, Steps=188237, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5374, Total reward=43.37, Steps=188258, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5375, Total reward=36.0, Steps=188284, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5376, Total reward=25.03, Steps=188317, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5377, Total reward=126.0, Steps=188409, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5378, Total reward=29.98, Steps=188434, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5379, Total reward=59.26, Steps=188480, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5380, Total reward=34.92, Steps=188518, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5381, Total reward=60.43, Steps=188549, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5382, Total reward=32.21, Steps=188576, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5383, Total reward=23.7, Steps=188626, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5384, Total reward=6.59, Steps=188663, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5385, Total reward=84.51, Steps=188746, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5386, Total reward=14.29, Steps=188766, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5387, Total reward=84.3, Steps=188829, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5388, Total reward=52.59, Steps=188865, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5389, Total reward=24.66, Steps=188884, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5390, Total reward=68.51, Steps=188945, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5391, Total reward=51.19, Steps=188985, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5392, Total reward=66.37, Steps=189028, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5393, Total reward=0.02, Steps=189046, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5394, Total reward=46.44, Steps=189068, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5395, Total reward=36.44, Steps=189096, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5396, Total reward=19.43, Steps=189114, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5397, Total reward=43.76, Steps=189168, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5398, Total reward=25.42, Steps=189185, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5399, Total reward=21.22, Steps=189218, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5400, Total reward=98.21, Steps=189282, Training iteration=107
Policy training> Surrogate loss=0.0027833234053105116, KL divergence=0.00013419675815384835, Entropy=0.4384678304195404, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.007843527011573315, KL divergence=0.0034342005383223295, Entropy=0.4535268247127533, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.037373267114162445, KL divergence=0.012260054238140583, Entropy=0.44082966446876526, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.047531794756650925, KL divergence=0.024335376918315887, Entropy=0.438772588968277, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06030694767832756, KL divergence=0.03757080063223839, Entropy=0.4304879605770111, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.060935359448194504, KL divergence=0.04624049365520477, Entropy=0.4370757043361664, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07536368817090988, KL divergence=0.058120351284742355, Entropy=0.42118534445762634, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09008318185806274, KL divergence=0.06458810716867447, Entropy=0.4210628569126129, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07189283519983292, KL divergence=0.06987182050943375, Entropy=0.41530683636665344, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06540720909833908, KL divergence=0.07976167649030685, Entropy=0.4292292296886444, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/400_Step-189282.ckpt']
Uploaded 3 files for checkpoint 400 in 0.48 seconds
saved intermediate frozen graph: current/model/model_400.pb
Best checkpoint number: 355, Last checkpoint number: 398
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'397'}
Training> Name=main_level/agent, Worker=0, Episode=5401, Total reward=69.27, Steps=189333, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5402, Total reward=31.74, Steps=189352, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5403, Total reward=6.87, Steps=189367, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5404, Total reward=115.37, Steps=189476, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5405, Total reward=49.54, Steps=189533, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5406, Total reward=73.02, Steps=189585, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5407, Total reward=58.62, Steps=189623, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5408, Total reward=36.67, Steps=189638, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5409, Total reward=50.28, Steps=189688, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5410, Total reward=76.04, Steps=189726, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5411, Total reward=40.83, Steps=189752, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5412, Total reward=0.01, Steps=189764, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5413, Total reward=44.98, Steps=189793, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5414, Total reward=31.58, Steps=189816, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5415, Total reward=24.75, Steps=189843, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5416, Total reward=23.42, Steps=189863, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5417, Total reward=11.49, Steps=189885, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5418, Total reward=74.52, Steps=189947, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5419, Total reward=48.53, Steps=189998, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5420, Total reward=88.8, Steps=190045, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5421, Total reward=68.17, Steps=190079, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5422, Total reward=35.96, Steps=190099, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5423, Total reward=84.42, Steps=190186, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5424, Total reward=0.02, Steps=190201, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5425, Total reward=104.9, Steps=190279, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5426, Total reward=107.01, Steps=190352, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5427, Total reward=65.36, Steps=190418, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5428, Total reward=59.1, Steps=190448, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5429, Total reward=23.58, Steps=190467, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5430, Total reward=69.21, Steps=190502, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5431, Total reward=49.31, Steps=190542, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5432, Total reward=55.66, Steps=190574, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5433, Total reward=0.02, Steps=190591, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5434, Total reward=42.7, Steps=190614, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5435, Total reward=19.01, Steps=190642, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5436, Total reward=23.06, Steps=190655, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5437, Total reward=78.19, Steps=190725, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5438, Total reward=28.68, Steps=190765, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5439, Total reward=64.86, Steps=190830, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5440, Total reward=81.82, Steps=190873, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5441, Total reward=114.09, Steps=190990, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5442, Total reward=45.65, Steps=191022, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5443, Total reward=0.03, Steps=191051, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5444, Total reward=102.14, Steps=191138, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5445, Total reward=13.41, Steps=191154, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5446, Total reward=91.27, Steps=191225, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5447, Total reward=76.18, Steps=191292, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5448, Total reward=68.26, Steps=191345, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5449, Total reward=66.51, Steps=191395, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5450, Total reward=29.81, Steps=191410, Training iteration=108
Policy training> Surrogate loss=0.007736387196928263, KL divergence=0.00023689711815677583, Entropy=0.44896504282951355, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.028978347778320312, KL divergence=0.008400056511163712, Entropy=0.45092934370040894, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0423939973115921, KL divergence=0.023870378732681274, Entropy=0.4481546878814697, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06259427964687347, KL divergence=0.03769873082637787, Entropy=0.4443817734718323, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06339164078235626, KL divergence=0.05086657777428627, Entropy=0.44341132044792175, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07174329459667206, KL divergence=0.059452153742313385, Entropy=0.4415525794029236, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.060500212013721466, KL divergence=0.0714854747056961, Entropy=0.4416777789592743, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07534460723400116, KL divergence=0.08012659102678299, Entropy=0.4417406916618347, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06371861696243286, KL divergence=0.08647948503494263, Entropy=0.44310620427131653, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07616108655929565, KL divergence=0.09216148406267166, Entropy=0.445317804813385, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/401_Step-191410.ckpt']
Uploaded 3 files for checkpoint 401 in 0.57 seconds
saved intermediate frozen graph: current/model/model_401.pb
Best checkpoint number: 355, Last checkpoint number: 399
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'398'}
Training> Name=main_level/agent, Worker=0, Episode=5451, Total reward=81.07, Steps=191450, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5452, Total reward=36.2, Steps=191481, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5453, Total reward=62.8, Steps=191510, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5454, Total reward=44.67, Steps=191532, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5455, Total reward=24.38, Steps=191560, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5456, Total reward=19.22, Steps=191575, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5457, Total reward=51.51, Steps=191642, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5458, Total reward=96.65, Steps=191711, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5459, Total reward=11.84, Steps=191733, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5460, Total reward=160.39, Steps=191844, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5461, Total reward=56.95, Steps=191875, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5462, Total reward=45.08, Steps=191899, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5463, Total reward=20.47, Steps=191935, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5464, Total reward=10.23, Steps=191962, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5465, Total reward=16.53, Steps=191977, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5466, Total reward=126.15, Steps=192064, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5467, Total reward=76.16, Steps=192128, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5468, Total reward=125.92, Steps=192201, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5469, Total reward=81.64, Steps=192249, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5470, Total reward=14.97, Steps=192271, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5471, Total reward=44.97, Steps=192307, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5472, Total reward=52.29, Steps=192337, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5473, Total reward=50.38, Steps=192367, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5474, Total reward=45.6, Steps=192389, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5475, Total reward=15.25, Steps=192416, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5476, Total reward=25.63, Steps=192431, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5477, Total reward=15.78, Steps=192451, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5478, Total reward=87.18, Steps=192569, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5479, Total reward=14.02, Steps=192583, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5480, Total reward=78.78, Steps=192653, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5481, Total reward=51.19, Steps=192706, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5482, Total reward=36.63, Steps=192730, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5483, Total reward=0.02, Steps=192751, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5484, Total reward=93.19, Steps=192862, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5485, Total reward=14.81, Steps=192883, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5486, Total reward=50.37, Steps=192937, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5487, Total reward=95.95, Steps=193018, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5488, Total reward=50.92, Steps=193045, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5489, Total reward=52.86, Steps=193101, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5490, Total reward=73.05, Steps=193146, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5491, Total reward=40.15, Steps=193175, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5492, Total reward=3.77, Steps=193187, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5493, Total reward=40.63, Steps=193209, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5494, Total reward=39.18, Steps=193230, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5495, Total reward=17.41, Steps=193257, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5496, Total reward=20.56, Steps=193275, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5497, Total reward=50.31, Steps=193324, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5498, Total reward=55.54, Steps=193381, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5499, Total reward=18.15, Steps=193401, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5500, Total reward=53.06, Steps=193443, Training iteration=109
Policy training> Surrogate loss=0.005023428704589605, KL divergence=0.00010138309880858287, Entropy=0.4691248834133148, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03142666444182396, KL divergence=0.003478423925116658, Entropy=0.4761016368865967, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03436170890927315, KL divergence=0.015249748714268208, Entropy=0.4602973461151123, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.028870031237602234, KL divergence=0.024216482415795326, Entropy=0.44841161370277405, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.018834449350833893, KL divergence=0.03992195054888725, Entropy=0.44958260655403137, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04639570415019989, KL divergence=0.04427774250507355, Entropy=0.45663270354270935, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06058916822075844, KL divergence=0.06554493308067322, Entropy=0.438603013753891, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0461433045566082, KL divergence=0.0740242525935173, Entropy=0.43993985652923584, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06833181530237198, KL divergence=0.07397715747356415, Entropy=0.4480677843093872, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07884756475687027, KL divergence=0.08497148752212524, Entropy=0.4437880516052246, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/402_Step-193443.ckpt']
Uploaded 3 files for checkpoint 402 in 0.50 seconds
saved intermediate frozen graph: current/model/model_402.pb
Best checkpoint number: 355, Last checkpoint number: 400
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'399'}
Training> Name=main_level/agent, Worker=0, Episode=5501, Total reward=29.8, Steps=193468, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5502, Total reward=57.31, Steps=193523, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5503, Total reward=21.71, Steps=193579, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5504, Total reward=7.19, Steps=193613, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5505, Total reward=13.34, Steps=193639, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5506, Total reward=22.3, Steps=193662, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5507, Total reward=45.31, Steps=193701, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5508, Total reward=69.09, Steps=193742, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5509, Total reward=27.59, Steps=193767, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5510, Total reward=54.3, Steps=193815, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5511, Total reward=54.06, Steps=193853, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5512, Total reward=46.62, Steps=193883, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5513, Total reward=44.06, Steps=193912, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5514, Total reward=38.51, Steps=193933, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5515, Total reward=24.67, Steps=193958, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5516, Total reward=25.94, Steps=193988, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5517, Total reward=6.09, Steps=193999, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5518, Total reward=29.7, Steps=194040, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5519, Total reward=11.47, Steps=194062, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5520, Total reward=82.29, Steps=194101, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5521, Total reward=18.03, Steps=194127, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5522, Total reward=42.88, Steps=194166, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5523, Total reward=0.02, Steps=194186, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5524, Total reward=11.72, Steps=194218, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5525, Total reward=65.54, Steps=194288, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5526, Total reward=48.16, Steps=194336, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5527, Total reward=21.62, Steps=194361, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5528, Total reward=80.23, Steps=194426, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5529, Total reward=85.17, Steps=194495, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5530, Total reward=95.85, Steps=194549, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5531, Total reward=41.13, Steps=194587, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5532, Total reward=30.43, Steps=194617, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5533, Total reward=56.52, Steps=194648, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5534, Total reward=37.27, Steps=194669, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5535, Total reward=24.88, Steps=194683, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5536, Total reward=54.91, Steps=194743, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5537, Total reward=45.93, Steps=194780, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5538, Total reward=29.2, Steps=194793, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5539, Total reward=80.49, Steps=194854, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5540, Total reward=76.97, Steps=194892, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5541, Total reward=62.33, Steps=194925, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5542, Total reward=35.65, Steps=194950, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5543, Total reward=6.66, Steps=194978, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5544, Total reward=3.52, Steps=195006, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5545, Total reward=145.25, Steps=195128, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5546, Total reward=93.45, Steps=195195, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5547, Total reward=64.8, Steps=195247, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5548, Total reward=69.82, Steps=195295, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5549, Total reward=27.06, Steps=195318, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5550, Total reward=72.98, Steps=195353, Training iteration=110
Policy training> Surrogate loss=-0.0008185195620171726, KL divergence=0.0001071918013622053, Entropy=0.45436587929725647, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.024321982637047768, KL divergence=0.006347451359033585, Entropy=0.46021589636802673, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.043000251054763794, KL divergence=0.016405243426561356, Entropy=0.44235706329345703, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06017254665493965, KL divergence=0.028757259249687195, Entropy=0.438894122838974, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04725572466850281, KL divergence=0.03945503011345863, Entropy=0.43312934041023254, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06988956034183502, KL divergence=0.05599192902445793, Entropy=0.4408520758152008, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06020693480968475, KL divergence=0.06490607559680939, Entropy=0.435015469789505, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.04120882973074913, KL divergence=0.07607764005661011, Entropy=0.4260588586330414, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0683804303407669, KL divergence=0.07851529866456985, Entropy=0.43570828437805176, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06274520605802536, KL divergence=0.08792046457529068, Entropy=0.43516650795936584, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/403_Step-195353.ckpt']
Uploaded 3 files for checkpoint 403 in 0.52 seconds
saved intermediate frozen graph: current/model/model_403.pb
Best checkpoint number: 355, Last checkpoint number: 401
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'400'}
Training> Name=main_level/agent, Worker=0, Episode=5551, Total reward=18.93, Steps=195366, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5552, Total reward=32.59, Steps=195393, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5553, Total reward=51.66, Steps=195415, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5554, Total reward=43.63, Steps=195437, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5555, Total reward=14.74, Steps=195464, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5556, Total reward=45.14, Steps=195513, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5557, Total reward=42.68, Steps=195546, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5558, Total reward=36.48, Steps=195567, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5559, Total reward=77.41, Steps=195621, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5560, Total reward=71.47, Steps=195661, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5561, Total reward=43.7, Steps=195691, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5562, Total reward=42.74, Steps=195722, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5563, Total reward=89.38, Steps=195825, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5564, Total reward=11.65, Steps=195862, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5565, Total reward=25.68, Steps=195899, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5566, Total reward=10.9, Steps=195924, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5567, Total reward=110.94, Steps=196007, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5568, Total reward=67.49, Steps=196043, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5569, Total reward=35.3, Steps=196077, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5570, Total reward=35.27, Steps=196107, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5571, Total reward=55.79, Steps=196137, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5572, Total reward=62.11, Steps=196169, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5573, Total reward=38.1, Steps=196189, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5574, Total reward=42.58, Steps=196210, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5575, Total reward=33.09, Steps=196240, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5576, Total reward=30.58, Steps=196270, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5577, Total reward=50.15, Steps=196309, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5578, Total reward=44.98, Steps=196359, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5579, Total reward=16.24, Steps=196384, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5580, Total reward=38.44, Steps=196440, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5581, Total reward=53.13, Steps=196468, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5582, Total reward=30.1, Steps=196487, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5583, Total reward=9.42, Steps=196533, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5584, Total reward=17.39, Steps=196562, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5585, Total reward=19.47, Steps=196578, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5586, Total reward=73.23, Steps=196644, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5587, Total reward=64.94, Steps=196686, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5588, Total reward=114.25, Steps=196768, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5589, Total reward=19.4, Steps=196792, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5590, Total reward=71.39, Steps=196844, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5591, Total reward=53.72, Steps=196883, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5592, Total reward=64.44, Steps=196914, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5593, Total reward=64.95, Steps=196944, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5594, Total reward=41.71, Steps=196965, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5595, Total reward=39.88, Steps=196993, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5596, Total reward=31.69, Steps=197036, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5597, Total reward=42.16, Steps=197074, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5598, Total reward=22.84, Steps=197098, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5599, Total reward=23.27, Steps=197126, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5600, Total reward=61.05, Steps=197178, Training iteration=111
Policy training> Surrogate loss=0.013508358038961887, KL divergence=0.00010871291306102648, Entropy=0.47084975242614746, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03191119804978371, KL divergence=0.005776233971118927, Entropy=0.48103657364845276, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.040067270398139954, KL divergence=0.01616329699754715, Entropy=0.4571005403995514, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.059265073388814926, KL divergence=0.02915852703154087, Entropy=0.45450925827026367, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05425811931490898, KL divergence=0.04001009091734886, Entropy=0.44980788230895996, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06505569070577621, KL divergence=0.05321216955780983, Entropy=0.44709262251853943, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07789745926856995, KL divergence=0.06459704041481018, Entropy=0.4565030038356781, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07515766471624374, KL divergence=0.07166583091020584, Entropy=0.4499434530735016, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05980278551578522, KL divergence=0.07806649059057236, Entropy=0.45139506459236145, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0644162967801094, KL divergence=0.0897265300154686, Entropy=0.4534313678741455, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/404_Step-197178.ckpt']
Uploaded 3 files for checkpoint 404 in 0.63 seconds
saved intermediate frozen graph: current/model/model_404.pb
Best checkpoint number: 355, Last checkpoint number: 402
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'401'}
Training> Name=main_level/agent, Worker=0, Episode=5601, Total reward=116.14, Steps=197297, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5602, Total reward=26.93, Steps=197313, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5603, Total reward=14.89, Steps=197359, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5604, Total reward=64.59, Steps=197423, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5605, Total reward=69.44, Steps=197496, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5606, Total reward=47.85, Steps=197533, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5607, Total reward=62.43, Steps=197574, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5608, Total reward=57.91, Steps=197616, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5609, Total reward=27.65, Steps=197631, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5610, Total reward=84.71, Steps=197677, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5611, Total reward=68.05, Steps=197712, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5612, Total reward=38.05, Steps=197743, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5613, Total reward=43.45, Steps=197769, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5614, Total reward=42.68, Steps=197789, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5615, Total reward=18.27, Steps=197815, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5616, Total reward=24.23, Steps=197835, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5617, Total reward=54.44, Steps=197885, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5618, Total reward=42.12, Steps=197908, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5619, Total reward=75.72, Steps=197970, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5620, Total reward=86.25, Steps=198014, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5621, Total reward=64.51, Steps=198045, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5622, Total reward=35.51, Steps=198075, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5623, Total reward=20.23, Steps=198112, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5624, Total reward=103.57, Steps=198203, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5625, Total reward=12.95, Steps=198242, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5626, Total reward=64.01, Steps=198289, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5627, Total reward=122.13, Steps=198396, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5628, Total reward=81.14, Steps=198465, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5629, Total reward=60.72, Steps=198522, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5630, Total reward=69.48, Steps=198572, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5631, Total reward=52.44, Steps=198616, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5632, Total reward=21.75, Steps=198644, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5633, Total reward=56.89, Steps=198672, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5634, Total reward=40.61, Steps=198694, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5635, Total reward=25.17, Steps=198720, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5636, Total reward=33.56, Steps=198769, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5637, Total reward=17.23, Steps=198785, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5638, Total reward=21.42, Steps=198799, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5639, Total reward=18.67, Steps=198822, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5640, Total reward=78.5, Steps=198874, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5641, Total reward=69.76, Steps=198905, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5642, Total reward=32.52, Steps=198933, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5643, Total reward=12.85, Steps=198969, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5644, Total reward=20.35, Steps=199007, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5645, Total reward=83.84, Steps=199064, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5646, Total reward=104.71, Steps=199155, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5647, Total reward=13.1, Steps=199191, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5648, Total reward=64.32, Steps=199246, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5649, Total reward=24.14, Steps=199278, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5650, Total reward=74.14, Steps=199324, Training iteration=112
Policy training> Surrogate loss=0.002705065067857504, KL divergence=0.0002791791339404881, Entropy=0.4531828761100769, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04090273380279541, KL divergence=0.007808706723153591, Entropy=0.45141661167144775, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0540284626185894, KL divergence=0.025382492691278458, Entropy=0.4451471269130707, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05956846475601196, KL divergence=0.04141265153884888, Entropy=0.43245211243629456, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06721725314855576, KL divergence=0.05746673420071602, Entropy=0.43085068464279175, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06744050979614258, KL divergence=0.06930577754974365, Entropy=0.42850932478904724, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07736483961343765, KL divergence=0.07808186858892441, Entropy=0.42708316445350647, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08067330718040466, KL divergence=0.08405324816703796, Entropy=0.42756539583206177, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07481595128774643, KL divergence=0.09058548510074615, Entropy=0.4262970983982086, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07293678820133209, KL divergence=0.09365208446979523, Entropy=0.4288368821144104, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/405_Step-199324.ckpt']
Uploaded 3 files for checkpoint 405 in 0.57 seconds
saved intermediate frozen graph: current/model/model_405.pb
Best checkpoint number: 355, Last checkpoint number: 403
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'402'}
Training> Name=main_level/agent, Worker=0, Episode=5651, Total reward=87.88, Steps=199374, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5652, Total reward=68.47, Steps=199415, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5653, Total reward=43.74, Steps=199434, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5654, Total reward=23.79, Steps=199457, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5655, Total reward=43.7, Steps=199487, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5656, Total reward=18.23, Steps=199522, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5657, Total reward=111.45, Steps=199613, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5658, Total reward=31.88, Steps=199626, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5659, Total reward=12.74, Steps=199647, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5660, Total reward=37.23, Steps=199684, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5661, Total reward=56.55, Steps=199714, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5662, Total reward=33.72, Steps=199734, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5663, Total reward=6.98, Steps=199757, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5664, Total reward=67.7, Steps=199844, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5665, Total reward=29.5, Steps=199900, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5666, Total reward=85.31, Steps=199953, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5667, Total reward=69.27, Steps=200015, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5668, Total reward=66.95, Steps=200063, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5669, Total reward=17.4, Steps=200078, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5670, Total reward=90.72, Steps=200112, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5671, Total reward=28.5, Steps=200131, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5672, Total reward=55.74, Steps=200173, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5673, Total reward=54.2, Steps=200200, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5674, Total reward=36.69, Steps=200221, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5675, Total reward=39.04, Steps=200251, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5676, Total reward=32.77, Steps=200293, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5677, Total reward=49.28, Steps=200329, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5678, Total reward=31.95, Steps=200364, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5679, Total reward=69.65, Steps=200419, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5680, Total reward=74.86, Steps=200455, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5681, Total reward=54.74, Steps=200485, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5682, Total reward=47.54, Steps=200521, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5683, Total reward=3.12, Steps=200544, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5684, Total reward=39.36, Steps=200617, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5685, Total reward=64.86, Steps=200688, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5686, Total reward=97.67, Steps=200759, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5687, Total reward=68.4, Steps=200821, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5688, Total reward=56.69, Steps=200860, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5689, Total reward=25.39, Steps=200884, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5690, Total reward=61.9, Steps=200926, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5691, Total reward=50.51, Steps=200964, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5692, Total reward=50.48, Steps=200994, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5693, Total reward=37.1, Steps=201015, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5694, Total reward=50.12, Steps=201038, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5695, Total reward=25.49, Steps=201065, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5696, Total reward=18.86, Steps=201083, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5697, Total reward=101.38, Steps=201173, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5698, Total reward=47.45, Steps=201194, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5699, Total reward=12.76, Steps=201212, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5700, Total reward=70.75, Steps=201249, Training iteration=113
Policy training> Surrogate loss=-0.008783253841102123, KL divergence=8.867649739840999e-05, Entropy=0.4352820813655853, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.028565319254994392, KL divergence=0.0036878029350191355, Entropy=0.4440089762210846, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03599725291132927, KL divergence=0.0123600447550416, Entropy=0.4288666546344757, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06349900364875793, KL divergence=0.022846398875117302, Entropy=0.4285218417644501, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06408758461475372, KL divergence=0.03463016822934151, Entropy=0.4149315059185028, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04596533253788948, KL divergence=0.04587613046169281, Entropy=0.4173877239227295, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07282956689596176, KL divergence=0.05575131997466087, Entropy=0.4120958745479584, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06834327429533005, KL divergence=0.06433791667222977, Entropy=0.4113100469112396, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06819332391023636, KL divergence=0.06880467385053635, Entropy=0.4162185490131378, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07490944862365723, KL divergence=0.07639393210411072, Entropy=0.41795477271080017, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/406_Step-201249.ckpt']
Uploaded 3 files for checkpoint 406 in 0.59 seconds
saved intermediate frozen graph: current/model/model_406.pb
Best checkpoint number: 355, Last checkpoint number: 404
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'403'}
Training> Name=main_level/agent, Worker=0, Episode=5701, Total reward=57.67, Steps=201289, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5702, Total reward=40.05, Steps=201333, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5703, Total reward=69.63, Steps=201438, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5704, Total reward=17.05, Steps=201471, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5705, Total reward=24.12, Steps=201511, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5706, Total reward=88.05, Steps=201574, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5707, Total reward=72.46, Steps=201646, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5708, Total reward=57.57, Steps=201675, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5709, Total reward=78.93, Steps=201731, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5710, Total reward=74.92, Steps=201779, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5711, Total reward=29.37, Steps=201803, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5712, Total reward=64.74, Steps=201847, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5713, Total reward=40.81, Steps=201867, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5714, Total reward=44.58, Steps=201889, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5715, Total reward=36.99, Steps=201917, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5716, Total reward=26.6, Steps=201934, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5717, Total reward=39.25, Steps=201966, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5718, Total reward=46.55, Steps=201990, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5719, Total reward=10.35, Steps=202012, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5720, Total reward=62.49, Steps=202057, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5721, Total reward=8.68, Steps=202073, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5722, Total reward=36.4, Steps=202093, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5723, Total reward=11.0, Steps=202141, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5724, Total reward=75.96, Steps=202210, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5725, Total reward=10.12, Steps=202228, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5726, Total reward=93.99, Steps=202285, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5727, Total reward=78.63, Steps=202347, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5728, Total reward=61.95, Steps=202377, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5729, Total reward=32.85, Steps=202411, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5730, Total reward=21.28, Steps=202441, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5731, Total reward=68.68, Steps=202478, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5732, Total reward=71.0, Steps=202520, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5733, Total reward=0.02, Steps=202536, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5734, Total reward=44.85, Steps=202568, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5735, Total reward=30.08, Steps=202595, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5736, Total reward=12.54, Steps=202610, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5737, Total reward=45.51, Steps=202653, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5738, Total reward=39.74, Steps=202688, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5739, Total reward=68.96, Steps=202754, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5740, Total reward=69.39, Steps=202790, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5741, Total reward=58.25, Steps=202826, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5742, Total reward=34.47, Steps=202847, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5743, Total reward=12.74, Steps=202881, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5744, Total reward=5.92, Steps=202910, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5745, Total reward=16.25, Steps=202934, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5746, Total reward=46.98, Steps=202981, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5747, Total reward=74.7, Steps=203023, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5748, Total reward=127.34, Steps=203100, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5749, Total reward=34.6, Steps=203148, Training iteration=114
Training> Name=main_level/agent, Worker=0, Episode=5750, Total reward=70.98, Steps=203197, Training iteration=114
Policy training> Surrogate loss=-0.008728764951229095, KL divergence=0.00012435820826794952, Entropy=0.46741223335266113, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.015549573116004467, KL divergence=0.0049026827327907085, Entropy=0.46955442428588867, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.039667945355176926, KL divergence=0.013884571380913258, Entropy=0.46291056275367737, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05572286620736122, KL divergence=0.028054751455783844, Entropy=0.46330228447914124, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05518348142504692, KL divergence=0.04037703946232796, Entropy=0.458690881729126, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05251728370785713, KL divergence=0.04965786263346672, Entropy=0.4591103494167328, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04659562185406685, KL divergence=0.060535430908203125, Entropy=0.4524538815021515, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08722098916769028, KL divergence=0.07264818996191025, Entropy=0.4555964767932892, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08426464349031448, KL divergence=0.07657334953546524, Entropy=0.4400298595428467, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07620617002248764, KL divergence=0.08563607186079025, Entropy=0.4526419937610626, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/407_Step-203197.ckpt']
Uploaded 3 files for checkpoint 407 in 0.52 seconds
saved intermediate frozen graph: current/model/model_407.pb
Best checkpoint number: 355, Last checkpoint number: 405
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'404'}
Training> Name=main_level/agent, Worker=0, Episode=5751, Total reward=49.3, Steps=203225, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5752, Total reward=51.83, Steps=203257, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5753, Total reward=49.2, Steps=203286, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5754, Total reward=40.13, Steps=203307, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5755, Total reward=37.54, Steps=203334, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5756, Total reward=38.29, Steps=203361, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5757, Total reward=44.49, Steps=203399, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5758, Total reward=34.51, Steps=203445, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5759, Total reward=16.9, Steps=203469, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5760, Total reward=51.93, Steps=203507, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5761, Total reward=53.37, Steps=203537, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5762, Total reward=41.55, Steps=203562, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5763, Total reward=62.23, Steps=203642, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5764, Total reward=13.08, Steps=203704, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5765, Total reward=90.49, Steps=203817, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5766, Total reward=14.25, Steps=203836, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5767, Total reward=72.66, Steps=203887, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5768, Total reward=66.19, Steps=203916, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5769, Total reward=22.99, Steps=203935, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5770, Total reward=73.21, Steps=203982, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5771, Total reward=73.82, Steps=204020, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5772, Total reward=57.94, Steps=204062, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5773, Total reward=47.15, Steps=204091, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5774, Total reward=27.68, Steps=204110, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5775, Total reward=36.76, Steps=204138, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5776, Total reward=22.9, Steps=204175, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5777, Total reward=46.33, Steps=204221, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5778, Total reward=104.78, Steps=204283, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5779, Total reward=23.08, Steps=204306, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5780, Total reward=82.28, Steps=204342, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5781, Total reward=47.72, Steps=204379, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5782, Total reward=40.02, Steps=204404, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5783, Total reward=0.02, Steps=204428, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5784, Total reward=6.99, Steps=204455, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5785, Total reward=104.92, Steps=204576, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5786, Total reward=113.22, Steps=204682, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5787, Total reward=71.28, Steps=204726, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5788, Total reward=59.0, Steps=204768, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5789, Total reward=38.8, Steps=204820, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5790, Total reward=45.85, Steps=204864, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5791, Total reward=60.08, Steps=204908, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5792, Total reward=7.53, Steps=204920, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5793, Total reward=46.36, Steps=204950, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5794, Total reward=44.51, Steps=204972, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5795, Total reward=29.75, Steps=205003, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5796, Total reward=20.39, Steps=205020, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5797, Total reward=66.36, Steps=205103, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5798, Total reward=23.2, Steps=205122, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5799, Total reward=81.33, Steps=205214, Training iteration=115
Training> Name=main_level/agent, Worker=0, Episode=5800, Total reward=82.19, Steps=205266, Training iteration=115
Policy training> Surrogate loss=-0.00211914349347353, KL divergence=0.0002619569713715464, Entropy=0.45954686403274536, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03438085690140724, KL divergence=0.00982108898460865, Entropy=0.4612305462360382, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04798731207847595, KL divergence=0.028219249099493027, Entropy=0.46045440435409546, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05867458134889603, KL divergence=0.04256891459226608, Entropy=0.4560372531414032, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06090567260980606, KL divergence=0.05655732378363609, Entropy=0.4535406827926636, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.059882380068302155, KL divergence=0.06771925091743469, Entropy=0.4507419466972351, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06304198503494263, KL divergence=0.07722800970077515, Entropy=0.4500049352645874, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06855372339487076, KL divergence=0.0837184339761734, Entropy=0.4506814479827881, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06477117538452148, KL divergence=0.08780695497989655, Entropy=0.45029014348983765, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07126736640930176, KL divergence=0.09412535279989243, Entropy=0.4521949887275696, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/408_Step-205266.ckpt']
Uploaded 3 files for checkpoint 408 in 0.52 seconds
saved intermediate frozen graph: current/model/model_408.pb
Best checkpoint number: 355, Last checkpoint number: 406
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'405'}
Training> Name=main_level/agent, Worker=0, Episode=5801, Total reward=58.64, Steps=205312, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5802, Total reward=34.96, Steps=205352, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5803, Total reward=0.01, Steps=205366, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5804, Total reward=20.22, Steps=205402, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5805, Total reward=78.58, Steps=205460, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5806, Total reward=72.01, Steps=205532, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5807, Total reward=61.97, Steps=205576, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5808, Total reward=68.99, Steps=205605, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5809, Total reward=27.29, Steps=205626, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5810, Total reward=65.71, Steps=205672, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5811, Total reward=43.48, Steps=205710, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5812, Total reward=44.67, Steps=205741, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5813, Total reward=42.33, Steps=205769, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5814, Total reward=34.67, Steps=205790, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5815, Total reward=31.07, Steps=205817, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5816, Total reward=24.69, Steps=205833, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5817, Total reward=128.35, Steps=205906, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5818, Total reward=55.57, Steps=205974, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5819, Total reward=77.38, Steps=206030, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5820, Total reward=72.42, Steps=206073, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5821, Total reward=54.36, Steps=206101, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5822, Total reward=21.19, Steps=206119, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5823, Total reward=5.3, Steps=206158, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5824, Total reward=11.2, Steps=206186, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5825, Total reward=26.86, Steps=206239, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5826, Total reward=87.4, Steps=206289, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5827, Total reward=23.52, Steps=206323, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5828, Total reward=66.12, Steps=206361, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5829, Total reward=60.67, Steps=206425, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5830, Total reward=15.34, Steps=206440, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5831, Total reward=29.12, Steps=206463, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5832, Total reward=53.32, Steps=206503, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5833, Total reward=38.9, Steps=206523, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5834, Total reward=39.84, Steps=206545, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5835, Total reward=23.97, Steps=206572, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5836, Total reward=21.05, Steps=206592, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5837, Total reward=130.16, Steps=206700, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5838, Total reward=31.17, Steps=206714, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5839, Total reward=58.19, Steps=206767, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5840, Total reward=93.78, Steps=206825, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5841, Total reward=58.73, Steps=206857, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5842, Total reward=33.3, Steps=206886, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5843, Total reward=0.02, Steps=206901, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5844, Total reward=64.48, Steps=206968, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5845, Total reward=66.24, Steps=207034, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5846, Total reward=103.91, Steps=207124, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5847, Total reward=52.57, Steps=207161, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5848, Total reward=63.14, Steps=207209, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5849, Total reward=43.84, Steps=207244, Training iteration=116
Training> Name=main_level/agent, Worker=0, Episode=5850, Total reward=32.89, Steps=207298, Training iteration=116
Policy training> Surrogate loss=-0.002012992277741432, KL divergence=6.185512756928802e-05, Entropy=0.480581670999527, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.015778671950101852, KL divergence=0.0029762873891741037, Entropy=0.46647319197654724, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.021324975416064262, KL divergence=0.01146146934479475, Entropy=0.4541703164577484, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04184238985180855, KL divergence=0.023420730605721474, Entropy=0.4524255096912384, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.03533255681395531, KL divergence=0.035213273018598557, Entropy=0.46179020404815674, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06226201355457306, KL divergence=0.04372238740324974, Entropy=0.4526742398738861, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05696259066462517, KL divergence=0.05175377056002617, Entropy=0.44034552574157715, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0718723013997078, KL divergence=0.05789601802825928, Entropy=0.4477410614490509, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07098261266946793, KL divergence=0.06522519141435623, Entropy=0.4496077001094818, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0715905949473381, KL divergence=0.07267531752586365, Entropy=0.4469020664691925, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/409_Step-207298.ckpt']
Uploaded 3 files for checkpoint 409 in 0.57 seconds
saved intermediate frozen graph: current/model/model_409.pb
Best checkpoint number: 355, Last checkpoint number: 407
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'406'}
Training> Name=main_level/agent, Worker=0, Episode=5851, Total reward=71.4, Steps=207334, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5852, Total reward=53.28, Steps=207367, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5853, Total reward=28.18, Steps=207386, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5854, Total reward=45.39, Steps=207417, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5855, Total reward=33.17, Steps=207446, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5856, Total reward=22.73, Steps=207477, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5857, Total reward=56.69, Steps=207515, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5858, Total reward=30.78, Steps=207552, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5859, Total reward=75.98, Steps=207628, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5860, Total reward=94.75, Steps=207699, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5861, Total reward=74.14, Steps=207729, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5862, Total reward=36.26, Steps=207773, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5863, Total reward=72.81, Steps=207853, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5864, Total reward=10.11, Steps=207890, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5865, Total reward=22.56, Steps=207906, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5866, Total reward=76.64, Steps=207955, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5867, Total reward=61.78, Steps=207995, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5868, Total reward=102.39, Steps=208070, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5869, Total reward=15.81, Steps=208085, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5870, Total reward=81.14, Steps=208132, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5871, Total reward=62.94, Steps=208170, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5872, Total reward=47.17, Steps=208203, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5873, Total reward=25.2, Steps=208214, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5874, Total reward=42.61, Steps=208235, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5875, Total reward=37.07, Steps=208264, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5876, Total reward=22.86, Steps=208281, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5877, Total reward=50.34, Steps=208332, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5878, Total reward=71.99, Steps=208381, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5879, Total reward=15.75, Steps=208401, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5880, Total reward=80.65, Steps=208441, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5881, Total reward=56.28, Steps=208475, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5882, Total reward=16.44, Steps=208490, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5883, Total reward=7.24, Steps=208528, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5884, Total reward=73.17, Steps=208618, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5885, Total reward=18.45, Steps=208639, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5886, Total reward=80.79, Steps=208683, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5887, Total reward=110.98, Steps=208773, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5888, Total reward=61.39, Steps=208817, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5889, Total reward=86.61, Steps=208879, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5890, Total reward=40.36, Steps=208903, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5891, Total reward=28.94, Steps=208947, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5892, Total reward=59.07, Steps=208980, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5893, Total reward=37.83, Steps=209000, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5894, Total reward=36.4, Steps=209021, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5895, Total reward=28.38, Steps=209053, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5896, Total reward=24.05, Steps=209072, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5897, Total reward=56.93, Steps=209119, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5898, Total reward=50.69, Steps=209154, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5899, Total reward=9.76, Steps=209176, Training iteration=117
Training> Name=main_level/agent, Worker=0, Episode=5900, Total reward=53.69, Steps=209210, Training iteration=117
Policy training> Surrogate loss=0.0057663447223603725, KL divergence=8.330481796292588e-05, Entropy=0.45464038848876953, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.034010354429483414, KL divergence=0.003701587440446019, Entropy=0.4577622413635254, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.044405728578567505, KL divergence=0.012375235557556152, Entropy=0.4444471299648285, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04534873366355896, KL divergence=0.02337745390832424, Entropy=0.43407759070396423, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.038262080401182175, KL divergence=0.037022631615400314, Entropy=0.4348304271697998, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04927778244018555, KL divergence=0.04633541777729988, Entropy=0.43970194458961487, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05573497340083122, KL divergence=0.05459733307361603, Entropy=0.422661691904068, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06622092425823212, KL divergence=0.05670883134007454, Entropy=0.42417871952056885, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05315139889717102, KL divergence=0.0677926316857338, Entropy=0.42377832531929016, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08274146169424057, KL divergence=0.07672342658042908, Entropy=0.4212733209133148, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/410_Step-209210.ckpt']
Uploaded 3 files for checkpoint 410 in 0.52 seconds
saved intermediate frozen graph: current/model/model_410.pb
Best checkpoint number: 355, Last checkpoint number: 408
Copying the frozen checkpoint from ./frozen_models/agent/model_355.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'407'}
Training> Name=main_level/agent, Worker=0, Episode=5901, Total reward=65.86, Steps=209267, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5902, Total reward=28.47, Steps=209289, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5903, Total reward=13.11, Steps=209318, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5904, Total reward=64.38, Steps=209401, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5905, Total reward=96.79, Steps=209471, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5906, Total reward=24.08, Steps=209496, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5907, Total reward=63.53, Steps=209538, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5908, Total reward=140.82, Steps=209627, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5909, Total reward=67.49, Steps=209687, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5910, Total reward=47.66, Steps=209735, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5911, Total reward=33.97, Steps=209779, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5912, Total reward=70.12, Steps=209821, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5913, Total reward=43.55, Steps=209842, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5914, Total reward=40.79, Steps=209864, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5915, Total reward=36.27, Steps=209890, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5916, Total reward=23.58, Steps=209920, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5917, Total reward=102.61, Steps=210014, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5918, Total reward=34.62, Steps=210034, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5919, Total reward=16.99, Steps=210057, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5920, Total reward=69.62, Steps=210108, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5921, Total reward=69.35, Steps=210157, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5922, Total reward=49.22, Steps=210224, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5923, Total reward=3.42, Steps=210244, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5924, Total reward=19.66, Steps=210284, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5925, Total reward=9.78, Steps=210299, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5926, Total reward=59.42, Steps=210355, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5927, Total reward=51.97, Steps=210395, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5928, Total reward=42.21, Steps=210422, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5929, Total reward=64.05, Steps=210469, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5930, Total reward=67.26, Steps=210516, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5931, Total reward=48.9, Steps=210555, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5932, Total reward=65.55, Steps=210598, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5933, Total reward=40.44, Steps=210619, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5934, Total reward=43.4, Steps=210640, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5935, Total reward=17.25, Steps=210655, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5936, Total reward=21.97, Steps=210688, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5937, Total reward=78.53, Steps=210779, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5938, Total reward=36.76, Steps=210803, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5939, Total reward=67.4, Steps=210869, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5940, Total reward=63.44, Steps=210914, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5941, Total reward=63.32, Steps=210944, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5942, Total reward=34.5, Steps=210968, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5943, Total reward=13.45, Steps=211007, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5944, Total reward=90.35, Steps=211116, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5945, Total reward=16.73, Steps=211132, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5946, Total reward=67.8, Steps=211199, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5947, Total reward=57.96, Steps=211243, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5948, Total reward=54.92, Steps=211272, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5949, Total reward=71.58, Steps=211328, Training iteration=118
Training> Name=main_level/agent, Worker=0, Episode=5950, Total reward=63.33, Steps=211375, Training iteration=118
Policy training> Surrogate loss=0.00152116478420794, KL divergence=0.000528092437889427, Entropy=0.4880228042602539, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.030410010367631912, KL divergence=0.00899141002446413, Entropy=0.4826306104660034, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05392047017812729, KL divergence=0.020941920578479767, Entropy=0.4804690480232239, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06310948729515076, KL divergence=0.03561585769057274, Entropy=0.47011828422546387, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.07006541639566422, KL divergence=0.04956701397895813, Entropy=0.47192293405532837, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07655808329582214, KL divergence=0.0607876181602478, Entropy=0.46294474601745605, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06381934136152267, KL divergence=0.06845727562904358, Entropy=0.46817487478256226, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07254594564437866, KL divergence=0.07611481100320816, Entropy=0.4688429832458496, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07766726613044739, KL divergence=0.0812644511461258, Entropy=0.47002410888671875, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07848112285137177, KL divergence=0.0853172019124031, Entropy=0.4661707282066345, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/411_Step-211375.ckpt']
Uploaded 3 files for checkpoint 411 in 0.52 seconds
saved intermediate frozen graph: current/model/model_411.pb
Best checkpoint number: 409, Last checkpoint number: 409
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'355'}
Training> Name=main_level/agent, Worker=0, Episode=5951, Total reward=78.14, Steps=211414, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5952, Total reward=60.42, Steps=211447, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5953, Total reward=37.01, Steps=211466, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5954, Total reward=19.39, Steps=211488, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5955, Total reward=18.28, Steps=211542, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5956, Total reward=21.16, Steps=211559, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5957, Total reward=81.56, Steps=211638, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5958, Total reward=123.91, Steps=211716, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5959, Total reward=50.94, Steps=211770, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5960, Total reward=82.07, Steps=211821, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5961, Total reward=69.29, Steps=211875, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5962, Total reward=35.94, Steps=211902, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5963, Total reward=14.68, Steps=211950, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5964, Total reward=55.77, Steps=212015, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5965, Total reward=13.07, Steps=212041, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5966, Total reward=80.1, Steps=212102, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5967, Total reward=52.16, Steps=212143, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5968, Total reward=106.11, Steps=212215, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5969, Total reward=78.48, Steps=212262, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5970, Total reward=24.59, Steps=212291, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5971, Total reward=54.32, Steps=212329, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5972, Total reward=44.52, Steps=212360, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5973, Total reward=52.67, Steps=212389, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5974, Total reward=28.82, Steps=212401, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5975, Total reward=27.66, Steps=212433, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5976, Total reward=23.88, Steps=212449, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5977, Total reward=54.38, Steps=212510, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5978, Total reward=38.89, Steps=212532, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5979, Total reward=98.35, Steps=212619, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5980, Total reward=45.39, Steps=212658, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5981, Total reward=42.66, Steps=212679, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5982, Total reward=41.56, Steps=212707, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5983, Total reward=10.61, Steps=212734, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5984, Total reward=122.12, Steps=212860, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5985, Total reward=63.19, Steps=212940, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5986, Total reward=119.1, Steps=213035, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5987, Total reward=69.56, Steps=213075, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5988, Total reward=59.32, Steps=213130, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5989, Total reward=56.69, Steps=213193, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5990, Total reward=72.97, Steps=213259, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5991, Total reward=76.92, Steps=213302, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5992, Total reward=50.04, Steps=213333, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5993, Total reward=31.48, Steps=213357, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5994, Total reward=29.93, Steps=213368, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5995, Total reward=0.01, Steps=213382, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5996, Total reward=21.94, Steps=213417, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5997, Total reward=46.41, Steps=213453, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5998, Total reward=43.65, Steps=213475, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=5999, Total reward=8.22, Steps=213488, Training iteration=119
Training> Name=main_level/agent, Worker=0, Episode=6000, Total reward=81.74, Steps=213544, Training iteration=119
Policy training> Surrogate loss=0.007767281029373407, KL divergence=0.000322208070429042, Entropy=0.48781540989875793, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03285990655422211, KL divergence=0.007223453372716904, Entropy=0.48189327120780945, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05437454208731651, KL divergence=0.01977377198636532, Entropy=0.4770774841308594, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0605565682053566, KL divergence=0.03371585160493851, Entropy=0.464407742023468, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06643745303153992, KL divergence=0.04707757383584976, Entropy=0.45872414112091064, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06494422256946564, KL divergence=0.05683746561408043, Entropy=0.4538130760192871, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08303848654031754, KL divergence=0.06685447692871094, Entropy=0.45317021012306213, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08054700493812561, KL divergence=0.07367405295372009, Entropy=0.45195460319519043, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06946638226509094, KL divergence=0.07985047996044159, Entropy=0.4534710943698883, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08160372078418732, KL divergence=0.08406209945678711, Entropy=0.4584459662437439, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/412_Step-213544.ckpt']
Uploaded 3 files for checkpoint 412 in 0.68 seconds
saved intermediate frozen graph: current/model/model_412.pb
Best checkpoint number: 409, Last checkpoint number: 410
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'408'}
Training> Name=main_level/agent, Worker=0, Episode=6001, Total reward=65.12, Steps=213583, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6002, Total reward=34.76, Steps=213623, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6003, Total reward=3.2, Steps=213650, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6004, Total reward=152.42, Steps=213750, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6005, Total reward=78.8, Steps=213826, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6006, Total reward=94.03, Steps=213875, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6007, Total reward=63.78, Steps=213929, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6008, Total reward=62.19, Steps=213956, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6009, Total reward=83.69, Steps=214012, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6010, Total reward=71.5, Steps=214064, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6011, Total reward=54.63, Steps=214091, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6012, Total reward=54.62, Steps=214122, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6013, Total reward=60.93, Steps=214153, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6014, Total reward=44.72, Steps=214184, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6015, Total reward=30.2, Steps=214210, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6016, Total reward=22.19, Steps=214241, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6017, Total reward=43.94, Steps=214276, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6018, Total reward=67.43, Steps=214381, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6019, Total reward=78.89, Steps=214432, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6020, Total reward=86.11, Steps=214472, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6021, Total reward=60.81, Steps=214503, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6022, Total reward=28.93, Steps=214520, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6023, Total reward=64.16, Steps=214628, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6024, Total reward=20.54, Steps=214655, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6025, Total reward=112.83, Steps=214771, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6026, Total reward=60.74, Steps=214814, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6027, Total reward=75.86, Steps=214864, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6028, Total reward=56.91, Steps=214892, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6029, Total reward=68.45, Steps=214942, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6030, Total reward=54.08, Steps=214991, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6031, Total reward=78.97, Steps=215028, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6032, Total reward=39.53, Steps=215059, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6033, Total reward=39.2, Steps=215081, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6034, Total reward=45.9, Steps=215103, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6035, Total reward=35.48, Steps=215130, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6036, Total reward=25.08, Steps=215148, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6037, Total reward=49.55, Steps=215196, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6038, Total reward=86.21, Steps=215266, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6039, Total reward=54.53, Steps=215326, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6040, Total reward=66.13, Steps=215385, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6041, Total reward=67.73, Steps=215416, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6042, Total reward=40.61, Steps=215466, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6043, Total reward=2.99, Steps=215490, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6044, Total reward=24.68, Steps=215539, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6045, Total reward=6.68, Steps=215555, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6046, Total reward=24.95, Steps=215604, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6047, Total reward=109.35, Steps=215705, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6048, Total reward=77.17, Steps=215768, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6049, Total reward=27.83, Steps=215807, Training iteration=120
Training> Name=main_level/agent, Worker=0, Episode=6050, Total reward=63.45, Steps=215844, Training iteration=120
Policy training> Surrogate loss=-0.008686421439051628, KL divergence=0.00032317591831088066, Entropy=0.46003246307373047, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.026553887873888016, KL divergence=0.007231419440358877, Entropy=0.4603836238384247, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04304151237010956, KL divergence=0.021306905895471573, Entropy=0.4542723000049591, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05113022029399872, KL divergence=0.03522887080907822, Entropy=0.4513559937477112, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06545165181159973, KL divergence=0.049927085638046265, Entropy=0.4426462650299072, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08336734026670456, KL divergence=0.05828186124563217, Entropy=0.4451354146003723, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05675280839204788, KL divergence=0.06827893108129501, Entropy=0.43901777267456055, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.061909791082143784, KL divergence=0.0747227594256401, Entropy=0.4375542998313904, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07349343597888947, KL divergence=0.07701799273490906, Entropy=0.44817501306533813, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06700894981622696, KL divergence=0.08265305310487747, Entropy=0.44826024770736694, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/413_Step-215844.ckpt']
Uploaded 3 files for checkpoint 413 in 0.52 seconds
saved intermediate frozen graph: current/model/model_413.pb
Best checkpoint number: 409, Last checkpoint number: 411
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'410'}
Training> Name=main_level/agent, Worker=0, Episode=6051, Total reward=29.98, Steps=215874, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6052, Total reward=53.84, Steps=215915, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6053, Total reward=0.01, Steps=215928, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6054, Total reward=51.43, Steps=215950, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6055, Total reward=8.32, Steps=215965, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6056, Total reward=21.65, Steps=215995, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6057, Total reward=9.96, Steps=216016, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6058, Total reward=25.42, Steps=216029, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6059, Total reward=143.54, Steps=216165, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6060, Total reward=51.87, Steps=216207, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6061, Total reward=41.2, Steps=216236, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6062, Total reward=28.3, Steps=216258, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6063, Total reward=5.83, Steps=216282, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6064, Total reward=128.59, Steps=216403, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6065, Total reward=78.75, Steps=216464, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6066, Total reward=90.35, Steps=216546, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6067, Total reward=74.29, Steps=216614, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6068, Total reward=134.39, Steps=216698, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6069, Total reward=53.43, Steps=216758, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6070, Total reward=78.95, Steps=216809, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6071, Total reward=12.75, Steps=216834, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6072, Total reward=51.73, Steps=216875, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6073, Total reward=37.91, Steps=216896, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6074, Total reward=37.77, Steps=216917, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6075, Total reward=7.05, Steps=216926, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6076, Total reward=23.64, Steps=216945, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6077, Total reward=57.87, Steps=216993, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6078, Total reward=42.77, Steps=217035, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6079, Total reward=82.42, Steps=217088, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6080, Total reward=66.64, Steps=217127, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6081, Total reward=42.28, Steps=217156, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6082, Total reward=40.1, Steps=217181, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6083, Total reward=16.43, Steps=217214, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6084, Total reward=62.35, Steps=217270, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6085, Total reward=16.76, Steps=217303, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6086, Total reward=120.61, Steps=217401, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6087, Total reward=64.02, Steps=217457, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6088, Total reward=46.01, Steps=217473, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6089, Total reward=85.58, Steps=217544, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6090, Total reward=68.78, Steps=217591, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6091, Total reward=91.08, Steps=217633, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6092, Total reward=41.62, Steps=217665, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6093, Total reward=55.69, Steps=217697, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6094, Total reward=42.85, Steps=217718, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6095, Total reward=23.95, Steps=217749, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6096, Total reward=20.06, Steps=217783, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6097, Total reward=34.56, Steps=217816, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6098, Total reward=34.07, Steps=217837, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6099, Total reward=9.45, Steps=217853, Training iteration=121
Training> Name=main_level/agent, Worker=0, Episode=6100, Total reward=46.69, Steps=217888, Training iteration=121
Policy training> Surrogate loss=-0.006496982183307409, KL divergence=0.00013650269829668105, Entropy=0.4760059416294098, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.021104438230395317, KL divergence=0.004170229192823172, Entropy=0.48192477226257324, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.053521979600191116, KL divergence=0.01382018905133009, Entropy=0.46466413140296936, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0391019769012928, KL divergence=0.02265278995037079, Entropy=0.46908318996429443, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05937987565994263, KL divergence=0.03292270004749298, Entropy=0.4681837260723114, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.053550586104393005, KL divergence=0.0438898466527462, Entropy=0.4611797332763672, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07100513577461243, KL divergence=0.05184326693415642, Entropy=0.454552561044693, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07414358854293823, KL divergence=0.06062201038002968, Entropy=0.4582531452178955, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08034706860780716, KL divergence=0.06569265574216843, Entropy=0.4507113993167877, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07006581127643585, KL divergence=0.07266779989004135, Entropy=0.4486003816127777, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/414_Step-217888.ckpt']
Uploaded 3 files for checkpoint 414 in 0.53 seconds
saved intermediate frozen graph: current/model/model_414.pb
Best checkpoint number: 409, Last checkpoint number: 412
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'411'}
Training> Name=main_level/agent, Worker=0, Episode=6101, Total reward=59.17, Steps=217922, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6102, Total reward=25.49, Steps=217939, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6103, Total reward=30.42, Steps=218011, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6104, Total reward=14.04, Steps=218060, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6105, Total reward=6.85, Steps=218075, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6106, Total reward=110.92, Steps=218161, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6107, Total reward=61.81, Steps=218203, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6108, Total reward=51.74, Steps=218235, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6109, Total reward=90.56, Steps=218281, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6110, Total reward=70.63, Steps=218341, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6111, Total reward=66.71, Steps=218378, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6112, Total reward=63.82, Steps=218420, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6113, Total reward=46.32, Steps=218450, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6114, Total reward=27.36, Steps=218461, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6115, Total reward=29.32, Steps=218490, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6116, Total reward=24.92, Steps=218521, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6117, Total reward=59.27, Steps=218554, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6118, Total reward=34.15, Steps=218589, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6119, Total reward=186.63, Steps=218769, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6120, Total reward=90.59, Steps=218808, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6121, Total reward=63.06, Steps=218840, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6122, Total reward=25.17, Steps=218855, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6123, Total reward=4.8, Steps=218894, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6124, Total reward=8.93, Steps=218926, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6125, Total reward=79.93, Steps=219001, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6126, Total reward=82.09, Steps=219052, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6127, Total reward=74.43, Steps=219119, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6128, Total reward=65.93, Steps=219164, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6129, Total reward=66.7, Steps=219218, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6130, Total reward=102.27, Steps=219266, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6131, Total reward=49.68, Steps=219301, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6132, Total reward=66.0, Steps=219344, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6133, Total reward=47.44, Steps=219374, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6134, Total reward=31.9, Steps=219385, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6135, Total reward=26.74, Steps=219418, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6136, Total reward=18.97, Steps=219440, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6137, Total reward=41.4, Steps=219470, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6138, Total reward=37.64, Steps=219496, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6139, Total reward=21.61, Steps=219518, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6140, Total reward=49.54, Steps=219559, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6141, Total reward=41.28, Steps=219587, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6142, Total reward=117.18, Steps=219698, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6143, Total reward=8.53, Steps=219737, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6144, Total reward=16.38, Steps=219763, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6145, Total reward=16.72, Steps=219789, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6146, Total reward=71.43, Steps=219837, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6147, Total reward=71.1, Steps=219891, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6148, Total reward=105.31, Steps=219965, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6149, Total reward=25.35, Steps=220000, Training iteration=122
Training> Name=main_level/agent, Worker=0, Episode=6150, Total reward=58.93, Steps=220038, Training iteration=122
Policy training> Surrogate loss=-0.0010641085682436824, KL divergence=0.00023173700901679695, Entropy=0.4839920103549957, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03663107380270958, KL divergence=0.005903058685362339, Entropy=0.48406511545181274, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04244394972920418, KL divergence=0.018967682495713234, Entropy=0.48350250720977783, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05223863944411278, KL divergence=0.032020408660173416, Entropy=0.479320228099823, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06031732261180878, KL divergence=0.043959684669971466, Entropy=0.47160395979881287, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.059966739267110825, KL divergence=0.05535324662923813, Entropy=0.46793749928474426, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06403005123138428, KL divergence=0.06206004321575165, Entropy=0.46821922063827515, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07198335230350494, KL divergence=0.07098472863435745, Entropy=0.4658696949481964, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07043807208538055, KL divergence=0.0754077136516571, Entropy=0.47007614374160767, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06650321185588837, KL divergence=0.08028966933488846, Entropy=0.4684906005859375, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/415_Step-220038.ckpt']
Uploaded 3 files for checkpoint 415 in 0.57 seconds
saved intermediate frozen graph: current/model/model_415.pb
Best checkpoint number: 409, Last checkpoint number: 413
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'412'}
Training> Name=main_level/agent, Worker=0, Episode=6151, Total reward=73.38, Steps=220076, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6152, Total reward=47.58, Steps=220106, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6153, Total reward=42.28, Steps=220128, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6154, Total reward=37.76, Steps=220149, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6155, Total reward=29.65, Steps=220176, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6156, Total reward=28.71, Steps=220194, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6157, Total reward=49.01, Steps=220227, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6158, Total reward=28.92, Steps=220240, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6159, Total reward=67.85, Steps=220284, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6160, Total reward=89.07, Steps=220345, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6161, Total reward=40.49, Steps=220365, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6162, Total reward=30.45, Steps=220399, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6163, Total reward=70.58, Steps=220485, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6164, Total reward=7.45, Steps=220502, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6165, Total reward=19.08, Steps=220540, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6166, Total reward=77.57, Steps=220597, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6167, Total reward=34.71, Steps=220633, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6168, Total reward=50.5, Steps=220658, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6169, Total reward=32.85, Steps=220684, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6170, Total reward=25.64, Steps=220711, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6171, Total reward=81.26, Steps=220764, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6172, Total reward=43.74, Steps=220795, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6173, Total reward=49.39, Steps=220825, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6174, Total reward=50.57, Steps=220848, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6175, Total reward=17.7, Steps=220863, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6176, Total reward=18.73, Steps=220897, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6177, Total reward=34.46, Steps=220927, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6178, Total reward=31.47, Steps=220940, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6179, Total reward=60.52, Steps=220997, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6180, Total reward=62.38, Steps=221035, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6181, Total reward=61.89, Steps=221066, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6182, Total reward=18.48, Steps=221083, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6183, Total reward=60.59, Steps=221199, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6184, Total reward=13.44, Steps=221228, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6185, Total reward=84.25, Steps=221298, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6186, Total reward=75.3, Steps=221345, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6187, Total reward=59.76, Steps=221388, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6188, Total reward=97.45, Steps=221446, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6189, Total reward=24.39, Steps=221468, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6190, Total reward=79.78, Steps=221504, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6191, Total reward=40.65, Steps=221531, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6192, Total reward=59.23, Steps=221563, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6193, Total reward=37.29, Steps=221594, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6194, Total reward=46.39, Steps=221616, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6195, Total reward=18.21, Steps=221632, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6196, Total reward=21.45, Steps=221666, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6197, Total reward=48.57, Steps=221702, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6198, Total reward=76.42, Steps=221768, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6199, Total reward=77.24, Steps=221822, Training iteration=123
Training> Name=main_level/agent, Worker=0, Episode=6200, Total reward=81.5, Steps=221865, Training iteration=123
Policy training> Surrogate loss=0.009671956300735474, KL divergence=0.00012795608199667186, Entropy=0.4758850038051605, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.022502923384308815, KL divergence=0.0061924047768116, Entropy=0.475649356842041, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04455168545246124, KL divergence=0.019715504720807076, Entropy=0.47126278281211853, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06479602307081223, KL divergence=0.028493186458945274, Entropy=0.4626903831958771, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.053674500435590744, KL divergence=0.043174758553504944, Entropy=0.45756348967552185, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.03908546641469002, KL divergence=0.052488524466753006, Entropy=0.4676210582256317, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05203736200928688, KL divergence=0.06233500316739082, Entropy=0.4635326564311981, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05310774967074394, KL divergence=0.07518736273050308, Entropy=0.46586236357688904, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08252163231372833, KL divergence=0.0860602855682373, Entropy=0.4711526334285736, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09776884317398071, KL divergence=0.09028055518865585, Entropy=0.46096137166023254, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/416_Step-221865.ckpt']
Uploaded 3 files for checkpoint 416 in 0.54 seconds
saved intermediate frozen graph: current/model/model_416.pb
Best checkpoint number: 409, Last checkpoint number: 414
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'413'}
Training> Name=main_level/agent, Worker=0, Episode=6201, Total reward=62.38, Steps=221895, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6202, Total reward=26.61, Steps=221912, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6203, Total reward=5.95, Steps=221935, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6204, Total reward=6.35, Steps=221963, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6205, Total reward=28.68, Steps=221994, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6206, Total reward=101.82, Steps=222064, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6207, Total reward=60.82, Steps=222106, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6208, Total reward=57.1, Steps=222148, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6209, Total reward=86.41, Steps=222224, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6210, Total reward=91.75, Steps=222272, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6211, Total reward=58.49, Steps=222311, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6212, Total reward=58.5, Steps=222352, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6213, Total reward=47.5, Steps=222382, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6214, Total reward=44.0, Steps=222404, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6215, Total reward=22.72, Steps=222433, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6216, Total reward=20.85, Steps=222462, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6217, Total reward=32.41, Steps=222490, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6218, Total reward=35.06, Steps=222514, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6219, Total reward=35.93, Steps=222552, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6220, Total reward=58.01, Steps=222592, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6221, Total reward=37.05, Steps=222621, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6222, Total reward=32.03, Steps=222640, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6223, Total reward=3.4, Steps=222662, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6224, Total reward=29.31, Steps=222739, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6225, Total reward=130.81, Steps=222855, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6226, Total reward=134.1, Steps=222951, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6227, Total reward=55.68, Steps=222996, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6228, Total reward=80.04, Steps=223079, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6229, Total reward=17.5, Steps=223099, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6230, Total reward=89.67, Steps=223135, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6231, Total reward=34.22, Steps=223164, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6232, Total reward=33.84, Steps=223194, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6233, Total reward=53.22, Steps=223224, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6234, Total reward=44.73, Steps=223246, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6235, Total reward=24.15, Steps=223274, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6236, Total reward=31.28, Steps=223337, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6237, Total reward=228.82, Steps=223515, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6238, Total reward=44.12, Steps=223537, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6239, Total reward=58.92, Steps=223582, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6240, Total reward=78.07, Steps=223670, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6241, Total reward=79.31, Steps=223723, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6242, Total reward=28.01, Steps=223743, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6243, Total reward=19.24, Steps=223778, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6244, Total reward=6.34, Steps=223804, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6245, Total reward=60.22, Steps=223873, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6246, Total reward=88.29, Steps=223929, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6247, Total reward=50.33, Steps=223968, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6248, Total reward=52.03, Steps=223997, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6249, Total reward=27.35, Steps=224025, Training iteration=124
Training> Name=main_level/agent, Worker=0, Episode=6250, Total reward=86.12, Steps=224072, Training iteration=124
Policy training> Surrogate loss=-0.010659201070666313, KL divergence=0.00033762602834030986, Entropy=0.4905603229999542, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03953921049833298, KL divergence=0.006844494491815567, Entropy=0.4964892268180847, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05483577027916908, KL divergence=0.019460653886198997, Entropy=0.49565789103507996, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04960840567946434, KL divergence=0.03309009224176407, Entropy=0.48311930894851685, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06678316742181778, KL divergence=0.0452575758099556, Entropy=0.48302820324897766, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07509291172027588, KL divergence=0.0547095388174057, Entropy=0.4810066819190979, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06733284145593643, KL divergence=0.06531805545091629, Entropy=0.4786883294582367, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07416897267103195, KL divergence=0.07284105569124222, Entropy=0.4850326478481293, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08545228838920593, KL divergence=0.07646583765745163, Entropy=0.4784126877784729, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08107255399227142, KL divergence=0.08028339594602585, Entropy=0.48110878467559814, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/417_Step-224072.ckpt']
Uploaded 3 files for checkpoint 417 in 0.55 seconds
saved intermediate frozen graph: current/model/model_417.pb
Best checkpoint number: 409, Last checkpoint number: 415
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'414'}
Training> Name=main_level/agent, Worker=0, Episode=6251, Total reward=75.8, Steps=224111, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6252, Total reward=39.78, Steps=224142, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6253, Total reward=49.78, Steps=224173, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6254, Total reward=32.49, Steps=224193, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6255, Total reward=22.75, Steps=224218, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6256, Total reward=24.75, Steps=224248, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6257, Total reward=100.63, Steps=224328, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6258, Total reward=59.81, Steps=224379, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6259, Total reward=19.27, Steps=224401, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6260, Total reward=93.65, Steps=224450, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6261, Total reward=69.72, Steps=224516, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6262, Total reward=37.11, Steps=224535, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6263, Total reward=53.48, Steps=224627, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6264, Total reward=9.27, Steps=224660, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6265, Total reward=10.16, Steps=224681, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6266, Total reward=77.34, Steps=224752, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6267, Total reward=84.14, Steps=224812, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6268, Total reward=65.43, Steps=224868, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6269, Total reward=30.18, Steps=224900, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6270, Total reward=57.99, Steps=224958, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6271, Total reward=62.59, Steps=224986, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6272, Total reward=22.65, Steps=225007, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6273, Total reward=48.47, Steps=225037, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6274, Total reward=32.38, Steps=225059, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6275, Total reward=21.79, Steps=225085, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6276, Total reward=21.69, Steps=225104, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6277, Total reward=37.68, Steps=225138, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6278, Total reward=29.76, Steps=225160, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6279, Total reward=47.99, Steps=225207, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6280, Total reward=65.61, Steps=225248, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6281, Total reward=33.82, Steps=225272, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6282, Total reward=51.44, Steps=225319, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6283, Total reward=20.78, Steps=225355, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6284, Total reward=83.38, Steps=225457, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6285, Total reward=13.4, Steps=225479, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6286, Total reward=60.18, Steps=225528, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6287, Total reward=109.83, Steps=225621, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6288, Total reward=63.97, Steps=225649, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6289, Total reward=17.24, Steps=225679, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6290, Total reward=67.92, Steps=225715, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6291, Total reward=27.16, Steps=225744, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6292, Total reward=50.6, Steps=225774, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6293, Total reward=38.71, Steps=225795, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6294, Total reward=41.78, Steps=225816, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6295, Total reward=20.06, Steps=225844, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6296, Total reward=25.98, Steps=225863, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6297, Total reward=47.22, Steps=225901, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6298, Total reward=50.09, Steps=225948, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6299, Total reward=21.07, Steps=225982, Training iteration=125
Training> Name=main_level/agent, Worker=0, Episode=6300, Total reward=61.56, Steps=226020, Training iteration=125
Policy training> Surrogate loss=0.01208816934376955, KL divergence=0.00021123354963492602, Entropy=0.5165556073188782, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.05139230191707611, KL divergence=0.005741285625845194, Entropy=0.5190943479537964, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03098621405661106, KL divergence=0.016561444848775864, Entropy=0.5087706446647644, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05287237837910652, KL divergence=0.025467338040471077, Entropy=0.49430522322654724, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.059517789632081985, KL divergence=0.03611906245350838, Entropy=0.49663642048835754, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07775125652551651, KL divergence=0.051610320806503296, Entropy=0.48544561862945557, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07507108896970749, KL divergence=0.06314348429441452, Entropy=0.47804689407348633, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0580311119556427, KL divergence=0.06760822981595993, Entropy=0.4893341064453125, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05769683048129082, KL divergence=0.077724389731884, Entropy=0.48432230949401855, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.058956801891326904, KL divergence=0.08420994132757187, Entropy=0.4855596721172333, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/418_Step-226020.ckpt']
Uploaded 3 files for checkpoint 418 in 0.56 seconds
saved intermediate frozen graph: current/model/model_418.pb
Best checkpoint number: 409, Last checkpoint number: 416
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'415'}
Training> Name=main_level/agent, Worker=0, Episode=6301, Total reward=61.5, Steps=226073, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6302, Total reward=39.69, Steps=226110, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6303, Total reward=81.37, Steps=226210, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6304, Total reward=31.06, Steps=226283, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6305, Total reward=130.75, Steps=226392, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6306, Total reward=90.13, Steps=226459, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6307, Total reward=27.49, Steps=226493, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6308, Total reward=114.23, Steps=226587, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6309, Total reward=33.52, Steps=226624, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6310, Total reward=56.12, Steps=226667, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6311, Total reward=54.46, Steps=226694, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6312, Total reward=12.23, Steps=226714, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6313, Total reward=5.78, Steps=226732, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6314, Total reward=42.59, Steps=226753, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6315, Total reward=31.86, Steps=226778, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6316, Total reward=84.31, Steps=226869, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6317, Total reward=38.03, Steps=226900, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6318, Total reward=90.87, Steps=226965, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6319, Total reward=18.25, Steps=227003, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6320, Total reward=45.29, Steps=227040, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6321, Total reward=60.98, Steps=227072, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6322, Total reward=35.31, Steps=227094, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6323, Total reward=16.42, Steps=227141, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6324, Total reward=6.37, Steps=227168, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6325, Total reward=13.05, Steps=227193, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6326, Total reward=91.1, Steps=227245, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6327, Total reward=48.06, Steps=227284, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6328, Total reward=54.87, Steps=227324, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6329, Total reward=22.83, Steps=227351, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6330, Total reward=60.3, Steps=227401, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6331, Total reward=45.63, Steps=227427, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6332, Total reward=42.88, Steps=227457, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6333, Total reward=51.06, Steps=227479, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6334, Total reward=34.02, Steps=227499, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6335, Total reward=13.61, Steps=227531, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6336, Total reward=25.68, Steps=227547, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6337, Total reward=145.95, Steps=227709, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6338, Total reward=26.92, Steps=227733, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6339, Total reward=22.99, Steps=227766, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6340, Total reward=58.03, Steps=227820, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6341, Total reward=58.46, Steps=227850, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6342, Total reward=32.47, Steps=227868, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6343, Total reward=15.44, Steps=227901, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6344, Total reward=62.28, Steps=227972, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6345, Total reward=58.89, Steps=228035, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6346, Total reward=119.07, Steps=228128, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6347, Total reward=71.73, Steps=228179, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6348, Total reward=44.13, Steps=228196, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6349, Total reward=39.64, Steps=228221, Training iteration=126
Training> Name=main_level/agent, Worker=0, Episode=6350, Total reward=57.28, Steps=228259, Training iteration=126
Policy training> Surrogate loss=0.006785348057746887, KL divergence=0.0002772592124529183, Entropy=0.4922166168689728, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.030907468870282173, KL divergence=0.007190927863121033, Entropy=0.48491787910461426, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04780493676662445, KL divergence=0.01947653666138649, Entropy=0.47727006673812866, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04591643810272217, KL divergence=0.03185634687542915, Entropy=0.47344136238098145, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.059757597744464874, KL divergence=0.044084738940000534, Entropy=0.47899481654167175, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.054566118866205215, KL divergence=0.052411630749702454, Entropy=0.47318652272224426, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07874352484941483, KL divergence=0.0581524558365345, Entropy=0.46897053718566895, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08058495074510574, KL divergence=0.06639398634433746, Entropy=0.47588786482810974, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07897134125232697, KL divergence=0.07137812674045563, Entropy=0.47748783230781555, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08660919219255447, KL divergence=0.07385384291410446, Entropy=0.4797394573688507, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/419_Step-228259.ckpt']
Uploaded 3 files for checkpoint 419 in 0.54 seconds
saved intermediate frozen graph: current/model/model_419.pb
Best checkpoint number: 409, Last checkpoint number: 417
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'416'}
Training> Name=main_level/agent, Worker=0, Episode=6351, Total reward=44.01, Steps=228288, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6352, Total reward=53.67, Steps=228321, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6353, Total reward=50.34, Steps=228350, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6354, Total reward=40.67, Steps=228371, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6355, Total reward=23.29, Steps=228400, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6356, Total reward=24.9, Steps=228435, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6357, Total reward=29.58, Steps=228458, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6358, Total reward=38.5, Steps=228480, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6359, Total reward=25.63, Steps=228512, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6360, Total reward=73.61, Steps=228551, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6361, Total reward=55.73, Steps=228584, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6362, Total reward=26.61, Steps=228601, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6363, Total reward=59.0, Steps=228703, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6364, Total reward=21.52, Steps=228763, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6365, Total reward=46.04, Steps=228813, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6366, Total reward=38.85, Steps=228854, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6367, Total reward=62.04, Steps=228897, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6368, Total reward=66.3, Steps=228927, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6369, Total reward=23.39, Steps=228954, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6370, Total reward=74.24, Steps=228990, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6371, Total reward=23.85, Steps=229035, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6372, Total reward=30.52, Steps=229066, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6373, Total reward=53.61, Steps=229094, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6374, Total reward=41.88, Steps=229115, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6375, Total reward=17.18, Steps=229140, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6376, Total reward=25.63, Steps=229172, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6377, Total reward=40.08, Steps=229219, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6378, Total reward=68.56, Steps=229276, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6379, Total reward=129.49, Steps=229419, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6380, Total reward=65.07, Steps=229459, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6381, Total reward=60.95, Steps=229488, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6382, Total reward=37.32, Steps=229516, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6383, Total reward=67.34, Steps=229599, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6384, Total reward=80.88, Steps=229674, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6385, Total reward=14.8, Steps=229700, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6386, Total reward=65.92, Steps=229747, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6387, Total reward=55.25, Steps=229824, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6388, Total reward=60.42, Steps=229854, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6389, Total reward=91.21, Steps=229916, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6390, Total reward=11.36, Steps=229930, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6391, Total reward=66.41, Steps=229969, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6392, Total reward=56.03, Steps=230012, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6393, Total reward=44.22, Steps=230034, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6394, Total reward=47.5, Steps=230056, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6395, Total reward=30.32, Steps=230085, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6396, Total reward=19.54, Steps=230105, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6397, Total reward=41.77, Steps=230145, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6398, Total reward=38.28, Steps=230167, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6399, Total reward=7.81, Steps=230190, Training iteration=127
Training> Name=main_level/agent, Worker=0, Episode=6400, Total reward=58.53, Steps=230229, Training iteration=127
Policy training> Surrogate loss=-0.0018382432172074914, KL divergence=6.110741378506646e-05, Entropy=0.4846322238445282, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.035115569829940796, KL divergence=0.0031852219253778458, Entropy=0.46632519364356995, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05722858011722565, KL divergence=0.010514303110539913, Entropy=0.46673712134361267, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06603723019361496, KL divergence=0.02128581702709198, Entropy=0.4659975469112396, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.049382369965314865, KL divergence=0.032542884349823, Entropy=0.4629605710506439, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06316864490509033, KL divergence=0.04349939897656441, Entropy=0.46384397149086, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08261964470148087, KL divergence=0.0520426444709301, Entropy=0.46319839358329773, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08346480131149292, KL divergence=0.06082640215754509, Entropy=0.4546128511428833, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0657709538936615, KL divergence=0.06579302251338959, Entropy=0.4534410536289215, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06540942937135696, KL divergence=0.07240652292966843, Entropy=0.4619932174682617, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/420_Step-230229.ckpt']
Uploaded 3 files for checkpoint 420 in 0.53 seconds
saved intermediate frozen graph: current/model/model_420.pb
Best checkpoint number: 409, Last checkpoint number: 418
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'417'}
Training> Name=main_level/agent, Worker=0, Episode=6401, Total reward=64.79, Steps=230260, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6402, Total reward=51.93, Steps=230292, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6403, Total reward=7.17, Steps=230309, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6404, Total reward=25.1, Steps=230349, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6405, Total reward=43.14, Steps=230439, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6406, Total reward=72.26, Steps=230491, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6407, Total reward=58.84, Steps=230533, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6408, Total reward=65.16, Steps=230563, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6409, Total reward=24.99, Steps=230589, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6410, Total reward=11.21, Steps=230601, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6411, Total reward=58.22, Steps=230637, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6412, Total reward=23.5, Steps=230664, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6413, Total reward=50.36, Steps=230693, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6414, Total reward=34.65, Steps=230713, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6415, Total reward=31.3, Steps=230740, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6416, Total reward=20.07, Steps=230774, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6417, Total reward=15.41, Steps=230787, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6418, Total reward=102.39, Steps=230854, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6419, Total reward=1.65, Steps=230868, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6420, Total reward=79.39, Steps=230920, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6421, Total reward=58.86, Steps=230951, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6422, Total reward=36.53, Steps=231000, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6423, Total reward=12.52, Steps=231039, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6424, Total reward=4.96, Steps=231057, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6425, Total reward=77.97, Steps=231137, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6426, Total reward=76.1, Steps=231204, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6427, Total reward=46.42, Steps=231264, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6428, Total reward=70.34, Steps=231331, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6429, Total reward=28.98, Steps=231357, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6430, Total reward=115.83, Steps=231417, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6431, Total reward=53.29, Steps=231460, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6432, Total reward=41.81, Steps=231491, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6433, Total reward=34.78, Steps=231512, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6434, Total reward=40.16, Steps=231534, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6435, Total reward=32.45, Steps=231562, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6436, Total reward=18.69, Steps=231595, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6437, Total reward=53.2, Steps=231627, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6438, Total reward=43.77, Steps=231650, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6439, Total reward=9.48, Steps=231683, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6440, Total reward=52.19, Steps=231723, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6441, Total reward=42.69, Steps=231751, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6442, Total reward=45.83, Steps=231779, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6443, Total reward=0.02, Steps=231799, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6444, Total reward=70.63, Steps=231876, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6445, Total reward=45.4, Steps=231926, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6446, Total reward=16.97, Steps=231956, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6447, Total reward=123.53, Steps=232047, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6448, Total reward=57.12, Steps=232077, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6449, Total reward=21.1, Steps=232112, Training iteration=128
Training> Name=main_level/agent, Worker=0, Episode=6450, Total reward=59.97, Steps=232161, Training iteration=128
Policy training> Surrogate loss=-0.01010182499885559, KL divergence=6.560799374710768e-05, Entropy=0.5053282380104065, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03638214245438576, KL divergence=0.002826399402692914, Entropy=0.5031360983848572, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0370243638753891, KL divergence=0.012487094849348068, Entropy=0.5036270618438721, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05438658967614174, KL divergence=0.023892978206276894, Entropy=0.48942676186561584, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06971795856952667, KL divergence=0.03704659640789032, Entropy=0.4840679168701172, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07445309311151505, KL divergence=0.04478268697857857, Entropy=0.4754447937011719, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05957217514514923, KL divergence=0.054517943412065506, Entropy=0.4678688049316406, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06960584968328476, KL divergence=0.061705976724624634, Entropy=0.470628023147583, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08048760145902634, KL divergence=0.0669718012213707, Entropy=0.4673381745815277, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10002220422029495, KL divergence=0.07080451399087906, Entropy=0.45647570490837097, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/421_Step-232161.ckpt']
Uploaded 3 files for checkpoint 421 in 0.52 seconds
saved intermediate frozen graph: current/model/model_421.pb
Best checkpoint number: 409, Last checkpoint number: 419
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'418'}
Training> Name=main_level/agent, Worker=0, Episode=6451, Total reward=16.51, Steps=232184, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6452, Total reward=7.51, Steps=232194, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6453, Total reward=21.86, Steps=232205, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6454, Total reward=45.05, Steps=232228, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6455, Total reward=19.82, Steps=232254, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6456, Total reward=26.87, Steps=232291, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6457, Total reward=48.04, Steps=232329, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6458, Total reward=28.35, Steps=232343, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6459, Total reward=19.3, Steps=232362, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6460, Total reward=78.08, Steps=232408, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6461, Total reward=66.31, Steps=232438, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6462, Total reward=29.86, Steps=232456, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6463, Total reward=3.4, Steps=232477, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6464, Total reward=16.49, Steps=232507, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6465, Total reward=118.8, Steps=232608, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6466, Total reward=14.26, Steps=232625, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6467, Total reward=61.51, Steps=232668, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6468, Total reward=90.1, Steps=232750, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6469, Total reward=53.73, Steps=232799, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6470, Total reward=81.19, Steps=232835, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6471, Total reward=22.92, Steps=232856, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6472, Total reward=37.08, Steps=232888, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6473, Total reward=2.81, Steps=232906, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6474, Total reward=46.7, Steps=232928, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6475, Total reward=28.47, Steps=232958, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6476, Total reward=26.3, Steps=232989, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6477, Total reward=49.55, Steps=233027, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6478, Total reward=65.08, Steps=233095, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6479, Total reward=10.74, Steps=233108, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6480, Total reward=73.63, Steps=233152, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6481, Total reward=61.95, Steps=233185, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6482, Total reward=25.96, Steps=233203, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6483, Total reward=0.03, Steps=233228, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6484, Total reward=4.61, Steps=233250, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6485, Total reward=75.42, Steps=233311, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6486, Total reward=78.86, Steps=233367, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6487, Total reward=63.39, Steps=233409, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6488, Total reward=67.51, Steps=233448, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6489, Total reward=61.02, Steps=233483, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6490, Total reward=99.12, Steps=233531, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6491, Total reward=36.95, Steps=233563, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6492, Total reward=5.56, Steps=233574, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6493, Total reward=53.46, Steps=233605, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6494, Total reward=38.36, Steps=233626, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6495, Total reward=16.84, Steps=233654, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6496, Total reward=29.36, Steps=233688, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6497, Total reward=36.36, Steps=233724, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6498, Total reward=35.58, Steps=233737, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6499, Total reward=63.47, Steps=233805, Training iteration=129
Training> Name=main_level/agent, Worker=0, Episode=6500, Total reward=47.73, Steps=233845, Training iteration=129
Policy training> Surrogate loss=-0.011754627339541912, KL divergence=0.0001899084309116006, Entropy=0.47844502329826355, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.036101650446653366, KL divergence=0.00430659344419837, Entropy=0.47556909918785095, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.051682066172361374, KL divergence=0.01327824592590332, Entropy=0.4788840711116791, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03952943906188011, KL divergence=0.02376793883740902, Entropy=0.4749099016189575, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05245794728398323, KL divergence=0.03380317613482475, Entropy=0.47084346413612366, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06892841309309006, KL divergence=0.04450760409235954, Entropy=0.470113068819046, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07077939063310623, KL divergence=0.05493142083287239, Entropy=0.46618497371673584, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06597963720560074, KL divergence=0.06250334531068802, Entropy=0.46435752511024475, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05519070848822594, KL divergence=0.06978194415569305, Entropy=0.464972585439682, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07926221191883087, KL divergence=0.076313816010952, Entropy=0.4587811529636383, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/422_Step-233845.ckpt']
Uploaded 3 files for checkpoint 422 in 0.54 seconds
saved intermediate frozen graph: current/model/model_422.pb
Best checkpoint number: 409, Last checkpoint number: 420
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'419'}
Training> Name=main_level/agent, Worker=0, Episode=6501, Total reward=67.5, Steps=233885, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6502, Total reward=48.32, Steps=233932, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6503, Total reward=59.38, Steps=234018, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6504, Total reward=7.88, Steps=234063, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6505, Total reward=15.18, Steps=234094, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6506, Total reward=96.64, Steps=234184, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6507, Total reward=60.24, Steps=234222, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6508, Total reward=121.52, Steps=234301, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6509, Total reward=66.37, Steps=234367, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6510, Total reward=37.78, Steps=234400, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6511, Total reward=37.22, Steps=234442, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6512, Total reward=61.57, Steps=234475, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6513, Total reward=52.02, Steps=234505, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6514, Total reward=33.4, Steps=234526, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6515, Total reward=22.49, Steps=234556, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6516, Total reward=19.62, Steps=234590, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6517, Total reward=33.39, Steps=234611, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6518, Total reward=23.35, Steps=234633, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6519, Total reward=15.35, Steps=234664, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6520, Total reward=81.87, Steps=234713, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6521, Total reward=59.01, Steps=234743, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6522, Total reward=28.19, Steps=234770, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6523, Total reward=7.7, Steps=234800, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6524, Total reward=13.93, Steps=234831, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6525, Total reward=19.99, Steps=234866, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6526, Total reward=17.52, Steps=234909, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6527, Total reward=73.45, Steps=234953, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6528, Total reward=97.3, Steps=235026, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6529, Total reward=60.83, Steps=235075, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6530, Total reward=25.49, Steps=235092, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6531, Total reward=88.06, Steps=235134, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6532, Total reward=57.28, Steps=235167, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6533, Total reward=59.35, Steps=235197, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6534, Total reward=44.71, Steps=235219, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6535, Total reward=28.67, Steps=235247, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6536, Total reward=21.16, Steps=235268, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6537, Total reward=113.46, Steps=235355, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6538, Total reward=49.53, Steps=235393, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6539, Total reward=9.25, Steps=235421, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6540, Total reward=71.85, Steps=235465, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6541, Total reward=44.1, Steps=235521, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6542, Total reward=50.64, Steps=235572, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6543, Total reward=98.07, Steps=235675, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6544, Total reward=9.83, Steps=235704, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6545, Total reward=53.25, Steps=235766, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6546, Total reward=105.68, Steps=235833, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6547, Total reward=66.63, Steps=235877, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6548, Total reward=58.18, Steps=235903, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6549, Total reward=75.31, Steps=235967, Training iteration=130
Training> Name=main_level/agent, Worker=0, Episode=6550, Total reward=57.03, Steps=236013, Training iteration=130
Policy training> Surrogate loss=0.0021777842193841934, KL divergence=0.00017283271881751716, Entropy=0.4873959422111511, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03205333650112152, KL divergence=0.006046813912689686, Entropy=0.4864436984062195, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04693847522139549, KL divergence=0.020390355959534645, Entropy=0.48411494493484497, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.053422026336193085, KL divergence=0.03386900573968887, Entropy=0.47852665185928345, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06409291923046112, KL divergence=0.04447425529360771, Entropy=0.4817683696746826, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06510721892118454, KL divergence=0.05371695011854172, Entropy=0.47644826769828796, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.064899742603302, KL divergence=0.06260446459054947, Entropy=0.4783927798271179, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06690618395805359, KL divergence=0.06856991350650787, Entropy=0.4758976101875305, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0805523693561554, KL divergence=0.07537926733493805, Entropy=0.4737320840358734, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08236350864171982, KL divergence=0.07950260490179062, Entropy=0.47136998176574707, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/423_Step-236013.ckpt']
Uploaded 3 files for checkpoint 423 in 0.54 seconds
saved intermediate frozen graph: current/model/model_423.pb
Best checkpoint number: 409, Last checkpoint number: 421
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'420'}
Training> Name=main_level/agent, Worker=0, Episode=6551, Total reward=24.58, Steps=236043, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6552, Total reward=55.59, Steps=236075, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6553, Total reward=51.76, Steps=236104, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6554, Total reward=38.48, Steps=236125, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6555, Total reward=34.75, Steps=236154, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6556, Total reward=23.06, Steps=236172, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6557, Total reward=40.13, Steps=236213, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6558, Total reward=80.96, Steps=236287, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6559, Total reward=63.47, Steps=236345, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6560, Total reward=83.49, Steps=236384, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6561, Total reward=49.99, Steps=236420, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6562, Total reward=41.74, Steps=236444, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6563, Total reward=12.23, Steps=236483, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6564, Total reward=31.25, Steps=236527, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6565, Total reward=19.76, Steps=236575, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6566, Total reward=83.88, Steps=236628, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6567, Total reward=62.17, Steps=236676, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6568, Total reward=67.31, Steps=236713, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6569, Total reward=26.57, Steps=236737, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6570, Total reward=77.31, Steps=236785, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6571, Total reward=55.63, Steps=236825, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6572, Total reward=59.71, Steps=236866, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6573, Total reward=46.09, Steps=236895, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6574, Total reward=47.54, Steps=236917, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6575, Total reward=28.18, Steps=236948, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6576, Total reward=17.53, Steps=236969, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6577, Total reward=44.22, Steps=237008, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6578, Total reward=109.56, Steps=237082, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6579, Total reward=17.72, Steps=237114, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6580, Total reward=91.15, Steps=237154, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6581, Total reward=229.35, Steps=237306, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6582, Total reward=51.02, Steps=237341, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6583, Total reward=129.28, Steps=237484, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6584, Total reward=140.95, Steps=237630, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6585, Total reward=3.39, Steps=237666, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6586, Total reward=87.07, Steps=237717, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6587, Total reward=69.02, Steps=237767, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6588, Total reward=76.33, Steps=237818, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6589, Total reward=58.38, Steps=237878, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6590, Total reward=106.84, Steps=237927, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6591, Total reward=43.71, Steps=237968, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6592, Total reward=64.24, Steps=238001, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6593, Total reward=47.68, Steps=238033, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6594, Total reward=41.11, Steps=238055, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6595, Total reward=27.11, Steps=238084, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6596, Total reward=25.05, Steps=238114, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6597, Total reward=18.02, Steps=238128, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6598, Total reward=42.29, Steps=238154, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6599, Total reward=59.73, Steps=238206, Training iteration=131
Training> Name=main_level/agent, Worker=0, Episode=6600, Total reward=88.17, Steps=238245, Training iteration=131
Policy training> Surrogate loss=-0.0003776084631681442, KL divergence=0.00022479164181277156, Entropy=0.5006431937217712, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03016100637614727, KL divergence=0.0065523115918040276, Entropy=0.4955311715602875, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04480813071131706, KL divergence=0.01911981962621212, Entropy=0.4861412048339844, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06036360189318657, KL divergence=0.030464526265859604, Entropy=0.47594836354255676, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06666785478591919, KL divergence=0.03838660940527916, Entropy=0.4774564504623413, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07540600001811981, KL divergence=0.050730153918266296, Entropy=0.4747541844844818, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07960481941699982, KL divergence=0.061369746923446655, Entropy=0.4766138792037964, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0654420405626297, KL divergence=0.06816279888153076, Entropy=0.48364827036857605, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07506442070007324, KL divergence=0.07298282533884048, Entropy=0.4831174314022064, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07472185790538788, KL divergence=0.07830298691987991, Entropy=0.48380404710769653, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/424_Step-238245.ckpt']
Uploaded 3 files for checkpoint 424 in 0.60 seconds
saved intermediate frozen graph: current/model/model_424.pb
Best checkpoint number: 409, Last checkpoint number: 422
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'421'}
Training> Name=main_level/agent, Worker=0, Episode=6601, Total reward=43.01, Steps=238282, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6602, Total reward=31.84, Steps=238301, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6603, Total reward=26.32, Steps=238345, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6604, Total reward=57.09, Steps=238419, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6605, Total reward=69.07, Steps=238487, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6606, Total reward=73.95, Steps=238552, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6607, Total reward=68.91, Steps=238634, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6608, Total reward=63.3, Steps=238665, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6609, Total reward=27.52, Steps=238692, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6610, Total reward=76.12, Steps=238743, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6611, Total reward=79.37, Steps=238797, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6612, Total reward=54.58, Steps=238840, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6613, Total reward=36.22, Steps=238862, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6614, Total reward=33.36, Steps=238873, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6615, Total reward=21.21, Steps=238887, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6616, Total reward=36.26, Steps=238932, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6617, Total reward=46.59, Steps=238975, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6618, Total reward=86.44, Steps=239044, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6619, Total reward=66.11, Steps=239120, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6620, Total reward=54.21, Steps=239159, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6621, Total reward=77.38, Steps=239213, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6622, Total reward=38.79, Steps=239239, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6623, Total reward=3.32, Steps=239259, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6624, Total reward=69.89, Steps=239353, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6625, Total reward=80.1, Steps=239414, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6626, Total reward=86.45, Steps=239468, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6627, Total reward=51.84, Steps=239508, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6628, Total reward=59.74, Steps=239538, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6629, Total reward=79.92, Steps=239593, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6630, Total reward=70.25, Steps=239645, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6631, Total reward=50.29, Steps=239685, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6632, Total reward=66.21, Steps=239726, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6633, Total reward=49.48, Steps=239757, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6634, Total reward=39.85, Steps=239778, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6635, Total reward=17.47, Steps=239804, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6636, Total reward=26.89, Steps=239822, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6637, Total reward=54.4, Steps=239870, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6638, Total reward=24.36, Steps=239884, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6639, Total reward=79.35, Steps=239936, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6640, Total reward=78.94, Steps=239974, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6641, Total reward=57.09, Steps=240003, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6642, Total reward=33.78, Steps=240025, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6643, Total reward=21.05, Steps=240077, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6644, Total reward=20.29, Steps=240126, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6645, Total reward=40.53, Steps=240189, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6646, Total reward=76.55, Steps=240246, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6647, Total reward=45.21, Steps=240286, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6648, Total reward=66.7, Steps=240320, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6649, Total reward=34.44, Steps=240345, Training iteration=132
Training> Name=main_level/agent, Worker=0, Episode=6650, Total reward=22.55, Steps=240369, Training iteration=132
Policy training> Surrogate loss=-0.0020563488360494375, KL divergence=0.00022821270977146924, Entropy=0.49021536111831665, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.027338968589901924, KL divergence=0.007260347716510296, Entropy=0.48559099435806274, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0532565638422966, KL divergence=0.019680321216583252, Entropy=0.48011982440948486, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05325263366103172, KL divergence=0.0323101207613945, Entropy=0.4736593961715698, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0641707107424736, KL divergence=0.04349764436483383, Entropy=0.4732101559638977, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06182243674993515, KL divergence=0.054476380348205566, Entropy=0.4701135754585266, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07385385781526566, KL divergence=0.06266172230243683, Entropy=0.46902167797088623, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07062825560569763, KL divergence=0.06727907806634903, Entropy=0.47126686573028564, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06601136177778244, KL divergence=0.06993213295936584, Entropy=0.4690811038017273, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06394752860069275, KL divergence=0.07380623370409012, Entropy=0.46788811683654785, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/425_Step-240369.ckpt']
Uploaded 3 files for checkpoint 425 in 0.49 seconds
saved intermediate frozen graph: current/model/model_425.pb
Best checkpoint number: 409, Last checkpoint number: 423
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'422'}
Training> Name=main_level/agent, Worker=0, Episode=6651, Total reward=22.36, Steps=240400, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6652, Total reward=3.76, Steps=240411, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6653, Total reward=58.57, Steps=240442, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6654, Total reward=49.82, Steps=240464, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6655, Total reward=19.47, Steps=240487, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6656, Total reward=24.07, Steps=240505, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6657, Total reward=99.63, Steps=240588, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6658, Total reward=24.47, Steps=240611, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6659, Total reward=22.78, Steps=240632, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6660, Total reward=26.68, Steps=240656, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6661, Total reward=48.56, Steps=240686, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6662, Total reward=34.87, Steps=240731, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6663, Total reward=84.95, Steps=240807, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6664, Total reward=101.42, Steps=240925, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6665, Total reward=60.54, Steps=240989, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6666, Total reward=152.57, Steps=241102, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6667, Total reward=73.38, Steps=241156, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6668, Total reward=65.86, Steps=241212, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6669, Total reward=70.13, Steps=241277, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6670, Total reward=106.02, Steps=241334, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6671, Total reward=52.95, Steps=241376, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6672, Total reward=72.76, Steps=241418, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6673, Total reward=46.16, Steps=241439, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6674, Total reward=46.27, Steps=241461, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6675, Total reward=28.68, Steps=241492, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6676, Total reward=28.81, Steps=241523, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6677, Total reward=27.03, Steps=241555, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6678, Total reward=43.43, Steps=241579, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6679, Total reward=55.88, Steps=241625, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6680, Total reward=96.78, Steps=241676, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6681, Total reward=63.85, Steps=241724, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6682, Total reward=43.05, Steps=241765, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6683, Total reward=21.84, Steps=241810, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6684, Total reward=8.08, Steps=241838, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6685, Total reward=19.82, Steps=241870, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6686, Total reward=7.19, Steps=241882, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6687, Total reward=62.65, Steps=241938, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6688, Total reward=65.25, Steps=241970, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6689, Total reward=26.19, Steps=242007, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6690, Total reward=98.09, Steps=242055, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6691, Total reward=66.53, Steps=242097, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6692, Total reward=65.7, Steps=242130, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6693, Total reward=37.22, Steps=242151, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6694, Total reward=35.15, Steps=242174, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6695, Total reward=31.11, Steps=242203, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6696, Total reward=24.49, Steps=242238, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6697, Total reward=97.74, Steps=242317, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6698, Total reward=95.0, Steps=242392, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6699, Total reward=29.04, Steps=242438, Training iteration=133
Training> Name=main_level/agent, Worker=0, Episode=6700, Total reward=40.47, Steps=242480, Training iteration=133
Policy training> Surrogate loss=0.0025260369293391705, KL divergence=0.00013522231893148273, Entropy=0.49165812134742737, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03252541646361351, KL divergence=0.005788438953459263, Entropy=0.49299025535583496, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.055484697222709656, KL divergence=0.018624793738126755, Entropy=0.4860585331916809, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.055068910121917725, KL divergence=0.03404197841882706, Entropy=0.48089027404785156, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05929400771856308, KL divergence=0.047235194593667984, Entropy=0.481497198343277, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06090814247727394, KL divergence=0.059632107615470886, Entropy=0.48021847009658813, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07035096734762192, KL divergence=0.06797683238983154, Entropy=0.48052483797073364, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07827786356210709, KL divergence=0.07738643884658813, Entropy=0.48282670974731445, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08089132606983185, KL divergence=0.08262385427951813, Entropy=0.4821562170982361, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07830827683210373, KL divergence=0.08595789968967438, Entropy=0.48342907428741455, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/426_Step-242480.ckpt']
Uploaded 3 files for checkpoint 426 in 0.51 seconds
saved intermediate frozen graph: current/model/model_426.pb
Best checkpoint number: 409, Last checkpoint number: 424
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'423'}
Training> Name=main_level/agent, Worker=0, Episode=6701, Total reward=45.49, Steps=242511, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6702, Total reward=33.45, Steps=242534, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6703, Total reward=0.02, Steps=242556, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6704, Total reward=19.64, Steps=242584, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6705, Total reward=94.05, Steps=242659, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6706, Total reward=87.46, Steps=242719, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6707, Total reward=61.2, Steps=242761, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6708, Total reward=32.37, Steps=242773, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6709, Total reward=24.35, Steps=242789, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6710, Total reward=42.29, Steps=242826, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6711, Total reward=26.3, Steps=242854, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6712, Total reward=43.24, Steps=242882, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6713, Total reward=50.25, Steps=242911, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6714, Total reward=29.98, Steps=242922, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6715, Total reward=35.12, Steps=242953, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6716, Total reward=21.59, Steps=242971, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6717, Total reward=46.9, Steps=243020, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6718, Total reward=31.79, Steps=243044, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6719, Total reward=86.62, Steps=243144, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6720, Total reward=91.16, Steps=243195, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6721, Total reward=42.8, Steps=243215, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6722, Total reward=40.36, Steps=243257, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6723, Total reward=18.33, Steps=243325, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6724, Total reward=9.71, Steps=243350, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6725, Total reward=18.07, Steps=243375, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6726, Total reward=91.38, Steps=243426, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6727, Total reward=63.39, Steps=243467, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6728, Total reward=76.55, Steps=243514, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6729, Total reward=30.09, Steps=243540, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6730, Total reward=58.38, Steps=243581, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6731, Total reward=15.92, Steps=243608, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6732, Total reward=66.68, Steps=243640, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6733, Total reward=0.02, Steps=243657, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6734, Total reward=39.9, Steps=243679, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6735, Total reward=28.83, Steps=243710, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6736, Total reward=50.12, Steps=243753, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6737, Total reward=31.41, Steps=243775, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6738, Total reward=30.02, Steps=243814, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6739, Total reward=44.45, Steps=243872, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6740, Total reward=95.04, Steps=243915, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6741, Total reward=67.52, Steps=243949, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6742, Total reward=29.02, Steps=243966, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6743, Total reward=0.02, Steps=243981, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6744, Total reward=24.02, Steps=244014, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6745, Total reward=124.06, Steps=244114, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6746, Total reward=88.27, Steps=244162, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6747, Total reward=51.32, Steps=244201, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6748, Total reward=54.82, Steps=244229, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6749, Total reward=24.39, Steps=244258, Training iteration=134
Training> Name=main_level/agent, Worker=0, Episode=6750, Total reward=33.52, Steps=244315, Training iteration=134
Policy training> Surrogate loss=-0.009467280469834805, KL divergence=6.158160249469802e-05, Entropy=0.5064978003501892, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03996525704860687, KL divergence=0.004171888809651136, Entropy=0.5137075781822205, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.043441835790872574, KL divergence=0.012129048816859722, Entropy=0.505784273147583, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04307269677519798, KL divergence=0.022205008193850517, Entropy=0.5110324621200562, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06607065349817276, KL divergence=0.03407501056790352, Entropy=0.5123744606971741, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0687035620212555, KL divergence=0.041775237768888474, Entropy=0.49914130568504333, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08463538438081741, KL divergence=0.05076150223612785, Entropy=0.5021362900733948, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06318696588277817, KL divergence=0.057809244841337204, Entropy=0.4962587356567383, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06691468507051468, KL divergence=0.06462801992893219, Entropy=0.4978583753108978, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0822330117225647, KL divergence=0.06873200088739395, Entropy=0.489437460899353, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/427_Step-244315.ckpt']
Uploaded 3 files for checkpoint 427 in 0.52 seconds
saved intermediate frozen graph: current/model/model_427.pb
Best checkpoint number: 409, Last checkpoint number: 425
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'424'}
Training> Name=main_level/agent, Worker=0, Episode=6751, Total reward=94.22, Steps=244365, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6752, Total reward=80.94, Steps=244406, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6753, Total reward=39.54, Steps=244436, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6754, Total reward=32.42, Steps=244458, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6755, Total reward=13.36, Steps=244487, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6756, Total reward=19.86, Steps=244509, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6757, Total reward=108.05, Steps=244597, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6758, Total reward=39.36, Steps=244610, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6759, Total reward=29.11, Steps=244662, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6760, Total reward=77.22, Steps=244701, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6761, Total reward=65.12, Steps=244734, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6762, Total reward=39.02, Steps=244760, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6763, Total reward=91.49, Steps=244872, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6764, Total reward=9.96, Steps=244900, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6765, Total reward=16.95, Steps=244939, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6766, Total reward=54.34, Steps=244993, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6767, Total reward=61.21, Steps=245036, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6768, Total reward=68.21, Steps=245073, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6769, Total reward=23.95, Steps=245091, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6770, Total reward=45.28, Steps=245118, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6771, Total reward=62.23, Steps=245159, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6772, Total reward=60.82, Steps=245200, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6773, Total reward=64.26, Steps=245231, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6774, Total reward=35.89, Steps=245253, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6775, Total reward=30.1, Steps=245285, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6776, Total reward=17.81, Steps=245304, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6777, Total reward=51.93, Steps=245336, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6778, Total reward=75.11, Steps=245390, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6779, Total reward=11.58, Steps=245407, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6780, Total reward=129.14, Steps=245531, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6781, Total reward=60.6, Steps=245562, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6782, Total reward=48.71, Steps=245602, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6783, Total reward=13.69, Steps=245631, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6784, Total reward=0.02, Steps=245646, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6785, Total reward=106.38, Steps=245759, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6786, Total reward=89.48, Steps=245830, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6787, Total reward=78.17, Steps=245909, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6788, Total reward=66.66, Steps=245947, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6789, Total reward=27.31, Steps=245969, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6790, Total reward=101.41, Steps=246028, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6791, Total reward=35.27, Steps=246049, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6792, Total reward=36.72, Steps=246077, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6793, Total reward=57.02, Steps=246108, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6794, Total reward=40.45, Steps=246130, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6795, Total reward=31.02, Steps=246161, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6796, Total reward=30.0, Steps=246228, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6797, Total reward=56.29, Steps=246279, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6798, Total reward=44.32, Steps=246312, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6799, Total reward=1.66, Steps=246332, Training iteration=135
Training> Name=main_level/agent, Worker=0, Episode=6800, Total reward=11.6, Steps=246357, Training iteration=135
Policy training> Surrogate loss=0.01041433960199356, KL divergence=7.051620195852593e-05, Entropy=0.5056479573249817, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.00781871099025011, KL divergence=0.0028622860554605722, Entropy=0.5038275122642517, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04842313751578331, KL divergence=0.010471329092979431, Entropy=0.49183419346809387, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07570929825305939, KL divergence=0.021959466859698296, Entropy=0.485809326171875, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06631901115179062, KL divergence=0.035194966942071915, Entropy=0.48141101002693176, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05937347188591957, KL divergence=0.04682648181915283, Entropy=0.4784358739852905, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08101321011781693, KL divergence=0.053364988416433334, Entropy=0.4823291301727295, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07321316748857498, KL divergence=0.06097769737243652, Entropy=0.4808008670806885, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07853773236274719, KL divergence=0.06968908756971359, Entropy=0.4868769943714142, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.061640262603759766, KL divergence=0.07309020310640335, Entropy=0.48333248496055603, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/428_Step-246357.ckpt']
Uploaded 3 files for checkpoint 428 in 0.55 seconds
saved intermediate frozen graph: current/model/model_428.pb
Best checkpoint number: 409, Last checkpoint number: 426
Copying the frozen checkpoint from ./frozen_models/agent/model_409.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'425'}
Training> Name=main_level/agent, Worker=0, Episode=6801, Total reward=59.09, Steps=246397, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6802, Total reward=44.94, Steps=246434, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6803, Total reward=2.18, Steps=246473, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6804, Total reward=4.8, Steps=246506, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6805, Total reward=19.9, Steps=246530, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6806, Total reward=62.76, Steps=246577, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6807, Total reward=124.5, Steps=246675, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6808, Total reward=139.73, Steps=246763, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6809, Total reward=17.47, Steps=246783, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6810, Total reward=110.23, Steps=246839, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6811, Total reward=46.93, Steps=246879, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6812, Total reward=57.87, Steps=246910, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6813, Total reward=29.37, Steps=246931, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6814, Total reward=44.89, Steps=246953, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6815, Total reward=24.99, Steps=246985, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6816, Total reward=22.35, Steps=247011, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6817, Total reward=39.46, Steps=247064, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6818, Total reward=20.52, Steps=247077, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6819, Total reward=24.91, Steps=247108, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6820, Total reward=85.2, Steps=247173, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6821, Total reward=64.22, Steps=247204, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6822, Total reward=44.5, Steps=247232, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6823, Total reward=7.67, Steps=247244, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6824, Total reward=73.02, Steps=247314, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6825, Total reward=19.84, Steps=247362, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6826, Total reward=89.45, Steps=247417, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6827, Total reward=110.87, Steps=247513, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6828, Total reward=77.77, Steps=247565, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6829, Total reward=18.73, Steps=247580, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6830, Total reward=68.13, Steps=247624, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6831, Total reward=52.49, Steps=247660, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6832, Total reward=53.84, Steps=247691, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6833, Total reward=47.13, Steps=247713, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6834, Total reward=43.06, Steps=247735, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6835, Total reward=16.64, Steps=247763, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6836, Total reward=21.82, Steps=247797, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6837, Total reward=111.97, Steps=247878, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6838, Total reward=39.93, Steps=247917, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6839, Total reward=6.14, Steps=247941, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6840, Total reward=86.01, Steps=248004, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6841, Total reward=67.24, Steps=248036, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6842, Total reward=10.57, Steps=248050, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6843, Total reward=31.28, Steps=248120, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6844, Total reward=0.02, Steps=248137, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6845, Total reward=22.89, Steps=248167, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6846, Total reward=90.7, Steps=248220, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6847, Total reward=59.23, Steps=248258, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6848, Total reward=56.44, Steps=248336, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6849, Total reward=24.85, Steps=248364, Training iteration=136
Training> Name=main_level/agent, Worker=0, Episode=6850, Total reward=57.98, Steps=248418, Training iteration=136
Policy training> Surrogate loss=0.003405973082408309, KL divergence=0.00021140268654562533, Entropy=0.4923672676086426, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0357271209359169, KL divergence=0.006718047894537449, Entropy=0.4864412248134613, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05013878643512726, KL divergence=0.020337071269750595, Entropy=0.4821207821369171, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05836541950702667, KL divergence=0.03554960712790489, Entropy=0.4792662560939789, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06632392108440399, KL divergence=0.047745443880558014, Entropy=0.47661253809928894, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07016071677207947, KL divergence=0.05875936895608902, Entropy=0.47312942147254944, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06926634162664413, KL divergence=0.06685949116945267, Entropy=0.4713141918182373, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07314914464950562, KL divergence=0.07329867780208588, Entropy=0.47090086340904236, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07274200767278671, KL divergence=0.07848919183015823, Entropy=0.47023627161979675, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07430736720561981, KL divergence=0.08231134712696075, Entropy=0.46840983629226685, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/429_Step-248418.ckpt']
Uploaded 3 files for checkpoint 429 in 0.54 seconds
saved intermediate frozen graph: current/model/model_429.pb
Best checkpoint number: 427, Last checkpoint number: 427
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'409'}
Training> Name=main_level/agent, Worker=0, Episode=6851, Total reward=39.87, Steps=248442, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6852, Total reward=62.45, Steps=248475, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6853, Total reward=41.68, Steps=248505, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6854, Total reward=43.98, Steps=248527, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6855, Total reward=23.03, Steps=248556, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6856, Total reward=28.11, Steps=248573, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6857, Total reward=83.08, Steps=248656, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6858, Total reward=102.12, Steps=248745, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6859, Total reward=69.53, Steps=248822, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6860, Total reward=72.85, Steps=248862, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6861, Total reward=45.42, Steps=248890, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6862, Total reward=67.46, Steps=248953, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6863, Total reward=10.8, Steps=248992, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6864, Total reward=66.4, Steps=249072, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6865, Total reward=4.78, Steps=249096, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6866, Total reward=52.92, Steps=249146, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6867, Total reward=15.56, Steps=249173, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6868, Total reward=61.5, Steps=249203, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6869, Total reward=35.62, Steps=249231, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6870, Total reward=33.31, Steps=249266, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6871, Total reward=24.03, Steps=249289, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6872, Total reward=67.02, Steps=249321, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6873, Total reward=34.71, Steps=249341, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6874, Total reward=58.39, Steps=249364, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6875, Total reward=25.23, Steps=249391, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6876, Total reward=25.12, Steps=249409, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6877, Total reward=116.97, Steps=249488, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6878, Total reward=33.79, Steps=249525, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6879, Total reward=17.34, Steps=249552, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6880, Total reward=114.32, Steps=249671, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6881, Total reward=56.56, Steps=249702, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6882, Total reward=33.11, Steps=249720, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6883, Total reward=10.41, Steps=249766, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6884, Total reward=6.21, Steps=249823, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6885, Total reward=73.74, Steps=249910, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6886, Total reward=84.88, Steps=249967, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6887, Total reward=64.73, Steps=250008, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6888, Total reward=57.69, Steps=250049, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6889, Total reward=14.17, Steps=250065, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6890, Total reward=57.09, Steps=250111, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6891, Total reward=60.36, Steps=250140, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6892, Total reward=51.03, Steps=250172, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6893, Total reward=46.63, Steps=250202, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6894, Total reward=40.2, Steps=250225, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6895, Total reward=25.52, Steps=250258, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6896, Total reward=26.68, Steps=250278, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6897, Total reward=105.74, Steps=250362, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6898, Total reward=32.41, Steps=250390, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6899, Total reward=14.5, Steps=250425, Training iteration=137
Training> Name=main_level/agent, Worker=0, Episode=6900, Total reward=70.3, Steps=250467, Training iteration=137
Policy training> Surrogate loss=0.0005149450153112411, KL divergence=0.0002717433962970972, Entropy=0.5034502148628235, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.036904845386743546, KL divergence=0.00797372218221426, Entropy=0.5008609294891357, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.051854535937309265, KL divergence=0.021314140409231186, Entropy=0.4945515990257263, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06008916348218918, KL divergence=0.033952273428440094, Entropy=0.4907734990119934, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06505688279867172, KL divergence=0.044843144714832306, Entropy=0.48761922121047974, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06954354047775269, KL divergence=0.05442977696657181, Entropy=0.4854469895362854, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07207679748535156, KL divergence=0.06221623718738556, Entropy=0.48466968536376953, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07408598065376282, KL divergence=0.06849244236946106, Entropy=0.48379844427108765, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07635922729969025, KL divergence=0.07360698282718658, Entropy=0.4838602542877197, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07734283059835434, KL divergence=0.07787704467773438, Entropy=0.4837716221809387, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/430_Step-250467.ckpt']
Uploaded 3 files for checkpoint 430 in 0.57 seconds
saved intermediate frozen graph: current/model/model_430.pb
Best checkpoint number: 427, Last checkpoint number: 428
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'426'}
Training> Name=main_level/agent, Worker=0, Episode=6901, Total reward=42.58, Steps=250498, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6902, Total reward=42.21, Steps=250524, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6903, Total reward=31.33, Steps=250556, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6904, Total reward=96.5, Steps=250656, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6905, Total reward=13.96, Steps=250690, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6906, Total reward=7.53, Steps=250706, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6907, Total reward=55.2, Steps=250751, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6908, Total reward=74.23, Steps=250798, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6909, Total reward=19.49, Steps=250819, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6910, Total reward=75.9, Steps=250868, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6911, Total reward=46.3, Steps=250909, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6912, Total reward=21.13, Steps=250933, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6913, Total reward=57.53, Steps=250963, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6914, Total reward=37.97, Steps=250985, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6915, Total reward=12.81, Steps=251010, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6916, Total reward=25.81, Steps=251028, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6917, Total reward=52.41, Steps=251081, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6918, Total reward=93.72, Steps=251172, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6919, Total reward=85.25, Steps=251261, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6920, Total reward=59.19, Steps=251306, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6921, Total reward=65.88, Steps=251337, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6922, Total reward=32.22, Steps=251359, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6923, Total reward=18.01, Steps=251390, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6924, Total reward=58.88, Steps=251464, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6925, Total reward=75.31, Steps=251553, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6926, Total reward=44.41, Steps=251599, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6927, Total reward=73.02, Steps=251639, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6928, Total reward=62.2, Steps=251709, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6929, Total reward=68.94, Steps=251767, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6930, Total reward=24.44, Steps=251813, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6931, Total reward=18.55, Steps=251842, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6932, Total reward=46.93, Steps=251875, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6933, Total reward=52.47, Steps=251905, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6934, Total reward=41.86, Steps=251928, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6935, Total reward=22.7, Steps=251959, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6936, Total reward=27.29, Steps=251989, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6937, Total reward=54.28, Steps=252026, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6938, Total reward=36.12, Steps=252076, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6939, Total reward=47.54, Steps=252140, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6940, Total reward=82.89, Steps=252193, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6941, Total reward=52.9, Steps=252229, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6942, Total reward=27.58, Steps=252245, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6943, Total reward=0.02, Steps=252262, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6944, Total reward=74.48, Steps=252337, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6945, Total reward=101.44, Steps=252440, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6946, Total reward=31.54, Steps=252473, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6947, Total reward=37.07, Steps=252511, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6948, Total reward=64.91, Steps=252547, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6949, Total reward=78.38, Steps=252615, Training iteration=138
Training> Name=main_level/agent, Worker=0, Episode=6950, Total reward=72.53, Steps=252662, Training iteration=138
Policy training> Surrogate loss=0.006318450905382633, KL divergence=8.479869575239718e-05, Entropy=0.49473369121551514, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.031114406883716583, KL divergence=0.004771854728460312, Entropy=0.49502667784690857, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.051824700087308884, KL divergence=0.016944192349910736, Entropy=0.4877716600894928, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.060351572930812836, KL divergence=0.02962382137775421, Entropy=0.47949111461639404, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06414210796356201, KL divergence=0.04110080748796463, Entropy=0.4752662777900696, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06126738712191582, KL divergence=0.05031532049179077, Entropy=0.4745141267776489, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06614771485328674, KL divergence=0.06050039827823639, Entropy=0.47597628831863403, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07178223878145218, KL divergence=0.06487506628036499, Entropy=0.4768567681312561, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06441347301006317, KL divergence=0.07071218639612198, Entropy=0.4825737774372101, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08128727227449417, KL divergence=0.07432921230792999, Entropy=0.480113685131073, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/431_Step-252662.ckpt']
Uploaded 3 files for checkpoint 431 in 0.54 seconds
saved intermediate frozen graph: current/model/model_431.pb
Best checkpoint number: 427, Last checkpoint number: 429
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'428'}
Training> Name=main_level/agent, Worker=0, Episode=6951, Total reward=63.23, Steps=252701, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6952, Total reward=24.06, Steps=252731, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6953, Total reward=59.34, Steps=252762, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6954, Total reward=43.38, Steps=252783, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6955, Total reward=32.6, Steps=252810, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6956, Total reward=21.44, Steps=252846, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6957, Total reward=34.0, Steps=252874, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6958, Total reward=32.87, Steps=252908, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6959, Total reward=59.51, Steps=252962, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6960, Total reward=74.13, Steps=253016, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6961, Total reward=78.25, Steps=253065, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6962, Total reward=50.85, Steps=253109, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6963, Total reward=3.31, Steps=253128, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6964, Total reward=1.95, Steps=253147, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6965, Total reward=57.03, Steps=253213, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6966, Total reward=92.2, Steps=253277, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6967, Total reward=74.95, Steps=253339, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6968, Total reward=59.22, Steps=253368, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6969, Total reward=88.99, Steps=253427, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6970, Total reward=14.57, Steps=253441, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6971, Total reward=54.53, Steps=253467, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6972, Total reward=65.46, Steps=253499, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6973, Total reward=40.78, Steps=253520, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6974, Total reward=35.63, Steps=253542, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6975, Total reward=29.37, Steps=253572, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6976, Total reward=11.05, Steps=253587, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6977, Total reward=85.77, Steps=253666, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6978, Total reward=48.41, Steps=253715, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6979, Total reward=60.14, Steps=253784, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6980, Total reward=86.61, Steps=253846, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6981, Total reward=64.04, Steps=253876, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6982, Total reward=35.83, Steps=253906, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6983, Total reward=10.64, Steps=253925, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6984, Total reward=50.68, Steps=253993, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6985, Total reward=59.76, Steps=254055, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6986, Total reward=65.52, Steps=254150, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6987, Total reward=71.77, Steps=254214, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6988, Total reward=58.41, Steps=254242, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6989, Total reward=29.68, Steps=254271, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6990, Total reward=34.96, Steps=254301, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6991, Total reward=44.93, Steps=254337, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6992, Total reward=80.95, Steps=254380, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6993, Total reward=39.08, Steps=254401, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6994, Total reward=35.21, Steps=254422, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6995, Total reward=42.49, Steps=254450, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6996, Total reward=20.38, Steps=254467, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6997, Total reward=48.6, Steps=254519, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6998, Total reward=28.24, Steps=254532, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=6999, Total reward=9.12, Steps=254557, Training iteration=139
Training> Name=main_level/agent, Worker=0, Episode=7000, Total reward=41.42, Steps=254598, Training iteration=139
Policy training> Surrogate loss=-0.008720540441572666, KL divergence=6.421945727197453e-05, Entropy=0.5166007876396179, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.012762117199599743, KL divergence=0.0027708355337381363, Entropy=0.49873414635658264, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0500321090221405, KL divergence=0.0126573471352458, Entropy=0.498155802488327, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06119289621710777, KL divergence=0.02368772216141224, Entropy=0.49478110671043396, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06259146332740784, KL divergence=0.03694763034582138, Entropy=0.4964970052242279, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05635754391551018, KL divergence=0.04327348992228508, Entropy=0.4826853275299072, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06593478471040726, KL divergence=0.05867144465446472, Entropy=0.48299455642700195, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07830014824867249, KL divergence=0.06447026878595352, Entropy=0.47874951362609863, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07586906105279922, KL divergence=0.07092481851577759, Entropy=0.4778784215450287, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08296855539083481, KL divergence=0.07800699025392532, Entropy=0.4770117700099945, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/432_Step-254598.ckpt']
Uploaded 3 files for checkpoint 432 in 0.59 seconds
saved intermediate frozen graph: current/model/model_432.pb
Best checkpoint number: 427, Last checkpoint number: 430
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'429'}
Training> Name=main_level/agent, Worker=0, Episode=7001, Total reward=53.86, Steps=254639, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7002, Total reward=35.18, Steps=254660, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7003, Total reward=120.08, Steps=254779, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7004, Total reward=86.69, Steps=254870, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7005, Total reward=10.04, Steps=254891, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7006, Total reward=69.54, Steps=254945, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7007, Total reward=59.65, Steps=254987, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7008, Total reward=52.55, Steps=255029, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7009, Total reward=20.91, Steps=255054, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7010, Total reward=112.37, Steps=255105, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7011, Total reward=18.62, Steps=255129, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7012, Total reward=65.37, Steps=255171, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7013, Total reward=54.67, Steps=255201, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7014, Total reward=38.72, Steps=255222, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7015, Total reward=40.72, Steps=255249, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7016, Total reward=22.7, Steps=255271, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7017, Total reward=50.41, Steps=255308, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7018, Total reward=25.18, Steps=255353, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7019, Total reward=70.77, Steps=255412, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7020, Total reward=52.02, Steps=255482, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7021, Total reward=59.08, Steps=255526, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7022, Total reward=127.32, Steps=255630, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7023, Total reward=42.67, Steps=255709, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7024, Total reward=12.03, Steps=255741, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7025, Total reward=94.66, Steps=255825, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7026, Total reward=87.03, Steps=255893, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7027, Total reward=49.85, Steps=255931, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7028, Total reward=62.16, Steps=255959, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7029, Total reward=29.77, Steps=255994, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7030, Total reward=71.51, Steps=256051, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7031, Total reward=29.96, Steps=256068, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7032, Total reward=63.87, Steps=256107, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7033, Total reward=47.47, Steps=256129, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7034, Total reward=34.97, Steps=256150, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7035, Total reward=27.73, Steps=256177, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7036, Total reward=20.58, Steps=256207, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7037, Total reward=59.82, Steps=256253, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7038, Total reward=101.39, Steps=256323, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7039, Total reward=71.82, Steps=256381, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7040, Total reward=73.21, Steps=256426, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7041, Total reward=79.81, Steps=256485, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7042, Total reward=34.29, Steps=256514, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7043, Total reward=29.35, Steps=256548, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7044, Total reward=9.16, Steps=256575, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7045, Total reward=26.96, Steps=256616, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7046, Total reward=120.42, Steps=256712, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7047, Total reward=66.54, Steps=256751, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7048, Total reward=60.64, Steps=256783, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7049, Total reward=31.19, Steps=256828, Training iteration=140
Training> Name=main_level/agent, Worker=0, Episode=7050, Total reward=7.61, Steps=256848, Training iteration=140
Policy training> Surrogate loss=-0.012448528781533241, KL divergence=0.00025832728715613484, Entropy=0.5090430378913879, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03891993686556816, KL divergence=0.009267708286643028, Entropy=0.5066454410552979, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06324931234121323, KL divergence=0.02346571907401085, Entropy=0.5036000609397888, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0614810511469841, KL divergence=0.03604729101061821, Entropy=0.4877101182937622, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05650793015956879, KL divergence=0.04683293402194977, Entropy=0.4898279905319214, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.060156531631946564, KL divergence=0.059649091213941574, Entropy=0.4893241226673126, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07375692576169968, KL divergence=0.06893602013587952, Entropy=0.493979811668396, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05803532153367996, KL divergence=0.07594597339630127, Entropy=0.48808303475379944, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06177063286304474, KL divergence=0.0808027982711792, Entropy=0.49343767762184143, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0845184400677681, KL divergence=0.0851830467581749, Entropy=0.49338045716285706, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/433_Step-256848.ckpt']
Uploaded 3 files for checkpoint 433 in 0.61 seconds
saved intermediate frozen graph: current/model/model_433.pb
Best checkpoint number: 427, Last checkpoint number: 431
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'430'}
Training> Name=main_level/agent, Worker=0, Episode=7051, Total reward=11.51, Steps=256861, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7052, Total reward=42.5, Steps=256894, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7053, Total reward=42.53, Steps=256925, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7054, Total reward=39.43, Steps=256945, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7055, Total reward=24.87, Steps=256972, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7056, Total reward=23.29, Steps=256989, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7057, Total reward=63.56, Steps=257055, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7058, Total reward=89.09, Steps=257117, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7059, Total reward=16.24, Steps=257139, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7060, Total reward=39.15, Steps=257173, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7061, Total reward=70.98, Steps=257208, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7062, Total reward=32.04, Steps=257226, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7063, Total reward=15.09, Steps=257261, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7064, Total reward=95.3, Steps=257379, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7065, Total reward=91.12, Steps=257453, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7066, Total reward=83.23, Steps=257521, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7067, Total reward=125.88, Steps=257611, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7068, Total reward=63.57, Steps=257655, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7069, Total reward=25.64, Steps=257692, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7070, Total reward=78.06, Steps=257737, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7071, Total reward=14.43, Steps=257762, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7072, Total reward=68.98, Steps=257803, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7073, Total reward=46.2, Steps=257835, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7074, Total reward=36.46, Steps=257856, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7075, Total reward=28.66, Steps=257889, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7076, Total reward=18.0, Steps=257905, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7077, Total reward=99.2, Steps=257987, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7078, Total reward=26.17, Steps=258001, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7079, Total reward=23.43, Steps=258032, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7080, Total reward=67.14, Steps=258072, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7081, Total reward=65.38, Steps=258102, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7082, Total reward=41.01, Steps=258126, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7083, Total reward=11.36, Steps=258166, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7084, Total reward=84.57, Steps=258237, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7085, Total reward=109.86, Steps=258344, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7086, Total reward=76.1, Steps=258397, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7087, Total reward=56.96, Steps=258464, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7088, Total reward=65.24, Steps=258494, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7089, Total reward=40.26, Steps=258529, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7090, Total reward=35.27, Steps=258557, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7091, Total reward=70.54, Steps=258595, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7092, Total reward=69.79, Steps=258637, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7093, Total reward=39.39, Steps=258659, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7094, Total reward=41.3, Steps=258680, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7095, Total reward=24.64, Steps=258707, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7096, Total reward=24.24, Steps=258727, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7097, Total reward=49.93, Steps=258788, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7098, Total reward=37.87, Steps=258808, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7099, Total reward=10.23, Steps=258829, Training iteration=141
Training> Name=main_level/agent, Worker=0, Episode=7100, Total reward=93.47, Steps=258888, Training iteration=141
Policy training> Surrogate loss=-0.005121237598359585, KL divergence=7.030412234598771e-05, Entropy=0.49347081780433655, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0018704725662246346, KL divergence=0.0025585764087736607, Entropy=0.49020063877105713, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.032622989267110825, KL divergence=0.011197715997695923, Entropy=0.4743053913116455, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05965391919016838, KL divergence=0.023677587509155273, Entropy=0.4823296070098877, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04904260113835335, KL divergence=0.03531287983059883, Entropy=0.49115678668022156, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06014877185225487, KL divergence=0.0453321635723114, Entropy=0.4720698893070221, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05914038047194481, KL divergence=0.05128741264343262, Entropy=0.48649731278419495, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0513642318546772, KL divergence=0.060435470193624496, Entropy=0.4802480936050415, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06853322684764862, KL divergence=0.06492423266172409, Entropy=0.4789468050003052, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.04499809816479683, KL divergence=0.06872796267271042, Entropy=0.4818180799484253, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/434_Step-258888.ckpt']
Uploaded 3 files for checkpoint 434 in 0.61 seconds
saved intermediate frozen graph: current/model/model_434.pb
Best checkpoint number: 427, Last checkpoint number: 432
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'431'}
Training> Name=main_level/agent, Worker=0, Episode=7101, Total reward=59.44, Steps=258922, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7102, Total reward=36.32, Steps=258958, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7103, Total reward=6.56, Steps=259005, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7104, Total reward=83.91, Steps=259121, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7105, Total reward=103.94, Steps=259209, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7106, Total reward=102.3, Steps=259297, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7107, Total reward=60.99, Steps=259362, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7108, Total reward=61.54, Steps=259395, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7109, Total reward=27.61, Steps=259427, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7110, Total reward=96.49, Steps=259474, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7111, Total reward=10.67, Steps=259494, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7112, Total reward=42.63, Steps=259525, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7113, Total reward=20.85, Steps=259545, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7114, Total reward=38.06, Steps=259567, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7115, Total reward=35.46, Steps=259596, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7116, Total reward=12.64, Steps=259612, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7117, Total reward=61.22, Steps=259667, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7118, Total reward=40.29, Steps=259693, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7119, Total reward=15.0, Steps=259728, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7120, Total reward=74.23, Steps=259777, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7121, Total reward=65.65, Steps=259806, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7122, Total reward=61.44, Steps=259856, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7123, Total reward=51.92, Steps=259909, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7124, Total reward=9.69, Steps=259936, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7125, Total reward=10.05, Steps=259967, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7126, Total reward=75.34, Steps=260018, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7127, Total reward=118.97, Steps=260113, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7128, Total reward=75.86, Steps=260165, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7129, Total reward=50.74, Steps=260216, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7130, Total reward=58.44, Steps=260269, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7131, Total reward=49.32, Steps=260311, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7132, Total reward=47.66, Steps=260344, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7133, Total reward=53.48, Steps=260373, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7134, Total reward=34.23, Steps=260393, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7135, Total reward=21.84, Steps=260418, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7136, Total reward=21.68, Steps=260438, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7137, Total reward=132.94, Steps=260564, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7138, Total reward=49.18, Steps=260599, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7139, Total reward=8.93, Steps=260614, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7140, Total reward=86.78, Steps=260666, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7141, Total reward=44.42, Steps=260694, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7142, Total reward=45.22, Steps=260731, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7143, Total reward=3.13, Steps=260756, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7144, Total reward=15.0, Steps=260787, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7145, Total reward=67.91, Steps=260865, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7146, Total reward=92.11, Steps=260912, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7147, Total reward=75.01, Steps=260973, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7148, Total reward=65.89, Steps=261024, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7149, Total reward=22.9, Steps=261070, Training iteration=142
Training> Name=main_level/agent, Worker=0, Episode=7150, Total reward=30.53, Steps=261102, Training iteration=142
Policy training> Surrogate loss=0.004842916503548622, KL divergence=0.0001540077500976622, Entropy=0.4989042282104492, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04135029762983322, KL divergence=0.00457493681460619, Entropy=0.4934341609477997, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.056134115904569626, KL divergence=0.01690075546503067, Entropy=0.4960924983024597, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05949315056204796, KL divergence=0.030763443559408188, Entropy=0.4874300956726074, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06567750126123428, KL divergence=0.041309259831905365, Entropy=0.48646193742752075, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06860587000846863, KL divergence=0.05158236622810364, Entropy=0.4852955639362335, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07229871302843094, KL divergence=0.05795608088374138, Entropy=0.48375204205513, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07038111984729767, KL divergence=0.06593866646289825, Entropy=0.48439016938209534, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0793648213148117, KL divergence=0.06941182166337967, Entropy=0.483393132686615, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0737905353307724, KL divergence=0.07488255202770233, Entropy=0.48363667726516724, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/435_Step-261102.ckpt']
Uploaded 3 files for checkpoint 435 in 0.55 seconds
saved intermediate frozen graph: current/model/model_435.pb
Best checkpoint number: 427, Last checkpoint number: 433
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'432'}
Training> Name=main_level/agent, Worker=0, Episode=7151, Total reward=49.99, Steps=261137, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7152, Total reward=47.6, Steps=261168, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7153, Total reward=53.33, Steps=261198, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7154, Total reward=41.82, Steps=261220, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7155, Total reward=13.85, Steps=261251, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7156, Total reward=20.29, Steps=261269, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7157, Total reward=86.01, Steps=261354, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7158, Total reward=34.55, Steps=261387, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7159, Total reward=5.67, Steps=261415, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7160, Total reward=170.65, Steps=261558, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7161, Total reward=62.0, Steps=261590, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7162, Total reward=36.62, Steps=261612, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7163, Total reward=10.84, Steps=261660, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7164, Total reward=9.39, Steps=261684, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7165, Total reward=131.35, Steps=261801, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7166, Total reward=76.5, Steps=261850, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7167, Total reward=65.24, Steps=261899, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7168, Total reward=71.16, Steps=261949, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7169, Total reward=57.93, Steps=262017, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7170, Total reward=57.53, Steps=262053, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7171, Total reward=38.49, Steps=262099, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7172, Total reward=67.92, Steps=262129, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7173, Total reward=42.71, Steps=262160, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7174, Total reward=45.2, Steps=262182, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7175, Total reward=27.38, Steps=262207, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7176, Total reward=20.11, Steps=262240, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7177, Total reward=35.09, Steps=262277, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7178, Total reward=92.08, Steps=262379, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7179, Total reward=10.72, Steps=262405, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7180, Total reward=68.64, Steps=262445, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7181, Total reward=66.85, Steps=262507, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7182, Total reward=33.96, Steps=262535, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7183, Total reward=64.01, Steps=262610, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7184, Total reward=131.25, Steps=262724, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7185, Total reward=116.98, Steps=262838, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7186, Total reward=96.79, Steps=262936, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7187, Total reward=91.4, Steps=263026, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7188, Total reward=114.23, Steps=263104, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7189, Total reward=28.94, Steps=263123, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7190, Total reward=65.53, Steps=263175, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7191, Total reward=51.05, Steps=263210, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7192, Total reward=53.53, Steps=263240, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7193, Total reward=29.41, Steps=263261, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7194, Total reward=43.66, Steps=263282, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7195, Total reward=37.66, Steps=263312, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7196, Total reward=21.16, Steps=263332, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7197, Total reward=45.97, Steps=263361, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7198, Total reward=39.27, Steps=263397, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7199, Total reward=13.73, Steps=263418, Training iteration=143
Training> Name=main_level/agent, Worker=0, Episode=7200, Total reward=84.6, Steps=263468, Training iteration=143
Policy training> Surrogate loss=0.0007746713235974312, KL divergence=0.0004789489321410656, Entropy=0.5130958557128906, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03717384487390518, KL divergence=0.010189076885581017, Entropy=0.49510079622268677, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04213281720876694, KL divergence=0.02287125214934349, Entropy=0.49369487166404724, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.052994512021541595, KL divergence=0.034108106046915054, Entropy=0.4775241017341614, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.07395178079605103, KL divergence=0.04573303833603859, Entropy=0.48893147706985474, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06527964770793915, KL divergence=0.05458873137831688, Entropy=0.48381394147872925, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06417593359947205, KL divergence=0.058882929384708405, Entropy=0.47640103101730347, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.061642494052648544, KL divergence=0.06930004060268402, Entropy=0.48727208375930786, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09657932817935944, KL divergence=0.07329860329627991, Entropy=0.47919878363609314, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08672790229320526, KL divergence=0.07626998424530029, Entropy=0.48811036348342896, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/436_Step-263468.ckpt']
Uploaded 3 files for checkpoint 436 in 0.57 seconds
saved intermediate frozen graph: current/model/model_436.pb
Best checkpoint number: 427, Last checkpoint number: 434
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'433'}
Training> Name=main_level/agent, Worker=0, Episode=7201, Total reward=60.13, Steps=263497, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7202, Total reward=33.76, Steps=263520, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7203, Total reward=33.62, Steps=263584, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7204, Total reward=13.89, Steps=263602, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7205, Total reward=15.15, Steps=263620, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7206, Total reward=27.25, Steps=263643, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7207, Total reward=27.95, Steps=263677, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7208, Total reward=72.02, Steps=263729, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7209, Total reward=26.93, Steps=263768, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7210, Total reward=60.75, Steps=263812, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7211, Total reward=70.06, Steps=263851, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7212, Total reward=65.98, Steps=263893, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7213, Total reward=48.38, Steps=263923, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7214, Total reward=46.88, Steps=263945, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7215, Total reward=17.84, Steps=263974, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7216, Total reward=20.52, Steps=263989, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7217, Total reward=65.65, Steps=264050, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7218, Total reward=25.01, Steps=264075, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7219, Total reward=18.01, Steps=264108, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7220, Total reward=50.74, Steps=264144, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7221, Total reward=55.48, Steps=264174, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7222, Total reward=114.45, Steps=264281, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7223, Total reward=36.89, Steps=264337, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7224, Total reward=4.01, Steps=264355, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7225, Total reward=99.76, Steps=264461, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7226, Total reward=96.65, Steps=264523, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7227, Total reward=57.84, Steps=264565, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7228, Total reward=57.34, Steps=264608, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7229, Total reward=35.01, Steps=264641, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7230, Total reward=18.64, Steps=264668, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7231, Total reward=55.43, Steps=264706, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7232, Total reward=69.49, Steps=264739, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7233, Total reward=42.56, Steps=264761, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7234, Total reward=29.32, Steps=264772, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7235, Total reward=32.75, Steps=264801, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7236, Total reward=20.59, Steps=264827, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7237, Total reward=118.83, Steps=264921, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7238, Total reward=40.69, Steps=264960, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7239, Total reward=76.05, Steps=265026, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7240, Total reward=15.29, Steps=265041, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7241, Total reward=58.31, Steps=265070, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7242, Total reward=37.3, Steps=265116, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7243, Total reward=18.41, Steps=265158, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7244, Total reward=17.36, Steps=265181, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7245, Total reward=116.95, Steps=265296, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7246, Total reward=105.63, Steps=265354, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7247, Total reward=116.24, Steps=265447, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7248, Total reward=55.28, Steps=265474, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7249, Total reward=32.47, Steps=265506, Training iteration=144
Training> Name=main_level/agent, Worker=0, Episode=7250, Total reward=109.03, Steps=265555, Training iteration=144
Policy training> Surrogate loss=0.0016182237304747105, KL divergence=0.00017798753106035292, Entropy=0.5143953561782837, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03263918310403824, KL divergence=0.006122026592493057, Entropy=0.5120899677276611, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04460461437702179, KL divergence=0.018809711560606956, Entropy=0.5011734962463379, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06018521264195442, KL divergence=0.033629409968853, Entropy=0.4952774941921234, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06120622903108597, KL divergence=0.045785535126924515, Entropy=0.49004092812538147, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06622881442308426, KL divergence=0.054598309099674225, Entropy=0.48812925815582275, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06814301759004593, KL divergence=0.06244365870952606, Entropy=0.4878228008747101, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07361102104187012, KL divergence=0.0678483173251152, Entropy=0.48992347717285156, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07158882915973663, KL divergence=0.07367098331451416, Entropy=0.4900091290473938, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07890858501195908, KL divergence=0.07819407433271408, Entropy=0.4915498197078705, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/437_Step-265555.ckpt']
Uploaded 3 files for checkpoint 437 in 0.53 seconds
saved intermediate frozen graph: current/model/model_437.pb
Best checkpoint number: 427, Last checkpoint number: 435
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'434'}
Training> Name=main_level/agent, Worker=0, Episode=7251, Total reward=17.47, Steps=265578, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7252, Total reward=42.36, Steps=265609, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7253, Total reward=33.9, Steps=265635, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7254, Total reward=41.32, Steps=265656, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7255, Total reward=25.25, Steps=265669, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7256, Total reward=20.45, Steps=265706, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7257, Total reward=57.85, Steps=265744, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7258, Total reward=38.15, Steps=265769, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7259, Total reward=25.4, Steps=265797, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7260, Total reward=63.8, Steps=265840, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7261, Total reward=75.41, Steps=265911, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7262, Total reward=57.34, Steps=265945, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7263, Total reward=10.47, Steps=265975, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7264, Total reward=3.27, Steps=266002, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7265, Total reward=81.9, Steps=266113, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7266, Total reward=64.57, Steps=266163, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7267, Total reward=62.35, Steps=266204, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7268, Total reward=66.2, Steps=266232, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7269, Total reward=32.16, Steps=266264, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7270, Total reward=75.78, Steps=266311, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7271, Total reward=78.73, Steps=266348, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7272, Total reward=70.29, Steps=266390, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7273, Total reward=45.66, Steps=266419, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7274, Total reward=47.38, Steps=266441, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7275, Total reward=20.17, Steps=266469, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7276, Total reward=22.48, Steps=266490, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7277, Total reward=53.63, Steps=266522, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7278, Total reward=93.87, Steps=266595, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7279, Total reward=16.34, Steps=266626, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7280, Total reward=57.49, Steps=266663, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7281, Total reward=32.56, Steps=266690, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7282, Total reward=38.89, Steps=266738, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7283, Total reward=83.67, Steps=266816, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7284, Total reward=79.97, Steps=266889, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7285, Total reward=16.79, Steps=266915, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7286, Total reward=68.49, Steps=266965, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7287, Total reward=60.96, Steps=267026, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7288, Total reward=59.81, Steps=267054, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7289, Total reward=30.58, Steps=267080, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7290, Total reward=71.5, Steps=267125, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7291, Total reward=23.94, Steps=267155, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7292, Total reward=22.29, Steps=267185, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7293, Total reward=51.22, Steps=267213, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7294, Total reward=39.16, Steps=267234, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7295, Total reward=16.92, Steps=267262, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7296, Total reward=37.35, Steps=267307, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7297, Total reward=82.94, Steps=267389, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7298, Total reward=20.99, Steps=267406, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7299, Total reward=58.29, Steps=267467, Training iteration=145
Training> Name=main_level/agent, Worker=0, Episode=7300, Total reward=15.05, Steps=267479, Training iteration=145
Policy training> Surrogate loss=0.004538707435131073, KL divergence=0.00010094817844219506, Entropy=0.5010135173797607, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.026209823787212372, KL divergence=0.004109113942831755, Entropy=0.49218931794166565, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.057597000151872635, KL divergence=0.013148856349289417, Entropy=0.49411559104919434, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.027326390147209167, KL divergence=0.024975821375846863, Entropy=0.48513031005859375, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.07350697368383408, KL divergence=0.03493231162428856, Entropy=0.46928834915161133, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07654807716608047, KL divergence=0.04488210380077362, Entropy=0.46413660049438477, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08672838658094406, KL divergence=0.0545671246945858, Entropy=0.4629446566104889, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06429716944694519, KL divergence=0.06411255896091461, Entropy=0.46639883518218994, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07051721215248108, KL divergence=0.06935737282037735, Entropy=0.4704199731349945, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08081096410751343, KL divergence=0.07429373264312744, Entropy=0.4549314081668854, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/438_Step-267479.ckpt']
Uploaded 3 files for checkpoint 438 in 0.59 seconds
saved intermediate frozen graph: current/model/model_438.pb
Best checkpoint number: 427, Last checkpoint number: 436
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'435'}
Training> Name=main_level/agent, Worker=0, Episode=7301, Total reward=35.76, Steps=267507, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7302, Total reward=39.48, Steps=267556, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7303, Total reward=6.75, Steps=267575, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7304, Total reward=7.41, Steps=267606, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7305, Total reward=3.41, Steps=267631, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7306, Total reward=60.51, Steps=267681, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7307, Total reward=86.85, Steps=267767, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7308, Total reward=54.99, Steps=267794, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7309, Total reward=20.19, Steps=267812, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7310, Total reward=29.25, Steps=267835, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7311, Total reward=40.08, Steps=267860, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7312, Total reward=71.7, Steps=267900, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7313, Total reward=42.35, Steps=267932, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7314, Total reward=28.7, Steps=267943, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7315, Total reward=17.64, Steps=267993, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7316, Total reward=17.09, Steps=268008, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7317, Total reward=132.41, Steps=268154, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7318, Total reward=47.94, Steps=268196, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7319, Total reward=12.36, Steps=268215, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7320, Total reward=37.22, Steps=268248, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7321, Total reward=39.42, Steps=268277, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7322, Total reward=42.07, Steps=268301, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7323, Total reward=6.95, Steps=268328, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7324, Total reward=65.41, Steps=268396, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7325, Total reward=9.59, Steps=268416, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7326, Total reward=74.65, Steps=268467, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7327, Total reward=62.45, Steps=268518, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7328, Total reward=62.94, Steps=268546, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7329, Total reward=42.25, Steps=268584, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7330, Total reward=93.84, Steps=268628, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7331, Total reward=51.05, Steps=268664, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7332, Total reward=56.46, Steps=268696, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7333, Total reward=45.8, Steps=268727, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7334, Total reward=42.19, Steps=268748, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7335, Total reward=48.89, Steps=268778, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7336, Total reward=20.49, Steps=268796, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7337, Total reward=63.49, Steps=268829, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7338, Total reward=36.48, Steps=268873, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7339, Total reward=74.89, Steps=268924, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7340, Total reward=72.31, Steps=268983, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7341, Total reward=66.54, Steps=269016, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7342, Total reward=157.98, Steps=269159, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7343, Total reward=15.67, Steps=269175, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7344, Total reward=20.19, Steps=269207, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7345, Total reward=67.78, Steps=269271, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7346, Total reward=90.81, Steps=269336, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7347, Total reward=51.85, Steps=269381, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7348, Total reward=73.4, Steps=269430, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7349, Total reward=87.99, Steps=269492, Training iteration=146
Training> Name=main_level/agent, Worker=0, Episode=7350, Total reward=11.22, Steps=269506, Training iteration=146
Policy training> Surrogate loss=-0.0041093602776527405, KL divergence=7.13656991138123e-05, Entropy=0.5153148770332336, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02094818651676178, KL divergence=0.00284560420550406, Entropy=0.5060219764709473, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04190853238105774, KL divergence=0.01101748924702406, Entropy=0.493234783411026, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07258255034685135, KL divergence=0.023212334141135216, Entropy=0.497456818819046, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06819278001785278, KL divergence=0.03324691206216812, Entropy=0.4803687632083893, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07954927533864975, KL divergence=0.0437295138835907, Entropy=0.49012288451194763, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06289863586425781, KL divergence=0.05351020023226738, Entropy=0.47680529952049255, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06238796189427376, KL divergence=0.0618552602827549, Entropy=0.482370525598526, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.062306907027959824, KL divergence=0.07170706987380981, Entropy=0.47762903571128845, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06356707215309143, KL divergence=0.07146882265806198, Entropy=0.47361984848976135, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/439_Step-269506.ckpt']
Uploaded 3 files for checkpoint 439 in 0.55 seconds
saved intermediate frozen graph: current/model/model_439.pb
Best checkpoint number: 427, Last checkpoint number: 437
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'436'}
Training> Name=main_level/agent, Worker=0, Episode=7351, Total reward=15.08, Steps=269518, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7352, Total reward=69.64, Steps=269552, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7353, Total reward=41.65, Steps=269573, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7354, Total reward=42.74, Steps=269594, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7355, Total reward=37.31, Steps=269625, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7356, Total reward=28.51, Steps=269654, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7357, Total reward=84.36, Steps=269742, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7358, Total reward=32.39, Steps=269767, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7359, Total reward=4.41, Steps=269781, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7360, Total reward=72.15, Steps=269824, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7361, Total reward=60.93, Steps=269864, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7362, Total reward=38.19, Steps=269889, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7363, Total reward=77.98, Steps=269965, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7364, Total reward=3.02, Steps=269988, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7365, Total reward=26.98, Steps=270019, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7366, Total reward=63.93, Steps=270069, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7367, Total reward=56.33, Steps=270111, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7368, Total reward=64.43, Steps=270152, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7369, Total reward=14.0, Steps=270167, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7370, Total reward=80.25, Steps=270211, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7371, Total reward=81.84, Steps=270249, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7372, Total reward=56.48, Steps=270283, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7373, Total reward=54.83, Steps=270313, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7374, Total reward=36.85, Steps=270335, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7375, Total reward=24.87, Steps=270377, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7376, Total reward=18.63, Steps=270392, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7377, Total reward=136.82, Steps=270508, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7378, Total reward=38.59, Steps=270534, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7379, Total reward=80.06, Steps=270587, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7380, Total reward=77.55, Steps=270662, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7381, Total reward=120.77, Steps=270793, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7382, Total reward=34.23, Steps=270810, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7383, Total reward=9.55, Steps=270852, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7384, Total reward=13.29, Steps=270879, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7385, Total reward=72.01, Steps=270951, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7386, Total reward=127.14, Steps=271058, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7387, Total reward=71.11, Steps=271123, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7388, Total reward=68.56, Steps=271160, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7389, Total reward=28.28, Steps=271185, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7390, Total reward=83.64, Steps=271232, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7391, Total reward=37.24, Steps=271263, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7392, Total reward=76.71, Steps=271305, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7393, Total reward=34.41, Steps=271326, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7394, Total reward=44.09, Steps=271347, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7395, Total reward=35.66, Steps=271376, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7396, Total reward=19.42, Steps=271396, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7397, Total reward=60.65, Steps=271442, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7398, Total reward=24.59, Steps=271468, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7399, Total reward=75.74, Steps=271539, Training iteration=147
Training> Name=main_level/agent, Worker=0, Episode=7400, Total reward=67.27, Steps=271584, Training iteration=147
Policy training> Surrogate loss=0.0038332007825374603, KL divergence=0.00020309288811404258, Entropy=0.5091472864151001, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04112211987376213, KL divergence=0.009040099568665028, Entropy=0.5063159465789795, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.054227299988269806, KL divergence=0.02456427365541458, Entropy=0.49906063079833984, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06234079971909523, KL divergence=0.03975154459476471, Entropy=0.4934389591217041, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06515824049711227, KL divergence=0.053639717400074005, Entropy=0.4921515882015228, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06621923297643661, KL divergence=0.06261003017425537, Entropy=0.48646509647369385, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07644182443618774, KL divergence=0.07319977134466171, Entropy=0.4883148670196533, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08088906109333038, KL divergence=0.07898509502410889, Entropy=0.4871879816055298, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07642979919910431, KL divergence=0.08473359793424606, Entropy=0.49020227789878845, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08109565824270248, KL divergence=0.08925628662109375, Entropy=0.4938877522945404, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/440_Step-271584.ckpt']
Uploaded 3 files for checkpoint 440 in 0.58 seconds
saved intermediate frozen graph: current/model/model_440.pb
Best checkpoint number: 427, Last checkpoint number: 438
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'437'}
Training> Name=main_level/agent, Worker=0, Episode=7401, Total reward=36.02, Steps=271613, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7402, Total reward=33.09, Steps=271638, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7403, Total reward=22.5, Steps=271702, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7404, Total reward=32.62, Steps=271758, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7405, Total reward=68.92, Steps=271837, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7406, Total reward=69.05, Steps=271903, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7407, Total reward=11.37, Steps=271936, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7408, Total reward=71.7, Steps=271986, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7409, Total reward=59.92, Steps=272031, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7410, Total reward=50.18, Steps=272066, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7411, Total reward=32.38, Steps=272085, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7412, Total reward=69.87, Steps=272126, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7413, Total reward=61.32, Steps=272156, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7414, Total reward=37.87, Steps=272177, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7415, Total reward=15.96, Steps=272204, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7416, Total reward=30.05, Steps=272237, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7417, Total reward=54.39, Steps=272271, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7418, Total reward=97.4, Steps=272331, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7419, Total reward=11.39, Steps=272354, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7420, Total reward=7.64, Steps=272377, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7421, Total reward=48.99, Steps=272415, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7422, Total reward=37.89, Steps=272459, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7423, Total reward=6.92, Steps=272478, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7424, Total reward=12.36, Steps=272513, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7425, Total reward=88.72, Steps=272579, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7426, Total reward=101.52, Steps=272648, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7427, Total reward=44.8, Steps=272682, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7428, Total reward=81.84, Steps=272754, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7429, Total reward=32.31, Steps=272777, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7430, Total reward=56.75, Steps=272817, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7431, Total reward=31.44, Steps=272851, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7432, Total reward=0.01, Steps=272861, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7433, Total reward=58.19, Steps=272889, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7434, Total reward=36.77, Steps=272911, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7435, Total reward=11.67, Steps=272924, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7436, Total reward=17.85, Steps=272948, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7437, Total reward=37.19, Steps=272983, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7438, Total reward=21.6, Steps=272995, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7439, Total reward=9.03, Steps=273008, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7440, Total reward=80.43, Steps=273044, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7441, Total reward=59.73, Steps=273075, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7442, Total reward=50.24, Steps=273102, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7443, Total reward=0.01, Steps=273115, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7444, Total reward=7.12, Steps=273138, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7445, Total reward=94.06, Steps=273210, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7446, Total reward=136.73, Steps=273313, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7447, Total reward=58.34, Steps=273372, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7448, Total reward=71.53, Steps=273426, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7449, Total reward=89.03, Steps=273471, Training iteration=148
Training> Name=main_level/agent, Worker=0, Episode=7450, Total reward=83.55, Steps=273532, Training iteration=148
Policy training> Surrogate loss=0.008524472825229168, KL divergence=0.00010515991743886843, Entropy=0.5177018642425537, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.028470242395997047, KL divergence=0.0031566128600388765, Entropy=0.4992060661315918, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.02801772765815258, KL divergence=0.014124228619039059, Entropy=0.4973583519458771, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.037285175174474716, KL divergence=0.02393973432481289, Entropy=0.49670419096946716, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0735001340508461, KL divergence=0.035906556993722916, Entropy=0.48326730728149414, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0786723718047142, KL divergence=0.048416946083307266, Entropy=0.48506030440330505, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07162155210971832, KL divergence=0.05913609266281128, Entropy=0.4839754104614258, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07612534612417221, KL divergence=0.06507030129432678, Entropy=0.47907575964927673, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0842762291431427, KL divergence=0.07500121742486954, Entropy=0.48349201679229736, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0652884766459465, KL divergence=0.07474207133054733, Entropy=0.47778987884521484, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/441_Step-273532.ckpt']
Uploaded 3 files for checkpoint 441 in 0.52 seconds
saved intermediate frozen graph: current/model/model_441.pb
Best checkpoint number: 427, Last checkpoint number: 439
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'438'}
Training> Name=main_level/agent, Worker=0, Episode=7451, Total reward=49.58, Steps=273558, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7452, Total reward=31.74, Steps=273585, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7453, Total reward=62.2, Steps=273617, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7454, Total reward=46.64, Steps=273639, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7455, Total reward=33.25, Steps=273671, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7456, Total reward=29.77, Steps=273688, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7457, Total reward=63.04, Steps=273751, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7458, Total reward=34.82, Steps=273790, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7459, Total reward=4.18, Steps=273807, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7460, Total reward=85.82, Steps=273875, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7461, Total reward=67.76, Steps=273907, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7462, Total reward=27.07, Steps=273925, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7463, Total reward=6.73, Steps=273952, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7464, Total reward=12.52, Steps=273985, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7465, Total reward=38.41, Steps=274037, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7466, Total reward=83.76, Steps=274107, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7467, Total reward=70.6, Steps=274154, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7468, Total reward=65.94, Steps=274208, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7469, Total reward=12.84, Steps=274224, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7470, Total reward=85.95, Steps=274275, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7471, Total reward=49.7, Steps=274302, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7472, Total reward=59.17, Steps=274343, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7473, Total reward=48.43, Steps=274372, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7474, Total reward=37.84, Steps=274391, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7475, Total reward=34.43, Steps=274420, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7476, Total reward=26.76, Steps=274471, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7477, Total reward=116.86, Steps=274545, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7478, Total reward=30.04, Steps=274560, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7479, Total reward=84.27, Steps=274624, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7480, Total reward=61.66, Steps=274660, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7481, Total reward=75.42, Steps=274692, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7482, Total reward=46.16, Steps=274741, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7483, Total reward=9.17, Steps=274779, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7484, Total reward=3.19, Steps=274794, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7485, Total reward=59.65, Steps=274875, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7486, Total reward=3.58, Steps=274886, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7487, Total reward=63.61, Steps=274931, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7488, Total reward=68.93, Steps=274978, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7489, Total reward=20.4, Steps=274992, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7490, Total reward=76.56, Steps=275028, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7491, Total reward=61.83, Steps=275069, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7492, Total reward=8.73, Steps=275089, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7493, Total reward=49.52, Steps=275117, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7494, Total reward=35.95, Steps=275138, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7495, Total reward=27.97, Steps=275165, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7496, Total reward=15.18, Steps=275198, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7497, Total reward=16.63, Steps=275213, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7498, Total reward=45.72, Steps=275235, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7499, Total reward=71.06, Steps=275287, Training iteration=149
Training> Name=main_level/agent, Worker=0, Episode=7500, Total reward=67.47, Steps=275331, Training iteration=149
Policy training> Surrogate loss=-0.023559262976050377, KL divergence=7.89946861914359e-05, Entropy=0.5165475010871887, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.036336805671453476, KL divergence=0.0033115476835519075, Entropy=0.5058577656745911, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03969402238726616, KL divergence=0.011132906191051006, Entropy=0.5016114115715027, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05498313531279564, KL divergence=0.022439906373620033, Entropy=0.49973535537719727, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.07419861108064651, KL divergence=0.03378286585211754, Entropy=0.48363885283470154, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08041823655366898, KL divergence=0.03938692808151245, Entropy=0.4770694673061371, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.052173495292663574, KL divergence=0.050497110933065414, Entropy=0.4850926399230957, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0840865969657898, KL divergence=0.05649242177605629, Entropy=0.486907035112381, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08203055709600449, KL divergence=0.06532277911901474, Entropy=0.4757024049758911, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07807421684265137, KL divergence=0.07395903021097183, Entropy=0.48714113235473633, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/442_Step-275331.ckpt']
Uploaded 3 files for checkpoint 442 in 0.57 seconds
saved intermediate frozen graph: current/model/model_442.pb
Best checkpoint number: 427, Last checkpoint number: 440
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'439'}
Training> Name=main_level/agent, Worker=0, Episode=7501, Total reward=78.71, Steps=275403, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7502, Total reward=43.17, Steps=275428, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7503, Total reward=3.36, Steps=275449, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7504, Total reward=11.11, Steps=275480, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7505, Total reward=70.05, Steps=275552, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7506, Total reward=61.76, Steps=275607, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7507, Total reward=27.1, Steps=275647, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7508, Total reward=94.73, Steps=275717, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7509, Total reward=37.25, Steps=275750, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7510, Total reward=83.71, Steps=275796, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7511, Total reward=56.54, Steps=275832, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7512, Total reward=46.86, Steps=275863, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7513, Total reward=36.03, Steps=275885, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7514, Total reward=31.06, Steps=275896, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7515, Total reward=23.63, Steps=275931, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7516, Total reward=22.89, Steps=275950, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7517, Total reward=109.59, Steps=276033, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7518, Total reward=51.39, Steps=276073, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7519, Total reward=67.84, Steps=276127, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7520, Total reward=95.12, Steps=276175, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7521, Total reward=107.26, Steps=276275, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7522, Total reward=40.53, Steps=276297, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7523, Total reward=3.62, Steps=276318, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7524, Total reward=19.08, Steps=276387, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7525, Total reward=17.83, Steps=276424, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7526, Total reward=17.98, Steps=276442, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7527, Total reward=70.95, Steps=276486, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7528, Total reward=74.47, Steps=276550, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7529, Total reward=69.07, Steps=276617, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7530, Total reward=79.37, Steps=276665, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7531, Total reward=44.37, Steps=276705, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7532, Total reward=62.08, Steps=276747, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7533, Total reward=49.2, Steps=276776, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7534, Total reward=38.88, Steps=276797, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7535, Total reward=21.53, Steps=276823, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7536, Total reward=20.28, Steps=276842, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7537, Total reward=103.18, Steps=276956, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7538, Total reward=38.91, Steps=276977, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7539, Total reward=83.74, Steps=277030, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7540, Total reward=71.87, Steps=277071, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7541, Total reward=60.36, Steps=277103, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7542, Total reward=39.9, Steps=277146, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7543, Total reward=10.73, Steps=277182, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7544, Total reward=16.34, Steps=277220, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7545, Total reward=64.14, Steps=277292, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7546, Total reward=116.38, Steps=277374, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7547, Total reward=67.78, Steps=277433, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7548, Total reward=73.44, Steps=277473, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7549, Total reward=46.66, Steps=277507, Training iteration=150
Training> Name=main_level/agent, Worker=0, Episode=7550, Total reward=86.4, Steps=277556, Training iteration=150
Policy training> Surrogate loss=0.0026867911219596863, KL divergence=0.0004055387107655406, Entropy=0.5011917352676392, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.033336296677589417, KL divergence=0.009231144562363625, Entropy=0.4881312847137451, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.041400834918022156, KL divergence=0.02362373284995556, Entropy=0.4811801314353943, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06262952089309692, KL divergence=0.03485744446516037, Entropy=0.47733694314956665, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06593642383813858, KL divergence=0.04707798734307289, Entropy=0.4701312482357025, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06903776526451111, KL divergence=0.056297559291124344, Entropy=0.4672437608242035, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07408216595649719, KL divergence=0.06512945890426636, Entropy=0.4642203152179718, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06461899727582932, KL divergence=0.07148218154907227, Entropy=0.4655890166759491, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07095462828874588, KL divergence=0.07629553973674774, Entropy=0.4620550870895386, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07467814534902573, KL divergence=0.08027461171150208, Entropy=0.4667903184890747, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/443_Step-277556.ckpt']
Uploaded 3 files for checkpoint 443 in 0.57 seconds
saved intermediate frozen graph: current/model/model_443.pb
Best checkpoint number: 427, Last checkpoint number: 441
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'440'}
Training> Name=main_level/agent, Worker=0, Episode=7551, Total reward=26.52, Steps=277601, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7552, Total reward=68.61, Steps=277642, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7553, Total reward=56.12, Steps=277672, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7554, Total reward=43.58, Steps=277694, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7555, Total reward=34.93, Steps=277727, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7556, Total reward=30.68, Steps=277760, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7557, Total reward=45.56, Steps=277796, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7558, Total reward=38.78, Steps=277817, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7559, Total reward=71.78, Steps=277885, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7560, Total reward=87.24, Steps=277925, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7561, Total reward=56.84, Steps=277955, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7562, Total reward=38.55, Steps=277988, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7563, Total reward=26.94, Steps=278018, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7564, Total reward=93.1, Steps=278121, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7565, Total reward=18.67, Steps=278154, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7566, Total reward=84.66, Steps=278224, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7567, Total reward=9.32, Steps=278246, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7568, Total reward=105.99, Steps=278326, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7569, Total reward=16.88, Steps=278339, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7570, Total reward=56.99, Steps=278388, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7571, Total reward=41.94, Steps=278432, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7572, Total reward=62.1, Steps=278465, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7573, Total reward=44.41, Steps=278493, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7574, Total reward=42.46, Steps=278515, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7575, Total reward=21.23, Steps=278543, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7576, Total reward=23.17, Steps=278560, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7577, Total reward=52.83, Steps=278623, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7578, Total reward=33.77, Steps=278637, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7579, Total reward=79.03, Steps=278698, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7580, Total reward=92.85, Steps=278739, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7581, Total reward=133.58, Steps=278846, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7582, Total reward=38.54, Steps=278872, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7583, Total reward=103.59, Steps=278962, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7584, Total reward=8.93, Steps=278988, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7585, Total reward=16.95, Steps=279010, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7586, Total reward=94.42, Steps=279079, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7587, Total reward=83.13, Steps=279177, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7588, Total reward=58.61, Steps=279204, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7589, Total reward=65.52, Steps=279275, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7590, Total reward=0.01, Steps=279286, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7591, Total reward=12.52, Steps=279316, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7592, Total reward=66.56, Steps=279349, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7593, Total reward=40.36, Steps=279381, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7594, Total reward=42.07, Steps=279403, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7595, Total reward=28.27, Steps=279432, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7596, Total reward=23.45, Steps=279462, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7597, Total reward=51.76, Steps=279513, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7598, Total reward=32.75, Steps=279535, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7599, Total reward=17.21, Steps=279558, Training iteration=151
Training> Name=main_level/agent, Worker=0, Episode=7600, Total reward=56.39, Steps=279596, Training iteration=151
Policy training> Surrogate loss=-0.026915987953543663, KL divergence=0.00014708429807797074, Entropy=0.4689284861087799, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.031201468780636787, KL divergence=0.005184829235076904, Entropy=0.45589640736579895, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.041655443608760834, KL divergence=0.011978133581578732, Entropy=0.449973464012146, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.041895437985658646, KL divergence=0.02320636250078678, Entropy=0.44866743683815, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0729418694972992, KL divergence=0.031995248049497604, Entropy=0.4518323242664337, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06708084791898727, KL divergence=0.04265282675623894, Entropy=0.44105958938598633, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07775306701660156, KL divergence=0.05413581803441048, Entropy=0.44037091732025146, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.062217384576797485, KL divergence=0.05959099903702736, Entropy=0.4473237991333008, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09154057502746582, KL divergence=0.06648335605859756, Entropy=0.435530424118042, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.062151480466127396, KL divergence=0.07237092405557632, Entropy=0.445770263671875, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/444_Step-279596.ckpt']
Uploaded 3 files for checkpoint 444 in 0.61 seconds
saved intermediate frozen graph: current/model/model_444.pb
Best checkpoint number: 427, Last checkpoint number: 442
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'441'}
Training> Name=main_level/agent, Worker=0, Episode=7601, Total reward=150.17, Steps=279715, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7602, Total reward=31.04, Steps=279737, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7603, Total reward=13.13, Steps=279758, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7604, Total reward=20.12, Steps=279782, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7605, Total reward=87.58, Steps=279861, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7606, Total reward=143.83, Steps=279964, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7607, Total reward=61.57, Steps=280005, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7608, Total reward=73.65, Steps=280057, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7609, Total reward=16.44, Steps=280072, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7610, Total reward=75.71, Steps=280120, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7611, Total reward=56.9, Steps=280146, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7612, Total reward=62.81, Steps=280177, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7613, Total reward=21.89, Steps=280188, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7614, Total reward=20.95, Steps=280209, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7615, Total reward=26.85, Steps=280235, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7616, Total reward=16.05, Steps=280269, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7617, Total reward=45.64, Steps=280309, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7618, Total reward=32.04, Steps=280334, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7619, Total reward=57.34, Steps=280393, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7620, Total reward=54.49, Steps=280440, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7621, Total reward=67.42, Steps=280495, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7622, Total reward=32.68, Steps=280516, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7623, Total reward=6.37, Steps=280553, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7624, Total reward=3.52, Steps=280573, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7625, Total reward=22.13, Steps=280614, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7626, Total reward=81.68, Steps=280665, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7627, Total reward=68.85, Steps=280709, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7628, Total reward=61.87, Steps=280738, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7629, Total reward=74.36, Steps=280789, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7630, Total reward=47.97, Steps=280842, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7631, Total reward=54.79, Steps=280871, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7632, Total reward=62.95, Steps=280912, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7633, Total reward=47.09, Steps=280942, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7634, Total reward=32.28, Steps=280962, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7635, Total reward=28.74, Steps=280992, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7636, Total reward=25.98, Steps=281008, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7637, Total reward=34.3, Steps=281036, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7638, Total reward=38.26, Steps=281061, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7639, Total reward=83.97, Steps=281113, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7640, Total reward=90.88, Steps=281155, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7641, Total reward=56.64, Steps=281189, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7642, Total reward=31.72, Steps=281214, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7643, Total reward=21.48, Steps=281285, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7644, Total reward=36.33, Steps=281322, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7645, Total reward=80.09, Steps=281405, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7646, Total reward=94.13, Steps=281477, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7647, Total reward=47.29, Steps=281506, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7648, Total reward=124.05, Steps=281580, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7649, Total reward=32.66, Steps=281606, Training iteration=152
Training> Name=main_level/agent, Worker=0, Episode=7650, Total reward=74.27, Steps=281655, Training iteration=152
Policy training> Surrogate loss=-0.0008360352367162704, KL divergence=0.00027973472606390715, Entropy=0.48246660828590393, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03841095417737961, KL divergence=0.007283871993422508, Entropy=0.481092631816864, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05139382556080818, KL divergence=0.020567260682582855, Entropy=0.4736441373825073, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.055244751274585724, KL divergence=0.03348549082875252, Entropy=0.46588408946990967, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06092318892478943, KL divergence=0.044914111495018005, Entropy=0.46211713552474976, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06619523465633392, KL divergence=0.0545802116394043, Entropy=0.4619290232658386, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07043854892253876, KL divergence=0.06331224739551544, Entropy=0.46324294805526733, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06738348305225372, KL divergence=0.06969951093196869, Entropy=0.4648096561431885, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0724983885884285, KL divergence=0.07585649192333221, Entropy=0.46566495299339294, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07400007545948029, KL divergence=0.08014009892940521, Entropy=0.466722697019577, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/445_Step-281655.ckpt']
Uploaded 3 files for checkpoint 445 in 0.50 seconds
saved intermediate frozen graph: current/model/model_445.pb
Best checkpoint number: 427, Last checkpoint number: 443
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'442'}
Training> Name=main_level/agent, Worker=0, Episode=7651, Total reward=43.78, Steps=281684, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7652, Total reward=63.4, Steps=281726, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7653, Total reward=61.59, Steps=281757, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7654, Total reward=38.01, Steps=281779, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7655, Total reward=33.33, Steps=281812, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7656, Total reward=17.93, Steps=281829, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7657, Total reward=96.75, Steps=281922, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7658, Total reward=43.62, Steps=281955, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7659, Total reward=45.71, Steps=282012, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7660, Total reward=53.58, Steps=282045, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7661, Total reward=53.67, Steps=282074, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7662, Total reward=57.93, Steps=282123, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7663, Total reward=12.73, Steps=282161, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7664, Total reward=114.15, Steps=282268, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7665, Total reward=22.91, Steps=282304, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7666, Total reward=50.93, Steps=282361, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7667, Total reward=60.43, Steps=282417, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7668, Total reward=82.78, Steps=282502, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7669, Total reward=63.1, Steps=282568, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7670, Total reward=47.95, Steps=282603, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7671, Total reward=44.87, Steps=282634, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7672, Total reward=56.23, Steps=282665, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7673, Total reward=48.43, Steps=282696, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7674, Total reward=41.23, Steps=282718, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7675, Total reward=39.81, Steps=282746, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7676, Total reward=40.21, Steps=282795, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7677, Total reward=40.67, Steps=282826, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7678, Total reward=54.95, Steps=282885, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7679, Total reward=9.67, Steps=282902, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7680, Total reward=34.6, Steps=282941, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7681, Total reward=47.09, Steps=282972, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7682, Total reward=42.11, Steps=282999, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7683, Total reward=7.13, Steps=283048, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7684, Total reward=66.87, Steps=283139, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7685, Total reward=12.67, Steps=283156, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7686, Total reward=7.17, Steps=283173, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7687, Total reward=118.07, Steps=283258, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7688, Total reward=147.5, Steps=283335, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7689, Total reward=14.03, Steps=283350, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7690, Total reward=49.49, Steps=283387, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7691, Total reward=62.66, Steps=283428, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7692, Total reward=65.03, Steps=283462, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7693, Total reward=17.65, Steps=283473, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7694, Total reward=41.28, Steps=283494, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7695, Total reward=27.6, Steps=283522, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7696, Total reward=26.01, Steps=283551, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7697, Total reward=67.68, Steps=283588, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7698, Total reward=40.97, Steps=283635, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7699, Total reward=21.9, Steps=283668, Training iteration=153
Training> Name=main_level/agent, Worker=0, Episode=7700, Total reward=72.1, Steps=283760, Training iteration=153
Policy training> Surrogate loss=-0.0007485896348953247, KL divergence=0.00016908260295167565, Entropy=0.48296472430229187, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03418010473251343, KL divergence=0.008313514292240143, Entropy=0.47307470440864563, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05069607123732567, KL divergence=0.022293955087661743, Entropy=0.4651215076446533, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.056764982640743256, KL divergence=0.0367843359708786, Entropy=0.45361506938934326, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0636671930551529, KL divergence=0.04900924488902092, Entropy=0.4496248662471771, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.061790648847818375, KL divergence=0.05920197814702988, Entropy=0.44335606694221497, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06804664433002472, KL divergence=0.06750752031803131, Entropy=0.44317445158958435, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07089076191186905, KL divergence=0.07354172319173813, Entropy=0.44757622480392456, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07152463495731354, KL divergence=0.07864382117986679, Entropy=0.44389697909355164, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07158520072698593, KL divergence=0.08156369626522064, Entropy=0.44905030727386475, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/446_Step-283760.ckpt']
Uploaded 3 files for checkpoint 446 in 0.71 seconds
saved intermediate frozen graph: current/model/model_446.pb
Best checkpoint number: 427, Last checkpoint number: 444
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'443'}
Training> Name=main_level/agent, Worker=0, Episode=7701, Total reward=69.85, Steps=283792, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7702, Total reward=36.54, Steps=283823, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7703, Total reward=7.38, Steps=283839, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7704, Total reward=10.12, Steps=283866, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7705, Total reward=69.98, Steps=283935, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7706, Total reward=56.94, Steps=283984, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7707, Total reward=65.78, Steps=284027, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7708, Total reward=67.44, Steps=284084, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7709, Total reward=34.81, Steps=284130, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7710, Total reward=63.41, Steps=284179, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7711, Total reward=79.48, Steps=284233, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7712, Total reward=37.95, Steps=284266, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7713, Total reward=24.85, Steps=284277, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7714, Total reward=45.48, Steps=284298, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7715, Total reward=23.45, Steps=284320, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7716, Total reward=22.88, Steps=284354, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7717, Total reward=53.15, Steps=284407, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7718, Total reward=38.3, Steps=284429, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7719, Total reward=14.49, Steps=284450, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7720, Total reward=54.59, Steps=284496, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7721, Total reward=63.38, Steps=284548, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7722, Total reward=34.95, Steps=284574, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7723, Total reward=6.38, Steps=284602, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7724, Total reward=125.89, Steps=284731, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7725, Total reward=13.82, Steps=284771, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7726, Total reward=91.96, Steps=284823, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7727, Total reward=5.05, Steps=284837, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7728, Total reward=64.83, Steps=284866, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7729, Total reward=64.68, Steps=284927, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7730, Total reward=46.68, Steps=284963, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7731, Total reward=55.95, Steps=285001, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7732, Total reward=63.88, Steps=285041, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7733, Total reward=51.31, Steps=285072, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7734, Total reward=42.09, Steps=285093, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7735, Total reward=30.56, Steps=285123, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7736, Total reward=23.23, Steps=285149, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7737, Total reward=56.79, Steps=285197, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7738, Total reward=27.97, Steps=285218, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7739, Total reward=18.2, Steps=285247, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7740, Total reward=27.53, Steps=285272, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7741, Total reward=55.12, Steps=285302, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7742, Total reward=34.32, Steps=285322, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7743, Total reward=32.89, Steps=285380, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7744, Total reward=77.47, Steps=285456, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7745, Total reward=125.16, Steps=285562, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7746, Total reward=77.22, Steps=285615, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7747, Total reward=66.4, Steps=285670, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7748, Total reward=74.04, Steps=285712, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7749, Total reward=19.42, Steps=285735, Training iteration=154
Training> Name=main_level/agent, Worker=0, Episode=7750, Total reward=29.65, Steps=285760, Training iteration=154
Policy training> Surrogate loss=-0.010947220027446747, KL divergence=6.771300104446709e-05, Entropy=0.4749681055545807, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.012944325804710388, KL divergence=0.003366585820913315, Entropy=0.477925181388855, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06421501189470291, KL divergence=0.012112821452319622, Entropy=0.46556583046913147, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03834657371044159, KL divergence=0.020815962925553322, Entropy=0.4579026401042938, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05830654501914978, KL divergence=0.03311900049448013, Entropy=0.44780707359313965, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08288350701332092, KL divergence=0.0452670156955719, Entropy=0.44713494181632996, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05438021942973137, KL divergence=0.05497904121875763, Entropy=0.44683837890625, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06440139561891556, KL divergence=0.061870843172073364, Entropy=0.4400840997695923, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09083393961191177, KL divergence=0.06895596534013748, Entropy=0.4463549554347992, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05967005714774132, KL divergence=0.07299959659576416, Entropy=0.4346071481704712, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/447_Step-285760.ckpt']
Uploaded 3 files for checkpoint 447 in 0.65 seconds
saved intermediate frozen graph: current/model/model_447.pb
Best checkpoint number: 427, Last checkpoint number: 445
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'444'}
Training> Name=main_level/agent, Worker=0, Episode=7751, Total reward=46.98, Steps=285805, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7752, Total reward=49.85, Steps=285836, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7753, Total reward=30.61, Steps=285862, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7754, Total reward=32.91, Steps=285884, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7755, Total reward=33.47, Steps=285915, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7756, Total reward=12.64, Steps=285939, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7757, Total reward=128.09, Steps=286019, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7758, Total reward=38.79, Steps=286045, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7759, Total reward=31.35, Steps=286074, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7760, Total reward=95.15, Steps=286117, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7761, Total reward=70.1, Steps=286147, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7762, Total reward=42.87, Steps=286176, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7763, Total reward=17.8, Steps=286221, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7764, Total reward=7.98, Steps=286240, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7765, Total reward=18.08, Steps=286264, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7766, Total reward=93.03, Steps=286327, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7767, Total reward=60.05, Steps=286369, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7768, Total reward=84.99, Steps=286449, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7769, Total reward=23.42, Steps=286475, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7770, Total reward=82.11, Steps=286522, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7771, Total reward=84.32, Steps=286570, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7772, Total reward=35.45, Steps=286601, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7773, Total reward=56.26, Steps=286630, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7774, Total reward=42.63, Steps=286652, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7775, Total reward=12.11, Steps=286680, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7776, Total reward=18.25, Steps=286712, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7777, Total reward=21.12, Steps=286738, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7778, Total reward=46.23, Steps=286765, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7779, Total reward=14.08, Steps=286795, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7780, Total reward=85.55, Steps=286836, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7781, Total reward=166.36, Steps=286938, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7782, Total reward=35.88, Steps=286976, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7783, Total reward=3.44, Steps=286996, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7784, Total reward=6.5, Steps=287024, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7785, Total reward=19.35, Steps=287061, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7786, Total reward=132.84, Steps=287154, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7787, Total reward=67.1, Steps=287192, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7788, Total reward=81.07, Steps=287245, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7789, Total reward=57.48, Steps=287294, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7790, Total reward=58.5, Steps=287331, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7791, Total reward=47.74, Steps=287370, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7792, Total reward=58.81, Steps=287403, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7793, Total reward=36.25, Steps=287424, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7794, Total reward=35.92, Steps=287445, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7795, Total reward=21.08, Steps=287476, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7796, Total reward=14.57, Steps=287493, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7797, Total reward=59.45, Steps=287542, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7798, Total reward=37.52, Steps=287569, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7799, Total reward=13.85, Steps=287590, Training iteration=155
Training> Name=main_level/agent, Worker=0, Episode=7800, Total reward=81.16, Steps=287626, Training iteration=155
Policy training> Surrogate loss=-0.004823943134397268, KL divergence=7.448872929671779e-05, Entropy=0.4754748046398163, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0212835893034935, KL divergence=0.0028672590851783752, Entropy=0.4782024323940277, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07147472351789474, KL divergence=0.011826093308627605, Entropy=0.4710237979888916, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.054565925151109695, KL divergence=0.022047899663448334, Entropy=0.470615029335022, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05674627423286438, KL divergence=0.03333074972033501, Entropy=0.45424413681030273, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.055478084832429886, KL divergence=0.044481098651885986, Entropy=0.4648880958557129, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05779057741165161, KL divergence=0.05624769628047943, Entropy=0.46522316336631775, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07733651995658875, KL divergence=0.06281314790248871, Entropy=0.454585462808609, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06281005591154099, KL divergence=0.06853277236223221, Entropy=0.45087429881095886, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05646834895014763, KL divergence=0.07648389786481857, Entropy=0.45969876646995544, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/448_Step-287626.ckpt']
Uploaded 3 files for checkpoint 448 in 0.50 seconds
saved intermediate frozen graph: current/model/model_448.pb
Best checkpoint number: 427, Last checkpoint number: 446
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'445'}
Training> Name=main_level/agent, Worker=0, Episode=7801, Total reward=59.48, Steps=287656, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7802, Total reward=39.94, Steps=287683, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7803, Total reward=3.75, Steps=287699, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7804, Total reward=117.54, Steps=287807, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7805, Total reward=70.89, Steps=287891, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7806, Total reward=24.85, Steps=287910, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7807, Total reward=69.93, Steps=287958, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7808, Total reward=78.2, Steps=288009, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7809, Total reward=92.82, Steps=288064, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7810, Total reward=80.17, Steps=288112, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7811, Total reward=64.13, Steps=288151, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7812, Total reward=71.24, Steps=288192, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7813, Total reward=43.54, Steps=288215, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7814, Total reward=39.97, Steps=288237, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7815, Total reward=25.66, Steps=288252, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7816, Total reward=11.83, Steps=288267, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7817, Total reward=35.79, Steps=288334, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7818, Total reward=15.36, Steps=288343, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7819, Total reward=12.33, Steps=288359, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7820, Total reward=101.52, Steps=288439, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7821, Total reward=54.28, Steps=288470, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7822, Total reward=42.0, Steps=288495, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7823, Total reward=0.02, Steps=288517, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7824, Total reward=81.66, Steps=288586, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7825, Total reward=13.47, Steps=288611, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7826, Total reward=82.48, Steps=288697, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7827, Total reward=67.45, Steps=288741, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7828, Total reward=70.36, Steps=288783, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7829, Total reward=52.72, Steps=288846, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7830, Total reward=73.49, Steps=288881, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7831, Total reward=35.71, Steps=288910, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7832, Total reward=59.87, Steps=288941, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7833, Total reward=50.73, Steps=288972, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7834, Total reward=40.83, Steps=288994, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7835, Total reward=12.98, Steps=289021, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7836, Total reward=18.33, Steps=289056, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7837, Total reward=62.95, Steps=289129, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7838, Total reward=96.13, Steps=289191, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7839, Total reward=151.38, Steps=289326, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7840, Total reward=59.03, Steps=289375, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7841, Total reward=61.74, Steps=289429, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7842, Total reward=36.22, Steps=289454, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7843, Total reward=53.69, Steps=289537, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7844, Total reward=3.48, Steps=289551, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7845, Total reward=78.59, Steps=289627, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7846, Total reward=157.64, Steps=289734, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7847, Total reward=59.86, Steps=289774, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7848, Total reward=70.96, Steps=289825, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7849, Total reward=20.23, Steps=289841, Training iteration=156
Training> Name=main_level/agent, Worker=0, Episode=7850, Total reward=21.09, Steps=289874, Training iteration=156
Policy training> Surrogate loss=0.0021146037615835667, KL divergence=0.0004424512735567987, Entropy=0.47756707668304443, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.01536908932030201, KL divergence=0.008284730836749077, Entropy=0.48179829120635986, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.043287165462970734, KL divergence=0.021678732708096504, Entropy=0.4688805341720581, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.053772129118442535, KL divergence=0.03766925632953644, Entropy=0.46663162112236023, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06853919476270676, KL divergence=0.05195252597332001, Entropy=0.46554669737815857, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06353998929262161, KL divergence=0.05976812541484833, Entropy=0.46588727831840515, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06895887851715088, KL divergence=0.0694984495639801, Entropy=0.46138307452201843, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08544550091028214, KL divergence=0.07660423219203949, Entropy=0.4653257727622986, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07422395795583725, KL divergence=0.08355186879634857, Entropy=0.4692631959915161, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07635790854692459, KL divergence=0.08788489550352097, Entropy=0.46843764185905457, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/449_Step-289874.ckpt']
Uploaded 3 files for checkpoint 449 in 0.60 seconds
saved intermediate frozen graph: current/model/model_449.pb
Best checkpoint number: 427, Last checkpoint number: 447
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'446'}
Training> Name=main_level/agent, Worker=0, Episode=7851, Total reward=82.6, Steps=289913, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7852, Total reward=52.92, Steps=289946, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7853, Total reward=51.01, Steps=289976, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7854, Total reward=39.41, Steps=289997, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7855, Total reward=25.1, Steps=290021, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7856, Total reward=14.89, Steps=290039, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7857, Total reward=95.5, Steps=290117, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7858, Total reward=103.89, Steps=290194, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7859, Total reward=14.51, Steps=290216, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7860, Total reward=85.44, Steps=290253, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7861, Total reward=59.32, Steps=290292, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7862, Total reward=38.86, Steps=290321, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7863, Total reward=6.48, Steps=290360, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7864, Total reward=75.27, Steps=290424, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7865, Total reward=16.35, Steps=290450, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7866, Total reward=87.17, Steps=290543, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7867, Total reward=68.46, Steps=290600, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7868, Total reward=76.81, Steps=290652, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7869, Total reward=30.51, Steps=290676, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7870, Total reward=66.98, Steps=290723, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7871, Total reward=46.47, Steps=290755, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7872, Total reward=53.04, Steps=290789, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7873, Total reward=35.99, Steps=290810, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7874, Total reward=43.47, Steps=290831, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7875, Total reward=24.94, Steps=290861, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7876, Total reward=25.76, Steps=290890, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7877, Total reward=83.59, Steps=290953, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7878, Total reward=41.18, Steps=290976, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7879, Total reward=12.08, Steps=290992, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7880, Total reward=74.5, Steps=291031, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7881, Total reward=66.44, Steps=291064, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7882, Total reward=37.29, Steps=291085, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7883, Total reward=16.88, Steps=291103, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7884, Total reward=23.55, Steps=291137, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7885, Total reward=11.76, Steps=291154, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7886, Total reward=125.32, Steps=291243, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7887, Total reward=66.33, Steps=291297, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7888, Total reward=112.25, Steps=291374, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7889, Total reward=32.4, Steps=291397, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7890, Total reward=98.07, Steps=291460, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7891, Total reward=65.41, Steps=291486, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7892, Total reward=58.26, Steps=291526, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7893, Total reward=53.5, Steps=291557, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7894, Total reward=43.6, Steps=291579, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7895, Total reward=39.78, Steps=291609, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7896, Total reward=23.24, Steps=291641, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7897, Total reward=86.97, Steps=291750, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7898, Total reward=28.3, Steps=291763, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7899, Total reward=41.08, Steps=291802, Training iteration=157
Training> Name=main_level/agent, Worker=0, Episode=7900, Total reward=169.14, Steps=291916, Training iteration=157
Policy training> Surrogate loss=0.022217489778995514, KL divergence=9.373608190799132e-05, Entropy=0.4755301773548126, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03973749652504921, KL divergence=0.003574271919205785, Entropy=0.4800708293914795, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03914378955960274, KL divergence=0.012509770691394806, Entropy=0.48079773783683777, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.042705461382865906, KL divergence=0.022789902985095978, Entropy=0.47105345129966736, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04525505378842354, KL divergence=0.03264671191573143, Entropy=0.46317777037620544, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07010537385940552, KL divergence=0.04159954562783241, Entropy=0.45115160942077637, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04763816297054291, KL divergence=0.04985353350639343, Entropy=0.45126691460609436, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07506842166185379, KL divergence=0.06138336658477783, Entropy=0.45926031470298767, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.075420081615448, KL divergence=0.06461644172668457, Entropy=0.4504674971103668, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06975708156824112, KL divergence=0.07440254837274551, Entropy=0.45358601212501526, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/450_Step-291916.ckpt']
Uploaded 3 files for checkpoint 450 in 0.57 seconds
saved intermediate frozen graph: current/model/model_450.pb
Best checkpoint number: 427, Last checkpoint number: 448
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'447'}
Training> Name=main_level/agent, Worker=0, Episode=7901, Total reward=62.96, Steps=291946, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7902, Total reward=19.74, Steps=291962, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7903, Total reward=21.34, Steps=291995, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7904, Total reward=5.44, Steps=292023, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7905, Total reward=29.62, Steps=292061, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7906, Total reward=95.6, Steps=292133, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7907, Total reward=77.02, Steps=292195, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7908, Total reward=56.54, Steps=292234, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7909, Total reward=59.31, Steps=292287, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7910, Total reward=30.82, Steps=292318, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7911, Total reward=54.94, Steps=292355, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7912, Total reward=72.11, Steps=292396, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7913, Total reward=40.7, Steps=292427, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7914, Total reward=32.38, Steps=292447, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7915, Total reward=34.94, Steps=292477, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7916, Total reward=13.97, Steps=292502, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7917, Total reward=56.85, Steps=292551, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7918, Total reward=67.64, Steps=292635, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7919, Total reward=85.41, Steps=292686, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7920, Total reward=15.44, Steps=292702, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7921, Total reward=65.53, Steps=292734, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7922, Total reward=15.53, Steps=292748, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7923, Total reward=17.71, Steps=292790, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7924, Total reward=12.55, Steps=292823, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7925, Total reward=75.83, Steps=292887, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7926, Total reward=96.6, Steps=292941, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7927, Total reward=52.25, Steps=292979, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7928, Total reward=44.0, Steps=293003, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7929, Total reward=74.31, Steps=293057, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7930, Total reward=69.3, Steps=293104, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7931, Total reward=52.6, Steps=293141, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7932, Total reward=74.87, Steps=293184, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7933, Total reward=58.37, Steps=293215, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7934, Total reward=38.08, Steps=293236, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7935, Total reward=28.63, Steps=293266, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7936, Total reward=23.31, Steps=293300, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7937, Total reward=99.86, Steps=293416, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7938, Total reward=101.39, Steps=293482, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7939, Total reward=16.84, Steps=293513, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7940, Total reward=66.08, Steps=293557, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7941, Total reward=60.26, Steps=293588, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7942, Total reward=32.28, Steps=293608, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7943, Total reward=6.64, Steps=293627, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7944, Total reward=24.67, Steps=293668, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7945, Total reward=74.38, Steps=293759, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7946, Total reward=62.07, Steps=293823, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7947, Total reward=62.36, Steps=293876, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7948, Total reward=102.0, Steps=293937, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7949, Total reward=21.18, Steps=293952, Training iteration=158
Training> Name=main_level/agent, Worker=0, Episode=7950, Total reward=35.07, Steps=293992, Training iteration=158
Policy training> Surrogate loss=-0.00018872134387493134, KL divergence=0.00032551391632296145, Entropy=0.4680324196815491, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03898739069700241, KL divergence=0.008598480373620987, Entropy=0.4668624997138977, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05291200429201126, KL divergence=0.021063970401883125, Entropy=0.45979633927345276, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06054278835654259, KL divergence=0.035004399716854095, Entropy=0.4536992907524109, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.061433158814907074, KL divergence=0.048680368810892105, Entropy=0.44890040159225464, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07074061781167984, KL divergence=0.05956536903977394, Entropy=0.4463443458080292, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06479562819004059, KL divergence=0.06864378601312637, Entropy=0.44446325302124023, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07051486521959305, KL divergence=0.07574114948511124, Entropy=0.4451911449432373, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07003399729728699, KL divergence=0.08193668723106384, Entropy=0.4477521777153015, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07165279984474182, KL divergence=0.08496363461017609, Entropy=0.4469708204269409, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/451_Step-293992.ckpt']
Uploaded 3 files for checkpoint 451 in 0.53 seconds
saved intermediate frozen graph: current/model/model_451.pb
Best checkpoint number: 427, Last checkpoint number: 449
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'448'}
Training> Name=main_level/agent, Worker=0, Episode=7951, Total reward=14.83, Steps=294027, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7952, Total reward=54.03, Steps=294067, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7953, Total reward=57.72, Steps=294098, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7954, Total reward=36.22, Steps=294119, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7955, Total reward=26.05, Steps=294148, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7956, Total reward=21.4, Steps=294185, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7957, Total reward=61.36, Steps=294231, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7958, Total reward=81.87, Steps=294342, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7959, Total reward=84.36, Steps=294391, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7960, Total reward=80.9, Steps=294430, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7961, Total reward=66.2, Steps=294482, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7962, Total reward=39.56, Steps=294507, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7963, Total reward=14.59, Steps=294544, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7964, Total reward=73.72, Steps=294620, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7965, Total reward=30.48, Steps=294655, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7966, Total reward=3.57, Steps=294668, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7967, Total reward=134.41, Steps=294751, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7968, Total reward=65.96, Steps=294798, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7969, Total reward=25.49, Steps=294821, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7970, Total reward=40.44, Steps=294859, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7971, Total reward=50.88, Steps=294887, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7972, Total reward=3.76, Steps=294898, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7973, Total reward=47.19, Steps=294927, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7974, Total reward=38.46, Steps=294950, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7975, Total reward=24.33, Steps=294975, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7976, Total reward=15.72, Steps=295000, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7977, Total reward=50.28, Steps=295034, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7978, Total reward=4.81, Steps=295045, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7979, Total reward=11.0, Steps=295066, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7980, Total reward=156.05, Steps=295183, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7981, Total reward=66.29, Steps=295215, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7982, Total reward=64.2, Steps=295270, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7983, Total reward=29.18, Steps=295313, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7984, Total reward=31.57, Steps=295343, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7985, Total reward=13.32, Steps=295361, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7986, Total reward=93.85, Steps=295429, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7987, Total reward=66.88, Steps=295475, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7988, Total reward=69.55, Steps=295519, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7989, Total reward=30.03, Steps=295557, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7990, Total reward=68.97, Steps=295610, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7991, Total reward=66.11, Steps=295648, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7992, Total reward=71.25, Steps=295690, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7993, Total reward=39.82, Steps=295711, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7994, Total reward=35.83, Steps=295732, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7995, Total reward=17.21, Steps=295763, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7996, Total reward=15.73, Steps=295780, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7997, Total reward=91.81, Steps=295857, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7998, Total reward=31.88, Steps=295881, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=7999, Total reward=60.25, Steps=295933, Training iteration=159
Training> Name=main_level/agent, Worker=0, Episode=8000, Total reward=15.38, Steps=295946, Training iteration=159
Policy training> Surrogate loss=0.005601154640316963, KL divergence=0.00013893493451178074, Entropy=0.46755552291870117, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0210107509046793, KL divergence=0.0048052663914859295, Entropy=0.47290122509002686, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04928228631615639, KL divergence=0.014238398522138596, Entropy=0.46889153122901917, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.032677456736564636, KL divergence=0.02402719110250473, Entropy=0.4569801092147827, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05034692585468292, KL divergence=0.03416898474097252, Entropy=0.452160120010376, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05049163103103638, KL divergence=0.04640531167387962, Entropy=0.4548877477645874, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.03105088137090206, KL divergence=0.05531357601284981, Entropy=0.45952653884887695, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08252843469381332, KL divergence=0.06504055112600327, Entropy=0.4584568440914154, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08682934194803238, KL divergence=0.06628814339637756, Entropy=0.44975224137306213, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07804988324642181, KL divergence=0.07570135593414307, Entropy=0.44755053520202637, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/452_Step-295946.ckpt']
Uploaded 3 files for checkpoint 452 in 0.57 seconds
saved intermediate frozen graph: current/model/model_452.pb
Best checkpoint number: 427, Last checkpoint number: 450
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'449'}
Training> Name=main_level/agent, Worker=0, Episode=8001, Total reward=62.64, Steps=295978, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8002, Total reward=39.73, Steps=295997, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8003, Total reward=21.03, Steps=296051, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8004, Total reward=92.49, Steps=296139, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8005, Total reward=62.86, Steps=296194, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8006, Total reward=76.77, Steps=296247, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8007, Total reward=120.57, Steps=296326, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8008, Total reward=66.17, Steps=296390, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8009, Total reward=21.32, Steps=296406, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8010, Total reward=54.21, Steps=296433, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8011, Total reward=12.18, Steps=296454, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8012, Total reward=57.07, Steps=296485, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8013, Total reward=58.84, Steps=296515, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8014, Total reward=35.15, Steps=296536, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8015, Total reward=29.01, Steps=296566, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8016, Total reward=26.96, Steps=296585, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8017, Total reward=202.1, Steps=296722, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8018, Total reward=83.02, Steps=296792, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8019, Total reward=69.19, Steps=296844, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8020, Total reward=65.15, Steps=296887, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8021, Total reward=68.83, Steps=296956, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8022, Total reward=31.88, Steps=296980, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8023, Total reward=3.13, Steps=297003, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8024, Total reward=14.46, Steps=297026, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8025, Total reward=91.85, Steps=297108, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8026, Total reward=40.06, Steps=297146, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8027, Total reward=70.35, Steps=297203, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8028, Total reward=63.44, Steps=297233, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8029, Total reward=29.94, Steps=297249, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8030, Total reward=65.72, Steps=297289, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8031, Total reward=30.56, Steps=297311, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8032, Total reward=59.44, Steps=297344, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8033, Total reward=38.16, Steps=297365, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8034, Total reward=46.93, Steps=297387, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8035, Total reward=21.83, Steps=297415, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8036, Total reward=24.66, Steps=297435, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8037, Total reward=55.66, Steps=297474, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8038, Total reward=27.45, Steps=297495, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8039, Total reward=71.91, Steps=297548, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8040, Total reward=24.19, Steps=297569, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8041, Total reward=58.24, Steps=297600, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8042, Total reward=23.89, Steps=297619, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8043, Total reward=0.02, Steps=297642, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8044, Total reward=14.41, Steps=297680, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8045, Total reward=70.84, Steps=297744, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8046, Total reward=22.09, Steps=297768, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8047, Total reward=63.57, Steps=297807, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8048, Total reward=66.48, Steps=297838, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8049, Total reward=12.75, Steps=297853, Training iteration=160
Training> Name=main_level/agent, Worker=0, Episode=8050, Total reward=63.55, Steps=297897, Training iteration=160
Policy training> Surrogate loss=-0.024470709264278412, KL divergence=6.797091191401705e-05, Entropy=0.4966449439525604, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.027401044964790344, KL divergence=0.0024054425302892923, Entropy=0.49605610966682434, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.043519046157598495, KL divergence=0.008445106446743011, Entropy=0.49402299523353577, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.061364542692899704, KL divergence=0.0195646770298481, Entropy=0.48365136981010437, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06948161870241165, KL divergence=0.02720407396554947, Entropy=0.47529956698417664, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06884243339300156, KL divergence=0.03985745459794998, Entropy=0.4753694534301758, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07214519381523132, KL divergence=0.04587319493293762, Entropy=0.4649991989135742, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0816420465707779, KL divergence=0.05364617705345154, Entropy=0.4677528440952301, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.049183666706085205, KL divergence=0.05757981538772583, Entropy=0.4688389003276825, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.076862633228302, KL divergence=0.06512751430273056, Entropy=0.47517311573028564, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/453_Step-297897.ckpt']
Uploaded 3 files for checkpoint 453 in 0.52 seconds
saved intermediate frozen graph: current/model/model_453.pb
Best checkpoint number: 427, Last checkpoint number: 451
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'450'}
Training> Name=main_level/agent, Worker=0, Episode=8051, Total reward=82.54, Steps=297937, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8052, Total reward=64.52, Steps=297980, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8053, Total reward=45.37, Steps=298011, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8054, Total reward=39.86, Steps=298032, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8055, Total reward=26.47, Steps=298061, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8056, Total reward=27.21, Steps=298080, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8057, Total reward=61.13, Steps=298115, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8058, Total reward=47.35, Steps=298151, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8059, Total reward=13.41, Steps=298171, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8060, Total reward=88.83, Steps=298232, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8061, Total reward=66.93, Steps=298264, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8062, Total reward=60.11, Steps=298307, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8063, Total reward=81.35, Steps=298388, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8064, Total reward=15.82, Steps=298417, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8065, Total reward=6.84, Steps=298440, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8066, Total reward=154.24, Steps=298531, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8067, Total reward=9.99, Steps=298560, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8068, Total reward=59.86, Steps=298587, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8069, Total reward=39.53, Steps=298616, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8070, Total reward=88.37, Steps=298651, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8071, Total reward=59.17, Steps=298679, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8072, Total reward=54.61, Steps=298710, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8073, Total reward=43.36, Steps=298739, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8074, Total reward=33.02, Steps=298759, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8075, Total reward=37.58, Steps=298789, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8076, Total reward=26.58, Steps=298819, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8077, Total reward=92.9, Steps=298933, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8078, Total reward=95.74, Steps=299000, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8079, Total reward=66.14, Steps=299047, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8080, Total reward=83.52, Steps=299088, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8081, Total reward=62.74, Steps=299119, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8082, Total reward=53.18, Steps=299177, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8083, Total reward=12.27, Steps=299203, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8084, Total reward=79.62, Steps=299295, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8085, Total reward=24.83, Steps=299323, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8086, Total reward=97.76, Steps=299385, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8087, Total reward=64.33, Steps=299427, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8088, Total reward=59.32, Steps=299455, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8089, Total reward=29.92, Steps=299497, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8090, Total reward=39.01, Steps=299526, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8091, Total reward=18.45, Steps=299549, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8092, Total reward=75.31, Steps=299592, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8093, Total reward=48.94, Steps=299623, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8094, Total reward=43.76, Steps=299645, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8095, Total reward=35.49, Steps=299673, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8096, Total reward=26.7, Steps=299691, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8097, Total reward=139.95, Steps=299809, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8098, Total reward=31.82, Steps=299846, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8099, Total reward=5.86, Steps=299863, Training iteration=161
Training> Name=main_level/agent, Worker=0, Episode=8100, Total reward=58.82, Steps=299904, Training iteration=161
Policy training> Surrogate loss=-0.0024442372377961874, KL divergence=4.381206235848367e-05, Entropy=0.470877081155777, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.01909131370484829, KL divergence=0.0029873151797801256, Entropy=0.485821396112442, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.050375740975141525, KL divergence=0.012267562560737133, Entropy=0.46408334374427795, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07701373845338821, KL divergence=0.023165741935372353, Entropy=0.4695053994655609, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.052876848727464676, KL divergence=0.031695663928985596, Entropy=0.45777276158332825, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.058307141065597534, KL divergence=0.04325823858380318, Entropy=0.46367669105529785, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0419309027493, KL divergence=0.05072161927819252, Entropy=0.46186837553977966, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0575098879635334, KL divergence=0.06024031713604927, Entropy=0.4628605842590332, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07102451473474503, KL divergence=0.06466040760278702, Entropy=0.4596479833126068, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08306688070297241, KL divergence=0.07059415429830551, Entropy=0.45056381821632385, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/454_Step-299904.ckpt']
Uploaded 3 files for checkpoint 454 in 0.52 seconds
saved intermediate frozen graph: current/model/model_454.pb
Best checkpoint number: 427, Last checkpoint number: 452
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'451'}
Training> Name=main_level/agent, Worker=0, Episode=8101, Total reward=64.33, Steps=299945, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8102, Total reward=37.45, Steps=299964, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8103, Total reward=27.59, Steps=300002, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8104, Total reward=66.18, Steps=300076, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8105, Total reward=22.87, Steps=300100, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8106, Total reward=6.6, Steps=300132, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8107, Total reward=39.24, Steps=300169, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8108, Total reward=77.03, Steps=300218, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8109, Total reward=79.99, Steps=300274, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8110, Total reward=46.7, Steps=300311, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8111, Total reward=52.62, Steps=300336, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8112, Total reward=73.38, Steps=300376, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8113, Total reward=41.72, Steps=300397, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8114, Total reward=31.75, Steps=300419, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8115, Total reward=36.77, Steps=300449, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8116, Total reward=18.22, Steps=300474, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8117, Total reward=144.07, Steps=300632, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8118, Total reward=28.15, Steps=300667, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8119, Total reward=54.35, Steps=300725, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8120, Total reward=76.74, Steps=300768, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8121, Total reward=51.83, Steps=300796, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8122, Total reward=70.97, Steps=300846, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8123, Total reward=61.85, Steps=300926, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8124, Total reward=27.06, Steps=300949, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8125, Total reward=9.88, Steps=300978, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8126, Total reward=74.27, Steps=301032, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8127, Total reward=64.0, Steps=301089, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8128, Total reward=41.64, Steps=301114, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8129, Total reward=36.31, Steps=301154, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8130, Total reward=37.94, Steps=301176, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8131, Total reward=68.64, Steps=301214, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8132, Total reward=11.23, Steps=301234, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8133, Total reward=47.05, Steps=301262, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8134, Total reward=43.14, Steps=301284, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8135, Total reward=14.96, Steps=301312, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8136, Total reward=22.38, Steps=301332, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8137, Total reward=57.65, Steps=301368, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8138, Total reward=17.66, Steps=301381, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8139, Total reward=13.2, Steps=301400, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8140, Total reward=69.09, Steps=301439, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8141, Total reward=134.65, Steps=301535, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8142, Total reward=35.68, Steps=301556, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8143, Total reward=7.25, Steps=301587, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8144, Total reward=13.41, Steps=301626, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8145, Total reward=37.02, Steps=301687, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8146, Total reward=54.22, Steps=301724, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8147, Total reward=76.77, Steps=301783, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8148, Total reward=65.67, Steps=301830, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8149, Total reward=17.99, Steps=301845, Training iteration=162
Training> Name=main_level/agent, Worker=0, Episode=8150, Total reward=47.51, Steps=301895, Training iteration=162
Policy training> Surrogate loss=-0.014780928380787373, KL divergence=0.00015420062118209898, Entropy=0.48946788907051086, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03683086857199669, KL divergence=0.004115027841180563, Entropy=0.489967942237854, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04386724531650543, KL divergence=0.013173733837902546, Entropy=0.4782053530216217, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06442324817180634, KL divergence=0.02432030439376831, Entropy=0.4587536156177521, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.071914441883564, KL divergence=0.03507447987794876, Entropy=0.47113409638404846, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06342864781618118, KL divergence=0.040551017969846725, Entropy=0.45239460468292236, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07084738463163376, KL divergence=0.0543038547039032, Entropy=0.4642024040222168, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.060511138290166855, KL divergence=0.05959941819310188, Entropy=0.45734670758247375, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07187789678573608, KL divergence=0.06431277096271515, Entropy=0.4597606658935547, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06630206853151321, KL divergence=0.06940910965204239, Entropy=0.45357975363731384, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/455_Step-301895.ckpt']
Uploaded 3 files for checkpoint 455 in 0.57 seconds
saved intermediate frozen graph: current/model/model_455.pb
Best checkpoint number: 427, Last checkpoint number: 453
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'452'}
Training> Name=main_level/agent, Worker=0, Episode=8151, Total reward=50.8, Steps=301925, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8152, Total reward=61.57, Steps=301959, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8153, Total reward=0.02, Steps=301974, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8154, Total reward=44.88, Steps=301997, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8155, Total reward=35.49, Steps=302026, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8156, Total reward=20.51, Steps=302062, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8157, Total reward=50.61, Steps=302097, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8158, Total reward=92.14, Steps=302196, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8159, Total reward=22.35, Steps=302227, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8160, Total reward=73.99, Steps=302270, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8161, Total reward=67.13, Steps=302303, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8162, Total reward=57.57, Steps=302359, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8163, Total reward=6.93, Steps=302378, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8164, Total reward=11.23, Steps=302407, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8165, Total reward=31.83, Steps=302461, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8166, Total reward=66.55, Steps=302509, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8167, Total reward=61.07, Steps=302564, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8168, Total reward=45.93, Steps=302614, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8169, Total reward=30.99, Steps=302639, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8170, Total reward=7.59, Steps=302654, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8171, Total reward=76.11, Steps=302692, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8172, Total reward=62.39, Steps=302735, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8173, Total reward=50.52, Steps=302765, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8174, Total reward=43.17, Steps=302787, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8175, Total reward=27.98, Steps=302816, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8176, Total reward=22.11, Steps=302833, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8177, Total reward=114.86, Steps=302910, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8178, Total reward=86.74, Steps=302968, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8179, Total reward=43.28, Steps=303011, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8180, Total reward=74.31, Steps=303066, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8181, Total reward=65.0, Steps=303106, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8182, Total reward=30.56, Steps=303125, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8183, Total reward=17.22, Steps=303162, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8184, Total reward=83.48, Steps=303239, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8185, Total reward=79.8, Steps=303299, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8186, Total reward=102.08, Steps=303369, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8187, Total reward=142.99, Steps=303474, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8188, Total reward=119.26, Steps=303566, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8189, Total reward=29.73, Steps=303589, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8190, Total reward=18.47, Steps=303603, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8191, Total reward=54.74, Steps=303642, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8192, Total reward=52.33, Steps=303674, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8193, Total reward=46.65, Steps=303695, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8194, Total reward=36.16, Steps=303717, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8195, Total reward=38.9, Steps=303747, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8196, Total reward=22.22, Steps=303764, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8197, Total reward=41.67, Steps=303802, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8198, Total reward=40.97, Steps=303839, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8199, Total reward=80.77, Steps=303902, Training iteration=163
Training> Name=main_level/agent, Worker=0, Episode=8200, Total reward=100.37, Steps=303942, Training iteration=163
Policy training> Surrogate loss=0.0030241806525737047, KL divergence=7.10718086338602e-05, Entropy=0.48905837535858154, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0072241066955029964, KL divergence=0.0031947873067110777, Entropy=0.48310795426368713, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.040434323251247406, KL divergence=0.011373594403266907, Entropy=0.4746049642562866, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.025243669748306274, KL divergence=0.023585185408592224, Entropy=0.4731779992580414, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04602126404643059, KL divergence=0.03281508386135101, Entropy=0.474105566740036, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.057626139372587204, KL divergence=0.04193023964762688, Entropy=0.45623382925987244, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06242945417761803, KL divergence=0.04959322139620781, Entropy=0.45466533303260803, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.040086545050144196, KL divergence=0.06045205518603325, Entropy=0.4704805910587311, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06561353802680969, KL divergence=0.06565677374601364, Entropy=0.4616508483886719, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.044431257992982864, KL divergence=0.06899023801088333, Entropy=0.45827388763427734, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/456_Step-303942.ckpt']
Uploaded 3 files for checkpoint 456 in 0.58 seconds
saved intermediate frozen graph: current/model/model_456.pb
Best checkpoint number: 427, Last checkpoint number: 454
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'453'}
Training> Name=main_level/agent, Worker=0, Episode=8201, Total reward=64.92, Steps=303975, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8202, Total reward=22.94, Steps=303992, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8203, Total reward=46.83, Steps=304069, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8204, Total reward=14.68, Steps=304104, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8205, Total reward=88.3, Steps=304171, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8206, Total reward=3.68, Steps=304186, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8207, Total reward=66.15, Steps=304229, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8208, Total reward=62.19, Steps=304260, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8209, Total reward=86.8, Steps=304317, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8210, Total reward=32.99, Steps=304353, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8211, Total reward=62.62, Steps=304392, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8212, Total reward=49.67, Steps=304424, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8213, Total reward=65.24, Steps=304455, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8214, Total reward=47.33, Steps=304476, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8215, Total reward=25.06, Steps=304500, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8216, Total reward=24.23, Steps=304519, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8217, Total reward=59.46, Steps=304557, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8218, Total reward=32.39, Steps=304578, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8219, Total reward=21.18, Steps=304612, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8220, Total reward=56.51, Steps=304661, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8221, Total reward=60.53, Steps=304718, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8222, Total reward=34.87, Steps=304744, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8223, Total reward=9.02, Steps=304782, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8224, Total reward=24.91, Steps=304816, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8225, Total reward=14.67, Steps=304838, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8226, Total reward=111.39, Steps=304943, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8227, Total reward=36.63, Steps=304982, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8228, Total reward=136.34, Steps=305077, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8229, Total reward=15.91, Steps=305090, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8230, Total reward=33.19, Steps=305127, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8231, Total reward=56.09, Steps=305162, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8232, Total reward=59.45, Steps=305195, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8233, Total reward=43.46, Steps=305226, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8234, Total reward=35.98, Steps=305248, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8235, Total reward=16.41, Steps=305276, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8236, Total reward=22.82, Steps=305308, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8237, Total reward=46.23, Steps=305361, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8238, Total reward=40.76, Steps=305393, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8239, Total reward=10.86, Steps=305425, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8240, Total reward=78.47, Steps=305462, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8241, Total reward=72.59, Steps=305521, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8242, Total reward=31.2, Steps=305543, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8243, Total reward=18.98, Steps=305587, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8244, Total reward=10.01, Steps=305608, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8245, Total reward=12.86, Steps=305625, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8246, Total reward=41.02, Steps=305669, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8247, Total reward=66.11, Steps=305729, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8248, Total reward=57.73, Steps=305758, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8249, Total reward=17.42, Steps=305772, Training iteration=164
Training> Name=main_level/agent, Worker=0, Episode=8250, Total reward=14.68, Steps=305796, Training iteration=164
Policy training> Surrogate loss=-0.02233087457716465, KL divergence=0.00011331345740472898, Entropy=0.4703094959259033, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.023974036797881126, KL divergence=0.004843267612159252, Entropy=0.4723050892353058, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05115959420800209, KL divergence=0.01610584557056427, Entropy=0.4704085886478424, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07152887433767319, KL divergence=0.030970498919487, Entropy=0.4659171998500824, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.062150392681360245, KL divergence=0.042017001658678055, Entropy=0.46250972151756287, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08383650332689285, KL divergence=0.05118435621261597, Entropy=0.45592474937438965, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.058069903403520584, KL divergence=0.05898463726043701, Entropy=0.45454660058021545, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06928079575300217, KL divergence=0.0699135884642601, Entropy=0.4443746507167816, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09108590334653854, KL divergence=0.07979798316955566, Entropy=0.44906988739967346, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06531494110822678, KL divergence=0.08027787506580353, Entropy=0.4524286091327667, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/457_Step-305796.ckpt']
Uploaded 3 files for checkpoint 457 in 0.64 seconds
saved intermediate frozen graph: current/model/model_457.pb
Best checkpoint number: 427, Last checkpoint number: 455
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'454'}
Training> Name=main_level/agent, Worker=0, Episode=8251, Total reward=25.13, Steps=305819, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8252, Total reward=55.25, Steps=305852, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8253, Total reward=44.95, Steps=305881, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8254, Total reward=51.23, Steps=305904, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8255, Total reward=20.27, Steps=305930, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8256, Total reward=24.66, Steps=305974, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8257, Total reward=43.17, Steps=306011, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8258, Total reward=35.17, Steps=306036, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8259, Total reward=27.41, Steps=306069, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8260, Total reward=155.45, Steps=306212, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8261, Total reward=68.29, Steps=306249, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8262, Total reward=46.63, Steps=306288, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8263, Total reward=3.41, Steps=306307, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8264, Total reward=15.03, Steps=306345, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8265, Total reward=16.85, Steps=306365, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8266, Total reward=109.7, Steps=306446, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8267, Total reward=94.17, Steps=306538, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8268, Total reward=65.65, Steps=306578, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8269, Total reward=16.03, Steps=306595, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8270, Total reward=55.98, Steps=306630, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8271, Total reward=44.41, Steps=306659, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8272, Total reward=7.46, Steps=306679, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8273, Total reward=40.59, Steps=306705, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8274, Total reward=34.89, Steps=306726, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8275, Total reward=22.03, Steps=306752, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8276, Total reward=24.88, Steps=306770, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8277, Total reward=36.47, Steps=306814, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8278, Total reward=31.35, Steps=306838, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8279, Total reward=10.01, Steps=306863, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8280, Total reward=71.37, Steps=306902, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8281, Total reward=51.52, Steps=306932, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8282, Total reward=60.78, Steps=306977, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8283, Total reward=76.39, Steps=307047, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8284, Total reward=10.22, Steps=307075, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8285, Total reward=142.81, Steps=307185, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8286, Total reward=86.59, Steps=307250, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8287, Total reward=66.65, Steps=307305, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8288, Total reward=67.43, Steps=307345, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8289, Total reward=31.67, Steps=307378, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8290, Total reward=56.07, Steps=307423, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8291, Total reward=39.52, Steps=307448, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8292, Total reward=60.66, Steps=307479, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8293, Total reward=37.76, Steps=307500, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8294, Total reward=43.62, Steps=307522, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8295, Total reward=17.1, Steps=307551, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8296, Total reward=43.26, Steps=307604, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8297, Total reward=135.86, Steps=307702, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8298, Total reward=39.45, Steps=307728, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8299, Total reward=72.98, Steps=307786, Training iteration=165
Training> Name=main_level/agent, Worker=0, Episode=8300, Total reward=83.83, Steps=307826, Training iteration=165
Policy training> Surrogate loss=-0.012589611113071442, KL divergence=0.00011349905980750918, Entropy=0.4825972616672516, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0318845696747303, KL divergence=0.003653985448181629, Entropy=0.4908004701137543, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06469585001468658, KL divergence=0.011755061335861683, Entropy=0.48657551407814026, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04152446612715721, KL divergence=0.022752517834305763, Entropy=0.4793449640274048, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.061200182884931564, KL divergence=0.0334613099694252, Entropy=0.46979761123657227, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07005378603935242, KL divergence=0.045055970549583435, Entropy=0.47170260548591614, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0817297175526619, KL divergence=0.05420650914311409, Entropy=0.460502028465271, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06796333193778992, KL divergence=0.06238476559519768, Entropy=0.4734327793121338, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04061629995703697, KL divergence=0.07019823044538498, Entropy=0.47539034485816956, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09451207518577576, KL divergence=0.07554654031991959, Entropy=0.472013384103775, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/458_Step-307826.ckpt']
Uploaded 3 files for checkpoint 458 in 0.61 seconds
saved intermediate frozen graph: current/model/model_458.pb
Best checkpoint number: 427, Last checkpoint number: 456
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'455'}
Training> Name=main_level/agent, Worker=0, Episode=8301, Total reward=45.46, Steps=307846, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8302, Total reward=129.36, Steps=307933, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8303, Total reward=28.29, Steps=307982, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8304, Total reward=108.97, Steps=308070, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8305, Total reward=27.01, Steps=308091, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8306, Total reward=121.42, Steps=308201, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8307, Total reward=108.67, Steps=308292, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8308, Total reward=70.14, Steps=308322, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8309, Total reward=36.62, Steps=308364, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8310, Total reward=11.53, Steps=308376, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8311, Total reward=43.37, Steps=308414, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8312, Total reward=49.98, Steps=308446, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8313, Total reward=53.72, Steps=308475, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8314, Total reward=36.43, Steps=308496, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8315, Total reward=20.12, Steps=308519, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8316, Total reward=25.62, Steps=308537, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8317, Total reward=92.65, Steps=308613, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8318, Total reward=37.01, Steps=308636, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8319, Total reward=19.52, Steps=308657, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8320, Total reward=85.67, Steps=308700, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8321, Total reward=60.07, Steps=308737, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8322, Total reward=70.19, Steps=308791, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8323, Total reward=18.81, Steps=308826, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8324, Total reward=5.49, Steps=308857, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8325, Total reward=19.46, Steps=308896, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8326, Total reward=85.77, Steps=308956, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8327, Total reward=61.74, Steps=308999, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8328, Total reward=65.98, Steps=309038, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8329, Total reward=84.14, Steps=309096, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8330, Total reward=26.73, Steps=309114, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8331, Total reward=54.36, Steps=309154, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8332, Total reward=62.63, Steps=309187, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8333, Total reward=46.77, Steps=309217, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8334, Total reward=39.88, Steps=309238, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8335, Total reward=12.49, Steps=309268, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8336, Total reward=23.37, Steps=309288, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8337, Total reward=117.41, Steps=309370, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8338, Total reward=107.47, Steps=309449, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8339, Total reward=66.71, Steps=309503, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8340, Total reward=80.74, Steps=309568, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8341, Total reward=91.92, Steps=309639, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8342, Total reward=29.85, Steps=309658, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8343, Total reward=12.78, Steps=309686, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8344, Total reward=92.82, Steps=309764, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8345, Total reward=133.16, Steps=309871, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8346, Total reward=169.04, Steps=309985, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8347, Total reward=69.85, Steps=310053, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8348, Total reward=67.15, Steps=310103, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8349, Total reward=12.69, Steps=310118, Training iteration=166
Training> Name=main_level/agent, Worker=0, Episode=8350, Total reward=91.33, Steps=310167, Training iteration=166
Policy training> Surrogate loss=-0.0006134524010121822, KL divergence=0.0001212807692354545, Entropy=0.4713197350502014, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.01602039486169815, KL divergence=0.004537777975201607, Entropy=0.4712566137313843, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.057553403079509735, KL divergence=0.015628550201654434, Entropy=0.46934959292411804, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06149911880493164, KL divergence=0.027959970757365227, Entropy=0.4622936546802521, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06527527421712875, KL divergence=0.03920785337686539, Entropy=0.4616841673851013, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.058496907353401184, KL divergence=0.050047799944877625, Entropy=0.45420849323272705, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07101379334926605, KL divergence=0.05685480684041977, Entropy=0.4546715021133423, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05007661506533623, KL divergence=0.06501057744026184, Entropy=0.45826655626296997, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06929367035627365, KL divergence=0.06987513601779938, Entropy=0.4512197971343994, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07553311437368393, KL divergence=0.07588151097297668, Entropy=0.45701441168785095, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/459_Step-310167.ckpt']
Uploaded 3 files for checkpoint 459 in 0.60 seconds
saved intermediate frozen graph: current/model/model_459.pb
Best checkpoint number: 427, Last checkpoint number: 457
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'456'}
Training> Name=main_level/agent, Worker=0, Episode=8351, Total reward=55.84, Steps=310209, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8352, Total reward=53.02, Steps=310254, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8353, Total reward=41.84, Steps=310285, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8354, Total reward=44.54, Steps=310307, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8355, Total reward=18.66, Steps=310337, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8356, Total reward=28.77, Steps=310356, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8357, Total reward=38.85, Steps=310413, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8358, Total reward=37.63, Steps=310453, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8359, Total reward=24.22, Steps=310485, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8360, Total reward=149.12, Steps=310605, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8361, Total reward=49.23, Steps=310635, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8362, Total reward=38.27, Steps=310678, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8363, Total reward=8.2, Steps=310707, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8364, Total reward=72.81, Steps=310780, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8365, Total reward=97.08, Steps=310893, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8366, Total reward=7.16, Steps=310905, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8367, Total reward=60.13, Steps=310944, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8368, Total reward=38.13, Steps=310971, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8369, Total reward=30.99, Steps=311007, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8370, Total reward=100.34, Steps=311054, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8371, Total reward=46.12, Steps=311096, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8372, Total reward=58.48, Steps=311138, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8373, Total reward=0.02, Steps=311153, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8374, Total reward=46.72, Steps=311176, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8375, Total reward=26.02, Steps=311205, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8376, Total reward=23.46, Steps=311225, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8377, Total reward=59.49, Steps=311262, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8378, Total reward=40.5, Steps=311297, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8379, Total reward=13.14, Steps=311316, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8380, Total reward=68.71, Steps=311355, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8381, Total reward=53.35, Steps=311391, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8382, Total reward=38.25, Steps=311411, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8383, Total reward=7.72, Steps=311438, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8384, Total reward=18.08, Steps=311475, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8385, Total reward=9.89, Steps=311498, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8386, Total reward=14.13, Steps=311518, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8387, Total reward=77.58, Steps=311581, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8388, Total reward=65.5, Steps=311612, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8389, Total reward=36.46, Steps=311636, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8390, Total reward=61.16, Steps=311673, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8391, Total reward=56.57, Steps=311716, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8392, Total reward=27.8, Steps=311747, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8393, Total reward=41.9, Steps=311778, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8394, Total reward=41.11, Steps=311799, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8395, Total reward=35.5, Steps=311823, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8396, Total reward=78.95, Steps=311909, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8397, Total reward=52.39, Steps=311962, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8398, Total reward=36.2, Steps=312006, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8399, Total reward=21.73, Steps=312037, Training iteration=167
Training> Name=main_level/agent, Worker=0, Episode=8400, Total reward=66.25, Steps=312082, Training iteration=167
Policy training> Surrogate loss=0.001859168172813952, KL divergence=5.380895163398236e-05, Entropy=0.4841421842575073, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03565186634659767, KL divergence=0.0030096459668129683, Entropy=0.48582401871681213, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0300362017005682, KL divergence=0.011706770397722721, Entropy=0.47491705417633057, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05807957053184509, KL divergence=0.021133998408913612, Entropy=0.4631338119506836, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06496158987283707, KL divergence=0.03208940103650093, Entropy=0.4604673683643341, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04987054690718651, KL divergence=0.03920692577958107, Entropy=0.4555644094944, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06669339537620544, KL divergence=0.04941248893737793, Entropy=0.46257686614990234, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0725826621055603, KL divergence=0.05553804710507393, Entropy=0.4440251886844635, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04759601131081581, KL divergence=0.06187335029244423, Entropy=0.4553675651550293, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0832754597067833, KL divergence=0.06597480177879333, Entropy=0.445819616317749, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/460_Step-312082.ckpt']
Uploaded 3 files for checkpoint 460 in 0.64 seconds
saved intermediate frozen graph: current/model/model_460.pb
Best checkpoint number: 427, Last checkpoint number: 458
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'457'}
Training> Name=main_level/agent, Worker=0, Episode=8401, Total reward=62.79, Steps=312124, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8402, Total reward=37.61, Steps=312150, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8403, Total reward=8.97, Steps=312189, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8404, Total reward=90.06, Steps=312264, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8405, Total reward=107.23, Steps=312363, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8406, Total reward=75.91, Steps=312410, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8407, Total reward=79.8, Steps=312474, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8408, Total reward=68.7, Steps=312529, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8409, Total reward=26.56, Steps=312548, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8410, Total reward=92.56, Steps=312592, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8411, Total reward=45.97, Steps=312629, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8412, Total reward=54.87, Steps=312661, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8413, Total reward=52.97, Steps=312690, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8414, Total reward=44.64, Steps=312713, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8415, Total reward=19.7, Steps=312738, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8416, Total reward=46.76, Steps=312788, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8417, Total reward=142.12, Steps=312895, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8418, Total reward=42.9, Steps=312920, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8419, Total reward=24.47, Steps=312947, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8420, Total reward=69.57, Steps=312988, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8421, Total reward=60.01, Steps=313019, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8422, Total reward=48.11, Steps=313068, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8423, Total reward=31.68, Steps=313103, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8424, Total reward=82.9, Steps=313183, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8425, Total reward=31.77, Steps=313222, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8426, Total reward=128.84, Steps=313320, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8427, Total reward=58.18, Steps=313378, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8428, Total reward=59.2, Steps=313410, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8429, Total reward=50.54, Steps=313473, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8430, Total reward=18.34, Steps=313503, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8431, Total reward=43.34, Steps=313533, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8432, Total reward=67.99, Steps=313566, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8433, Total reward=11.07, Steps=313586, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8434, Total reward=34.63, Steps=313607, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8435, Total reward=38.35, Steps=313636, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8436, Total reward=22.45, Steps=313661, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8437, Total reward=54.83, Steps=313698, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8438, Total reward=28.37, Steps=313736, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8439, Total reward=78.86, Steps=313791, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8440, Total reward=118.03, Steps=313866, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8441, Total reward=65.19, Steps=313899, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8442, Total reward=39.7, Steps=313931, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8443, Total reward=8.99, Steps=313956, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8444, Total reward=9.06, Steps=313993, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8445, Total reward=82.76, Steps=314055, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8446, Total reward=66.13, Steps=314106, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8447, Total reward=79.8, Steps=314181, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8448, Total reward=58.41, Steps=314247, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8449, Total reward=28.42, Steps=314263, Training iteration=168
Training> Name=main_level/agent, Worker=0, Episode=8450, Total reward=81.24, Steps=314307, Training iteration=168
Policy training> Surrogate loss=0.006918476894497871, KL divergence=0.00020405786926858127, Entropy=0.4861506521701813, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02472565323114395, KL divergence=0.005876818206161261, Entropy=0.48206469416618347, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.052238427102565765, KL divergence=0.016863783821463585, Entropy=0.47075673937797546, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.062153320759534836, KL divergence=0.031216848641633987, Entropy=0.46726953983306885, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0677596926689148, KL divergence=0.04416670277714729, Entropy=0.4622133672237396, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06021585315465927, KL divergence=0.05585257709026337, Entropy=0.46394675970077515, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06975626945495605, KL divergence=0.06477472186088562, Entropy=0.46864640712738037, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06993886083364487, KL divergence=0.06751766055822372, Entropy=0.4652375876903534, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0634346455335617, KL divergence=0.07436611503362656, Entropy=0.46532896161079407, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07529556751251221, KL divergence=0.08013683557510376, Entropy=0.46904873847961426, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/461_Step-314307.ckpt']
Uploaded 3 files for checkpoint 461 in 0.54 seconds
saved intermediate frozen graph: current/model/model_461.pb
Best checkpoint number: 427, Last checkpoint number: 459
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'458'}
Training> Name=main_level/agent, Worker=0, Episode=8451, Total reward=42.78, Steps=314348, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8452, Total reward=52.85, Steps=314391, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8453, Total reward=32.81, Steps=314417, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8454, Total reward=40.47, Steps=314438, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8455, Total reward=25.31, Steps=314463, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8456, Total reward=22.03, Steps=314496, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8457, Total reward=44.87, Steps=314532, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8458, Total reward=28.03, Steps=314567, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8459, Total reward=3.28, Steps=314579, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8460, Total reward=71.02, Steps=314639, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8461, Total reward=99.31, Steps=314706, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8462, Total reward=35.09, Steps=314727, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8463, Total reward=17.14, Steps=314764, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8464, Total reward=75.76, Steps=314854, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8465, Total reward=57.05, Steps=314928, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8466, Total reward=41.68, Steps=314957, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8467, Total reward=64.8, Steps=314996, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8468, Total reward=74.08, Steps=315042, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8469, Total reward=91.11, Steps=315090, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8470, Total reward=22.31, Steps=315112, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8471, Total reward=7.75, Steps=315141, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8472, Total reward=57.47, Steps=315173, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8473, Total reward=46.93, Steps=315204, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8474, Total reward=37.19, Steps=315225, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8475, Total reward=13.53, Steps=315254, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8476, Total reward=14.41, Steps=315269, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8477, Total reward=55.59, Steps=315305, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8478, Total reward=27.74, Steps=315320, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8479, Total reward=80.86, Steps=315381, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8480, Total reward=157.06, Steps=315500, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8481, Total reward=43.99, Steps=315530, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8482, Total reward=31.99, Steps=315548, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8483, Total reward=6.46, Steps=315570, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8484, Total reward=112.49, Steps=315702, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8485, Total reward=96.1, Steps=315814, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8486, Total reward=97.67, Steps=315889, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8487, Total reward=48.54, Steps=315947, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8488, Total reward=105.6, Steps=316029, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8489, Total reward=41.84, Steps=316097, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8490, Total reward=7.58, Steps=316121, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8491, Total reward=88.74, Steps=316172, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8492, Total reward=53.77, Steps=316208, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8493, Total reward=34.21, Steps=316228, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8494, Total reward=22.66, Steps=316251, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8495, Total reward=22.42, Steps=316280, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8496, Total reward=24.5, Steps=316315, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8497, Total reward=97.16, Steps=316395, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8498, Total reward=40.21, Steps=316429, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8499, Total reward=51.32, Steps=316481, Training iteration=169
Training> Name=main_level/agent, Worker=0, Episode=8500, Total reward=69.66, Steps=316521, Training iteration=169
Policy training> Surrogate loss=0.005110443569719791, KL divergence=0.0001606818550499156, Entropy=0.4614851474761963, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.032737668603658676, KL divergence=0.005805471912026405, Entropy=0.46276646852493286, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05468444526195526, KL divergence=0.018408501520752907, Entropy=0.45337212085723877, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05838221311569214, KL divergence=0.034284815192222595, Entropy=0.4435155391693115, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.060626331716775894, KL divergence=0.04321439936757088, Entropy=0.43400144577026367, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.056147824972867966, KL divergence=0.056492455303668976, Entropy=0.43248695135116577, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06403982639312744, KL divergence=0.06293196231126785, Entropy=0.43569618463516235, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07660405337810516, KL divergence=0.07018566876649857, Entropy=0.4383096396923065, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07598722726106644, KL divergence=0.07582654058933258, Entropy=0.4416236877441406, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0665840208530426, KL divergence=0.07955251634120941, Entropy=0.44029271602630615, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/462_Step-316521.ckpt']
Uploaded 3 files for checkpoint 462 in 0.56 seconds
saved intermediate frozen graph: current/model/model_462.pb
Best checkpoint number: 427, Last checkpoint number: 460
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'459'}
Training> Name=main_level/agent, Worker=0, Episode=8501, Total reward=50.76, Steps=316549, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8502, Total reward=54.95, Steps=316608, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8503, Total reward=86.43, Steps=316688, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8504, Total reward=47.25, Steps=316734, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8505, Total reward=76.81, Steps=316799, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8506, Total reward=86.15, Steps=316851, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8507, Total reward=56.13, Steps=316892, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8508, Total reward=46.12, Steps=316918, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8509, Total reward=106.97, Steps=316997, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8510, Total reward=115.84, Steps=317045, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8511, Total reward=86.58, Steps=317097, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8512, Total reward=69.37, Steps=317137, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8513, Total reward=43.73, Steps=317168, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8514, Total reward=40.85, Steps=317191, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8515, Total reward=28.97, Steps=317220, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8516, Total reward=29.48, Steps=317257, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8517, Total reward=55.05, Steps=317307, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8518, Total reward=40.42, Steps=317333, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8519, Total reward=16.64, Steps=317353, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8520, Total reward=91.44, Steps=317394, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8521, Total reward=67.23, Steps=317426, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8522, Total reward=38.43, Steps=317475, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8523, Total reward=0.02, Steps=317494, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8524, Total reward=59.47, Steps=317564, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8525, Total reward=84.94, Steps=317650, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8526, Total reward=60.67, Steps=317706, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8527, Total reward=126.1, Steps=317794, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8528, Total reward=144.84, Steps=317881, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8529, Total reward=31.82, Steps=317912, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8530, Total reward=91.04, Steps=317962, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8531, Total reward=31.16, Steps=317990, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8532, Total reward=49.63, Steps=318022, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8533, Total reward=26.63, Steps=318045, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8534, Total reward=53.65, Steps=318069, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8535, Total reward=34.29, Steps=318098, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8536, Total reward=27.85, Steps=318133, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8537, Total reward=46.69, Steps=318170, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8538, Total reward=17.51, Steps=318188, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8539, Total reward=8.21, Steps=318210, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8540, Total reward=73.86, Steps=318246, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8541, Total reward=65.32, Steps=318278, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8542, Total reward=37.77, Steps=318298, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8543, Total reward=8.33, Steps=318330, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8544, Total reward=73.05, Steps=318397, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8545, Total reward=87.63, Steps=318460, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8546, Total reward=77.19, Steps=318512, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8547, Total reward=61.71, Steps=318554, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8548, Total reward=68.16, Steps=318598, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8549, Total reward=33.89, Steps=318622, Training iteration=170
Training> Name=main_level/agent, Worker=0, Episode=8550, Total reward=24.95, Steps=318661, Training iteration=170
Policy training> Surrogate loss=-0.0008652841206640005, KL divergence=0.00029178266413509846, Entropy=0.4545472264289856, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.036945994943380356, KL divergence=0.009138801135122776, Entropy=0.4490698575973511, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05053398385643959, KL divergence=0.021277278661727905, Entropy=0.4356310963630676, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05580081790685654, KL divergence=0.03763952851295471, Entropy=0.43141278624534607, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06092715263366699, KL divergence=0.05098986625671387, Entropy=0.428618848323822, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06988680362701416, KL divergence=0.062279343605041504, Entropy=0.4309723973274231, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0744616836309433, KL divergence=0.07035696506500244, Entropy=0.43127644062042236, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07800102233886719, KL divergence=0.07968313246965408, Entropy=0.43625473976135254, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07600389420986176, KL divergence=0.08273737132549286, Entropy=0.4358644187450409, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07501711696386337, KL divergence=0.08802102506160736, Entropy=0.4383775591850281, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/463_Step-318661.ckpt']
Uploaded 3 files for checkpoint 463 in 0.55 seconds
saved intermediate frozen graph: current/model/model_463.pb
Best checkpoint number: 427, Last checkpoint number: 461
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'460'}
Training> Name=main_level/agent, Worker=0, Episode=8551, Total reward=38.17, Steps=318693, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8552, Total reward=52.88, Steps=318725, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8553, Total reward=58.53, Steps=318755, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8554, Total reward=47.84, Steps=318778, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8555, Total reward=18.46, Steps=318806, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8556, Total reward=18.82, Steps=318836, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8557, Total reward=143.44, Steps=318937, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8558, Total reward=29.7, Steps=318952, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8559, Total reward=8.37, Steps=318972, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8560, Total reward=73.66, Steps=319042, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8561, Total reward=58.77, Steps=319102, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8562, Total reward=58.42, Steps=319152, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8563, Total reward=7.7, Steps=319169, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8564, Total reward=72.76, Steps=319256, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8565, Total reward=14.05, Steps=319300, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8566, Total reward=20.79, Steps=319326, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8567, Total reward=57.81, Steps=319372, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8568, Total reward=69.15, Steps=319413, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8569, Total reward=29.69, Steps=319437, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8570, Total reward=64.94, Steps=319490, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8571, Total reward=53.62, Steps=319529, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8572, Total reward=51.08, Steps=319574, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8573, Total reward=47.94, Steps=319605, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8574, Total reward=34.08, Steps=319617, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8575, Total reward=27.29, Steps=319646, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8576, Total reward=20.74, Steps=319665, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8577, Total reward=96.86, Steps=319756, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8578, Total reward=96.77, Steps=319825, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8579, Total reward=17.06, Steps=319843, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8580, Total reward=77.79, Steps=319882, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8581, Total reward=73.47, Steps=319913, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8582, Total reward=35.28, Steps=319933, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8583, Total reward=3.8, Steps=319951, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8584, Total reward=141.38, Steps=320065, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8585, Total reward=87.67, Steps=320153, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8586, Total reward=120.2, Steps=320240, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8587, Total reward=67.83, Steps=320287, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8588, Total reward=66.95, Steps=320317, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8589, Total reward=52.18, Steps=320351, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8590, Total reward=86.84, Steps=320398, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8591, Total reward=34.21, Steps=320417, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8592, Total reward=70.98, Steps=320459, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8593, Total reward=29.59, Steps=320479, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8594, Total reward=41.24, Steps=320500, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8595, Total reward=15.85, Steps=320524, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8596, Total reward=21.2, Steps=320554, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8597, Total reward=33.37, Steps=320585, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8598, Total reward=38.49, Steps=320627, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8599, Total reward=9.5, Steps=320644, Training iteration=171
Training> Name=main_level/agent, Worker=0, Episode=8600, Total reward=103.63, Steps=320705, Training iteration=171
Policy training> Surrogate loss=-0.0023496660869568586, KL divergence=5.3081497753737494e-05, Entropy=0.47494587302207947, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.014914100058376789, KL divergence=0.0032689732033759356, Entropy=0.4754887521266937, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05590757727622986, KL divergence=0.009606423787772655, Entropy=0.4774837791919708, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.056502360850572586, KL divergence=0.020959043875336647, Entropy=0.4771433174610138, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06400550156831741, KL divergence=0.032773759216070175, Entropy=0.4681781232357025, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.075301893055439, KL divergence=0.042798738926649094, Entropy=0.4621230363845825, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07586082071065903, KL divergence=0.0515216700732708, Entropy=0.4682539999485016, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05677628889679909, KL divergence=0.061103641986846924, Entropy=0.46237897872924805, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07536952942609787, KL divergence=0.06326239556074142, Entropy=0.46273815631866455, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06885301321744919, KL divergence=0.07302477955818176, Entropy=0.46082553267478943, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/464_Step-320705.ckpt']
Uploaded 3 files for checkpoint 464 in 0.56 seconds
saved intermediate frozen graph: current/model/model_464.pb
Best checkpoint number: 427, Last checkpoint number: 462
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'461'}
Training> Name=main_level/agent, Worker=0, Episode=8601, Total reward=142.27, Steps=320831, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8602, Total reward=32.48, Steps=320849, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8603, Total reward=7.12, Steps=320867, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8604, Total reward=87.65, Steps=320961, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8605, Total reward=16.48, Steps=320992, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8606, Total reward=115.19, Steps=321094, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8607, Total reward=70.65, Steps=321137, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8608, Total reward=53.25, Steps=321173, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8609, Total reward=43.8, Steps=321227, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8610, Total reward=39.67, Steps=321279, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8611, Total reward=73.6, Steps=321320, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8612, Total reward=54.86, Steps=321352, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8613, Total reward=59.15, Steps=321382, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8614, Total reward=47.09, Steps=321405, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8615, Total reward=23.31, Steps=321429, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8616, Total reward=25.9, Steps=321463, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8617, Total reward=101.23, Steps=321550, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8618, Total reward=40.41, Steps=321585, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8619, Total reward=67.84, Steps=321643, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8620, Total reward=40.66, Steps=321677, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8621, Total reward=64.89, Steps=321708, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8622, Total reward=38.02, Steps=321728, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8623, Total reward=11.01, Steps=321758, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8624, Total reward=3.19, Steps=321778, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8625, Total reward=73.39, Steps=321858, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8626, Total reward=10.77, Steps=321877, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8627, Total reward=61.14, Steps=321922, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8628, Total reward=81.56, Steps=321991, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8629, Total reward=22.77, Steps=322017, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8630, Total reward=19.24, Steps=322065, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8631, Total reward=57.81, Steps=322093, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8632, Total reward=7.51, Steps=322105, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8633, Total reward=48.19, Steps=322125, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8634, Total reward=36.39, Steps=322147, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8635, Total reward=26.65, Steps=322161, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8636, Total reward=19.87, Steps=322182, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8637, Total reward=58.97, Steps=322229, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8638, Total reward=94.43, Steps=322294, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8639, Total reward=118.55, Steps=322381, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8640, Total reward=58.19, Steps=322450, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8641, Total reward=58.83, Steps=322478, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8642, Total reward=33.86, Steps=322504, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8643, Total reward=6.22, Steps=322534, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8644, Total reward=6.71, Steps=322559, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8645, Total reward=18.29, Steps=322592, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8646, Total reward=73.04, Steps=322653, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8647, Total reward=77.02, Steps=322731, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8648, Total reward=47.73, Steps=322757, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8649, Total reward=26.54, Steps=322781, Training iteration=172
Training> Name=main_level/agent, Worker=0, Episode=8650, Total reward=11.24, Steps=322796, Training iteration=172
Policy training> Surrogate loss=0.0037355683743953705, KL divergence=0.00015309856098610908, Entropy=0.48478713631629944, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02973814308643341, KL divergence=0.007988497614860535, Entropy=0.47893667221069336, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05091201514005661, KL divergence=0.02192814275622368, Entropy=0.4711255431175232, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06046605110168457, KL divergence=0.03659909963607788, Entropy=0.46743327379226685, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.061732906848192215, KL divergence=0.04951748996973038, Entropy=0.4641960859298706, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06576397269964218, KL divergence=0.05747199058532715, Entropy=0.4632571041584015, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0665862038731575, KL divergence=0.0668049231171608, Entropy=0.4627445936203003, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07292867451906204, KL divergence=0.07344071567058563, Entropy=0.4625779986381531, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07635331153869629, KL divergence=0.07710257172584534, Entropy=0.4633080065250397, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0774012953042984, KL divergence=0.08238204568624496, Entropy=0.4634105861186981, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/465_Step-322796.ckpt']
Uploaded 3 files for checkpoint 465 in 0.54 seconds
saved intermediate frozen graph: current/model/model_465.pb
Best checkpoint number: 427, Last checkpoint number: 463
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'462'}
Training> Name=main_level/agent, Worker=0, Episode=8651, Total reward=40.7, Steps=322818, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8652, Total reward=51.39, Steps=322850, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8653, Total reward=45.11, Steps=322882, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8654, Total reward=34.22, Steps=322904, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8655, Total reward=13.05, Steps=322917, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8656, Total reward=24.96, Steps=322946, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8657, Total reward=51.78, Steps=322983, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8658, Total reward=99.98, Steps=323060, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8659, Total reward=26.01, Steps=323082, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8660, Total reward=104.27, Steps=323135, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8661, Total reward=25.21, Steps=323156, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8662, Total reward=35.43, Steps=323176, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8663, Total reward=3.51, Steps=323195, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8664, Total reward=29.34, Steps=323230, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8665, Total reward=74.59, Steps=323302, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8666, Total reward=89.53, Steps=323355, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8667, Total reward=59.36, Steps=323394, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8668, Total reward=61.17, Steps=323424, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8669, Total reward=38.97, Steps=323469, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8670, Total reward=66.38, Steps=323513, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8671, Total reward=52.33, Steps=323544, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8672, Total reward=53.91, Steps=323575, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8673, Total reward=37.18, Steps=323601, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8674, Total reward=28.3, Steps=323612, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8675, Total reward=34.55, Steps=323656, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8676, Total reward=22.54, Steps=323689, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8677, Total reward=43.09, Steps=323725, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8678, Total reward=30.26, Steps=323762, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8679, Total reward=6.62, Steps=323789, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8680, Total reward=90.44, Steps=323847, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8681, Total reward=69.18, Steps=323880, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8682, Total reward=59.41, Steps=323915, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8683, Total reward=13.66, Steps=323953, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8684, Total reward=133.75, Steps=324075, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8685, Total reward=13.25, Steps=324090, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8686, Total reward=0.02, Steps=324111, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8687, Total reward=110.88, Steps=324200, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8688, Total reward=61.27, Steps=324246, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8689, Total reward=24.48, Steps=324274, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8690, Total reward=47.84, Steps=324314, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8691, Total reward=54.09, Steps=324342, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8692, Total reward=3.77, Steps=324353, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8693, Total reward=33.85, Steps=324374, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8694, Total reward=27.98, Steps=324396, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8695, Total reward=35.62, Steps=324425, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8696, Total reward=18.86, Steps=324442, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8697, Total reward=28.26, Steps=324465, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8698, Total reward=16.14, Steps=324478, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8699, Total reward=58.73, Steps=324527, Training iteration=173
Training> Name=main_level/agent, Worker=0, Episode=8700, Total reward=72.36, Steps=324569, Training iteration=173
Policy training> Surrogate loss=-0.009777930565178394, KL divergence=6.422099977498874e-05, Entropy=0.4628227949142456, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03927524760365486, KL divergence=0.00348399649374187, Entropy=0.46482136845588684, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04598413407802582, KL divergence=0.01179009210318327, Entropy=0.45730242133140564, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0455259270966053, KL divergence=0.021922769024968147, Entropy=0.451833575963974, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05769190564751625, KL divergence=0.03344007581472397, Entropy=0.45071157813072205, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.048587482422590256, KL divergence=0.04284737631678581, Entropy=0.44506844878196716, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06744341552257538, KL divergence=0.049470361322164536, Entropy=0.42705926299095154, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06810270249843597, KL divergence=0.06139373406767845, Entropy=0.44223880767822266, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05454258993268013, KL divergence=0.06825703382492065, Entropy=0.4404093027114868, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06596672534942627, KL divergence=0.07135019451379776, Entropy=0.4333553612232208, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/466_Step-324569.ckpt']
Uploaded 3 files for checkpoint 466 in 0.51 seconds
saved intermediate frozen graph: current/model/model_466.pb
Best checkpoint number: 427, Last checkpoint number: 464
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'463'}
Training> Name=main_level/agent, Worker=0, Episode=8701, Total reward=76.31, Steps=324627, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8702, Total reward=33.63, Steps=324656, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8703, Total reward=18.84, Steps=324698, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8704, Total reward=69.05, Steps=324773, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8705, Total reward=107.99, Steps=324881, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8706, Total reward=7.14, Steps=324893, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8707, Total reward=68.89, Steps=324952, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8708, Total reward=70.65, Steps=325002, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8709, Total reward=77.98, Steps=325050, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8710, Total reward=46.03, Steps=325097, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8711, Total reward=81.02, Steps=325149, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8712, Total reward=67.79, Steps=325192, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8713, Total reward=43.89, Steps=325215, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8714, Total reward=46.45, Steps=325237, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8715, Total reward=41.17, Steps=325267, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8716, Total reward=39.71, Steps=325330, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8717, Total reward=11.77, Steps=325361, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8718, Total reward=24.88, Steps=325374, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8719, Total reward=7.13, Steps=325398, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8720, Total reward=39.92, Steps=325430, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8721, Total reward=77.42, Steps=325477, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8722, Total reward=43.95, Steps=325497, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8723, Total reward=0.02, Steps=325512, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8724, Total reward=109.76, Steps=325617, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8725, Total reward=77.88, Steps=325677, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8726, Total reward=69.9, Steps=325761, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8727, Total reward=57.82, Steps=325817, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8728, Total reward=68.46, Steps=325848, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8729, Total reward=16.81, Steps=325865, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8730, Total reward=59.75, Steps=325903, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8731, Total reward=46.64, Steps=325929, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8732, Total reward=77.85, Steps=325969, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8733, Total reward=51.84, Steps=325999, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8734, Total reward=43.97, Steps=326021, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8735, Total reward=41.3, Steps=326050, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8736, Total reward=20.52, Steps=326069, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8737, Total reward=49.5, Steps=326114, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8738, Total reward=43.48, Steps=326150, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8739, Total reward=21.18, Steps=326181, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8740, Total reward=78.46, Steps=326218, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8741, Total reward=44.9, Steps=326237, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8742, Total reward=40.09, Steps=326265, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8743, Total reward=20.23, Steps=326300, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8744, Total reward=1.94, Steps=326320, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8745, Total reward=58.31, Steps=326382, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8746, Total reward=121.39, Steps=326482, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8747, Total reward=12.93, Steps=326498, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8748, Total reward=50.16, Steps=326524, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8749, Total reward=87.34, Steps=326603, Training iteration=174
Training> Name=main_level/agent, Worker=0, Episode=8750, Total reward=49.05, Steps=326628, Training iteration=174
Policy training> Surrogate loss=-0.00041779910679906607, KL divergence=0.00014733875286765397, Entropy=0.48393872380256653, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.037706512957811356, KL divergence=0.0064993323758244514, Entropy=0.4715712368488312, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05339658260345459, KL divergence=0.019711920991539955, Entropy=0.4584706425666809, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.057441405951976776, KL divergence=0.03368338569998741, Entropy=0.4512506127357483, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06724385917186737, KL divergence=0.04505901038646698, Entropy=0.445335328578949, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07154111564159393, KL divergence=0.05468105524778366, Entropy=0.44276681542396545, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0720246210694313, KL divergence=0.06245909258723259, Entropy=0.44004878401756287, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07223965227603912, KL divergence=0.06957364082336426, Entropy=0.44083741307258606, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07593018561601639, KL divergence=0.07494636625051498, Entropy=0.44151389598846436, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07359912991523743, KL divergence=0.07896673679351807, Entropy=0.44143742322921753, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/467_Step-326628.ckpt']
Uploaded 3 files for checkpoint 467 in 0.45 seconds
saved intermediate frozen graph: current/model/model_467.pb
Best checkpoint number: 427, Last checkpoint number: 465
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'464'}
Training> Name=main_level/agent, Worker=0, Episode=8751, Total reward=45.85, Steps=326663, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8752, Total reward=46.28, Steps=326706, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8753, Total reward=59.26, Steps=326738, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8754, Total reward=45.33, Steps=326760, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8755, Total reward=25.92, Steps=326790, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8756, Total reward=28.54, Steps=326822, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8757, Total reward=58.6, Steps=326868, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8758, Total reward=102.35, Steps=326934, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8759, Total reward=54.08, Steps=327013, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8760, Total reward=84.22, Steps=327054, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8761, Total reward=65.24, Steps=327085, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8762, Total reward=39.94, Steps=327146, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8763, Total reward=0.01, Steps=327157, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8764, Total reward=16.37, Steps=327186, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8765, Total reward=13.4, Steps=327221, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8766, Total reward=135.33, Steps=327309, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8767, Total reward=63.37, Steps=327354, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8768, Total reward=128.77, Steps=327442, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8769, Total reward=17.59, Steps=327463, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8770, Total reward=49.57, Steps=327491, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8771, Total reward=45.2, Steps=327523, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8772, Total reward=42.04, Steps=327553, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8773, Total reward=42.81, Steps=327580, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8774, Total reward=34.48, Steps=327601, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8775, Total reward=8.11, Steps=327613, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8776, Total reward=22.07, Steps=327638, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8777, Total reward=38.03, Steps=327687, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8778, Total reward=50.7, Steps=327722, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8779, Total reward=76.39, Steps=327779, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8780, Total reward=89.37, Steps=327820, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8781, Total reward=71.65, Steps=327872, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8782, Total reward=50.12, Steps=327915, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8783, Total reward=125.29, Steps=328045, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8784, Total reward=20.67, Steps=328070, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8785, Total reward=93.75, Steps=328156, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8786, Total reward=176.17, Steps=328248, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8787, Total reward=89.88, Steps=328346, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8788, Total reward=64.18, Steps=328377, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8789, Total reward=94.52, Steps=328438, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8790, Total reward=76.86, Steps=328498, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8791, Total reward=67.39, Steps=328540, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8792, Total reward=47.1, Steps=328573, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8793, Total reward=46.81, Steps=328603, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8794, Total reward=40.88, Steps=328623, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8795, Total reward=43.63, Steps=328654, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8796, Total reward=27.32, Steps=328672, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8797, Total reward=130.57, Steps=328759, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8798, Total reward=43.5, Steps=328804, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8799, Total reward=73.02, Steps=328855, Training iteration=175
Training> Name=main_level/agent, Worker=0, Episode=8800, Total reward=72.18, Steps=328896, Training iteration=175
Policy training> Surrogate loss=-0.008778945542871952, KL divergence=0.00030419911490753293, Entropy=0.47750434279441833, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0250528734177351, KL divergence=0.009158847853541374, Entropy=0.47857534885406494, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04740983247756958, KL divergence=0.021754011511802673, Entropy=0.47185277938842773, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06608998030424118, KL divergence=0.03594928979873657, Entropy=0.4701746702194214, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06728677451610565, KL divergence=0.04844992235302925, Entropy=0.4631284475326538, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05456173047423363, KL divergence=0.055617332458496094, Entropy=0.46920111775398254, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06477245688438416, KL divergence=0.06711927056312561, Entropy=0.4613035023212433, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07648076862096786, KL divergence=0.07538460940122604, Entropy=0.462514191865921, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07836213707923889, KL divergence=0.07974906265735626, Entropy=0.4535003900527954, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08276761323213577, KL divergence=0.08492907136678696, Entropy=0.45746415853500366, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/468_Step-328896.ckpt']
Uploaded 3 files for checkpoint 468 in 0.64 seconds
saved intermediate frozen graph: current/model/model_468.pb
Best checkpoint number: 427, Last checkpoint number: 466
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'465'}
Training> Name=main_level/agent, Worker=0, Episode=8801, Total reward=73.42, Steps=328929, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8802, Total reward=147.41, Steps=329059, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8803, Total reward=78.55, Steps=329148, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8804, Total reward=21.52, Steps=329172, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8805, Total reward=19.78, Steps=329200, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8806, Total reward=86.72, Steps=329275, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8807, Total reward=3.25, Steps=329287, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8808, Total reward=60.5, Steps=329316, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8809, Total reward=18.31, Steps=329333, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8810, Total reward=62.66, Steps=329385, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8811, Total reward=88.88, Steps=329437, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8812, Total reward=63.73, Steps=329480, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8813, Total reward=53.97, Steps=329511, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8814, Total reward=46.64, Steps=329533, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8815, Total reward=38.14, Steps=329561, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8816, Total reward=26.24, Steps=329588, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8817, Total reward=126.43, Steps=329675, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8818, Total reward=32.67, Steps=329693, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8819, Total reward=12.62, Steps=329710, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8820, Total reward=87.74, Steps=329767, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8821, Total reward=61.83, Steps=329803, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8822, Total reward=43.18, Steps=329826, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8823, Total reward=103.54, Steps=329934, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8824, Total reward=7.84, Steps=329979, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8825, Total reward=13.24, Steps=329996, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8826, Total reward=124.98, Steps=330097, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8827, Total reward=32.09, Steps=330136, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8828, Total reward=51.62, Steps=330162, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8829, Total reward=25.66, Steps=330191, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8830, Total reward=57.78, Steps=330227, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8831, Total reward=64.0, Steps=330267, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8832, Total reward=54.82, Steps=330301, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8833, Total reward=26.93, Steps=330319, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8834, Total reward=11.34, Steps=330335, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8835, Total reward=43.1, Steps=330366, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8836, Total reward=14.5, Steps=330391, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8837, Total reward=24.36, Steps=330425, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8838, Total reward=40.95, Steps=330459, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8839, Total reward=51.29, Steps=330517, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8840, Total reward=62.74, Steps=330559, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8841, Total reward=52.06, Steps=330581, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8842, Total reward=42.96, Steps=330602, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8843, Total reward=73.91, Steps=330685, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8844, Total reward=105.48, Steps=330780, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8845, Total reward=6.78, Steps=330804, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8846, Total reward=74.55, Steps=330855, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8847, Total reward=65.07, Steps=330896, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8848, Total reward=60.74, Steps=330924, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8849, Total reward=46.82, Steps=330970, Training iteration=176
Training> Name=main_level/agent, Worker=0, Episode=8850, Total reward=14.76, Steps=330981, Training iteration=176
Policy training> Surrogate loss=-0.0024452712386846542, KL divergence=0.0001472819276386872, Entropy=0.46792536973953247, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.036166511476039886, KL divergence=0.007146277464926243, Entropy=0.4649268090724945, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05396809056401253, KL divergence=0.020738957449793816, Entropy=0.45504525303840637, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06404978781938553, KL divergence=0.03603934496641159, Entropy=0.44990378618240356, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06467428803443909, KL divergence=0.049081891775131226, Entropy=0.4442407190799713, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0653078705072403, KL divergence=0.06009440869092941, Entropy=0.43870875239372253, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0647818073630333, KL divergence=0.06844375282526016, Entropy=0.43720000982284546, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06887433677911758, KL divergence=0.07495343685150146, Entropy=0.4347154498100281, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07748528569936752, KL divergence=0.08021033555269241, Entropy=0.43697190284729004, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06697981059551239, KL divergence=0.08326970040798187, Entropy=0.4377872347831726, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/469_Step-330981.ckpt']
Uploaded 3 files for checkpoint 469 in 0.54 seconds
saved intermediate frozen graph: current/model/model_469.pb
Best checkpoint number: 427, Last checkpoint number: 467
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'466'}
Training> Name=main_level/agent, Worker=0, Episode=8851, Total reward=15.29, Steps=330993, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8852, Total reward=35.7, Steps=331032, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8853, Total reward=50.04, Steps=331063, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8854, Total reward=40.08, Steps=331094, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8855, Total reward=34.05, Steps=331125, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8856, Total reward=27.27, Steps=331153, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8857, Total reward=115.87, Steps=331229, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8858, Total reward=28.48, Steps=331255, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8859, Total reward=83.08, Steps=331320, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8860, Total reward=43.14, Steps=331360, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8861, Total reward=69.2, Steps=331420, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8862, Total reward=34.1, Steps=331438, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8863, Total reward=150.01, Steps=331577, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8864, Total reward=132.98, Steps=331694, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8865, Total reward=98.9, Steps=331802, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8866, Total reward=3.55, Steps=331813, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8867, Total reward=58.63, Steps=331862, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8868, Total reward=64.65, Steps=331892, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8869, Total reward=22.29, Steps=331922, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8870, Total reward=77.24, Steps=331968, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8871, Total reward=20.96, Steps=331993, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8872, Total reward=7.53, Steps=332004, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8873, Total reward=54.9, Steps=332034, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8874, Total reward=40.07, Steps=332057, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8875, Total reward=14.87, Steps=332084, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8876, Total reward=18.9, Steps=332105, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8877, Total reward=65.99, Steps=332176, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8878, Total reward=39.95, Steps=332231, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8879, Total reward=74.91, Steps=332282, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8880, Total reward=141.3, Steps=332399, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8881, Total reward=162.08, Steps=332505, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8882, Total reward=45.28, Steps=332534, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8883, Total reward=28.64, Steps=332567, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8884, Total reward=18.15, Steps=332593, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8885, Total reward=44.03, Steps=332650, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8886, Total reward=22.56, Steps=332677, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8887, Total reward=122.94, Steps=332764, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8888, Total reward=60.99, Steps=332794, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8889, Total reward=57.62, Steps=332849, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8890, Total reward=75.43, Steps=332896, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8891, Total reward=91.78, Steps=332938, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8892, Total reward=11.28, Steps=332950, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8893, Total reward=52.68, Steps=332979, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8894, Total reward=47.27, Steps=333001, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8895, Total reward=20.29, Steps=333016, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8896, Total reward=19.2, Steps=333038, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8897, Total reward=135.07, Steps=333152, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8898, Total reward=30.25, Steps=333173, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8899, Total reward=18.02, Steps=333207, Training iteration=177
Training> Name=main_level/agent, Worker=0, Episode=8900, Total reward=99.95, Steps=333264, Training iteration=177
Policy training> Surrogate loss=-0.00664945226162672, KL divergence=0.00025958428159356117, Entropy=0.47529900074005127, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.019747182726860046, KL divergence=0.00691122654825449, Entropy=0.4597027599811554, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.053784288465976715, KL divergence=0.01929367147386074, Entropy=0.4555227756500244, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.052903901785612106, KL divergence=0.03243681415915489, Entropy=0.44516271352767944, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06345482915639877, KL divergence=0.04642247036099434, Entropy=0.4436190128326416, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.057197850197553635, KL divergence=0.05413337051868439, Entropy=0.44075971841812134, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05509320646524429, KL divergence=0.06388340890407562, Entropy=0.4438622295856476, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06290427595376968, KL divergence=0.06775683164596558, Entropy=0.4439724087715149, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.061422958970069885, KL divergence=0.07467369735240936, Entropy=0.4421931505203247, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06745222210884094, KL divergence=0.07628040760755539, Entropy=0.4403513967990875, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/470_Step-333264.ckpt']
Uploaded 3 files for checkpoint 470 in 0.44 seconds
saved intermediate frozen graph: current/model/model_470.pb
Best checkpoint number: 427, Last checkpoint number: 468
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'467'}
Training> Name=main_level/agent, Worker=0, Episode=8901, Total reward=135.78, Steps=333367, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8902, Total reward=47.7, Steps=333392, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8903, Total reward=37.59, Steps=333457, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8904, Total reward=90.62, Steps=333542, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8905, Total reward=80.97, Steps=333608, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8906, Total reward=90.93, Steps=333659, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8907, Total reward=75.48, Steps=333701, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8908, Total reward=91.86, Steps=333768, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8909, Total reward=21.84, Steps=333783, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8910, Total reward=71.83, Steps=333819, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8911, Total reward=87.48, Steps=333872, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8912, Total reward=63.09, Steps=333905, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8913, Total reward=14.08, Steps=333924, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8914, Total reward=27.14, Steps=333934, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8915, Total reward=34.45, Steps=333966, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8916, Total reward=18.52, Steps=333986, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8917, Total reward=64.3, Steps=334045, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8918, Total reward=75.31, Steps=334111, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8919, Total reward=169.53, Steps=334244, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8920, Total reward=19.05, Steps=334260, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8921, Total reward=62.8, Steps=334301, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8922, Total reward=44.35, Steps=334329, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8923, Total reward=106.4, Steps=334472, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8924, Total reward=0.02, Steps=334489, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8925, Total reward=27.03, Steps=334526, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8926, Total reward=87.27, Steps=334582, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8927, Total reward=118.54, Steps=334670, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8928, Total reward=68.78, Steps=334700, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8929, Total reward=90.46, Steps=334757, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8930, Total reward=18.28, Steps=334798, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8931, Total reward=52.9, Steps=334830, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8932, Total reward=58.88, Steps=334863, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8933, Total reward=44.63, Steps=334892, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8934, Total reward=44.95, Steps=334923, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8935, Total reward=48.96, Steps=334951, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8936, Total reward=14.78, Steps=334987, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8937, Total reward=42.52, Steps=335043, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8938, Total reward=82.28, Steps=335144, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8939, Total reward=12.44, Steps=335163, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8940, Total reward=73.27, Steps=335215, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8941, Total reward=76.26, Steps=335273, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8942, Total reward=44.78, Steps=335297, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8943, Total reward=13.94, Steps=335333, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8944, Total reward=9.06, Steps=335352, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8945, Total reward=87.04, Steps=335417, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8946, Total reward=74.68, Steps=335492, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8947, Total reward=84.65, Steps=335565, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8948, Total reward=65.68, Steps=335604, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8949, Total reward=122.39, Steps=335674, Training iteration=178
Training> Name=main_level/agent, Worker=0, Episode=8950, Total reward=90.89, Steps=335722, Training iteration=178
Policy training> Surrogate loss=0.009259795770049095, KL divergence=0.0002147835912182927, Entropy=0.4528443217277527, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.043564148247241974, KL divergence=0.005143567454069853, Entropy=0.45196205377578735, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05981806665658951, KL divergence=0.017726104706525803, Entropy=0.44734063744544983, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.050223853439092636, KL divergence=0.03157353028655052, Entropy=0.44476985931396484, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.049582142382860184, KL divergence=0.04572230577468872, Entropy=0.43915480375289917, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05708134174346924, KL divergence=0.05525960028171539, Entropy=0.4328509569168091, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.060479536652565, KL divergence=0.06414808332920074, Entropy=0.4325528144836426, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06479865312576294, KL divergence=0.0702236145734787, Entropy=0.43882906436920166, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07439079880714417, KL divergence=0.07583515346050262, Entropy=0.4359476566314697, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08643496781587601, KL divergence=0.07858428359031677, Entropy=0.4364345669746399, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/471_Step-335722.ckpt']
Uploaded 3 files for checkpoint 471 in 0.49 seconds
saved intermediate frozen graph: current/model/model_471.pb
Best checkpoint number: 427, Last checkpoint number: 469
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'468'}
Training> Name=main_level/agent, Worker=0, Episode=8951, Total reward=101.91, Steps=335772, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8952, Total reward=47.37, Steps=335804, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8953, Total reward=51.36, Steps=335834, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8954, Total reward=37.32, Steps=335855, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8955, Total reward=20.68, Steps=335885, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8956, Total reward=20.08, Steps=335903, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8957, Total reward=48.3, Steps=335963, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8958, Total reward=31.19, Steps=335976, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8959, Total reward=91.94, Steps=336027, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8960, Total reward=88.88, Steps=336074, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8961, Total reward=60.59, Steps=336106, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8962, Total reward=37.87, Steps=336134, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8963, Total reward=39.32, Steps=336193, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8964, Total reward=9.46, Steps=336221, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8965, Total reward=63.04, Steps=336281, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8966, Total reward=86.12, Steps=336330, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8967, Total reward=57.84, Steps=336378, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8968, Total reward=119.97, Steps=336454, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8969, Total reward=80.63, Steps=336511, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8970, Total reward=66.0, Steps=336569, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8971, Total reward=80.4, Steps=336609, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8972, Total reward=12.47, Steps=336631, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8973, Total reward=50.82, Steps=336663, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8974, Total reward=48.46, Steps=336686, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8975, Total reward=35.22, Steps=336713, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8976, Total reward=25.48, Steps=336731, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8977, Total reward=50.61, Steps=336788, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8978, Total reward=30.21, Steps=336813, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8979, Total reward=20.57, Steps=336847, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8980, Total reward=79.24, Steps=336891, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8981, Total reward=79.06, Steps=336938, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8982, Total reward=66.81, Steps=336986, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8983, Total reward=8.94, Steps=337008, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8984, Total reward=7.34, Steps=337026, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8985, Total reward=39.56, Steps=337059, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8986, Total reward=81.46, Steps=337110, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8987, Total reward=58.76, Steps=337150, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8988, Total reward=75.26, Steps=337204, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8989, Total reward=36.66, Steps=337235, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8990, Total reward=76.58, Steps=337300, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8991, Total reward=54.82, Steps=337338, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8992, Total reward=65.29, Steps=337381, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8993, Total reward=43.66, Steps=337403, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8994, Total reward=45.39, Steps=337424, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8995, Total reward=12.98, Steps=337451, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8996, Total reward=27.66, Steps=337485, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8997, Total reward=51.09, Steps=337535, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8998, Total reward=30.52, Steps=337561, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=8999, Total reward=77.29, Steps=337634, Training iteration=179
Training> Name=main_level/agent, Worker=0, Episode=9000, Total reward=70.01, Steps=337683, Training iteration=179
Policy training> Surrogate loss=0.0015243975212797523, KL divergence=6.0436581406975165e-05, Entropy=0.46665525436401367, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.008528058417141438, KL divergence=0.004164120648056269, Entropy=0.46184107661247253, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03815467283129692, KL divergence=0.014805001206696033, Entropy=0.4650484025478363, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.031829506158828735, KL divergence=0.025226788595318794, Entropy=0.4518295228481293, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05555807426571846, KL divergence=0.038736578077077866, Entropy=0.4558391869068146, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04953889921307564, KL divergence=0.04819987714290619, Entropy=0.4512988030910492, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06186474859714508, KL divergence=0.06340333819389343, Entropy=0.4419044256210327, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06537265330553055, KL divergence=0.06257622689008713, Entropy=0.4481956958770752, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07827161997556686, KL divergence=0.07805778831243515, Entropy=0.44669973850250244, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05291584134101868, KL divergence=0.07808604836463928, Entropy=0.4490145444869995, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/472_Step-337683.ckpt']
Uploaded 3 files for checkpoint 472 in 0.57 seconds
saved intermediate frozen graph: current/model/model_472.pb
Best checkpoint number: 427, Last checkpoint number: 470
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'469'}
Training> Name=main_level/agent, Worker=0, Episode=9001, Total reward=180.26, Steps=337840, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9002, Total reward=38.55, Steps=337867, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9003, Total reward=19.94, Steps=337904, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9004, Total reward=8.42, Steps=337935, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9005, Total reward=16.79, Steps=337961, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9006, Total reward=116.5, Steps=338047, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9007, Total reward=51.95, Steps=338086, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9008, Total reward=66.7, Steps=338124, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9009, Total reward=28.8, Steps=338145, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9010, Total reward=69.37, Steps=338178, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9011, Total reward=84.54, Steps=338231, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9012, Total reward=58.81, Steps=338262, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9013, Total reward=55.95, Steps=338292, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9014, Total reward=44.18, Steps=338314, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9015, Total reward=26.64, Steps=338343, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9016, Total reward=29.97, Steps=338375, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9017, Total reward=113.05, Steps=338493, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9018, Total reward=41.14, Steps=338537, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9019, Total reward=75.53, Steps=338588, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9020, Total reward=50.5, Steps=338629, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9021, Total reward=66.76, Steps=338662, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9022, Total reward=28.19, Steps=338679, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9023, Total reward=3.4, Steps=338699, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9024, Total reward=12.02, Steps=338730, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9025, Total reward=138.73, Steps=338844, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9026, Total reward=21.17, Steps=338865, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9027, Total reward=124.87, Steps=338971, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9028, Total reward=131.06, Steps=339045, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9029, Total reward=64.78, Steps=339093, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9030, Total reward=67.08, Steps=339147, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9031, Total reward=11.23, Steps=339173, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9032, Total reward=43.31, Steps=339203, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9033, Total reward=31.55, Steps=339227, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9034, Total reward=26.69, Steps=339239, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9035, Total reward=18.12, Steps=339282, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9036, Total reward=22.29, Steps=339299, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9037, Total reward=49.67, Steps=339338, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9038, Total reward=40.52, Steps=339365, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9039, Total reward=20.85, Steps=339396, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9040, Total reward=65.83, Steps=339439, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9041, Total reward=90.26, Steps=339534, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9042, Total reward=38.4, Steps=339555, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9043, Total reward=30.46, Steps=339588, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9044, Total reward=19.06, Steps=339620, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9045, Total reward=115.82, Steps=339734, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9046, Total reward=99.94, Steps=339797, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9047, Total reward=113.23, Steps=339875, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9048, Total reward=108.03, Steps=339940, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9049, Total reward=91.21, Steps=340003, Training iteration=180
Training> Name=main_level/agent, Worker=0, Episode=9050, Total reward=50.7, Steps=340058, Training iteration=180
Policy training> Surrogate loss=-0.0066670821979641914, KL divergence=0.00016316621622536331, Entropy=0.47003602981567383, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.020055195316672325, KL divergence=0.006185424514114857, Entropy=0.46866118907928467, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0497150719165802, KL divergence=0.018463123589754105, Entropy=0.4610529839992523, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07805819064378738, KL divergence=0.032818570733070374, Entropy=0.450061559677124, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04880933091044426, KL divergence=0.04398098960518837, Entropy=0.450253427028656, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07421250641345978, KL divergence=0.05504583194851875, Entropy=0.4522622227668762, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0643942654132843, KL divergence=0.06192830204963684, Entropy=0.45816320180892944, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08152350783348083, KL divergence=0.06905853748321533, Entropy=0.4534660577774048, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06488112360239029, KL divergence=0.0731193870306015, Entropy=0.4531717598438263, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08073748648166656, KL divergence=0.07992145419120789, Entropy=0.4427538514137268, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/473_Step-340058.ckpt']
Uploaded 3 files for checkpoint 473 in 0.54 seconds
saved intermediate frozen graph: current/model/model_473.pb
Best checkpoint number: 427, Last checkpoint number: 471
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'470'}
Training> Name=main_level/agent, Worker=0, Episode=9051, Total reward=111.98, Steps=340108, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9052, Total reward=50.14, Steps=340140, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9053, Total reward=56.34, Steps=340171, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9054, Total reward=35.3, Steps=340192, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9055, Total reward=32.68, Steps=340217, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9056, Total reward=23.3, Steps=340250, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9057, Total reward=53.5, Steps=340298, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9058, Total reward=29.84, Steps=340321, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9059, Total reward=71.19, Steps=340382, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9060, Total reward=70.76, Steps=340426, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9061, Total reward=69.33, Steps=340459, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9062, Total reward=125.74, Steps=340580, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9063, Total reward=12.48, Steps=340597, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9064, Total reward=9.47, Steps=340632, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9065, Total reward=82.64, Steps=340710, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9066, Total reward=81.65, Steps=340762, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9067, Total reward=73.88, Steps=340828, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9068, Total reward=85.5, Steps=340893, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9069, Total reward=89.13, Steps=340949, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9070, Total reward=25.45, Steps=340990, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9071, Total reward=92.78, Steps=341031, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9072, Total reward=66.26, Steps=341073, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9073, Total reward=48.36, Steps=341103, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9074, Total reward=41.19, Steps=341124, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9075, Total reward=25.97, Steps=341147, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9076, Total reward=20.12, Steps=341176, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9077, Total reward=134.04, Steps=341283, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9078, Total reward=86.44, Steps=341360, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9079, Total reward=53.0, Steps=341410, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9080, Total reward=33.89, Steps=341449, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9081, Total reward=59.67, Steps=341479, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9082, Total reward=38.8, Steps=341521, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9083, Total reward=17.69, Steps=341552, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9084, Total reward=165.24, Steps=341684, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9085, Total reward=10.12, Steps=341712, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9086, Total reward=18.27, Steps=341738, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9087, Total reward=24.51, Steps=341768, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9088, Total reward=64.01, Steps=341801, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9089, Total reward=44.41, Steps=341835, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9090, Total reward=44.84, Steps=341855, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9091, Total reward=45.77, Steps=341883, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9092, Total reward=56.29, Steps=341915, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9093, Total reward=23.21, Steps=341939, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9094, Total reward=48.94, Steps=341961, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9095, Total reward=16.39, Steps=341975, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9096, Total reward=28.09, Steps=341993, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9097, Total reward=46.02, Steps=342030, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9098, Total reward=38.53, Steps=342054, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9099, Total reward=84.76, Steps=342130, Training iteration=181
Training> Name=main_level/agent, Worker=0, Episode=9100, Total reward=165.09, Steps=342258, Training iteration=181
Policy training> Surrogate loss=-0.0013046152889728546, KL divergence=0.00013355506234802306, Entropy=0.4767720699310303, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.035390764474868774, KL divergence=0.006455749273300171, Entropy=0.479092001914978, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.051154665648937225, KL divergence=0.0205874964594841, Entropy=0.46620991826057434, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.055086418986320496, KL divergence=0.03367874026298523, Entropy=0.4584745764732361, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06933353841304779, KL divergence=0.045054443180561066, Entropy=0.45513391494750977, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06721851229667664, KL divergence=0.053453229367733, Entropy=0.453644335269928, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06680522114038467, KL divergence=0.06366709619760513, Entropy=0.4511263370513916, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08015391230583191, KL divergence=0.07055667042732239, Entropy=0.45609259605407715, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07325328141450882, KL divergence=0.07431067526340485, Entropy=0.4551374912261963, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06645961850881577, KL divergence=0.08087372034788132, Entropy=0.45983120799064636, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/474_Step-342258.ckpt']
Uploaded 3 files for checkpoint 474 in 0.66 seconds
saved intermediate frozen graph: current/model/model_474.pb
Best checkpoint number: 427, Last checkpoint number: 472
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'471'}
Training> Name=main_level/agent, Worker=0, Episode=9101, Total reward=67.2, Steps=342294, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9102, Total reward=41.36, Steps=342316, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9103, Total reward=9.73, Steps=342336, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9104, Total reward=65.71, Steps=342423, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9105, Total reward=102.21, Steps=342534, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9106, Total reward=54.47, Steps=342583, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9107, Total reward=65.83, Steps=342624, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9108, Total reward=71.07, Steps=342671, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9109, Total reward=52.18, Steps=342717, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9110, Total reward=23.23, Steps=342749, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9111, Total reward=58.74, Steps=342776, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9112, Total reward=67.78, Steps=342816, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9113, Total reward=45.32, Steps=342847, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9114, Total reward=40.43, Steps=342869, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9115, Total reward=15.94, Steps=342893, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9116, Total reward=22.39, Steps=342926, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9117, Total reward=86.34, Steps=343017, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9118, Total reward=43.78, Steps=343041, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9119, Total reward=75.63, Steps=343102, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9120, Total reward=15.38, Steps=343115, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9121, Total reward=66.93, Steps=343172, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9122, Total reward=163.72, Steps=343309, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9123, Total reward=14.53, Steps=343335, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9124, Total reward=65.52, Steps=343413, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9125, Total reward=125.49, Steps=343529, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9126, Total reward=79.34, Steps=343596, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9127, Total reward=0.01, Steps=343608, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9128, Total reward=61.01, Steps=343638, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9129, Total reward=28.72, Steps=343659, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9130, Total reward=89.79, Steps=343707, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9131, Total reward=14.12, Steps=343741, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9132, Total reward=71.01, Steps=343783, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9133, Total reward=39.12, Steps=343812, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9134, Total reward=46.72, Steps=343834, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9135, Total reward=42.66, Steps=343863, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9136, Total reward=31.99, Steps=343901, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9137, Total reward=115.03, Steps=343984, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9138, Total reward=109.89, Steps=344047, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9139, Total reward=77.17, Steps=344102, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9140, Total reward=19.01, Steps=344118, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9141, Total reward=62.9, Steps=344152, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9142, Total reward=36.17, Steps=344188, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9143, Total reward=85.83, Steps=344283, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9144, Total reward=12.11, Steps=344311, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9145, Total reward=60.13, Steps=344371, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9146, Total reward=133.77, Steps=344477, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9147, Total reward=145.08, Steps=344566, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9148, Total reward=72.67, Steps=344605, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9149, Total reward=14.93, Steps=344619, Training iteration=182
Training> Name=main_level/agent, Worker=0, Episode=9150, Total reward=49.64, Steps=344656, Training iteration=182
Policy training> Surrogate loss=0.007817831821739674, KL divergence=0.00012099288869649172, Entropy=0.4874923825263977, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03298138827085495, KL divergence=0.006259642541408539, Entropy=0.4871962070465088, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05880407989025116, KL divergence=0.02020076848566532, Entropy=0.48396363854408264, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05018053948879242, KL divergence=0.03175551816821098, Entropy=0.4770505726337433, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0551687553524971, KL divergence=0.04120209440588951, Entropy=0.4813610017299652, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07048535346984863, KL divergence=0.05698266625404358, Entropy=0.48072218894958496, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06801889836788177, KL divergence=0.06287965178489685, Entropy=0.4802703857421875, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07011709362268448, KL divergence=0.07312370836734772, Entropy=0.47626686096191406, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06636208295822144, KL divergence=0.07531482726335526, Entropy=0.47182661294937134, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08763455599546432, KL divergence=0.0801825299859047, Entropy=0.4660593569278717, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/475_Step-344656.ckpt']
Uploaded 3 files for checkpoint 475 in 0.61 seconds
saved intermediate frozen graph: current/model/model_475.pb
Best checkpoint number: 427, Last checkpoint number: 473
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'472'}
Training> Name=main_level/agent, Worker=0, Episode=9151, Total reward=49.85, Steps=344691, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9152, Total reward=60.75, Steps=344735, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9153, Total reward=44.37, Steps=344766, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9154, Total reward=38.61, Steps=344788, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9155, Total reward=11.78, Steps=344818, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9156, Total reward=54.28, Steps=344881, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9157, Total reward=92.34, Steps=344961, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9158, Total reward=38.15, Steps=344995, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9159, Total reward=15.28, Steps=345016, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9160, Total reward=160.71, Steps=345187, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9161, Total reward=68.66, Steps=345219, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9162, Total reward=57.29, Steps=345262, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9163, Total reward=14.73, Steps=345302, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9164, Total reward=75.59, Steps=345374, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9165, Total reward=119.7, Steps=345486, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9166, Total reward=7.11, Steps=345504, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9167, Total reward=55.44, Steps=345545, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9168, Total reward=76.46, Steps=345601, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9169, Total reward=92.14, Steps=345677, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9170, Total reward=86.25, Steps=345728, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9171, Total reward=76.44, Steps=345781, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9172, Total reward=70.57, Steps=345823, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9173, Total reward=52.47, Steps=345854, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9174, Total reward=41.34, Steps=345876, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9175, Total reward=13.74, Steps=345906, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9176, Total reward=33.5, Steps=345934, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9177, Total reward=47.05, Steps=346003, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9178, Total reward=37.95, Steps=346029, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9179, Total reward=155.68, Steps=346161, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9180, Total reward=70.87, Steps=346204, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9181, Total reward=59.79, Steps=346235, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9182, Total reward=48.52, Steps=346269, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9183, Total reward=79.79, Steps=346348, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9184, Total reward=11.06, Steps=346371, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9185, Total reward=11.72, Steps=346393, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9186, Total reward=96.8, Steps=346472, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9187, Total reward=62.7, Steps=346537, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9188, Total reward=138.34, Steps=346622, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9189, Total reward=18.79, Steps=346643, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9190, Total reward=49.74, Steps=346683, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9191, Total reward=18.56, Steps=346704, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9192, Total reward=35.69, Steps=346736, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9193, Total reward=56.27, Steps=346767, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9194, Total reward=48.79, Steps=346790, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9195, Total reward=25.89, Steps=346809, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9196, Total reward=23.04, Steps=346827, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9197, Total reward=118.14, Steps=346907, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9198, Total reward=104.14, Steps=346974, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9199, Total reward=67.68, Steps=347064, Training iteration=183
Training> Name=main_level/agent, Worker=0, Episode=9200, Total reward=40.78, Steps=347104, Training iteration=183
Policy training> Surrogate loss=-0.004729690961539745, KL divergence=0.00024586747167631984, Entropy=0.49979084730148315, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.039889950305223465, KL divergence=0.00698217935860157, Entropy=0.4933336675167084, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05637292563915253, KL divergence=0.0209640059620142, Entropy=0.4801899194717407, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05753207951784134, KL divergence=0.03622828423976898, Entropy=0.4715383052825928, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.056767649948596954, KL divergence=0.05212818831205368, Entropy=0.46998876333236694, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08817733824253082, KL divergence=0.05864512547850609, Entropy=0.4602571129798889, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07295285165309906, KL divergence=0.06597859412431717, Entropy=0.46855661273002625, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06398892402648926, KL divergence=0.0725230872631073, Entropy=0.46889728307724, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07502550631761551, KL divergence=0.07853542268276215, Entropy=0.47011101245880127, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06773239374160767, KL divergence=0.08381408452987671, Entropy=0.46991056203842163, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/476_Step-347104.ckpt']
Uploaded 3 files for checkpoint 476 in 0.62 seconds
saved intermediate frozen graph: current/model/model_476.pb
Best checkpoint number: 427, Last checkpoint number: 474
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'473'}
Training> Name=main_level/agent, Worker=0, Episode=9201, Total reward=51.72, Steps=347166, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9202, Total reward=36.2, Steps=347208, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9203, Total reward=67.24, Steps=347299, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9204, Total reward=15.29, Steps=347347, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9205, Total reward=118.65, Steps=347464, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9206, Total reward=7.16, Steps=347477, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9207, Total reward=89.87, Steps=347562, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9208, Total reward=64.08, Steps=347603, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9209, Total reward=89.62, Steps=347677, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9210, Total reward=69.75, Steps=347724, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9211, Total reward=100.02, Steps=347773, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9212, Total reward=49.43, Steps=347817, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9213, Total reward=50.32, Steps=347847, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9214, Total reward=28.7, Steps=347858, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9215, Total reward=30.5, Steps=347886, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9216, Total reward=25.23, Steps=347905, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9217, Total reward=57.08, Steps=347967, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9218, Total reward=94.66, Steps=348027, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9219, Total reward=9.57, Steps=348047, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9220, Total reward=66.68, Steps=348092, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9221, Total reward=61.87, Steps=348122, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9222, Total reward=44.43, Steps=348174, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9223, Total reward=27.04, Steps=348213, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9224, Total reward=8.8, Steps=348243, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9225, Total reward=84.62, Steps=348306, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9226, Total reward=114.27, Steps=348401, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9227, Total reward=77.82, Steps=348473, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9228, Total reward=57.73, Steps=348502, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9229, Total reward=75.24, Steps=348551, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9230, Total reward=112.0, Steps=348600, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9231, Total reward=50.03, Steps=348643, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9232, Total reward=58.28, Steps=348674, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9233, Total reward=53.58, Steps=348705, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9234, Total reward=40.2, Steps=348726, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9235, Total reward=21.41, Steps=348751, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9236, Total reward=21.56, Steps=348784, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9237, Total reward=102.18, Steps=348871, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9238, Total reward=42.28, Steps=348894, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9239, Total reward=75.77, Steps=348963, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9240, Total reward=19.12, Steps=348982, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9241, Total reward=62.61, Steps=349012, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9242, Total reward=41.62, Steps=349039, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9243, Total reward=10.39, Steps=349058, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9244, Total reward=23.54, Steps=349092, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9245, Total reward=60.08, Steps=349157, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9246, Total reward=125.94, Steps=349262, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9247, Total reward=77.03, Steps=349318, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9248, Total reward=50.01, Steps=349356, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9249, Total reward=20.56, Steps=349373, Training iteration=184
Training> Name=main_level/agent, Worker=0, Episode=9250, Total reward=63.53, Steps=349409, Training iteration=184
Policy training> Surrogate loss=-0.004821978509426117, KL divergence=0.00029380686464719474, Entropy=0.49392324686050415, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.025696508586406708, KL divergence=0.008555199019610882, Entropy=0.48407042026519775, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03314371779561043, KL divergence=0.02275051735341549, Entropy=0.4789784848690033, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06420547515153885, KL divergence=0.03858889639377594, Entropy=0.4727962613105774, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.051112107932567596, KL divergence=0.0505538135766983, Entropy=0.4760286211967468, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08062884211540222, KL divergence=0.059439778327941895, Entropy=0.4727414846420288, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07707689702510834, KL divergence=0.06936979293823242, Entropy=0.4683418571949005, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07154496014118195, KL divergence=0.07359205186367035, Entropy=0.4694632887840271, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0800728052854538, KL divergence=0.07950811088085175, Entropy=0.4726369082927704, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07975038141012192, KL divergence=0.08122256398200989, Entropy=0.4634537696838379, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/477_Step-349409.ckpt']
Uploaded 3 files for checkpoint 477 in 0.53 seconds
saved intermediate frozen graph: current/model/model_477.pb
Best checkpoint number: 427, Last checkpoint number: 475
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'474'}
Training> Name=main_level/agent, Worker=0, Episode=9251, Total reward=40.93, Steps=349440, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9252, Total reward=65.29, Steps=349482, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9253, Total reward=52.84, Steps=349514, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9254, Total reward=37.18, Steps=349535, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9255, Total reward=23.41, Steps=349560, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9256, Total reward=23.29, Steps=349580, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9257, Total reward=49.69, Steps=349649, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9258, Total reward=105.85, Steps=349740, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9259, Total reward=103.89, Steps=349870, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9260, Total reward=79.57, Steps=349912, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9261, Total reward=66.16, Steps=349954, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9262, Total reward=38.16, Steps=349975, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9263, Total reward=7.19, Steps=349996, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9264, Total reward=2.15, Steps=350015, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9265, Total reward=71.72, Steps=350078, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9266, Total reward=129.69, Steps=350180, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9267, Total reward=63.21, Steps=350224, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9268, Total reward=74.73, Steps=350279, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9269, Total reward=17.4, Steps=350297, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9270, Total reward=19.11, Steps=350332, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9271, Total reward=67.98, Steps=350376, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9272, Total reward=54.58, Steps=350409, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9273, Total reward=60.03, Steps=350439, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9274, Total reward=52.63, Steps=350462, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9275, Total reward=24.06, Steps=350493, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9276, Total reward=23.73, Steps=350514, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9277, Total reward=43.78, Steps=350570, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9278, Total reward=91.81, Steps=350634, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9279, Total reward=12.74, Steps=350667, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9280, Total reward=60.87, Steps=350710, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9281, Total reward=114.54, Steps=350817, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9282, Total reward=35.41, Steps=350842, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9283, Total reward=143.48, Steps=350986, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9284, Total reward=88.45, Steps=351070, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9285, Total reward=23.29, Steps=351103, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9286, Total reward=69.6, Steps=351152, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9287, Total reward=73.61, Steps=351218, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9288, Total reward=71.51, Steps=351273, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9289, Total reward=9.3, Steps=351285, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9290, Total reward=26.43, Steps=351306, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9291, Total reward=11.37, Steps=351319, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9292, Total reward=72.69, Steps=351361, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9293, Total reward=41.85, Steps=351392, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9294, Total reward=45.42, Steps=351413, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9295, Total reward=19.0, Steps=351437, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9296, Total reward=64.45, Steps=351509, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9297, Total reward=107.81, Steps=351590, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9298, Total reward=32.47, Steps=351615, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9299, Total reward=54.0, Steps=351677, Training iteration=185
Training> Name=main_level/agent, Worker=0, Episode=9300, Total reward=84.34, Steps=351736, Training iteration=185
Policy training> Surrogate loss=5.3058378398418427e-05, KL divergence=0.00021374317293521017, Entropy=0.5015555024147034, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.032981500029563904, KL divergence=0.006291329860687256, Entropy=0.5001037120819092, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.051591310650110245, KL divergence=0.019782772287726402, Entropy=0.4947105050086975, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07039814442396164, KL divergence=0.033610664308071136, Entropy=0.4831680655479431, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0639900341629982, KL divergence=0.044650718569755554, Entropy=0.4809059500694275, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07673068344593048, KL divergence=0.057527847588062286, Entropy=0.4796590805053711, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07141688466072083, KL divergence=0.0667632669210434, Entropy=0.4761718511581421, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08083172142505646, KL divergence=0.07186728715896606, Entropy=0.47699835896492004, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06325800716876984, KL divergence=0.07888360321521759, Entropy=0.4811856150627136, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07428909838199615, KL divergence=0.08325910568237305, Entropy=0.47858285903930664, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/478_Step-351736.ckpt']
Uploaded 3 files for checkpoint 478 in 0.60 seconds
saved intermediate frozen graph: current/model/model_478.pb
Best checkpoint number: 427, Last checkpoint number: 476
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'475'}
Training> Name=main_level/agent, Worker=0, Episode=9301, Total reward=54.3, Steps=351776, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9302, Total reward=37.58, Steps=351796, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9303, Total reward=29.18, Steps=351834, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9304, Total reward=77.3, Steps=351905, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9305, Total reward=75.96, Steps=351973, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9306, Total reward=75.51, Steps=352023, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9307, Total reward=72.87, Steps=352067, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9308, Total reward=114.73, Steps=352153, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9309, Total reward=21.1, Steps=352172, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9310, Total reward=70.05, Steps=352207, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9311, Total reward=70.99, Steps=352246, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9312, Total reward=32.55, Steps=352277, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9313, Total reward=0.01, Steps=352291, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9314, Total reward=42.89, Steps=352313, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9315, Total reward=16.05, Steps=352339, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9316, Total reward=22.42, Steps=352358, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9317, Total reward=27.8, Steps=352402, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9318, Total reward=37.24, Steps=352430, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9319, Total reward=89.8, Steps=352518, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9320, Total reward=7.76, Steps=352531, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9321, Total reward=41.04, Steps=352551, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9322, Total reward=29.49, Steps=352573, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9323, Total reward=24.38, Steps=352604, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9324, Total reward=13.84, Steps=352652, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9325, Total reward=130.0, Steps=352771, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9326, Total reward=19.17, Steps=352806, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9327, Total reward=108.58, Steps=352884, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9328, Total reward=69.19, Steps=352914, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9329, Total reward=24.26, Steps=352942, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9330, Total reward=90.5, Steps=352991, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9331, Total reward=81.63, Steps=353033, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9332, Total reward=69.38, Steps=353076, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9333, Total reward=41.74, Steps=353098, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9334, Total reward=44.31, Steps=353119, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9335, Total reward=26.51, Steps=353147, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9336, Total reward=29.95, Steps=353173, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9337, Total reward=50.71, Steps=353212, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9338, Total reward=40.08, Steps=353239, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9339, Total reward=48.06, Steps=353328, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9340, Total reward=102.59, Steps=353416, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9341, Total reward=61.8, Steps=353451, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9342, Total reward=45.11, Steps=353492, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9343, Total reward=15.18, Steps=353541, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9344, Total reward=47.13, Steps=353611, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9345, Total reward=20.34, Steps=353633, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9346, Total reward=65.37, Steps=353689, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9347, Total reward=46.69, Steps=353720, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9348, Total reward=64.83, Steps=353750, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9349, Total reward=26.27, Steps=353767, Training iteration=186
Training> Name=main_level/agent, Worker=0, Episode=9350, Total reward=36.0, Steps=353817, Training iteration=186
Policy training> Surrogate loss=0.0022008661180734634, KL divergence=0.00028801962616853416, Entropy=0.4972460865974426, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03309135511517525, KL divergence=0.006046026013791561, Entropy=0.49669700860977173, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.052061568945646286, KL divergence=0.018186114728450775, Entropy=0.4945496916770935, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05910136178135872, KL divergence=0.03130520135164261, Entropy=0.4903109669685364, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06619804352521896, KL divergence=0.04546977952122688, Entropy=0.4874269664287567, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07210230827331543, KL divergence=0.05619797483086586, Entropy=0.48187416791915894, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0758662074804306, KL divergence=0.06572140008211136, Entropy=0.481516033411026, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07835093140602112, KL divergence=0.07266014069318771, Entropy=0.4790504276752472, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07499668002128601, KL divergence=0.07786780595779419, Entropy=0.48020368814468384, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08040584623813629, KL divergence=0.08253788948059082, Entropy=0.4793669581413269, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/479_Step-353817.ckpt']
Uploaded 3 files for checkpoint 479 in 0.51 seconds
saved intermediate frozen graph: current/model/model_479.pb
Best checkpoint number: 427, Last checkpoint number: 477
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'476'}
Training> Name=main_level/agent, Worker=0, Episode=9351, Total reward=75.09, Steps=353866, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9352, Total reward=73.25, Steps=353906, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9353, Total reward=51.54, Steps=353937, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9354, Total reward=30.18, Steps=353948, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9355, Total reward=18.79, Steps=353992, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9356, Total reward=27.97, Steps=354012, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9357, Total reward=54.39, Steps=354059, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9358, Total reward=85.67, Steps=354156, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9359, Total reward=79.05, Steps=354218, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9360, Total reward=151.64, Steps=354399, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9361, Total reward=48.01, Steps=354426, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9362, Total reward=55.63, Steps=354473, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9363, Total reward=63.83, Steps=354554, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9364, Total reward=119.93, Steps=354662, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9365, Total reward=23.34, Steps=354686, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9366, Total reward=72.94, Steps=354759, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9367, Total reward=125.0, Steps=354845, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9368, Total reward=66.48, Steps=354891, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9369, Total reward=27.3, Steps=354915, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9370, Total reward=88.02, Steps=354971, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9371, Total reward=84.63, Steps=355021, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9372, Total reward=65.31, Steps=355063, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9373, Total reward=68.15, Steps=355095, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9374, Total reward=46.11, Steps=355117, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9375, Total reward=35.84, Steps=355147, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9376, Total reward=36.08, Steps=355181, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9377, Total reward=97.97, Steps=355259, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9378, Total reward=39.28, Steps=355308, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9379, Total reward=14.17, Steps=355335, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9380, Total reward=70.1, Steps=355379, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9381, Total reward=34.87, Steps=355407, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9382, Total reward=39.14, Steps=355435, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9383, Total reward=14.03, Steps=355456, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9384, Total reward=19.97, Steps=355492, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9385, Total reward=69.96, Steps=355583, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9386, Total reward=7.04, Steps=355596, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9387, Total reward=42.96, Steps=355625, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9388, Total reward=59.8, Steps=355653, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9389, Total reward=28.09, Steps=355670, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9390, Total reward=107.34, Steps=355732, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9391, Total reward=74.94, Steps=355771, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9392, Total reward=63.55, Steps=355813, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9393, Total reward=54.68, Steps=355843, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9394, Total reward=43.16, Steps=355863, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9395, Total reward=30.51, Steps=355893, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9396, Total reward=29.77, Steps=355926, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9397, Total reward=49.67, Steps=355978, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9398, Total reward=35.78, Steps=356016, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9399, Total reward=7.16, Steps=356034, Training iteration=187
Training> Name=main_level/agent, Worker=0, Episode=9400, Total reward=89.19, Steps=356084, Training iteration=187
Policy training> Surrogate loss=-0.011490246281027794, KL divergence=0.0002720909542404115, Entropy=0.49013227224349976, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03457395359873772, KL divergence=0.007107974961400032, Entropy=0.48552700877189636, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04615313559770584, KL divergence=0.020339779555797577, Entropy=0.47427940368652344, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05974823236465454, KL divergence=0.034370679408311844, Entropy=0.4756114184856415, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0697946548461914, KL divergence=0.043828047811985016, Entropy=0.4680955410003662, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06344639509916306, KL divergence=0.05798264592885971, Entropy=0.46709975600242615, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08161875605583191, KL divergence=0.0635029524564743, Entropy=0.45800647139549255, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07921183109283447, KL divergence=0.07124525308609009, Entropy=0.46667489409446716, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07172724604606628, KL divergence=0.07533188164234161, Entropy=0.46540123224258423, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07011482119560242, KL divergence=0.07921047508716583, Entropy=0.4636894464492798, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/480_Step-356084.ckpt']
Uploaded 3 files for checkpoint 480 in 0.60 seconds
saved intermediate frozen graph: current/model/model_480.pb
Best checkpoint number: 427, Last checkpoint number: 478
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'477'}
Training> Name=main_level/agent, Worker=0, Episode=9401, Total reward=55.2, Steps=356117, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9402, Total reward=35.5, Steps=356145, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9403, Total reward=6.96, Steps=356180, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9404, Total reward=23.31, Steps=356205, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9405, Total reward=16.17, Steps=356230, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9406, Total reward=33.07, Steps=356265, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9407, Total reward=130.92, Steps=356356, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9408, Total reward=69.67, Steps=356393, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9409, Total reward=18.95, Steps=356408, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9410, Total reward=41.64, Steps=356438, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9411, Total reward=63.53, Steps=356478, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9412, Total reward=32.1, Steps=356507, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9413, Total reward=44.51, Steps=356538, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9414, Total reward=36.02, Steps=356558, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9415, Total reward=20.75, Steps=356586, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9416, Total reward=16.21, Steps=356612, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9417, Total reward=7.75, Steps=356623, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9418, Total reward=45.61, Steps=356666, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9419, Total reward=5.82, Steps=356684, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9420, Total reward=69.98, Steps=356727, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9421, Total reward=41.05, Steps=356763, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9422, Total reward=34.47, Steps=356782, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9423, Total reward=17.85, Steps=356818, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9424, Total reward=61.25, Steps=356921, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9425, Total reward=9.9, Steps=356945, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9426, Total reward=85.7, Steps=357013, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9427, Total reward=103.53, Steps=357096, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9428, Total reward=88.33, Steps=357198, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9429, Total reward=26.01, Steps=357234, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9430, Total reward=58.41, Steps=357286, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9431, Total reward=32.94, Steps=357315, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9432, Total reward=55.31, Steps=357349, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9433, Total reward=66.54, Steps=357380, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9434, Total reward=27.55, Steps=357400, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9435, Total reward=17.34, Steps=357431, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9436, Total reward=18.4, Steps=357451, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9437, Total reward=66.55, Steps=357502, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9438, Total reward=40.52, Steps=357523, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9439, Total reward=26.8, Steps=357572, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9440, Total reward=75.0, Steps=357610, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9441, Total reward=99.47, Steps=357677, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9442, Total reward=37.16, Steps=357701, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9443, Total reward=10.89, Steps=357738, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9444, Total reward=24.66, Steps=357771, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9445, Total reward=67.1, Steps=357838, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9446, Total reward=79.89, Steps=357898, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9447, Total reward=0.01, Steps=357907, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9448, Total reward=60.42, Steps=357934, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9449, Total reward=39.5, Steps=357962, Training iteration=188
Training> Name=main_level/agent, Worker=0, Episode=9450, Total reward=32.25, Steps=358003, Training iteration=188
Policy training> Surrogate loss=0.01746748946607113, KL divergence=0.00016717730613891035, Entropy=0.5006082653999329, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03127540275454521, KL divergence=0.0047394149005413055, Entropy=0.5071409344673157, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05458421632647514, KL divergence=0.01374848186969757, Entropy=0.49867430329322815, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07115871459245682, KL divergence=0.02725214697420597, Entropy=0.5004753470420837, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05298799276351929, KL divergence=0.04243822768330574, Entropy=0.48788997530937195, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06492391228675842, KL divergence=0.054833997040987015, Entropy=0.48560985922813416, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0929061695933342, KL divergence=0.06301841139793396, Entropy=0.46096882224082947, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0755889043211937, KL divergence=0.07593421638011932, Entropy=0.478243350982666, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08504170179367065, KL divergence=0.07870867848396301, Entropy=0.47673821449279785, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07493993639945984, KL divergence=0.08899575471878052, Entropy=0.47577324509620667, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/481_Step-358003.ckpt']
Uploaded 3 files for checkpoint 481 in 0.54 seconds
saved intermediate frozen graph: current/model/model_481.pb
Best checkpoint number: 427, Last checkpoint number: 479
Copying the frozen checkpoint from ./frozen_models/agent/model_427.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'478'}
Training> Name=main_level/agent, Worker=0, Episode=9451, Total reward=45.33, Steps=358030, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9452, Total reward=50.95, Steps=358063, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9453, Total reward=50.57, Steps=358093, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9454, Total reward=40.37, Steps=358114, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9455, Total reward=22.84, Steps=358129, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9456, Total reward=24.19, Steps=358152, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9457, Total reward=52.46, Steps=358207, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9458, Total reward=38.51, Steps=358251, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9459, Total reward=2.72, Steps=358287, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9460, Total reward=62.88, Steps=358329, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9461, Total reward=153.16, Steps=358438, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9462, Total reward=32.29, Steps=358457, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9463, Total reward=20.58, Steps=358518, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9464, Total reward=11.71, Steps=358551, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9465, Total reward=20.35, Steps=358572, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9466, Total reward=85.48, Steps=358623, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9467, Total reward=63.18, Steps=358663, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9468, Total reward=69.4, Steps=358713, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9469, Total reward=20.89, Steps=358737, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9470, Total reward=70.07, Steps=358774, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9471, Total reward=56.98, Steps=358820, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9472, Total reward=56.59, Steps=358864, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9473, Total reward=30.64, Steps=358885, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9474, Total reward=48.84, Steps=358907, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9475, Total reward=18.59, Steps=358934, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9476, Total reward=25.81, Steps=358953, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9477, Total reward=47.04, Steps=359013, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9478, Total reward=39.92, Steps=359059, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9479, Total reward=48.76, Steps=359130, Training iteration=189
Training> Name=main_level/agent, Worker=0, Episode=9480, Total reward=26.07, Steps=359161, Training iteration=189
