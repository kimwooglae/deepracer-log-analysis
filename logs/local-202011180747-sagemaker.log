21:C 17 Nov 2020 05:51:50.837 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
21:C 17 Nov 2020 05:51:50.837 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=21, just started
21:C 17 Nov 2020 05:51:50.837 # Configuration loaded
                _._                                                  
           _.-``__ ''-._                                             
      _.-``    `.  `_.  ''-._           Redis 5.0.8 (00000000/0) 64 bit
  .-`` .-```.  ```\/    _.,_ ''-._                                   
 (    '      ,       .-`  | `,    )     Running in standalone mode
 |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
 |    `-._   `._    /     _.-'    |     PID: 21
  `-._    `-._  `-./  _.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |           http://redis.io        
  `-._    `-._`-.__.-'_.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |                                  
  `-._    `-._`-.__.-'_.-'    _.-'                                   
      `-._    `-.__.-'    _.-'                                       
          `-._        _.-'                                           
              `-.__.-'                                               

21:M 17 Nov 2020 05:51:50.838 # WARNING: The TCP backlog setting of 512 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
21:M 17 Nov 2020 05:51:50.838 # Server initialized
21:M 17 Nov 2020 05:51:50.838 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
21:M 17 Nov 2020 05:51:50.838 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
21:M 17 Nov 2020 05:51:50.839 * Ready to accept connections
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2020-11-17 05:51:52,978 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training
2020-11-17 05:51:53,250 sagemaker-containers INFO     Invoking user script

Training Env:

{
    "additional_framework_parameters": {
        "sagemaker_estimator": "RLEstimator"
    },
    "channel_input_dirs": {},
    "current_host": "algo-1-l7kj8",
    "framework_module": "sagemaker_tensorflow_container.training:main",
    "hosts": [
        "algo-1-l7kj8"
    ],
    "hyperparameters": {
        "s3_bucket": "bucket",
        "s3_prefix": "current",
        "aws_region": "us-east-1",
        "model_metadata_s3_key": "s3://bucket/custom_files/model_metadata.json",
        "RLCOACH_PRESET": "deepracer",
        "batch_size": 256,
        "beta_entropy": 0.01,
        "discount_factor": 0.9995,
        "e_greedy_value": 0.05,
        "epsilon_steps": 10000,
        "exploration_type": "categorical",
        "loss_type": "huber",
        "lr": 5e-05,
        "num_episodes_between_training": 50,
        "num_epochs": 10,
        "stack_size": 1,
        "term_cond_avg_score": 100000.0,
        "term_cond_max_episodes": 10000,
        "pretrained_s3_bucket": "bucket",
        "pretrained_s3_prefix": "rl-deepracer-pretrained"
    },
    "input_config_dir": "/opt/ml/input/config",
    "input_data_config": {},
    "input_dir": "/opt/ml/input",
    "is_master": true,
    "job_name": "current",
    "log_level": 20,
    "master_hostname": "algo-1-l7kj8",
    "model_dir": "/opt/ml/model",
    "module_dir": "s3://bucket/current/source/sourcedir.tar.gz",
    "module_name": "training_worker",
    "network_interface_name": "eth0",
    "num_cpus": 16,
    "num_gpus": 3,
    "output_data_dir": "/opt/ml/output/data",
    "output_dir": "/opt/ml/output",
    "output_intermediate_dir": "/opt/ml/output/intermediate",
    "resource_config": {
        "current_host": "algo-1-l7kj8",
        "hosts": [
            "algo-1-l7kj8"
        ]
    },
    "user_entry_point": "training_worker.py"
}

Environment variables:

SM_HOSTS=["algo-1-l7kj8"]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":256,"beta_entropy":0.01,"discount_factor":0.9995,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"huber","lr":5e-05,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":50,"num_epochs":10,"pretrained_s3_bucket":"bucket","pretrained_s3_prefix":"rl-deepracer-pretrained","s3_bucket":"bucket","s3_prefix":"current","stack_size":1,"term_cond_avg_score":100000.0,"term_cond_max_episodes":10000}
SM_USER_ENTRY_POINT=training_worker.py
SM_FRAMEWORK_PARAMS={"sagemaker_estimator":"RLEstimator"}
SM_RESOURCE_CONFIG={"current_host":"algo-1-l7kj8","hosts":["algo-1-l7kj8"]}
SM_INPUT_DATA_CONFIG={}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=[]
SM_CURRENT_HOST=algo-1-l7kj8
SM_MODULE_NAME=training_worker
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=16
SM_NUM_GPUS=3
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://bucket/current/source/sourcedir.tar.gz
SM_TRAINING_ENV={"additional_framework_parameters":{"sagemaker_estimator":"RLEstimator"},"channel_input_dirs":{},"current_host":"algo-1-l7kj8","framework_module":"sagemaker_tensorflow_container.training:main","hosts":["algo-1-l7kj8"],"hyperparameters":{"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":256,"beta_entropy":0.01,"discount_factor":0.9995,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"huber","lr":5e-05,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":50,"num_epochs":10,"pretrained_s3_bucket":"bucket","pretrained_s3_prefix":"rl-deepracer-pretrained","s3_bucket":"bucket","s3_prefix":"current","stack_size":1,"term_cond_avg_score":100000.0,"term_cond_max_episodes":10000},"input_config_dir":"/opt/ml/input/config","input_data_config":{},"input_dir":"/opt/ml/input","is_master":true,"job_name":"current","log_level":20,"master_hostname":"algo-1-l7kj8","model_dir":"/opt/ml/model","module_dir":"s3://bucket/current/source/sourcedir.tar.gz","module_name":"training_worker","network_interface_name":"eth0","num_cpus":16,"num_gpus":3,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1-l7kj8","hosts":["algo-1-l7kj8"]},"user_entry_point":"training_worker.py"}
SM_USER_ARGS=["--RLCOACH_PRESET","deepracer","--aws_region","us-east-1","--batch_size","256","--beta_entropy","0.01","--discount_factor","0.9995","--e_greedy_value","0.05","--epsilon_steps","10000","--exploration_type","categorical","--loss_type","huber","--lr","5e-05","--model_metadata_s3_key","s3://bucket/custom_files/model_metadata.json","--num_episodes_between_training","50","--num_epochs","10","--pretrained_s3_bucket","bucket","--pretrained_s3_prefix","rl-deepracer-pretrained","--s3_bucket","bucket","--s3_prefix","current","--stack_size","1","--term_cond_avg_score","100000.0","--term_cond_max_episodes","10000"]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_HP_S3_BUCKET=bucket
SM_HP_S3_PREFIX=current
SM_HP_AWS_REGION=us-east-1
SM_HP_MODEL_METADATA_S3_KEY=s3://bucket/custom_files/model_metadata.json
SM_HP_RLCOACH_PRESET=deepracer
SM_HP_BATCH_SIZE=256
SM_HP_BETA_ENTROPY=0.01
SM_HP_DISCOUNT_FACTOR=0.9995
SM_HP_E_GREEDY_VALUE=0.05
SM_HP_EPSILON_STEPS=10000
SM_HP_EXPLORATION_TYPE=categorical
SM_HP_LOSS_TYPE=huber
SM_HP_LR=5e-05
SM_HP_NUM_EPISODES_BETWEEN_TRAINING=50
SM_HP_NUM_EPOCHS=10
SM_HP_STACK_SIZE=1
SM_HP_TERM_COND_AVG_SCORE=100000.0
SM_HP_TERM_COND_MAX_EPISODES=10000
SM_HP_PRETRAINED_S3_BUCKET=bucket
SM_HP_PRETRAINED_S3_PREFIX=rl-deepracer-pretrained
PYTHONPATH=/usr/local/bin:/opt/amazon:/opt/ml/code:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages

Invoking script with the following command:

/usr/bin/python training_worker.py --RLCOACH_PRESET deepracer --aws_region us-east-1 --batch_size 256 --beta_entropy 0.01 --discount_factor 0.9995 --e_greedy_value 0.05 --epsilon_steps 10000 --exploration_type categorical --loss_type huber --lr 5e-05 --model_metadata_s3_key s3://bucket/custom_files/model_metadata.json --num_episodes_between_training 50 --num_epochs 10 --pretrained_s3_bucket bucket --pretrained_s3_prefix rl-deepracer-pretrained --s3_bucket bucket --s3_prefix current --stack_size 1 --term_cond_avg_score 100000.0 --term_cond_max_episodes 10000


S3 bucket: bucket 
 S3 prefix: current 
 S3 endpoint URL: http://minio:9000
Initializing SageS3Client...
Successfully downloaded model metadata from custom_files/model_metadata.json.
Sensor list ['STEREO_CAMERAS', 'SECTOR_LIDAR'], network DEEP_CONVOLUTIONAL_NETWORK_SHALLOW, simapp_version 2.0
Loaded action space from file: [{'steering_angle': -30.0, 'speed': 1.5, 'index': 0}, {'steering_angle': -12.7506, 'speed': 1.9704, 'index': 1}, {'steering_angle': -7.0501, 'speed': 2.8773, 'index': 2}, {'steering_angle': -0.6292, 'speed': 1.7831, 'index': 3}, {'steering_angle': -0.0239, 'speed': 3.9238, 'index': 4}, {'steering_angle': 0.875, 'speed': 3.3736, 'index': 5}, {'steering_angle': 6.1889, 'speed': 2.3195, 'index': 6}, {'steering_angle': 7.3838, 'speed': 2.8651, 'index': 7}, {'steering_angle': 13.6619, 'speed': 1.785, 'index': 8}, {'steering_angle': 30.0, 'speed': 1.5, 'index': 9}]
Using the following hyper-parameters
{
  "batch_size": 256,
  "beta_entropy": 0.01,
  "discount_factor": 0.9995,
  "e_greedy_value": 0.05,
  "epsilon_steps": 10000,
  "exploration_type": "categorical",
  "loss_type": "huber",
  "lr": 5e-05,
  "num_episodes_between_training": 50,
  "num_epochs": 10,
  "stack_size": 1,
  "term_cond_avg_score": 100000.0,
  "term_cond_max_episodes": 10000
}
Uploaded hyperparameters.json to S3
Uploaded IP address information to S3: 172.18.0.5
Sensor list ['STEREO_CAMERAS', 'SECTOR_LIDAR'], network DEEP_CONVOLUTIONAL_NETWORK_SHALLOW, simapp_version 2.0
Unable to find best model data, using last model
## Creating graph - name: MultiAgentGraphManager
## Start physics before creating graph
## Create graph
## Creating agent - name: agent
## Created agent: agent
## Stop physics after creating graph
## Creating session
Checkpoint> Restoring from path=./pretrained_checkpoint/144_Step-107572.ckpt
Checkpoint> Saving in path=['./checkpoint/145_Step-0.ckpt']
Uploaded 3 files for checkpoint 145 in 0.34 seconds
saved intermediate frozen graph: current/model/model_145.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_145.pb to /opt/ml/model/agent/model.pb.
Uploaded 3 files for checkpoint 145 in 0.53 seconds
saved intermediate frozen graph: current/model/model_145.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_145.pb to /opt/ml/model/agent/model.pb.
DoorMan: installing SIGINT, SIGTERM
Training> Name=main_level/agent, Worker=0, Episode=1, Total reward=33.1, Steps=23, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=2, Total reward=26.87, Steps=43, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=3, Total reward=0.02, Steps=58, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=4, Total reward=31.86, Steps=95, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=5, Total reward=6.77, Steps=120, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=6, Total reward=11.58, Steps=143, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=7, Total reward=45.25, Steps=188, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=8, Total reward=85.23, Steps=234, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=9, Total reward=111.09, Steps=330, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=10, Total reward=55.31, Steps=393, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=11, Total reward=18.95, Steps=413, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=12, Total reward=41.6, Steps=441, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=13, Total reward=60.34, Steps=478, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=14, Total reward=24.54, Steps=504, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=15, Total reward=133.61, Steps=627, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=16, Total reward=25.22, Steps=669, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=17, Total reward=88.24, Steps=796, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=18, Total reward=43.21, Steps=834, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=19, Total reward=72.35, Steps=901, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=20, Total reward=57.91, Steps=940, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=21, Total reward=43.16, Steps=977, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=22, Total reward=75.24, Steps=1047, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=23, Total reward=3.47, Steps=1068, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=24, Total reward=30.26, Steps=1087, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=25, Total reward=12.01, Steps=1109, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=26, Total reward=55.71, Steps=1154, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=27, Total reward=16.61, Steps=1177, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=28, Total reward=77.76, Steps=1213, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=29, Total reward=23.67, Steps=1245, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=30, Total reward=14.46, Steps=1259, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=31, Total reward=40.63, Steps=1315, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=32, Total reward=79.07, Steps=1369, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=33, Total reward=25.51, Steps=1415, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=34, Total reward=18.01, Steps=1434, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=35, Total reward=67.39, Steps=1481, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=36, Total reward=9.95, Steps=1500, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=37, Total reward=3.85, Steps=1519, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=38, Total reward=61.27, Steps=1557, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=39, Total reward=64.29, Steps=1626, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=40, Total reward=80.42, Steps=1708, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=41, Total reward=50.93, Steps=1757, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=42, Total reward=14.16, Steps=1774, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=43, Total reward=14.26, Steps=1797, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=44, Total reward=19.49, Steps=1819, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=45, Total reward=13.66, Steps=1843, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=46, Total reward=63.64, Steps=1923, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=47, Total reward=79.66, Steps=1983, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=48, Total reward=123.15, Steps=2062, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=49, Total reward=34.94, Steps=2101, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=50, Total reward=28.23, Steps=2147, Training iteration=0
Policy training> Surrogate loss=0.0025377878919243813, KL divergence=0.0008272852865047753, Entropy=0.8578132390975952, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.057032063603401184, KL divergence=0.006833503022789955, Entropy=0.8518633842468262, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0728798508644104, KL divergence=0.012774171307682991, Entropy=0.8504513502120972, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07708196341991425, KL divergence=0.018113937228918076, Entropy=0.8489387035369873, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09380995482206345, KL divergence=0.022540457546710968, Entropy=0.8506849408149719, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09075921028852463, KL divergence=0.02659646049141884, Entropy=0.8428224325180054, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10371153801679611, KL divergence=0.029700834304094315, Entropy=0.8444352746009827, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.1083965077996254, KL divergence=0.0326041541993618, Entropy=0.8441613912582397, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09713758528232574, KL divergence=0.03550595045089722, Entropy=0.8460538983345032, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10902099311351776, KL divergence=0.03725392371416092, Entropy=0.8485219478607178, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/146_Step-2147.ckpt']
Uploaded 3 files for checkpoint 146 in 0.70 seconds
saved intermediate frozen graph: current/model/model_146.pb
Best checkpoint number: 145, Last checkpoint number: 145
Copying the frozen checkpoint from ./frozen_models/agent/model_145.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=51, Total reward=3.96, Steps=2155, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=52, Total reward=47.28, Steps=2191, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=53, Total reward=55.19, Steps=2221, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=54, Total reward=33.11, Steps=2269, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=55, Total reward=22.82, Steps=2295, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=56, Total reward=94.24, Steps=2372, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=57, Total reward=19.24, Steps=2393, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=58, Total reward=55.55, Steps=2444, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=59, Total reward=29.58, Steps=2479, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=60, Total reward=47.5, Steps=2520, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=61, Total reward=41.1, Steps=2589, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=62, Total reward=33.05, Steps=2608, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=63, Total reward=29.96, Steps=2636, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=64, Total reward=15.13, Steps=2655, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=65, Total reward=0.03, Steps=2684, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=66, Total reward=111.19, Steps=2809, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=67, Total reward=87.96, Steps=2880, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=68, Total reward=152.24, Steps=2985, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=69, Total reward=26.04, Steps=3014, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=70, Total reward=25.06, Steps=3046, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=71, Total reward=11.41, Steps=3063, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=72, Total reward=24.44, Steps=3094, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=73, Total reward=44.23, Steps=3119, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=74, Total reward=24.79, Steps=3147, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=75, Total reward=30.52, Steps=3173, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=76, Total reward=28.88, Steps=3204, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=77, Total reward=46.68, Steps=3262, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=78, Total reward=24.5, Steps=3282, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=79, Total reward=25.9, Steps=3306, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=80, Total reward=52.92, Steps=3366, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=81, Total reward=66.83, Steps=3443, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=82, Total reward=68.79, Steps=3512, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=83, Total reward=47.87, Steps=3591, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=84, Total reward=8.71, Steps=3614, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=85, Total reward=18.3, Steps=3662, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=86, Total reward=15.2, Steps=3687, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=87, Total reward=63.01, Steps=3724, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=88, Total reward=89.0, Steps=3783, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=89, Total reward=33.95, Steps=3818, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=90, Total reward=18.73, Steps=3831, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=91, Total reward=32.21, Steps=3868, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=92, Total reward=63.11, Steps=3904, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=93, Total reward=46.9, Steps=3930, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=94, Total reward=39.69, Steps=3976, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=95, Total reward=10.5, Steps=3989, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=96, Total reward=5.72, Steps=3999, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=97, Total reward=40.66, Steps=4061, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=98, Total reward=37.69, Steps=4141, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=99, Total reward=54.01, Steps=4199, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=100, Total reward=55.33, Steps=4240, Training iteration=1
Policy training> Surrogate loss=-0.001231001690030098, KL divergence=0.00022151229495648295, Entropy=0.8346287608146667, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.05532383173704147, KL divergence=0.005464037414640188, Entropy=0.8282347917556763, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07470504939556122, KL divergence=0.014611620455980301, Entropy=0.8239608407020569, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08849097788333893, KL divergence=0.022651299834251404, Entropy=0.8194729089736938, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08902078866958618, KL divergence=0.02876611426472664, Entropy=0.8131733536720276, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0959448590874672, KL divergence=0.034043602645397186, Entropy=0.8123631477355957, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10302653163671494, KL divergence=0.03691990673542023, Entropy=0.8155488967895508, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09905007481575012, KL divergence=0.040821246802806854, Entropy=0.8138193488121033, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10343927890062332, KL divergence=0.042905375361442566, Entropy=0.8167388439178467, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10813876986503601, KL divergence=0.044961556792259216, Entropy=0.8141228556632996, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/147_Step-4240.ckpt']
Uploaded 3 files for checkpoint 147 in 0.62 seconds
saved intermediate frozen graph: current/model/model_147.pb
Best checkpoint number: 145, Last checkpoint number: 145
Copying the frozen checkpoint from ./frozen_models/agent/model_145.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=101, Total reward=59.88, Steps=4297, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=102, Total reward=37.47, Steps=4327, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=103, Total reward=0.03, Steps=4352, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=104, Total reward=10.09, Steps=4370, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=105, Total reward=17.16, Steps=4394, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=106, Total reward=18.86, Steps=4439, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=107, Total reward=37.83, Steps=4505, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=108, Total reward=85.33, Steps=4546, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=109, Total reward=63.2, Steps=4604, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=110, Total reward=50.72, Steps=4668, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=111, Total reward=52.61, Steps=4725, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=112, Total reward=68.36, Steps=4779, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=113, Total reward=38.68, Steps=4820, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=114, Total reward=41.12, Steps=4845, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=115, Total reward=52.99, Steps=4895, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=116, Total reward=61.17, Steps=4976, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=117, Total reward=42.98, Steps=5035, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=118, Total reward=20.62, Steps=5049, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=119, Total reward=44.72, Steps=5083, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=120, Total reward=50.31, Steps=5120, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=121, Total reward=46.22, Steps=5186, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=122, Total reward=38.79, Steps=5216, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=123, Total reward=0.01, Steps=5228, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=124, Total reward=7.66, Steps=5247, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=125, Total reward=18.36, Steps=5270, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=126, Total reward=17.74, Steps=5305, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=127, Total reward=174.87, Steps=5439, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=128, Total reward=83.0, Steps=5496, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=129, Total reward=26.53, Steps=5523, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=130, Total reward=14.99, Steps=5568, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=131, Total reward=33.12, Steps=5588, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=132, Total reward=69.37, Steps=5636, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=133, Total reward=12.99, Steps=5679, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=134, Total reward=35.3, Steps=5705, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=135, Total reward=9.62, Steps=5715, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=136, Total reward=27.08, Steps=5750, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=137, Total reward=7.26, Steps=5769, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=138, Total reward=25.04, Steps=5786, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=139, Total reward=36.58, Steps=5836, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=140, Total reward=58.12, Steps=5895, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=141, Total reward=0.0, Steps=5896, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=142, Total reward=30.45, Steps=5929, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=143, Total reward=21.15, Steps=5953, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=144, Total reward=7.13, Steps=5973, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=145, Total reward=1.44, Steps=6018, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=146, Total reward=25.32, Steps=6043, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=147, Total reward=29.22, Steps=6077, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=148, Total reward=60.27, Steps=6107, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=149, Total reward=61.93, Steps=6189, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=150, Total reward=7.46, Steps=6204, Training iteration=2
Policy training> Surrogate loss=0.006073786877095699, KL divergence=0.0002467723097652197, Entropy=0.8598548769950867, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04890650510787964, KL divergence=0.007288059685379267, Entropy=0.8632477521896362, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0818619653582573, KL divergence=0.018487166613340378, Entropy=0.8532009124755859, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09220601618289948, KL divergence=0.02872721664607525, Entropy=0.8424490094184875, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.10578083246946335, KL divergence=0.03643616661429405, Entropy=0.846570611000061, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10457583516836166, KL divergence=0.04189141467213631, Entropy=0.8388798832893372, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.1100555807352066, KL divergence=0.04648604616522789, Entropy=0.847335696220398, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.1192476898431778, KL divergence=0.050602108240127563, Entropy=0.8386358618736267, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09534215927124023, KL divergence=0.05286923423409462, Entropy=0.8409487009048462, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10989407449960709, KL divergence=0.05407271534204483, Entropy=0.8392791152000427, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/148_Step-6204.ckpt']
Uploaded 3 files for checkpoint 148 in 0.58 seconds
saved intermediate frozen graph: current/model/model_148.pb
Best checkpoint number: 145, Last checkpoint number: 146
Copying the frozen checkpoint from ./frozen_models/agent/model_145.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'146'}
Training> Name=main_level/agent, Worker=0, Episode=151, Total reward=101.6, Steps=6274, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=152, Total reward=42.9, Steps=6308, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=153, Total reward=54.5, Steps=6346, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=154, Total reward=26.61, Steps=6370, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=155, Total reward=50.45, Steps=6422, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=156, Total reward=96.69, Steps=6514, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=157, Total reward=24.0, Steps=6538, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=158, Total reward=22.11, Steps=6553, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=159, Total reward=19.76, Steps=6581, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=160, Total reward=80.33, Steps=6669, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=161, Total reward=51.58, Steps=6741, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=162, Total reward=24.57, Steps=6774, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=163, Total reward=16.27, Steps=6819, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=164, Total reward=7.45, Steps=6850, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=165, Total reward=8.4, Steps=6875, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=166, Total reward=18.85, Steps=6901, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=167, Total reward=75.65, Steps=6969, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=168, Total reward=31.34, Steps=6987, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=169, Total reward=114.18, Steps=7072, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=170, Total reward=15.41, Steps=7090, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=171, Total reward=25.29, Steps=7146, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=172, Total reward=50.89, Steps=7183, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=173, Total reward=53.68, Steps=7212, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=174, Total reward=39.16, Steps=7238, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=175, Total reward=63.0, Steps=7322, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=176, Total reward=47.11, Steps=7381, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=177, Total reward=13.03, Steps=7400, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=178, Total reward=26.15, Steps=7420, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=179, Total reward=31.93, Steps=7446, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=180, Total reward=91.59, Steps=7542, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=181, Total reward=34.26, Steps=7567, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=182, Total reward=54.85, Steps=7625, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=183, Total reward=7.28, Steps=7646, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=184, Total reward=23.69, Steps=7679, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=185, Total reward=13.41, Steps=7697, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=186, Total reward=111.76, Steps=7829, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=187, Total reward=48.58, Steps=7888, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=188, Total reward=102.43, Steps=7963, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=189, Total reward=26.75, Steps=8007, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=190, Total reward=11.35, Steps=8051, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=191, Total reward=75.52, Steps=8124, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=192, Total reward=62.44, Steps=8159, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=193, Total reward=65.69, Steps=8196, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=194, Total reward=34.85, Steps=8221, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=195, Total reward=19.74, Steps=8234, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=196, Total reward=83.66, Steps=8330, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=197, Total reward=25.91, Steps=8353, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=198, Total reward=25.49, Steps=8372, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=199, Total reward=77.81, Steps=8455, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=200, Total reward=15.65, Steps=8482, Training iteration=3
Policy training> Surrogate loss=0.014797904528677464, KL divergence=0.000905181746929884, Entropy=0.8724631667137146, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.05284717679023743, KL divergence=0.012456687167286873, Entropy=0.8554813861846924, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08126011490821838, KL divergence=0.0225819144397974, Entropy=0.8473081588745117, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0727030336856842, KL divergence=0.030140865594148636, Entropy=0.842205286026001, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.082536481320858, KL divergence=0.03610342741012573, Entropy=0.8431491851806641, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09032008051872253, KL divergence=0.04236745089292526, Entropy=0.8369637131690979, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.11129023134708405, KL divergence=0.04480845853686333, Entropy=0.8278308510780334, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.11061830818653107, KL divergence=0.04973696172237396, Entropy=0.8207722902297974, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.11139574646949768, KL divergence=0.05089927837252617, Entropy=0.8228108882904053, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10561847686767578, KL divergence=0.052598945796489716, Entropy=0.8284047842025757, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/149_Step-8482.ckpt']
Uploaded 3 files for checkpoint 149 in 0.53 seconds
saved intermediate frozen graph: current/model/model_149.pb
Best checkpoint number: 147, Last checkpoint number: 147
Copying the frozen checkpoint from ./frozen_models/agent/model_147.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'148'}
Training> Name=main_level/agent, Worker=0, Episode=201, Total reward=26.27, Steps=8535, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=202, Total reward=36.24, Steps=8570, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=203, Total reward=41.96, Steps=8620, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=204, Total reward=7.07, Steps=8645, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=205, Total reward=12.86, Steps=8672, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=206, Total reward=52.15, Steps=8718, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=207, Total reward=57.07, Steps=8758, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=208, Total reward=79.27, Steps=8793, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=209, Total reward=58.43, Steps=8846, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=210, Total reward=36.28, Steps=8881, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=211, Total reward=20.19, Steps=8919, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=212, Total reward=90.51, Steps=8983, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=213, Total reward=54.15, Steps=9021, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=214, Total reward=38.6, Steps=9046, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=215, Total reward=110.41, Steps=9147, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=216, Total reward=28.94, Steps=9179, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=217, Total reward=40.97, Steps=9220, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=218, Total reward=21.58, Steps=9239, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=219, Total reward=12.57, Steps=9263, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=220, Total reward=80.73, Steps=9337, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=221, Total reward=64.4, Steps=9396, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=222, Total reward=42.9, Steps=9429, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=223, Total reward=3.38, Steps=9454, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=224, Total reward=5.03, Steps=9487, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=225, Total reward=15.5, Steps=9528, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=226, Total reward=15.08, Steps=9551, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=227, Total reward=45.81, Steps=9589, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=228, Total reward=117.26, Steps=9678, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=229, Total reward=24.67, Steps=9722, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=230, Total reward=14.78, Steps=9762, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=231, Total reward=54.09, Steps=9830, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=232, Total reward=32.6, Steps=9850, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=233, Total reward=52.36, Steps=9888, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=234, Total reward=31.34, Steps=9916, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=235, Total reward=15.86, Steps=9929, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=236, Total reward=34.54, Steps=9957, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=237, Total reward=19.54, Steps=9978, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=238, Total reward=62.7, Steps=10026, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=239, Total reward=68.81, Steps=10080, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=240, Total reward=71.0, Steps=10153, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=241, Total reward=61.6, Steps=10235, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=242, Total reward=24.64, Steps=10267, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=243, Total reward=99.51, Steps=10382, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=244, Total reward=11.58, Steps=10407, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=245, Total reward=79.69, Steps=10499, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=246, Total reward=26.17, Steps=10523, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=247, Total reward=19.79, Steps=10543, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=248, Total reward=80.16, Steps=10595, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=249, Total reward=29.5, Steps=10633, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=250, Total reward=23.91, Steps=10687, Training iteration=4
Policy training> Surrogate loss=0.00587836466729641, KL divergence=0.00030704596429131925, Entropy=0.8125534057617188, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06706521660089493, KL divergence=0.008626041933894157, Entropy=0.8100574016571045, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07574165612459183, KL divergence=0.02112862467765808, Entropy=0.7917540073394775, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09249293804168701, KL divergence=0.03221558779478073, Entropy=0.7928408980369568, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0801919624209404, KL divergence=0.04074374586343765, Entropy=0.7895511388778687, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09372632205486298, KL divergence=0.046222150325775146, Entropy=0.7904601693153381, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10675980895757675, KL divergence=0.04917919635772705, Entropy=0.787074089050293, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.11245733499526978, KL divergence=0.05386847257614136, Entropy=0.7902655601501465, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10637670755386353, KL divergence=0.05578221380710602, Entropy=0.7894045114517212, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10999572277069092, KL divergence=0.057846859097480774, Entropy=0.7897629737854004, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/150_Step-10687.ckpt']
Uploaded 3 files for checkpoint 150 in 0.53 seconds
saved intermediate frozen graph: current/model/model_150.pb
Best checkpoint number: 148, Last checkpoint number: 148
Copying the frozen checkpoint from ./frozen_models/agent/model_148.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'145'}
Training> Name=main_level/agent, Worker=0, Episode=251, Total reward=11.29, Steps=10710, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=252, Total reward=90.39, Steps=10749, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=253, Total reward=23.25, Steps=10760, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=254, Total reward=44.31, Steps=10821, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=255, Total reward=60.59, Steps=10872, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=256, Total reward=20.55, Steps=10905, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=257, Total reward=56.59, Steps=10943, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=258, Total reward=69.54, Steps=10994, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=259, Total reward=14.77, Steps=11021, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=260, Total reward=3.98, Steps=11033, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=261, Total reward=60.95, Steps=11073, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=262, Total reward=59.87, Steps=11127, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=263, Total reward=10.94, Steps=11157, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=264, Total reward=24.58, Steps=11187, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=265, Total reward=21.45, Steps=11236, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=266, Total reward=14.95, Steps=11260, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=267, Total reward=78.8, Steps=11312, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=268, Total reward=128.21, Steps=11411, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=269, Total reward=104.83, Steps=11503, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=270, Total reward=18.94, Steps=11522, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=271, Total reward=39.51, Steps=11572, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=272, Total reward=36.22, Steps=11595, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=273, Total reward=63.57, Steps=11629, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=274, Total reward=51.32, Steps=11667, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=275, Total reward=25.5, Steps=11696, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=276, Total reward=24.5, Steps=11714, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=277, Total reward=80.8, Steps=11818, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=278, Total reward=50.56, Steps=11864, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=279, Total reward=51.39, Steps=11936, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=280, Total reward=48.59, Steps=12023, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=281, Total reward=14.37, Steps=12044, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=282, Total reward=73.52, Steps=12124, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=283, Total reward=3.36, Steps=12159, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=284, Total reward=20.12, Steps=12203, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=285, Total reward=32.57, Steps=12236, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=286, Total reward=56.08, Steps=12291, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=287, Total reward=70.6, Steps=12348, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=288, Total reward=92.47, Steps=12435, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=289, Total reward=99.97, Steps=12527, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=290, Total reward=23.41, Steps=12545, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=291, Total reward=75.62, Steps=12594, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=292, Total reward=35.3, Steps=12628, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=293, Total reward=61.15, Steps=12664, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=294, Total reward=55.75, Steps=12702, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=295, Total reward=105.79, Steps=12777, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=296, Total reward=104.56, Steps=12893, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=297, Total reward=17.84, Steps=12905, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=298, Total reward=65.43, Steps=12955, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=299, Total reward=29.96, Steps=13018, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=300, Total reward=39.85, Steps=13088, Training iteration=5
Policy training> Surrogate loss=-0.00366609706543386, KL divergence=0.0007128595607355237, Entropy=0.8011993765830994, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.056701794266700745, KL divergence=0.013642328791320324, Entropy=0.7833406925201416, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07755282521247864, KL divergence=0.026936372742056847, Entropy=0.7704206705093384, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09429406374692917, KL divergence=0.0363590270280838, Entropy=0.7656307220458984, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09898711740970612, KL divergence=0.0436699315905571, Entropy=0.7705698013305664, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10042955726385117, KL divergence=0.048532187938690186, Entropy=0.774410605430603, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10355570912361145, KL divergence=0.051298949867486954, Entropy=0.7696936726570129, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09146270155906677, KL divergence=0.054488327354192734, Entropy=0.7726449966430664, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10411777347326279, KL divergence=0.05592518299818039, Entropy=0.7730188369750977, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.1046329140663147, KL divergence=0.05617457628250122, Entropy=0.7737752795219421, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/151_Step-13088.ckpt']
Uploaded 3 files for checkpoint 151 in 0.46 seconds
saved intermediate frozen graph: current/model/model_151.pb
Best checkpoint number: 148, Last checkpoint number: 149
Copying the frozen checkpoint from ./frozen_models/agent/model_148.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'145'}
Training> Name=main_level/agent, Worker=0, Episode=301, Total reward=75.43, Steps=13142, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=302, Total reward=55.95, Steps=13210, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=303, Total reward=17.7, Steps=13266, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=304, Total reward=18.74, Steps=13293, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=305, Total reward=0.0, Steps=13294, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=306, Total reward=35.57, Steps=13337, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=307, Total reward=57.86, Steps=13391, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=308, Total reward=126.14, Steps=13477, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=309, Total reward=28.49, Steps=13532, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=310, Total reward=36.52, Steps=13594, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=311, Total reward=102.45, Steps=13646, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=312, Total reward=23.78, Steps=13673, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=313, Total reward=61.93, Steps=13705, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=314, Total reward=42.25, Steps=13733, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=315, Total reward=136.16, Steps=13833, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=316, Total reward=41.33, Steps=13899, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=317, Total reward=121.45, Steps=13984, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=318, Total reward=48.23, Steps=14027, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=319, Total reward=11.38, Steps=14046, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=320, Total reward=54.71, Steps=14079, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=321, Total reward=70.92, Steps=14118, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=322, Total reward=34.08, Steps=14149, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=323, Total reward=6.76, Steps=14185, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=324, Total reward=40.3, Steps=14240, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=325, Total reward=12.18, Steps=14256, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=326, Total reward=17.61, Steps=14272, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=327, Total reward=48.77, Steps=14336, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=328, Total reward=30.56, Steps=14354, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=329, Total reward=39.58, Steps=14381, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=330, Total reward=83.38, Steps=14473, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=331, Total reward=14.16, Steps=14518, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=332, Total reward=46.14, Steps=14552, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=333, Total reward=51.97, Steps=14586, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=334, Total reward=38.58, Steps=14612, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=335, Total reward=16.36, Steps=14626, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=336, Total reward=59.09, Steps=14686, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=337, Total reward=59.02, Steps=14744, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=338, Total reward=122.65, Steps=14849, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=339, Total reward=58.38, Steps=14935, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=340, Total reward=86.78, Steps=15031, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=341, Total reward=44.15, Steps=15061, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=342, Total reward=34.28, Steps=15087, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=343, Total reward=10.07, Steps=15111, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=344, Total reward=10.2, Steps=15128, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=345, Total reward=25.26, Steps=15183, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=346, Total reward=7.34, Steps=15196, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=347, Total reward=101.63, Steps=15264, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=348, Total reward=78.75, Steps=15308, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=349, Total reward=53.25, Steps=15358, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=350, Total reward=48.48, Steps=15426, Training iteration=6
Policy training> Surrogate loss=0.0010544881224632263, KL divergence=0.000629631569609046, Entropy=0.8216158151626587, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06902480125427246, KL divergence=0.012719047255814075, Entropy=0.8107727766036987, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08535945415496826, KL divergence=0.030086368322372437, Entropy=0.8061164021492004, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09300363808870316, KL divergence=0.041882991790771484, Entropy=0.8053066730499268, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09748619794845581, KL divergence=0.04901466518640518, Entropy=0.8050943613052368, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10750211775302887, KL divergence=0.05468013137578964, Entropy=0.8072655200958252, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.1004553735256195, KL divergence=0.05782768502831459, Entropy=0.8036782741546631, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10424914956092834, KL divergence=0.060451991856098175, Entropy=0.8067162036895752, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10297203809022903, KL divergence=0.061549678444862366, Entropy=0.8066906332969666, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10630429536104202, KL divergence=0.06297408789396286, Entropy=0.8110980987548828, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/152_Step-15426.ckpt']
Uploaded 3 files for checkpoint 152 in 0.49 seconds
saved intermediate frozen graph: current/model/model_152.pb
Best checkpoint number: 148, Last checkpoint number: 150
Copying the frozen checkpoint from ./frozen_models/agent/model_148.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'149'}
Training> Name=main_level/agent, Worker=0, Episode=351, Total reward=31.78, Steps=15465, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=352, Total reward=53.71, Steps=15502, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=353, Total reward=17.12, Steps=15528, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=354, Total reward=209.35, Steps=15714, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=355, Total reward=55.97, Steps=15760, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=356, Total reward=60.22, Steps=15834, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=357, Total reward=61.86, Steps=15894, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=358, Total reward=28.92, Steps=15912, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=359, Total reward=36.26, Steps=15960, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=360, Total reward=68.06, Steps=16019, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=361, Total reward=76.98, Steps=16136, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=362, Total reward=106.99, Steps=16223, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=363, Total reward=84.9, Steps=16355, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=364, Total reward=9.8, Steps=16378, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=365, Total reward=21.86, Steps=16415, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=366, Total reward=45.19, Steps=16472, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=367, Total reward=113.8, Steps=16585, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=368, Total reward=173.37, Steps=16708, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=369, Total reward=28.82, Steps=16734, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=370, Total reward=25.26, Steps=16758, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=371, Total reward=32.32, Steps=16803, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=372, Total reward=92.76, Steps=16857, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=373, Total reward=25.16, Steps=16882, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=374, Total reward=54.18, Steps=16951, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=375, Total reward=68.1, Steps=17002, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=376, Total reward=11.42, Steps=17025, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=377, Total reward=13.53, Steps=17045, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=378, Total reward=82.4, Steps=17092, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=379, Total reward=33.62, Steps=17136, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=380, Total reward=11.39, Steps=17160, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=381, Total reward=27.25, Steps=17188, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=382, Total reward=58.93, Steps=17266, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=383, Total reward=13.75, Steps=17318, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=384, Total reward=22.33, Steps=17384, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=385, Total reward=5.03, Steps=17401, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=386, Total reward=63.27, Steps=17483, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=387, Total reward=58.48, Steps=17525, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=388, Total reward=60.8, Steps=17583, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=389, Total reward=128.88, Steps=17662, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=390, Total reward=64.85, Steps=17724, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=391, Total reward=34.91, Steps=17748, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=392, Total reward=32.25, Steps=17769, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=393, Total reward=51.89, Steps=17795, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=394, Total reward=35.78, Steps=17848, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=395, Total reward=27.2, Steps=17898, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=396, Total reward=120.45, Steps=18003, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=397, Total reward=19.07, Steps=18025, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=398, Total reward=46.87, Steps=18057, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=399, Total reward=64.23, Steps=18133, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=400, Total reward=52.61, Steps=18176, Training iteration=7
Policy training> Surrogate loss=0.003742152824997902, KL divergence=0.0011451723985373974, Entropy=0.7793506383895874, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04939129948616028, KL divergence=0.016678903251886368, Entropy=0.7761873006820679, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0827609971165657, KL divergence=0.03276928514242172, Entropy=0.7743655443191528, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0845663920044899, KL divergence=0.042938537895679474, Entropy=0.7613832354545593, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09076493978500366, KL divergence=0.05030784755945206, Entropy=0.7654483914375305, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0977623388171196, KL divergence=0.05462520569562912, Entropy=0.7586597800254822, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.11068067699670792, KL divergence=0.057712964713573456, Entropy=0.7633334398269653, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09702867269515991, KL divergence=0.05980824679136276, Entropy=0.7653163075447083, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.1009705439209938, KL divergence=0.061276186257600784, Entropy=0.7672708630561829, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09998004883527756, KL divergence=0.06399235129356384, Entropy=0.7661663293838501, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/153_Step-18176.ckpt']
Uploaded 3 files for checkpoint 153 in 0.59 seconds
saved intermediate frozen graph: current/model/model_153.pb
Best checkpoint number: 148, Last checkpoint number: 151
Copying the frozen checkpoint from ./frozen_models/agent/model_148.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'147'}
Training> Name=main_level/agent, Worker=0, Episode=401, Total reward=48.96, Steps=18230, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=402, Total reward=41.26, Steps=18286, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=403, Total reward=3.04, Steps=18312, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=404, Total reward=3.6, Steps=18334, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=405, Total reward=59.9, Steps=18409, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=406, Total reward=11.15, Steps=18423, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=407, Total reward=56.99, Steps=18469, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=408, Total reward=61.23, Steps=18502, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=409, Total reward=49.75, Steps=18560, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=410, Total reward=7.84, Steps=18577, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=411, Total reward=37.47, Steps=18627, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=412, Total reward=76.94, Steps=18684, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=413, Total reward=31.09, Steps=18729, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=414, Total reward=27.8, Steps=18752, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=415, Total reward=50.69, Steps=18796, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=416, Total reward=40.75, Steps=18863, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=417, Total reward=57.38, Steps=18939, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=418, Total reward=21.24, Steps=18958, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=419, Total reward=14.31, Steps=18988, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=420, Total reward=23.54, Steps=19026, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=421, Total reward=58.98, Steps=19075, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=422, Total reward=47.21, Steps=19113, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=423, Total reward=11.6, Steps=19154, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=424, Total reward=3.97, Steps=19173, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=425, Total reward=33.84, Steps=19252, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=426, Total reward=74.31, Steps=19369, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=427, Total reward=34.0, Steps=19407, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=428, Total reward=74.75, Steps=19442, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=429, Total reward=30.43, Steps=19463, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=430, Total reward=34.86, Steps=19527, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=431, Total reward=33.0, Steps=19583, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=432, Total reward=43.13, Steps=19624, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=433, Total reward=67.21, Steps=19660, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=434, Total reward=50.74, Steps=19688, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=435, Total reward=36.88, Steps=19716, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=436, Total reward=16.65, Steps=19752, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=437, Total reward=64.94, Steps=19816, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=438, Total reward=44.72, Steps=19856, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=439, Total reward=14.39, Steps=19876, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=440, Total reward=44.72, Steps=19916, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=441, Total reward=24.25, Steps=19934, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=442, Total reward=34.44, Steps=19964, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=443, Total reward=17.86, Steps=20011, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=444, Total reward=120.5, Steps=20109, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=445, Total reward=69.73, Steps=20189, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=446, Total reward=67.59, Steps=20279, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=447, Total reward=54.77, Steps=20321, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=448, Total reward=73.37, Steps=20378, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=449, Total reward=19.71, Steps=20405, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=450, Total reward=91.41, Steps=20488, Training iteration=8
Policy training> Surrogate loss=-0.00043881626334041357, KL divergence=0.001553389709442854, Entropy=0.7789586782455444, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06369203329086304, KL divergence=0.018449971452355385, Entropy=0.7651366591453552, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08346033841371536, KL divergence=0.034468598663806915, Entropy=0.7617186307907104, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09178206324577332, KL divergence=0.0459972620010376, Entropy=0.757727861404419, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09601937234401703, KL divergence=0.05354965478181839, Entropy=0.753192663192749, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09977909177541733, KL divergence=0.058164749294519424, Entropy=0.7507935762405396, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.1011449322104454, KL divergence=0.06132031977176666, Entropy=0.7520982623100281, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10430370271205902, KL divergence=0.0642438679933548, Entropy=0.7530825138092041, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10605673491954803, KL divergence=0.0660606399178505, Entropy=0.7523611187934875, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10655777901411057, KL divergence=0.06773719191551208, Entropy=0.7541524767875671, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/154_Step-20488.ckpt']
Uploaded 3 files for checkpoint 154 in 0.61 seconds
saved intermediate frozen graph: current/model/model_154.pb
Best checkpoint number: 148, Last checkpoint number: 152
Copying the frozen checkpoint from ./frozen_models/agent/model_148.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'150'}
Training> Name=main_level/agent, Worker=0, Episode=451, Total reward=24.12, Steps=20524, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=452, Total reward=51.03, Steps=20558, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=453, Total reward=56.57, Steps=20595, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=454, Total reward=35.41, Steps=20621, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=455, Total reward=123.59, Steps=20710, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=456, Total reward=96.43, Steps=20806, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=457, Total reward=28.53, Steps=20848, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=458, Total reward=46.06, Steps=20892, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=459, Total reward=12.86, Steps=20921, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=460, Total reward=57.62, Steps=20983, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=461, Total reward=79.72, Steps=21077, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=462, Total reward=63.82, Steps=21131, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=463, Total reward=0.02, Steps=21146, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=464, Total reward=6.99, Steps=21167, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=465, Total reward=9.77, Steps=21187, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=466, Total reward=85.99, Steps=21239, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=467, Total reward=80.8, Steps=21293, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=468, Total reward=93.6, Steps=21352, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=469, Total reward=72.78, Steps=21426, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=470, Total reward=18.13, Steps=21455, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=471, Total reward=27.03, Steps=21493, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=472, Total reward=52.76, Steps=21533, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=473, Total reward=57.79, Steps=21571, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=474, Total reward=68.27, Steps=21650, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=475, Total reward=52.54, Steps=21702, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=476, Total reward=65.37, Steps=21755, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=477, Total reward=12.77, Steps=21773, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=478, Total reward=47.24, Steps=21807, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=479, Total reward=16.37, Steps=21842, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=480, Total reward=21.57, Steps=21867, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=481, Total reward=64.11, Steps=21938, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=482, Total reward=17.9, Steps=21957, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=483, Total reward=73.77, Steps=22072, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=484, Total reward=18.55, Steps=22113, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=485, Total reward=14.54, Steps=22168, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=486, Total reward=89.04, Steps=22251, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=487, Total reward=129.86, Steps=22331, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=488, Total reward=72.39, Steps=22371, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=489, Total reward=62.8, Steps=22441, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=490, Total reward=28.61, Steps=22480, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=491, Total reward=60.82, Steps=22560, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=492, Total reward=56.73, Steps=22594, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=493, Total reward=33.05, Steps=22618, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=494, Total reward=45.58, Steps=22670, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=495, Total reward=15.8, Steps=22683, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=496, Total reward=65.15, Steps=22744, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=497, Total reward=9.93, Steps=22765, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=498, Total reward=94.4, Steps=22843, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=499, Total reward=58.27, Steps=22929, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=500, Total reward=50.15, Steps=22972, Training iteration=9
Policy training> Surrogate loss=-0.002551155397668481, KL divergence=0.00039637889130972326, Entropy=0.7943909764289856, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0470648929476738, KL divergence=0.015013019554316998, Entropy=0.7840253114700317, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07904145121574402, KL divergence=0.031129619106650352, Entropy=0.7937870025634766, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0822041928768158, KL divergence=0.042280010879039764, Entropy=0.7870972156524658, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08310586959123611, KL divergence=0.04771687090396881, Entropy=0.7773222327232361, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08890089392662048, KL divergence=0.05222516134381294, Entropy=0.7815045118331909, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09367218613624573, KL divergence=0.056489791721105576, Entropy=0.7820009589195251, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09693697094917297, KL divergence=0.05911413952708244, Entropy=0.7865569591522217, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09967407584190369, KL divergence=0.06161324307322502, Entropy=0.7834489345550537, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0992208793759346, KL divergence=0.06175301969051361, Entropy=0.7836882472038269, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/155_Step-22972.ckpt']
Uploaded 3 files for checkpoint 155 in 0.53 seconds
saved intermediate frozen graph: current/model/model_155.pb
Best checkpoint number: 148, Last checkpoint number: 153
Copying the frozen checkpoint from ./frozen_models/agent/model_148.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'151'}
Training> Name=main_level/agent, Worker=0, Episode=501, Total reward=58.28, Steps=23037, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=502, Total reward=33.37, Steps=23120, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=503, Total reward=24.57, Steps=23194, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=504, Total reward=73.78, Steps=23267, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=505, Total reward=11.55, Steps=23293, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=506, Total reward=77.58, Steps=23345, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=507, Total reward=74.32, Steps=23401, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=508, Total reward=159.32, Steps=23522, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=509, Total reward=18.98, Steps=23542, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=510, Total reward=18.77, Steps=23574, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=511, Total reward=100.63, Steps=23642, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=512, Total reward=86.34, Steps=23694, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=513, Total reward=53.56, Steps=23735, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=514, Total reward=51.5, Steps=23763, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=515, Total reward=43.43, Steps=23794, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=516, Total reward=20.04, Steps=23826, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=517, Total reward=82.59, Steps=23877, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=518, Total reward=51.91, Steps=23931, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=519, Total reward=74.74, Steps=24001, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=520, Total reward=64.83, Steps=24044, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=521, Total reward=63.06, Steps=24100, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=522, Total reward=31.56, Steps=24125, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=523, Total reward=11.03, Steps=24169, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=524, Total reward=10.76, Steps=24192, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=525, Total reward=6.75, Steps=24221, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=526, Total reward=31.86, Steps=24242, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=527, Total reward=32.17, Steps=24274, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=528, Total reward=84.93, Steps=24342, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=529, Total reward=19.77, Steps=24377, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=530, Total reward=11.56, Steps=24395, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=531, Total reward=50.75, Steps=24446, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=532, Total reward=95.68, Steps=24499, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=533, Total reward=51.91, Steps=24538, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=534, Total reward=36.41, Steps=24564, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=535, Total reward=76.79, Steps=24608, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=536, Total reward=31.23, Steps=24641, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=537, Total reward=34.26, Steps=24676, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=538, Total reward=56.81, Steps=24716, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=539, Total reward=140.93, Steps=24822, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=540, Total reward=31.06, Steps=24860, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=541, Total reward=60.8, Steps=24926, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=542, Total reward=24.08, Steps=24959, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=543, Total reward=8.02, Steps=25008, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=544, Total reward=7.62, Steps=25031, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=545, Total reward=35.1, Steps=25084, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=546, Total reward=137.36, Steps=25180, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=547, Total reward=95.6, Steps=25250, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=548, Total reward=65.06, Steps=25297, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=549, Total reward=91.43, Steps=25388, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=550, Total reward=32.81, Steps=25457, Training iteration=10
Policy training> Surrogate loss=5.7076413213508204e-05, KL divergence=0.0007157344371080399, Entropy=0.8223294019699097, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.060523584485054016, KL divergence=0.015124823898077011, Entropy=0.8126990795135498, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08769109100103378, KL divergence=0.030085716396570206, Entropy=0.8055765628814697, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0835861787199974, KL divergence=0.04050736129283905, Entropy=0.810382604598999, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.10196040570735931, KL divergence=0.04794148728251457, Entropy=0.812082052230835, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10027319937944412, KL divergence=0.05279424786567688, Entropy=0.8069379329681396, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.11709899455308914, KL divergence=0.05684445798397064, Entropy=0.8098957538604736, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10928648710250854, KL divergence=0.05938241258263588, Entropy=0.8036125898361206, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.1057714894413948, KL divergence=0.06276693940162659, Entropy=0.8072726726531982, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10857564210891724, KL divergence=0.06495404243469238, Entropy=0.8046243786811829, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/156_Step-25457.ckpt']
Uploaded 3 files for checkpoint 156 in 0.56 seconds
saved intermediate frozen graph: current/model/model_156.pb
Best checkpoint number: 148, Last checkpoint number: 154
Copying the frozen checkpoint from ./frozen_models/agent/model_148.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'152'}
Training> Name=main_level/agent, Worker=0, Episode=551, Total reward=22.67, Steps=25501, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=552, Total reward=91.58, Steps=25557, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=553, Total reward=26.13, Steps=25582, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=554, Total reward=36.99, Steps=25627, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=555, Total reward=30.95, Steps=25674, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=556, Total reward=171.65, Steps=25845, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=557, Total reward=81.73, Steps=25934, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=558, Total reward=30.33, Steps=25961, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=559, Total reward=12.33, Steps=25986, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=560, Total reward=67.41, Steps=26039, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=561, Total reward=11.21, Steps=26056, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=562, Total reward=44.75, Steps=26085, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=563, Total reward=26.2, Steps=26128, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=564, Total reward=1.82, Steps=26149, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=565, Total reward=0.03, Steps=26181, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=566, Total reward=38.43, Steps=26233, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=567, Total reward=105.3, Steps=26352, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=568, Total reward=62.73, Steps=26384, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=569, Total reward=52.24, Steps=26462, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=570, Total reward=7.35, Steps=26489, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=571, Total reward=61.88, Steps=26540, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=572, Total reward=37.53, Steps=26579, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=573, Total reward=59.55, Steps=26612, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=574, Total reward=56.6, Steps=26650, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=575, Total reward=48.47, Steps=26703, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=576, Total reward=15.25, Steps=26740, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=577, Total reward=46.72, Steps=26802, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=578, Total reward=59.79, Steps=26849, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=579, Total reward=47.99, Steps=26898, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=580, Total reward=32.13, Steps=26940, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=581, Total reward=46.55, Steps=26988, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=582, Total reward=41.13, Steps=27034, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=583, Total reward=15.35, Steps=27071, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=584, Total reward=24.09, Steps=27108, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=585, Total reward=19.35, Steps=27136, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=586, Total reward=25.19, Steps=27163, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=587, Total reward=60.01, Steps=27224, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=588, Total reward=70.3, Steps=27266, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=589, Total reward=0.0, Steps=27267, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=590, Total reward=15.26, Steps=27294, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=591, Total reward=35.2, Steps=27347, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=592, Total reward=72.35, Steps=27405, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=593, Total reward=37.53, Steps=27447, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=594, Total reward=27.76, Steps=27471, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=595, Total reward=141.33, Steps=27595, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=596, Total reward=91.61, Steps=27686, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=597, Total reward=59.87, Steps=27741, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=598, Total reward=241.44, Steps=27918, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=599, Total reward=16.89, Steps=27940, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=600, Total reward=79.48, Steps=28001, Training iteration=11
Policy training> Surrogate loss=0.0011822585947811604, KL divergence=0.0008766187820583582, Entropy=0.7938156127929688, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06975535303354263, KL divergence=0.0159364715218544, Entropy=0.7893269062042236, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07536489516496658, KL divergence=0.030855076387524605, Entropy=0.7760544419288635, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09248507022857666, KL divergence=0.042559120804071426, Entropy=0.7680034637451172, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09932005405426025, KL divergence=0.05083383619785309, Entropy=0.7741103172302246, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09858524054288864, KL divergence=0.054325200617313385, Entropy=0.7759191989898682, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0954689159989357, KL divergence=0.05816485360264778, Entropy=0.7802002429962158, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09213283658027649, KL divergence=0.05916859209537506, Entropy=0.7797998189926147, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.1195690929889679, KL divergence=0.0622834637761116, Entropy=0.7862984538078308, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10582215338945389, KL divergence=0.06364656984806061, Entropy=0.7954185009002686, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/157_Step-28001.ckpt']
Uploaded 3 files for checkpoint 157 in 0.50 seconds
saved intermediate frozen graph: current/model/model_157.pb
Best checkpoint number: 148, Last checkpoint number: 155
Copying the frozen checkpoint from ./frozen_models/agent/model_148.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'153'}
Training> Name=main_level/agent, Worker=0, Episode=601, Total reward=7.47, Steps=28013, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=602, Total reward=72.49, Steps=28087, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=603, Total reward=29.15, Steps=28139, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=604, Total reward=110.27, Steps=28238, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=605, Total reward=103.34, Steps=28340, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=606, Total reward=18.05, Steps=28357, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=607, Total reward=122.35, Steps=28459, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=608, Total reward=26.54, Steps=28474, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=609, Total reward=36.61, Steps=28550, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=610, Total reward=22.45, Steps=28566, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=611, Total reward=13.95, Steps=28603, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=612, Total reward=58.63, Steps=28636, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=613, Total reward=54.25, Steps=28670, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=614, Total reward=20.97, Steps=28689, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=615, Total reward=55.31, Steps=28767, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=616, Total reward=33.55, Steps=28803, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=617, Total reward=129.98, Steps=28918, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=618, Total reward=54.36, Steps=28982, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=619, Total reward=109.58, Steps=29062, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=620, Total reward=48.67, Steps=29107, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=621, Total reward=46.74, Steps=29157, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=622, Total reward=35.08, Steps=29176, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=623, Total reward=14.95, Steps=29195, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=624, Total reward=68.2, Steps=29276, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=625, Total reward=11.79, Steps=29297, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=626, Total reward=102.05, Steps=29371, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=627, Total reward=53.62, Steps=29433, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=628, Total reward=63.53, Steps=29464, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=629, Total reward=36.28, Steps=29489, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=630, Total reward=14.68, Steps=29514, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=631, Total reward=60.83, Steps=29597, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=632, Total reward=49.91, Steps=29629, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=633, Total reward=60.53, Steps=29668, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=634, Total reward=67.48, Steps=29745, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=635, Total reward=141.1, Steps=29841, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=636, Total reward=13.53, Steps=29862, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=637, Total reward=13.09, Steps=29881, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=638, Total reward=46.14, Steps=29910, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=639, Total reward=49.79, Steps=30000, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=640, Total reward=91.83, Steps=30080, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=641, Total reward=70.5, Steps=30127, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=642, Total reward=20.71, Steps=30150, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=643, Total reward=30.13, Steps=30218, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=644, Total reward=0.01, Steps=30232, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=645, Total reward=4.86, Steps=30257, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=646, Total reward=51.21, Steps=30301, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=647, Total reward=105.71, Steps=30389, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=648, Total reward=75.59, Steps=30425, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=649, Total reward=22.54, Steps=30455, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=650, Total reward=39.75, Steps=30530, Training iteration=12
Policy training> Surrogate loss=-0.006050406955182552, KL divergence=0.0008205941412597895, Entropy=0.8277584910392761, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.05975732579827309, KL divergence=0.01631390117108822, Entropy=0.8207077980041504, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.09620240330696106, KL divergence=0.03230688348412514, Entropy=0.81070476770401, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0944761410355568, KL divergence=0.04369106888771057, Entropy=0.8053345084190369, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08454786986112595, KL divergence=0.049751121550798416, Entropy=0.8145060539245605, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10134413838386536, KL divergence=0.05709610506892204, Entropy=0.8145012259483337, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10241162031888962, KL divergence=0.05845819041132927, Entropy=0.8183240294456482, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.11074884980916977, KL divergence=0.061436042189598083, Entropy=0.8126261234283447, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10015612840652466, KL divergence=0.06350747495889664, Entropy=0.8194418549537659, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.11537948995828629, KL divergence=0.0651194378733635, Entropy=0.8196958899497986, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/158_Step-30530.ckpt']
Uploaded 3 files for checkpoint 158 in 0.63 seconds
saved intermediate frozen graph: current/model/model_158.pb
Best checkpoint number: 148, Last checkpoint number: 156
Copying the frozen checkpoint from ./frozen_models/agent/model_148.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'154'}
Training> Name=main_level/agent, Worker=0, Episode=651, Total reward=23.13, Steps=30544, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=652, Total reward=80.74, Steps=30586, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=653, Total reward=0.02, Steps=30602, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=654, Total reward=40.47, Steps=30643, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=655, Total reward=43.08, Steps=30679, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=656, Total reward=21.8, Steps=30707, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=657, Total reward=30.13, Steps=30753, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=658, Total reward=23.92, Steps=30768, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=659, Total reward=10.71, Steps=30795, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=660, Total reward=66.48, Steps=30851, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=661, Total reward=15.28, Steps=30865, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=662, Total reward=43.48, Steps=30894, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=663, Total reward=9.33, Steps=30929, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=664, Total reward=18.67, Steps=30985, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=665, Total reward=6.01, Steps=31015, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=666, Total reward=43.17, Steps=31066, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=667, Total reward=72.9, Steps=31138, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=668, Total reward=115.08, Steps=31216, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=669, Total reward=60.09, Steps=31307, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=670, Total reward=20.43, Steps=31364, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=671, Total reward=39.86, Steps=31405, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=672, Total reward=68.32, Steps=31463, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=673, Total reward=37.15, Steps=31488, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=674, Total reward=111.2, Steps=31624, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=675, Total reward=71.26, Steps=31670, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=676, Total reward=32.41, Steps=31702, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=677, Total reward=53.97, Steps=31761, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=678, Total reward=72.5, Steps=31809, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=679, Total reward=71.25, Steps=31877, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=680, Total reward=42.82, Steps=31912, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=681, Total reward=47.35, Steps=31959, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=682, Total reward=45.34, Steps=32008, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=683, Total reward=6.31, Steps=32058, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=684, Total reward=0.02, Steps=32077, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=685, Total reward=6.66, Steps=32109, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=686, Total reward=82.11, Steps=32198, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=687, Total reward=38.42, Steps=32231, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=688, Total reward=118.47, Steps=32285, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=689, Total reward=53.17, Steps=32343, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=690, Total reward=17.81, Steps=32368, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=691, Total reward=29.31, Steps=32422, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=692, Total reward=53.85, Steps=32453, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=693, Total reward=6.59, Steps=32475, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=694, Total reward=23.56, Steps=32524, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=695, Total reward=15.94, Steps=32536, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=696, Total reward=21.01, Steps=32556, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=697, Total reward=92.33, Steps=32637, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=698, Total reward=27.73, Steps=32662, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=699, Total reward=85.36, Steps=32743, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=700, Total reward=82.16, Steps=32800, Training iteration=13
Policy training> Surrogate loss=0.0035270750522613525, KL divergence=0.00034267758019268513, Entropy=0.812039852142334, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.055244289338588715, KL divergence=0.012490027584135532, Entropy=0.8105477690696716, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08742430061101913, KL divergence=0.029659053310751915, Entropy=0.8115028738975525, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07356768101453781, KL divergence=0.04172395542263985, Entropy=0.8067629933357239, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09739509224891663, KL divergence=0.05114524066448212, Entropy=0.8066803216934204, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09429905563592911, KL divergence=0.05747184902429581, Entropy=0.8059560656547546, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09348854422569275, KL divergence=0.060925230383872986, Entropy=0.8091000318527222, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10228962451219559, KL divergence=0.06319983303546906, Entropy=0.8086519837379456, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09590781480073929, KL divergence=0.06341437995433807, Entropy=0.8132795691490173, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.11643879115581512, KL divergence=0.0661509782075882, Entropy=0.8064998984336853, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/159_Step-32800.ckpt']
Uploaded 3 files for checkpoint 159 in 0.54 seconds
saved intermediate frozen graph: current/model/model_159.pb
Best checkpoint number: 148, Last checkpoint number: 157
Copying the frozen checkpoint from ./frozen_models/agent/model_148.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'155'}
Training> Name=main_level/agent, Worker=0, Episode=701, Total reward=22.08, Steps=32823, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=702, Total reward=61.55, Steps=32898, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=703, Total reward=0.02, Steps=32913, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=704, Total reward=11.27, Steps=32934, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=705, Total reward=16.13, Steps=32971, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=706, Total reward=17.84, Steps=32985, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=707, Total reward=101.52, Steps=33087, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=708, Total reward=113.19, Steps=33166, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=709, Total reward=16.34, Steps=33181, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=710, Total reward=7.49, Steps=33195, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=711, Total reward=7.97, Steps=33230, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=712, Total reward=43.51, Steps=33263, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=713, Total reward=39.28, Steps=33287, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=714, Total reward=101.56, Steps=33390, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=715, Total reward=66.72, Steps=33438, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=716, Total reward=26.31, Steps=33468, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=717, Total reward=94.04, Steps=33559, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=718, Total reward=111.87, Steps=33640, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=719, Total reward=93.78, Steps=33738, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=720, Total reward=23.67, Steps=33784, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=721, Total reward=31.82, Steps=33808, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=722, Total reward=78.58, Steps=33869, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=723, Total reward=89.62, Steps=33977, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=724, Total reward=24.31, Steps=34014, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=725, Total reward=156.81, Steps=34172, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=726, Total reward=84.66, Steps=34253, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=727, Total reward=89.63, Steps=34353, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=728, Total reward=97.25, Steps=34395, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=729, Total reward=31.39, Steps=34424, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=730, Total reward=22.68, Steps=34454, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=731, Total reward=108.72, Steps=34522, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=732, Total reward=38.05, Steps=34551, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=733, Total reward=38.56, Steps=34575, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=734, Total reward=9.75, Steps=34588, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=735, Total reward=174.24, Steps=34732, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=736, Total reward=120.14, Steps=34840, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=737, Total reward=9.48, Steps=34858, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=738, Total reward=74.51, Steps=34895, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=739, Total reward=56.15, Steps=34974, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=740, Total reward=96.46, Steps=35043, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=741, Total reward=39.5, Steps=35093, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=742, Total reward=26.78, Steps=35129, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=743, Total reward=15.32, Steps=35152, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=744, Total reward=8.79, Steps=35171, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=745, Total reward=31.33, Steps=35251, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=746, Total reward=46.39, Steps=35296, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=747, Total reward=55.31, Steps=35361, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=748, Total reward=53.85, Steps=35387, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=749, Total reward=24.58, Steps=35415, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=750, Total reward=53.48, Steps=35462, Training iteration=14
Policy training> Surrogate loss=-0.0019298649858683348, KL divergence=0.00097274238942191, Entropy=0.8398900032043457, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.07101453840732574, KL divergence=0.017413204535841942, Entropy=0.8157981038093567, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08641742914915085, KL divergence=0.032835591584444046, Entropy=0.8075512647628784, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09075115621089935, KL divergence=0.04175955057144165, Entropy=0.8100347518920898, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09904685616493225, KL divergence=0.04668673500418663, Entropy=0.8075712323188782, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09656122326850891, KL divergence=0.049628108739852905, Entropy=0.811016857624054, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09916128218173981, KL divergence=0.05338456481695175, Entropy=0.811991810798645, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.11118695884943008, KL divergence=0.05547717958688736, Entropy=0.8160236477851868, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10441482067108154, KL divergence=0.05687572434544563, Entropy=0.8187392354011536, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10590551048517227, KL divergence=0.05909143015742302, Entropy=0.8245123028755188, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/160_Step-35462.ckpt']
Uploaded 3 files for checkpoint 160 in 0.53 seconds
saved intermediate frozen graph: current/model/model_160.pb
Best checkpoint number: 158, Last checkpoint number: 158
Copying the frozen checkpoint from ./frozen_models/agent/model_158.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'156'}
Training> Name=main_level/agent, Worker=0, Episode=751, Total reward=102.02, Steps=35525, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=752, Total reward=46.38, Steps=35559, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=753, Total reward=39.58, Steps=35593, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=754, Total reward=50.77, Steps=35637, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=755, Total reward=118.63, Steps=35725, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=756, Total reward=98.92, Steps=35813, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=757, Total reward=60.27, Steps=35884, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=758, Total reward=142.72, Steps=36003, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=759, Total reward=59.69, Steps=36083, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=760, Total reward=48.9, Steps=36120, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=761, Total reward=36.19, Steps=36148, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=762, Total reward=42.49, Steps=36174, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=763, Total reward=7.07, Steps=36203, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=764, Total reward=145.89, Steps=36369, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=765, Total reward=10.17, Steps=36395, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=766, Total reward=137.67, Steps=36517, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=767, Total reward=22.33, Steps=36560, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=768, Total reward=155.66, Steps=36659, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=769, Total reward=66.95, Steps=36706, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=770, Total reward=50.53, Steps=36752, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=771, Total reward=41.79, Steps=36804, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=772, Total reward=50.84, Steps=36836, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=773, Total reward=59.82, Steps=36878, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=774, Total reward=32.9, Steps=36907, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=775, Total reward=47.64, Steps=36961, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=776, Total reward=128.76, Steps=37090, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=777, Total reward=20.23, Steps=37120, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=778, Total reward=87.52, Steps=37227, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=779, Total reward=9.54, Steps=37262, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=780, Total reward=31.18, Steps=37285, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=781, Total reward=39.78, Steps=37319, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=782, Total reward=130.64, Steps=37436, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=783, Total reward=0.02, Steps=37451, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=784, Total reward=105.78, Steps=37547, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=785, Total reward=11.25, Steps=37575, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=786, Total reward=106.01, Steps=37671, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=787, Total reward=82.43, Steps=37767, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=788, Total reward=94.31, Steps=37813, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=789, Total reward=45.17, Steps=37853, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=790, Total reward=80.08, Steps=37907, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=791, Total reward=31.2, Steps=37937, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=792, Total reward=83.36, Steps=37986, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=793, Total reward=30.69, Steps=38010, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=794, Total reward=10.54, Steps=38027, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=795, Total reward=72.62, Steps=38062, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=796, Total reward=21.83, Steps=38093, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=797, Total reward=26.32, Steps=38140, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=798, Total reward=64.83, Steps=38191, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=799, Total reward=38.75, Steps=38255, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=800, Total reward=84.3, Steps=38313, Training iteration=15
Policy training> Surrogate loss=0.0045043304562568665, KL divergence=0.0009779275860637426, Entropy=0.845314621925354, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06952071934938431, KL divergence=0.02178485505282879, Entropy=0.8453152179718018, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08808433264493942, KL divergence=0.03878933563828468, Entropy=0.8391532897949219, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.10049852728843689, KL divergence=0.04746008291840553, Entropy=0.8363179564476013, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.10036765784025192, KL divergence=0.05339200422167778, Entropy=0.8353050351142883, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10609781742095947, KL divergence=0.058286771178245544, Entropy=0.8366057872772217, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10822560638189316, KL divergence=0.06131809949874878, Entropy=0.8390712738037109, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10941249877214432, KL divergence=0.06380822509527206, Entropy=0.840642511844635, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10925936698913574, KL divergence=0.06584258377552032, Entropy=0.841316282749176, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.1111285462975502, KL divergence=0.06782960146665573, Entropy=0.8438504934310913, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/161_Step-38313.ckpt']
Uploaded 3 files for checkpoint 161 in 0.49 seconds
saved intermediate frozen graph: current/model/model_161.pb
Best checkpoint number: 158, Last checkpoint number: 159
Copying the frozen checkpoint from ./frozen_models/agent/model_158.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'157'}
Training> Name=main_level/agent, Worker=0, Episode=801, Total reward=12.03, Steps=38327, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=802, Total reward=60.62, Steps=38386, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=803, Total reward=6.15, Steps=38410, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=804, Total reward=17.19, Steps=38471, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=805, Total reward=23.42, Steps=38511, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=806, Total reward=90.44, Steps=38585, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=807, Total reward=37.49, Steps=38619, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=808, Total reward=84.16, Steps=38657, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=809, Total reward=12.71, Steps=38688, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=810, Total reward=63.59, Steps=38743, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=811, Total reward=61.86, Steps=38800, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=812, Total reward=20.34, Steps=38836, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=813, Total reward=58.56, Steps=38875, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=814, Total reward=10.56, Steps=38895, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=815, Total reward=203.85, Steps=39045, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=816, Total reward=74.38, Steps=39115, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=817, Total reward=124.11, Steps=39244, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=818, Total reward=33.75, Steps=39265, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=819, Total reward=85.7, Steps=39340, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=820, Total reward=221.75, Steps=39570, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=821, Total reward=76.61, Steps=39624, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=822, Total reward=36.71, Steps=39654, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=823, Total reward=76.63, Steps=39754, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=824, Total reward=6.81, Steps=39789, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=825, Total reward=10.3, Steps=39814, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=826, Total reward=99.97, Steps=39878, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=827, Total reward=86.3, Steps=39955, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=828, Total reward=73.24, Steps=39993, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=829, Total reward=24.8, Steps=40038, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=830, Total reward=29.62, Steps=40072, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=831, Total reward=64.53, Steps=40130, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=832, Total reward=59.82, Steps=40166, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=833, Total reward=63.53, Steps=40198, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=834, Total reward=32.95, Steps=40239, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=835, Total reward=57.89, Steps=40305, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=836, Total reward=88.98, Steps=40376, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=837, Total reward=55.9, Steps=40420, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=838, Total reward=44.4, Steps=40447, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=839, Total reward=90.42, Steps=40511, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=840, Total reward=43.2, Steps=40553, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=841, Total reward=23.66, Steps=40582, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=842, Total reward=39.18, Steps=40617, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=843, Total reward=48.25, Steps=40698, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=844, Total reward=14.72, Steps=40716, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=845, Total reward=6.73, Steps=40755, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=846, Total reward=29.48, Steps=40813, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=847, Total reward=118.61, Steps=40916, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=848, Total reward=74.82, Steps=40961, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=849, Total reward=26.06, Steps=40985, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=850, Total reward=42.15, Steps=41039, Training iteration=16
Policy training> Surrogate loss=0.006072669755667448, KL divergence=0.0007529370486736298, Entropy=0.8627559542655945, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06660908460617065, KL divergence=0.016836252063512802, Entropy=0.8499510884284973, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08273138850927353, KL divergence=0.03311619907617569, Entropy=0.8438689112663269, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09903893619775772, KL divergence=0.04366802051663399, Entropy=0.8421670794487, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09690017998218536, KL divergence=0.05112486332654953, Entropy=0.8426526784896851, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10630206018686295, KL divergence=0.05482635647058487, Entropy=0.8412890434265137, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10629018396139145, KL divergence=0.057385146617889404, Entropy=0.8420323133468628, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.11460542678833008, KL divergence=0.05969615653157234, Entropy=0.8466631770133972, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.1050359234213829, KL divergence=0.06151852756738663, Entropy=0.8450756072998047, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10663838684558868, KL divergence=0.06451316177845001, Entropy=0.8592256307601929, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/162_Step-41039.ckpt']
Uploaded 3 files for checkpoint 162 in 0.63 seconds
saved intermediate frozen graph: current/model/model_162.pb
Best checkpoint number: 158, Last checkpoint number: 160
Copying the frozen checkpoint from ./frozen_models/agent/model_158.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'159'}
Training> Name=main_level/agent, Worker=0, Episode=851, Total reward=32.2, Steps=41077, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=852, Total reward=78.03, Steps=41123, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=853, Total reward=57.42, Steps=41161, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=854, Total reward=41.63, Steps=41209, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=855, Total reward=27.93, Steps=41235, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=856, Total reward=12.08, Steps=41254, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=857, Total reward=62.05, Steps=41311, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=858, Total reward=29.01, Steps=41350, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=859, Total reward=71.38, Steps=41420, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=860, Total reward=50.32, Steps=41461, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=861, Total reward=3.75, Steps=41472, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=862, Total reward=60.26, Steps=41534, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=863, Total reward=9.47, Steps=41615, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=864, Total reward=1.09, Steps=41639, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=865, Total reward=101.07, Steps=41764, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=866, Total reward=97.14, Steps=41897, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=867, Total reward=78.73, Steps=41987, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=868, Total reward=84.7, Steps=42022, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=869, Total reward=69.77, Steps=42098, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=870, Total reward=26.79, Steps=42125, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=871, Total reward=24.69, Steps=42172, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=872, Total reward=22.8, Steps=42197, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=873, Total reward=58.4, Steps=42239, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=874, Total reward=19.73, Steps=42259, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=875, Total reward=35.6, Steps=42290, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=876, Total reward=25.35, Steps=42324, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=877, Total reward=33.83, Steps=42349, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=878, Total reward=131.46, Steps=42435, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=879, Total reward=63.59, Steps=42492, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=880, Total reward=100.43, Steps=42568, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=881, Total reward=54.74, Steps=42619, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=882, Total reward=13.88, Steps=42638, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=883, Total reward=0.02, Steps=42653, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=884, Total reward=8.72, Steps=42673, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=885, Total reward=90.98, Steps=42781, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=886, Total reward=98.41, Steps=42889, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=887, Total reward=11.96, Steps=42934, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=888, Total reward=51.54, Steps=42963, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=889, Total reward=28.11, Steps=42990, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=890, Total reward=45.37, Steps=43052, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=891, Total reward=49.39, Steps=43093, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=892, Total reward=47.12, Steps=43126, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=893, Total reward=38.87, Steps=43165, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=894, Total reward=50.43, Steps=43193, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=895, Total reward=49.66, Steps=43226, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=896, Total reward=26.98, Steps=43246, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=897, Total reward=60.97, Steps=43300, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=898, Total reward=167.18, Steps=43412, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=899, Total reward=84.63, Steps=43516, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=900, Total reward=101.45, Steps=43588, Training iteration=17
Policy training> Surrogate loss=0.009886400774121284, KL divergence=0.0004697717959061265, Entropy=0.8657213449478149, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.05844718962907791, KL divergence=0.015372595749795437, Entropy=0.8506457209587097, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0786767527461052, KL divergence=0.033053264021873474, Entropy=0.8445706367492676, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.10199110954999924, KL divergence=0.042308542877435684, Entropy=0.8246896266937256, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09076246619224548, KL divergence=0.049698278307914734, Entropy=0.8220570087432861, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.11231471598148346, KL divergence=0.053110405802726746, Entropy=0.8207082748413086, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08781012147665024, KL divergence=0.05554307997226715, Entropy=0.822221040725708, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.1096087098121643, KL divergence=0.05818662419915199, Entropy=0.8247365951538086, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10611538589000702, KL divergence=0.05978355184197426, Entropy=0.8224246501922607, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0877879410982132, KL divergence=0.0628335103392601, Entropy=0.8246135115623474, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/163_Step-43588.ckpt']
Uploaded 3 files for checkpoint 163 in 0.57 seconds
saved intermediate frozen graph: current/model/model_163.pb
Best checkpoint number: 158, Last checkpoint number: 161
Copying the frozen checkpoint from ./frozen_models/agent/model_158.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'160'}
Training> Name=main_level/agent, Worker=0, Episode=901, Total reward=81.21, Steps=43633, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=902, Total reward=57.63, Steps=43691, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=903, Total reward=3.0, Steps=43717, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=904, Total reward=101.83, Steps=43820, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=905, Total reward=15.51, Steps=43870, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=906, Total reward=84.52, Steps=43940, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=907, Total reward=94.45, Steps=44021, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=908, Total reward=81.23, Steps=44060, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=909, Total reward=49.26, Steps=44118, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=910, Total reward=40.0, Steps=44143, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=911, Total reward=60.8, Steps=44193, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=912, Total reward=21.46, Steps=44231, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=913, Total reward=51.57, Steps=44267, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=914, Total reward=24.12, Steps=44290, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=915, Total reward=129.63, Steps=44368, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=916, Total reward=82.41, Steps=44444, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=917, Total reward=69.88, Steps=44529, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=918, Total reward=108.84, Steps=44616, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=919, Total reward=81.73, Steps=44687, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=920, Total reward=48.21, Steps=44715, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=921, Total reward=40.05, Steps=44762, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=922, Total reward=45.67, Steps=44791, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=923, Total reward=3.87, Steps=44806, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=924, Total reward=8.18, Steps=44828, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=925, Total reward=3.37, Steps=44848, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=926, Total reward=14.55, Steps=44875, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=927, Total reward=84.62, Steps=44948, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=928, Total reward=59.0, Steps=44983, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=929, Total reward=131.16, Steps=45071, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=930, Total reward=28.2, Steps=45110, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=931, Total reward=106.31, Steps=45178, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=932, Total reward=27.57, Steps=45200, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=933, Total reward=25.43, Steps=45223, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=934, Total reward=21.5, Steps=45245, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=935, Total reward=28.39, Steps=45276, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=936, Total reward=29.38, Steps=45296, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=937, Total reward=46.1, Steps=45362, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=938, Total reward=77.49, Steps=45416, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=939, Total reward=29.4, Steps=45449, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=940, Total reward=39.05, Steps=45471, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=941, Total reward=21.46, Steps=45490, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=942, Total reward=52.14, Steps=45547, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=943, Total reward=2.99, Steps=45563, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=944, Total reward=19.69, Steps=45600, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=945, Total reward=3.41, Steps=45615, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=946, Total reward=7.23, Steps=45626, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=947, Total reward=98.74, Steps=45689, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=948, Total reward=129.79, Steps=45778, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=949, Total reward=70.05, Steps=45865, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=950, Total reward=22.37, Steps=45902, Training iteration=18
Policy training> Surrogate loss=0.0024355947971343994, KL divergence=0.0005599790019914508, Entropy=0.8307022452354431, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06720112264156342, KL divergence=0.014882688410580158, Entropy=0.8241992592811584, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08706699311733246, KL divergence=0.032090961933135986, Entropy=0.818230926990509, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09650271385908127, KL divergence=0.04312916472554207, Entropy=0.816081166267395, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09863454848527908, KL divergence=0.05034187063574791, Entropy=0.8144611120223999, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10221225768327713, KL divergence=0.05496883764863014, Entropy=0.8156724572181702, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10181686282157898, KL divergence=0.05903871729969978, Entropy=0.8188393115997314, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10928051173686981, KL divergence=0.06151651591062546, Entropy=0.8202173709869385, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10839800536632538, KL divergence=0.06382530182600021, Entropy=0.8247807621955872, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10814988613128662, KL divergence=0.06574845314025879, Entropy=0.8255226612091064, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/164_Step-45902.ckpt']
Uploaded 3 files for checkpoint 164 in 0.57 seconds
saved intermediate frozen graph: current/model/model_164.pb
Best checkpoint number: 158, Last checkpoint number: 162
Copying the frozen checkpoint from ./frozen_models/agent/model_158.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'161'}
Training> Name=main_level/agent, Worker=0, Episode=951, Total reward=73.28, Steps=45948, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=952, Total reward=93.55, Steps=46032, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=953, Total reward=171.98, Steps=46230, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=954, Total reward=38.31, Steps=46255, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=955, Total reward=85.03, Steps=46332, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=956, Total reward=86.85, Steps=46404, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=957, Total reward=147.97, Steps=46520, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=958, Total reward=27.5, Steps=46541, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=959, Total reward=22.62, Steps=46575, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=960, Total reward=28.84, Steps=46595, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=961, Total reward=73.81, Steps=46635, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=962, Total reward=54.87, Steps=46706, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=963, Total reward=68.79, Steps=46796, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=964, Total reward=136.27, Steps=46966, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=965, Total reward=40.21, Steps=47022, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=966, Total reward=145.51, Steps=47133, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=967, Total reward=46.47, Steps=47196, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=968, Total reward=82.98, Steps=47249, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=969, Total reward=77.21, Steps=47323, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=970, Total reward=33.84, Steps=47374, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=971, Total reward=43.32, Steps=47429, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=972, Total reward=36.16, Steps=47491, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=973, Total reward=60.34, Steps=47527, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=974, Total reward=29.08, Steps=47551, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=975, Total reward=29.75, Steps=47568, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=976, Total reward=174.09, Steps=47709, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=977, Total reward=19.6, Steps=47731, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=978, Total reward=123.49, Steps=47821, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=979, Total reward=98.02, Steps=47938, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=980, Total reward=51.87, Steps=47971, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=981, Total reward=92.65, Steps=48072, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=982, Total reward=36.75, Steps=48112, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=983, Total reward=154.96, Steps=48279, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=984, Total reward=15.0, Steps=48307, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=985, Total reward=139.95, Steps=48426, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=986, Total reward=87.84, Steps=48502, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=987, Total reward=0.0, Steps=48503, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=988, Total reward=79.9, Steps=48538, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=989, Total reward=30.54, Steps=48573, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=990, Total reward=23.12, Steps=48611, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=991, Total reward=37.88, Steps=48645, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=992, Total reward=34.12, Steps=48673, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=993, Total reward=55.61, Steps=48710, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=994, Total reward=20.85, Steps=48732, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=995, Total reward=67.71, Steps=48777, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=996, Total reward=40.13, Steps=48844, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=997, Total reward=73.85, Steps=48889, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=998, Total reward=43.55, Steps=48916, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=999, Total reward=11.91, Steps=48937, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=1000, Total reward=89.43, Steps=48996, Training iteration=19
Policy training> Surrogate loss=-0.0035325519274920225, KL divergence=0.0021374982316046953, Entropy=0.8432065844535828, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.07234515249729156, KL divergence=0.02595243789255619, Entropy=0.8145938515663147, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08944842964410782, KL divergence=0.04011012986302376, Entropy=0.8072324395179749, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09733400493860245, KL divergence=0.04989974573254585, Entropy=0.8010521531105042, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09990206360816956, KL divergence=0.054411422461271286, Entropy=0.8012373447418213, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10379964113235474, KL divergence=0.05756441131234169, Entropy=0.8031153678894043, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10563332587480545, KL divergence=0.05993606150150299, Entropy=0.8043384552001953, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10782765597105026, KL divergence=0.06123226508498192, Entropy=0.8093416094779968, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.11130806058645248, KL divergence=0.06286869198083878, Entropy=0.8119192719459534, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.11234080046415329, KL divergence=0.06509871035814285, Entropy=0.811291515827179, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/165_Step-48996.ckpt']
Uploaded 3 files for checkpoint 165 in 0.53 seconds
saved intermediate frozen graph: current/model/model_165.pb
Best checkpoint number: 158, Last checkpoint number: 163
Copying the frozen checkpoint from ./frozen_models/agent/model_158.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'162'}
Training> Name=main_level/agent, Worker=0, Episode=1001, Total reward=130.81, Steps=49146, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1002, Total reward=35.57, Steps=49171, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1003, Total reward=130.49, Steps=49281, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1004, Total reward=22.3, Steps=49338, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1005, Total reward=9.81, Steps=49377, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1006, Total reward=7.2, Steps=49389, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1007, Total reward=102.37, Steps=49451, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1008, Total reward=192.24, Steps=49558, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1009, Total reward=48.56, Steps=49617, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1010, Total reward=42.38, Steps=49644, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1011, Total reward=42.93, Steps=49695, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1012, Total reward=72.44, Steps=49752, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1013, Total reward=50.14, Steps=49780, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1014, Total reward=29.5, Steps=49806, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1015, Total reward=26.56, Steps=49843, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1016, Total reward=95.22, Steps=49926, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1017, Total reward=94.17, Steps=49991, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1018, Total reward=133.96, Steps=50100, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1019, Total reward=54.97, Steps=50143, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1020, Total reward=15.75, Steps=50162, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1021, Total reward=62.4, Steps=50254, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1022, Total reward=53.28, Steps=50323, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1023, Total reward=11.7, Steps=50363, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1024, Total reward=54.29, Steps=50449, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1025, Total reward=64.24, Steps=50539, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1026, Total reward=22.46, Steps=50572, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1027, Total reward=77.68, Steps=50628, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1028, Total reward=88.02, Steps=50697, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1029, Total reward=56.96, Steps=50757, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1030, Total reward=20.87, Steps=50779, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1031, Total reward=108.32, Steps=50842, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1032, Total reward=65.2, Steps=50890, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1033, Total reward=60.15, Steps=50922, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1034, Total reward=41.26, Steps=50948, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1035, Total reward=92.43, Steps=51033, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1036, Total reward=85.68, Steps=51101, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1037, Total reward=32.22, Steps=51145, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1038, Total reward=20.17, Steps=51158, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1039, Total reward=91.48, Steps=51243, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1040, Total reward=16.47, Steps=51281, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1041, Total reward=67.57, Steps=51327, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1042, Total reward=60.45, Steps=51414, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1043, Total reward=104.48, Steps=51532, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1044, Total reward=7.54, Steps=51549, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1045, Total reward=51.23, Steps=51628, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1046, Total reward=29.56, Steps=51672, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1047, Total reward=47.0, Steps=51707, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1048, Total reward=60.56, Steps=51736, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1049, Total reward=30.53, Steps=51761, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1050, Total reward=18.58, Steps=51801, Training iteration=20
Policy training> Surrogate loss=-0.006367893423885107, KL divergence=0.0008467991137877107, Entropy=0.8467499017715454, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0714436024427414, KL divergence=0.018622146919369698, Entropy=0.8335227966308594, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08306684345006943, KL divergence=0.036230579018592834, Entropy=0.8265337944030762, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09899145364761353, KL divergence=0.04511203616857529, Entropy=0.8190032243728638, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09971805661916733, KL divergence=0.05050457641482353, Entropy=0.8078951835632324, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09751389175653458, KL divergence=0.05487943813204765, Entropy=0.8145360946655273, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.11112785339355469, KL divergence=0.05538412183523178, Entropy=0.8176204562187195, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10377000272274017, KL divergence=0.059284914284944534, Entropy=0.813960075378418, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10469351708889008, KL divergence=0.06059776991605759, Entropy=0.8251921534538269, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09884090721607208, KL divergence=0.06313329935073853, Entropy=0.8230469822883606, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/166_Step-51801.ckpt']
Uploaded 3 files for checkpoint 166 in 0.58 seconds
saved intermediate frozen graph: current/model/model_166.pb
Best checkpoint number: 164, Last checkpoint number: 164
Copying the frozen checkpoint from ./frozen_models/agent/model_164.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'158'}
Training> Name=main_level/agent, Worker=0, Episode=1051, Total reward=51.49, Steps=51851, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1052, Total reward=59.94, Steps=51888, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1053, Total reward=52.38, Steps=51929, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1054, Total reward=31.73, Steps=51965, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1055, Total reward=127.53, Steps=52073, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1056, Total reward=13.64, Steps=52091, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1057, Total reward=60.04, Steps=52150, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1058, Total reward=90.86, Steps=52234, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1059, Total reward=67.02, Steps=52308, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1060, Total reward=225.54, Steps=52507, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1061, Total reward=82.68, Steps=52577, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1062, Total reward=65.63, Steps=52649, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1063, Total reward=6.47, Steps=52673, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1064, Total reward=9.49, Steps=52697, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1065, Total reward=1.41, Steps=52711, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1066, Total reward=7.35, Steps=52741, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1067, Total reward=100.58, Steps=52808, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1068, Total reward=86.56, Steps=52850, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1069, Total reward=24.81, Steps=52875, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1070, Total reward=17.94, Steps=52902, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1071, Total reward=87.87, Steps=52952, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1072, Total reward=23.99, Steps=52982, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1073, Total reward=65.75, Steps=53017, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1074, Total reward=43.87, Steps=53060, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1075, Total reward=67.49, Steps=53105, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1076, Total reward=96.06, Steps=53181, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1077, Total reward=62.14, Steps=53247, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1078, Total reward=147.26, Steps=53329, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1079, Total reward=10.14, Steps=53364, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1080, Total reward=11.32, Steps=53391, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1081, Total reward=159.65, Steps=53520, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1082, Total reward=75.44, Steps=53597, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1083, Total reward=25.57, Steps=53640, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1084, Total reward=21.14, Steps=53698, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1085, Total reward=31.23, Steps=53748, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1086, Total reward=117.43, Steps=53817, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1087, Total reward=148.97, Steps=53946, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1088, Total reward=117.61, Steps=54031, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1089, Total reward=17.07, Steps=54063, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1090, Total reward=21.68, Steps=54087, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1091, Total reward=16.51, Steps=54111, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1092, Total reward=44.05, Steps=54143, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1093, Total reward=47.08, Steps=54167, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1094, Total reward=25.01, Steps=54193, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1095, Total reward=88.61, Steps=54282, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1096, Total reward=94.39, Steps=54359, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1097, Total reward=137.78, Steps=54457, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1098, Total reward=130.15, Steps=54539, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1099, Total reward=60.85, Steps=54624, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1100, Total reward=86.24, Steps=54689, Training iteration=21
Policy training> Surrogate loss=0.002279922366142273, KL divergence=0.000977362971752882, Entropy=0.838213324546814, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06816660612821579, KL divergence=0.020972447469830513, Entropy=0.830007016658783, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08138404041528702, KL divergence=0.03714632987976074, Entropy=0.8264749646186829, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09111857414245605, KL divergence=0.047652147710323334, Entropy=0.8220887780189514, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09709492325782776, KL divergence=0.05351056903600693, Entropy=0.8202989101409912, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10064338892698288, KL divergence=0.058145955204963684, Entropy=0.8205066919326782, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09930676966905594, KL divergence=0.06036790460348129, Entropy=0.8195237517356873, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10198197513818741, KL divergence=0.06192284822463989, Entropy=0.8199808597564697, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09914863109588623, KL divergence=0.06360211968421936, Entropy=0.8242722153663635, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.1082165464758873, KL divergence=0.06562639772891998, Entropy=0.8274382948875427, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/167_Step-54689.ckpt']
Uploaded 3 files for checkpoint 167 in 0.58 seconds
saved intermediate frozen graph: current/model/model_167.pb
Best checkpoint number: 164, Last checkpoint number: 165
Copying the frozen checkpoint from ./frozen_models/agent/model_164.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'163'}
Training> Name=main_level/agent, Worker=0, Episode=1101, Total reward=52.51, Steps=54724, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1102, Total reward=43.08, Steps=54774, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1103, Total reward=97.05, Steps=54897, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1104, Total reward=49.49, Steps=54978, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1105, Total reward=8.1, Steps=54992, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1106, Total reward=105.29, Steps=55063, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1107, Total reward=102.43, Steps=55114, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1108, Total reward=82.08, Steps=55153, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1109, Total reward=27.23, Steps=55175, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1110, Total reward=32.14, Steps=55205, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1111, Total reward=103.91, Steps=55277, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1112, Total reward=63.76, Steps=55317, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1113, Total reward=58.03, Steps=55357, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1114, Total reward=39.39, Steps=55384, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1115, Total reward=28.82, Steps=55401, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1116, Total reward=43.14, Steps=55434, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1117, Total reward=63.15, Steps=55495, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1118, Total reward=13.57, Steps=55507, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1119, Total reward=10.57, Steps=55526, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1120, Total reward=51.71, Steps=55596, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1121, Total reward=203.22, Steps=55792, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1122, Total reward=34.69, Steps=55818, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1123, Total reward=128.66, Steps=55937, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1124, Total reward=15.59, Steps=55999, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1125, Total reward=142.56, Steps=56142, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1126, Total reward=82.7, Steps=56211, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1127, Total reward=69.12, Steps=56267, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1128, Total reward=96.61, Steps=56327, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1129, Total reward=82.32, Steps=56383, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1130, Total reward=22.09, Steps=56413, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1131, Total reward=86.14, Steps=56468, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1132, Total reward=89.66, Steps=56509, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1133, Total reward=52.74, Steps=56547, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1134, Total reward=21.27, Steps=56566, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1135, Total reward=104.14, Steps=56667, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1136, Total reward=31.17, Steps=56689, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1137, Total reward=63.86, Steps=56742, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1138, Total reward=46.38, Steps=56799, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1139, Total reward=11.82, Steps=56822, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1140, Total reward=100.97, Steps=56910, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1141, Total reward=77.97, Steps=56989, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1142, Total reward=49.92, Steps=57075, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1143, Total reward=18.94, Steps=57093, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1144, Total reward=96.7, Steps=57170, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1145, Total reward=104.64, Steps=57286, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1146, Total reward=34.2, Steps=57341, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1147, Total reward=85.3, Steps=57402, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1148, Total reward=88.76, Steps=57462, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1149, Total reward=75.28, Steps=57528, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1150, Total reward=42.56, Steps=57576, Training iteration=22
Policy training> Surrogate loss=0.0017651170492172241, KL divergence=0.001278086332604289, Entropy=0.8566070795059204, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06636038422584534, KL divergence=0.019805528223514557, Entropy=0.8403205275535583, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08747107535600662, KL divergence=0.036743078380823135, Entropy=0.832768440246582, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09654355049133301, KL divergence=0.04570239409804344, Entropy=0.8345455527305603, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.10071542114019394, KL divergence=0.05187327042222023, Entropy=0.8343618512153625, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10099279135465622, KL divergence=0.055583931505680084, Entropy=0.8359760046005249, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09927085041999817, KL divergence=0.05861429125070572, Entropy=0.8417255878448486, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.11012948304414749, KL divergence=0.061740901321172714, Entropy=0.8409314751625061, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10616643726825714, KL divergence=0.06321047991514206, Entropy=0.8471176028251648, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10972356051206589, KL divergence=0.0655839741230011, Entropy=0.8460947275161743, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/168_Step-57576.ckpt']
Uploaded 3 files for checkpoint 168 in 0.52 seconds
saved intermediate frozen graph: current/model/model_168.pb
Best checkpoint number: 164, Last checkpoint number: 166
Copying the frozen checkpoint from ./frozen_models/agent/model_164.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'165'}
Training> Name=main_level/agent, Worker=0, Episode=1151, Total reward=112.77, Steps=57642, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1152, Total reward=32.86, Steps=57680, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1153, Total reward=55.31, Steps=57705, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1154, Total reward=45.21, Steps=57749, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1155, Total reward=23.12, Steps=57763, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1156, Total reward=30.06, Steps=57792, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1157, Total reward=5.56, Steps=57815, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1158, Total reward=54.43, Steps=57871, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1159, Total reward=71.27, Steps=57950, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1160, Total reward=57.42, Steps=58054, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1161, Total reward=95.82, Steps=58139, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1162, Total reward=37.54, Steps=58169, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1163, Total reward=24.72, Steps=58237, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1164, Total reward=173.02, Steps=58409, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1165, Total reward=9.91, Steps=58426, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1166, Total reward=25.91, Steps=58466, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1167, Total reward=92.47, Steps=58528, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1168, Total reward=80.3, Steps=58563, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1169, Total reward=52.38, Steps=58596, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1170, Total reward=67.45, Steps=58655, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1171, Total reward=33.66, Steps=58702, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1172, Total reward=80.84, Steps=58747, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1173, Total reward=46.45, Steps=58785, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1174, Total reward=58.78, Steps=58883, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1175, Total reward=27.9, Steps=58897, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1176, Total reward=245.86, Steps=59170, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1177, Total reward=136.85, Steps=59272, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1178, Total reward=33.03, Steps=59297, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1179, Total reward=93.75, Steps=59394, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1180, Total reward=47.74, Steps=59461, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1181, Total reward=135.47, Steps=59621, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1182, Total reward=175.82, Steps=59798, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1183, Total reward=48.71, Steps=59902, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1184, Total reward=161.36, Steps=60082, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1185, Total reward=69.43, Steps=60154, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1186, Total reward=51.03, Steps=60226, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1187, Total reward=166.81, Steps=60349, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1188, Total reward=73.49, Steps=60408, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1189, Total reward=30.21, Steps=60452, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1190, Total reward=34.48, Steps=60506, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1191, Total reward=49.71, Steps=60590, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1192, Total reward=59.23, Steps=60627, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1193, Total reward=50.03, Steps=60660, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1194, Total reward=111.51, Steps=60759, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1195, Total reward=21.74, Steps=60773, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1196, Total reward=21.19, Steps=60788, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1197, Total reward=67.45, Steps=60902, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1198, Total reward=130.8, Steps=61026, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1199, Total reward=247.51, Steps=61270, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1200, Total reward=54.16, Steps=61327, Training iteration=23
Policy training> Surrogate loss=-0.003461087355390191, KL divergence=0.0027033996302634478, Entropy=0.822752058506012, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06832882016897202, KL divergence=0.028664743527770042, Entropy=0.8038569688796997, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08961407095193863, KL divergence=0.045408595353364944, Entropy=0.8026027083396912, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09496240317821503, KL divergence=0.05389941483736038, Entropy=0.8006986975669861, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09787732362747192, KL divergence=0.058164410293102264, Entropy=0.802006721496582, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09426455944776535, KL divergence=0.06106521561741829, Entropy=0.80410236120224, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10401114076375961, KL divergence=0.06364423781633377, Entropy=0.8023624420166016, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09899787604808807, KL divergence=0.06474441289901733, Entropy=0.8019465804100037, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.1089608296751976, KL divergence=0.06645427644252777, Entropy=0.8026095032691956, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10914785414934158, KL divergence=0.06872039288282394, Entropy=0.8079445958137512, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/169_Step-61327.ckpt']
Uploaded 3 files for checkpoint 169 in 0.56 seconds
saved intermediate frozen graph: current/model/model_169.pb
Best checkpoint number: 164, Last checkpoint number: 167
Copying the frozen checkpoint from ./frozen_models/agent/model_164.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'166'}
Training> Name=main_level/agent, Worker=0, Episode=1201, Total reward=86.72, Steps=61414, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1202, Total reward=56.32, Steps=61459, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1203, Total reward=8.95, Steps=61477, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1204, Total reward=0.01, Steps=61486, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1205, Total reward=136.65, Steps=61613, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1206, Total reward=115.91, Steps=61706, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1207, Total reward=39.58, Steps=61789, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1208, Total reward=73.24, Steps=61824, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1209, Total reward=27.62, Steps=61843, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1210, Total reward=87.76, Steps=61934, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1211, Total reward=38.08, Steps=61984, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1212, Total reward=70.19, Steps=62032, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1213, Total reward=0.02, Steps=62050, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1214, Total reward=59.96, Steps=62092, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1215, Total reward=18.14, Steps=62104, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1216, Total reward=89.37, Steps=62193, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1217, Total reward=73.48, Steps=62247, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1218, Total reward=52.68, Steps=62315, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1219, Total reward=30.42, Steps=62390, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1220, Total reward=65.81, Steps=62455, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1221, Total reward=62.99, Steps=62496, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1222, Total reward=35.24, Steps=62519, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1223, Total reward=0.02, Steps=62539, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1224, Total reward=11.31, Steps=62551, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1225, Total reward=23.18, Steps=62588, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1226, Total reward=52.73, Steps=62636, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1227, Total reward=24.54, Steps=62662, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1228, Total reward=81.63, Steps=62699, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1229, Total reward=27.17, Steps=62740, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1230, Total reward=21.89, Steps=62753, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1231, Total reward=43.4, Steps=62793, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1232, Total reward=90.49, Steps=62839, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1233, Total reward=0.02, Steps=62855, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1234, Total reward=14.83, Steps=62881, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1235, Total reward=65.11, Steps=62929, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1236, Total reward=104.98, Steps=63034, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1237, Total reward=18.95, Steps=63055, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1238, Total reward=35.25, Steps=63079, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1239, Total reward=10.13, Steps=63095, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1240, Total reward=103.74, Steps=63195, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1241, Total reward=133.43, Steps=63320, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1242, Total reward=41.34, Steps=63381, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1243, Total reward=107.3, Steps=63489, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1244, Total reward=15.37, Steps=63518, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1245, Total reward=97.81, Steps=63613, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1246, Total reward=89.72, Steps=63678, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1247, Total reward=109.43, Steps=63745, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1248, Total reward=84.02, Steps=63786, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1249, Total reward=52.63, Steps=63842, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1250, Total reward=90.36, Steps=63897, Training iteration=24
Policy training> Surrogate loss=0.0012110484531149268, KL divergence=0.0011044502025470138, Entropy=0.8621250987052917, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0714743360877037, KL divergence=0.021257154643535614, Entropy=0.8496910333633423, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08625349402427673, KL divergence=0.040097255259752274, Entropy=0.8428322076797485, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09644774347543716, KL divergence=0.04992639273405075, Entropy=0.8398531079292297, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.10247135162353516, KL divergence=0.056334108114242554, Entropy=0.8398178219795227, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10318917036056519, KL divergence=0.060267411172389984, Entropy=0.8405847549438477, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10498046875, KL divergence=0.06318452954292297, Entropy=0.8411919474601746, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10853905975818634, KL divergence=0.06539220362901688, Entropy=0.841152012348175, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10786858946084976, KL divergence=0.0670047327876091, Entropy=0.840445876121521, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.11175217479467392, KL divergence=0.06872135400772095, Entropy=0.8404865264892578, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/170_Step-63897.ckpt']
Uploaded 3 files for checkpoint 170 in 0.54 seconds
saved intermediate frozen graph: current/model/model_170.pb
Best checkpoint number: 164, Last checkpoint number: 168
Copying the frozen checkpoint from ./frozen_models/agent/model_164.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'167'}
Training> Name=main_level/agent, Worker=0, Episode=1251, Total reward=22.57, Steps=63913, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1252, Total reward=60.41, Steps=63950, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1253, Total reward=40.97, Steps=63989, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1254, Total reward=42.47, Steps=64024, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1255, Total reward=142.7, Steps=64114, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1256, Total reward=72.94, Steps=64177, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1257, Total reward=59.92, Steps=64239, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1258, Total reward=34.49, Steps=64261, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1259, Total reward=30.55, Steps=64298, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1260, Total reward=7.82, Steps=64313, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1261, Total reward=52.35, Steps=64359, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1262, Total reward=40.43, Steps=64388, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1263, Total reward=106.1, Steps=64496, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1264, Total reward=117.24, Steps=64599, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1265, Total reward=28.39, Steps=64649, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1266, Total reward=85.74, Steps=64725, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1267, Total reward=77.15, Steps=64781, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1268, Total reward=102.58, Steps=64874, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1269, Total reward=68.4, Steps=64940, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1270, Total reward=25.87, Steps=64973, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1271, Total reward=46.08, Steps=65018, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1272, Total reward=92.89, Steps=65072, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1273, Total reward=33.73, Steps=65100, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1274, Total reward=36.37, Steps=65125, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1275, Total reward=82.82, Steps=65198, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1276, Total reward=25.61, Steps=65217, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1277, Total reward=124.72, Steps=65315, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1278, Total reward=138.29, Steps=65439, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1279, Total reward=62.5, Steps=65521, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1280, Total reward=112.11, Steps=65637, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1281, Total reward=184.09, Steps=65829, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1282, Total reward=31.8, Steps=65853, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1283, Total reward=22.8, Steps=65891, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1284, Total reward=25.17, Steps=65937, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1285, Total reward=18.66, Steps=65972, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1286, Total reward=106.6, Steps=66043, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1287, Total reward=72.06, Steps=66102, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1288, Total reward=114.89, Steps=66161, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1289, Total reward=101.71, Steps=66257, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1290, Total reward=67.23, Steps=66318, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1291, Total reward=29.7, Steps=66339, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1292, Total reward=42.85, Steps=66370, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1293, Total reward=85.68, Steps=66428, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1294, Total reward=45.98, Steps=66471, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1295, Total reward=67.7, Steps=66524, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1296, Total reward=91.38, Steps=66598, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1297, Total reward=128.28, Steps=66752, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1298, Total reward=16.29, Steps=66766, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1299, Total reward=148.63, Steps=66952, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1300, Total reward=40.88, Steps=66994, Training iteration=25
Policy training> Surrogate loss=-0.001328893005847931, KL divergence=0.0017997041577473283, Entropy=0.8893783688545227, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06912144273519516, KL divergence=0.02602284587919712, Entropy=0.8818125128746033, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08920545130968094, KL divergence=0.045586083084344864, Entropy=0.8695992827415466, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09943238645792007, KL divergence=0.055826038122177124, Entropy=0.8632394671440125, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09971810132265091, KL divergence=0.06091031804680824, Entropy=0.8591007590293884, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10369902104139328, KL divergence=0.06373552232980728, Entropy=0.8564834594726562, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10619783401489258, KL divergence=0.0666055753827095, Entropy=0.8558503985404968, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10820399969816208, KL divergence=0.06823359429836273, Entropy=0.8561219573020935, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.11159557104110718, KL divergence=0.07017344236373901, Entropy=0.8576838374137878, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.11108220368623734, KL divergence=0.07136937230825424, Entropy=0.8572583794593811, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/171_Step-66994.ckpt']
Uploaded 3 files for checkpoint 171 in 0.60 seconds
saved intermediate frozen graph: current/model/model_171.pb
Best checkpoint number: 164, Last checkpoint number: 169
Copying the frozen checkpoint from ./frozen_models/agent/model_164.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'168'}
Training> Name=main_level/agent, Worker=0, Episode=1301, Total reward=57.58, Steps=67040, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1302, Total reward=88.85, Steps=67157, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1303, Total reward=25.24, Steps=67209, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1304, Total reward=34.5, Steps=67263, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1305, Total reward=16.72, Steps=67299, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1306, Total reward=143.88, Steps=67401, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1307, Total reward=66.95, Steps=67471, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1308, Total reward=83.47, Steps=67527, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1309, Total reward=68.8, Steps=67585, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1310, Total reward=52.9, Steps=67639, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1311, Total reward=54.03, Steps=67694, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1312, Total reward=42.48, Steps=67734, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1313, Total reward=60.7, Steps=67771, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1314, Total reward=52.37, Steps=67810, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1315, Total reward=157.71, Steps=67942, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1316, Total reward=15.97, Steps=67983, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1317, Total reward=72.09, Steps=68033, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1318, Total reward=55.07, Steps=68070, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1319, Total reward=133.1, Steps=68213, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1320, Total reward=75.94, Steps=68267, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1321, Total reward=19.66, Steps=68286, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1322, Total reward=35.11, Steps=68336, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1323, Total reward=55.11, Steps=68446, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1324, Total reward=21.86, Steps=68480, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1325, Total reward=68.52, Steps=68559, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1326, Total reward=74.62, Steps=68619, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1327, Total reward=54.01, Steps=68674, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1328, Total reward=122.56, Steps=68752, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1329, Total reward=119.87, Steps=68814, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1330, Total reward=82.46, Steps=68873, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1331, Total reward=25.31, Steps=68907, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1332, Total reward=67.11, Steps=68967, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1333, Total reward=59.32, Steps=69007, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1334, Total reward=62.73, Steps=69058, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1335, Total reward=20.86, Steps=69071, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1336, Total reward=230.46, Steps=69294, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1337, Total reward=18.68, Steps=69318, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1338, Total reward=42.67, Steps=69354, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1339, Total reward=97.53, Steps=69426, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1340, Total reward=39.96, Steps=69489, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1341, Total reward=59.48, Steps=69548, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1342, Total reward=201.71, Steps=69749, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1343, Total reward=6.38, Steps=69775, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1344, Total reward=14.29, Steps=69826, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1345, Total reward=6.82, Steps=69845, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1346, Total reward=64.15, Steps=69923, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1347, Total reward=104.3, Steps=70036, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1348, Total reward=140.91, Steps=70126, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1349, Total reward=63.63, Steps=70200, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1350, Total reward=140.33, Steps=70273, Training iteration=26
Policy training> Surrogate loss=-0.0007826996152289212, KL divergence=0.0014166930923238397, Entropy=0.8417342305183411, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06918470561504364, KL divergence=0.023335667327046394, Entropy=0.8317506313323975, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0841207504272461, KL divergence=0.041071515530347824, Entropy=0.8271884918212891, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09760770201683044, KL divergence=0.049438636749982834, Entropy=0.8289084434509277, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.10231123119592667, KL divergence=0.05501890182495117, Entropy=0.8305488228797913, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.1001630425453186, KL divergence=0.05925854668021202, Entropy=0.8296474814414978, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10723529011011124, KL divergence=0.061467621475458145, Entropy=0.8305938839912415, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.1036938726902008, KL divergence=0.0646778866648674, Entropy=0.8339784741401672, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.11180748790502548, KL divergence=0.06601032614707947, Entropy=0.8326187133789062, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.11148446798324585, KL divergence=0.06706749647855759, Entropy=0.8299016356468201, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/172_Step-70273.ckpt']
Uploaded 3 files for checkpoint 172 in 0.67 seconds
saved intermediate frozen graph: current/model/model_172.pb
Best checkpoint number: 164, Last checkpoint number: 170
Copying the frozen checkpoint from ./frozen_models/agent/model_164.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'169'}
Training> Name=main_level/agent, Worker=0, Episode=1351, Total reward=38.8, Steps=70310, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1352, Total reward=72.09, Steps=70348, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1353, Total reward=53.97, Steps=70387, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1354, Total reward=59.71, Steps=70425, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1355, Total reward=155.39, Steps=70530, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1356, Total reward=75.82, Steps=70623, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1357, Total reward=68.44, Steps=70745, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1358, Total reward=19.0, Steps=70759, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1359, Total reward=44.87, Steps=70818, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1360, Total reward=76.95, Steps=70903, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1361, Total reward=63.57, Steps=70945, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1362, Total reward=95.19, Steps=71037, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1363, Total reward=18.61, Steps=71079, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1364, Total reward=69.2, Steps=71179, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1365, Total reward=21.52, Steps=71229, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1366, Total reward=184.83, Steps=71372, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1367, Total reward=93.35, Steps=71425, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1368, Total reward=98.71, Steps=71492, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1369, Total reward=26.88, Steps=71523, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1370, Total reward=37.49, Steps=71561, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1371, Total reward=74.8, Steps=71615, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1372, Total reward=48.86, Steps=71648, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1373, Total reward=50.47, Steps=71687, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1374, Total reward=31.73, Steps=71728, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1375, Total reward=21.15, Steps=71743, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1376, Total reward=156.69, Steps=71861, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1377, Total reward=0.0, Steps=71862, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1378, Total reward=84.98, Steps=71955, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1379, Total reward=43.1, Steps=72002, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1380, Total reward=191.12, Steps=72145, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1381, Total reward=175.96, Steps=72285, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1382, Total reward=52.79, Steps=72330, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1383, Total reward=70.37, Steps=72433, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1384, Total reward=89.11, Steps=72533, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1385, Total reward=70.02, Steps=72615, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1386, Total reward=174.81, Steps=72761, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1387, Total reward=103.67, Steps=72836, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1388, Total reward=147.53, Steps=72928, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1389, Total reward=123.94, Steps=73001, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1390, Total reward=66.07, Steps=73057, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1391, Total reward=94.29, Steps=73129, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1392, Total reward=70.57, Steps=73170, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1393, Total reward=13.1, Steps=73190, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1394, Total reward=39.31, Steps=73217, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1395, Total reward=166.82, Steps=73366, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1396, Total reward=47.23, Steps=73426, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1397, Total reward=111.55, Steps=73534, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1398, Total reward=44.49, Steps=73582, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1399, Total reward=44.39, Steps=73633, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1400, Total reward=194.41, Steps=73789, Training iteration=27
Policy training> Surrogate loss=-0.000230959543841891, KL divergence=0.0012059579603374004, Entropy=0.8484318256378174, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.07452207058668137, KL divergence=0.02240263670682907, Entropy=0.8317705988883972, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08602924644947052, KL divergence=0.03977152332663536, Entropy=0.8299289345741272, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09472526609897614, KL divergence=0.04697573930025101, Entropy=0.8334004878997803, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.10666423290967941, KL divergence=0.05271381884813309, Entropy=0.8283252716064453, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10461214184761047, KL divergence=0.056284014135599136, Entropy=0.8324713110923767, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10222511738538742, KL divergence=0.059153538197278976, Entropy=0.8315117359161377, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.11203066259622574, KL divergence=0.06057792529463768, Entropy=0.8329609632492065, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.106194406747818, KL divergence=0.06171347573399544, Entropy=0.8303934931755066, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.11375418305397034, KL divergence=0.06392074376344681, Entropy=0.8349710702896118, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/173_Step-73789.ckpt']
Uploaded 3 files for checkpoint 173 in 0.51 seconds
saved intermediate frozen graph: current/model/model_173.pb
Best checkpoint number: 164, Last checkpoint number: 171
Copying the frozen checkpoint from ./frozen_models/agent/model_164.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'170'}
Training> Name=main_level/agent, Worker=0, Episode=1401, Total reward=57.58, Steps=73870, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1402, Total reward=172.95, Steps=74062, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1403, Total reward=17.93, Steps=74098, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1404, Total reward=21.18, Steps=74155, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1405, Total reward=11.9, Steps=74172, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1406, Total reward=105.38, Steps=74250, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1407, Total reward=25.51, Steps=74279, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1408, Total reward=78.88, Steps=74317, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1409, Total reward=43.89, Steps=74359, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1410, Total reward=74.2, Steps=74427, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1411, Total reward=71.61, Steps=74493, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1412, Total reward=45.28, Steps=74525, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1413, Total reward=66.31, Steps=74567, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1414, Total reward=32.9, Steps=74591, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1415, Total reward=35.23, Steps=74608, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1416, Total reward=74.77, Steps=74672, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1417, Total reward=86.52, Steps=74731, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1418, Total reward=95.9, Steps=74844, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1419, Total reward=103.62, Steps=74917, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1420, Total reward=63.1, Steps=74989, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1421, Total reward=61.06, Steps=75034, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1422, Total reward=33.86, Steps=75069, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1423, Total reward=38.74, Steps=75127, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1424, Total reward=14.45, Steps=75179, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1425, Total reward=10.24, Steps=75201, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1426, Total reward=98.0, Steps=75288, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1427, Total reward=79.05, Steps=75348, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1428, Total reward=139.86, Steps=75438, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1429, Total reward=79.52, Steps=75512, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1430, Total reward=110.36, Steps=75592, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1431, Total reward=19.5, Steps=75609, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1432, Total reward=54.47, Steps=75651, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1433, Total reward=41.73, Steps=75675, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1434, Total reward=135.95, Steps=75786, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1435, Total reward=140.24, Steps=75902, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1436, Total reward=102.8, Steps=75992, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1437, Total reward=67.33, Steps=76033, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1438, Total reward=114.79, Steps=76113, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1439, Total reward=30.92, Steps=76176, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1440, Total reward=102.17, Steps=76274, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1441, Total reward=46.0, Steps=76304, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1442, Total reward=36.14, Steps=76329, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1443, Total reward=92.72, Steps=76444, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1444, Total reward=9.23, Steps=76469, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1445, Total reward=156.01, Steps=76596, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1446, Total reward=95.24, Steps=76659, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1447, Total reward=65.94, Steps=76702, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1448, Total reward=96.09, Steps=76762, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1449, Total reward=32.19, Steps=76787, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1450, Total reward=45.55, Steps=76835, Training iteration=28
Policy training> Surrogate loss=0.0004258997505530715, KL divergence=0.0010354873957112432, Entropy=0.8506973385810852, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0664936974644661, KL divergence=0.02187279984354973, Entropy=0.8334032297134399, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08316896110773087, KL divergence=0.03968033567070961, Entropy=0.820364236831665, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08734850585460663, KL divergence=0.0491943322122097, Entropy=0.8196649551391602, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.10006814450025558, KL divergence=0.05409654229879379, Entropy=0.8215656280517578, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08772418648004532, KL divergence=0.058467671275138855, Entropy=0.8226891160011292, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10034628212451935, KL divergence=0.060452427715063095, Entropy=0.8256544470787048, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10027222335338593, KL divergence=0.06088601425290108, Entropy=0.8180243968963623, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10957395285367966, KL divergence=0.06349007040262222, Entropy=0.8106930255889893, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.11924096941947937, KL divergence=0.06482875347137451, Entropy=0.8182472586631775, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/174_Step-76835.ckpt']
Uploaded 3 files for checkpoint 174 in 0.61 seconds
saved intermediate frozen graph: current/model/model_174.pb
Best checkpoint number: 164, Last checkpoint number: 172
Copying the frozen checkpoint from ./frozen_models/agent/model_164.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'171'}
Training> Name=main_level/agent, Worker=0, Episode=1451, Total reward=43.05, Steps=76865, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1452, Total reward=52.49, Steps=76898, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1453, Total reward=56.08, Steps=76932, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1454, Total reward=16.5, Steps=76954, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1455, Total reward=66.13, Steps=77002, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1456, Total reward=175.37, Steps=77151, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1457, Total reward=123.24, Steps=77245, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1458, Total reward=30.66, Steps=77265, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1459, Total reward=8.45, Steps=77283, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1460, Total reward=71.9, Steps=77341, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1461, Total reward=73.6, Steps=77383, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1462, Total reward=33.09, Steps=77405, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1463, Total reward=151.3, Steps=77549, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1464, Total reward=97.3, Steps=77647, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1465, Total reward=3.84, Steps=77670, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1466, Total reward=115.72, Steps=77733, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1467, Total reward=91.56, Steps=77790, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1468, Total reward=100.32, Steps=77835, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1469, Total reward=27.77, Steps=77855, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1470, Total reward=38.88, Steps=77890, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1471, Total reward=114.33, Steps=77953, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1472, Total reward=63.29, Steps=78013, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1473, Total reward=48.54, Steps=78053, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1474, Total reward=39.6, Steps=78096, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1475, Total reward=161.95, Steps=78219, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1476, Total reward=52.46, Steps=78275, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1477, Total reward=70.0, Steps=78320, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1478, Total reward=61.5, Steps=78357, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1479, Total reward=16.16, Steps=78382, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1480, Total reward=70.51, Steps=78462, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1481, Total reward=63.5, Steps=78533, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1482, Total reward=48.06, Steps=78574, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1483, Total reward=0.02, Steps=78591, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1484, Total reward=11.21, Steps=78619, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1485, Total reward=9.31, Steps=78637, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1486, Total reward=110.99, Steps=78718, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1487, Total reward=78.33, Steps=78776, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1488, Total reward=77.96, Steps=78814, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1489, Total reward=32.8, Steps=78840, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1490, Total reward=37.14, Steps=78886, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1491, Total reward=45.26, Steps=78935, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1492, Total reward=81.19, Steps=78985, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1493, Total reward=3.8, Steps=79006, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1494, Total reward=62.51, Steps=79059, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1495, Total reward=36.8, Steps=79087, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1496, Total reward=93.47, Steps=79170, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1497, Total reward=124.31, Steps=79278, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1498, Total reward=61.86, Steps=79333, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1499, Total reward=42.86, Steps=79387, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1500, Total reward=51.48, Steps=79416, Training iteration=29
Policy training> Surrogate loss=-0.001351039856672287, KL divergence=0.0009307473083026707, Entropy=0.8460538983345032, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.07130280882120132, KL divergence=0.020084330812096596, Entropy=0.8248903155326843, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08751839399337769, KL divergence=0.03908560797572136, Entropy=0.815610408782959, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09673638641834259, KL divergence=0.051008351147174835, Entropy=0.8100124597549438, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.10016047954559326, KL divergence=0.05800200626254082, Entropy=0.8056482076644897, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10447831451892853, KL divergence=0.06238139793276787, Entropy=0.8023824691772461, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.1024603620171547, KL divergence=0.06546002626419067, Entropy=0.8000918626785278, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.1050477996468544, KL divergence=0.06667041033506393, Entropy=0.8001283407211304, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10593964159488678, KL divergence=0.06800027191638947, Entropy=0.8009435534477234, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.11054166406393051, KL divergence=0.06881685554981232, Entropy=0.8007423281669617, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/175_Step-79416.ckpt']
Uploaded 3 files for checkpoint 175 in 0.49 seconds
saved intermediate frozen graph: current/model/model_175.pb
Best checkpoint number: 164, Last checkpoint number: 173
Copying the frozen checkpoint from ./frozen_models/agent/model_164.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'172'}
Training> Name=main_level/agent, Worker=0, Episode=1501, Total reward=35.54, Steps=79475, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1502, Total reward=35.28, Steps=79510, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1503, Total reward=3.58, Steps=79541, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1504, Total reward=18.12, Steps=79579, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1505, Total reward=58.66, Steps=79670, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1506, Total reward=101.85, Steps=79732, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1507, Total reward=10.29, Steps=79758, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1508, Total reward=103.8, Steps=79818, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1509, Total reward=49.93, Steps=79861, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1510, Total reward=46.13, Steps=79924, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1511, Total reward=75.91, Steps=79967, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1512, Total reward=25.37, Steps=79992, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1513, Total reward=61.82, Steps=80031, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1514, Total reward=38.52, Steps=80057, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1515, Total reward=67.31, Steps=80101, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1516, Total reward=17.34, Steps=80120, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1517, Total reward=45.74, Steps=80161, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1518, Total reward=65.89, Steps=80215, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1519, Total reward=118.51, Steps=80283, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1520, Total reward=81.03, Steps=80388, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1521, Total reward=104.79, Steps=80484, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1522, Total reward=159.5, Steps=80613, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1523, Total reward=113.26, Steps=80755, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1524, Total reward=4.02, Steps=80779, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1525, Total reward=10.15, Steps=80798, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1526, Total reward=62.16, Steps=80847, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1527, Total reward=11.46, Steps=80879, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1528, Total reward=68.76, Steps=80938, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1529, Total reward=135.23, Steps=81030, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1530, Total reward=30.39, Steps=81054, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1531, Total reward=54.4, Steps=81098, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1532, Total reward=36.76, Steps=81132, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1533, Total reward=32.57, Steps=81156, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1534, Total reward=36.82, Steps=81201, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1535, Total reward=99.78, Steps=81279, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1536, Total reward=106.78, Steps=81428, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1537, Total reward=67.18, Steps=81476, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1538, Total reward=80.86, Steps=81528, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1539, Total reward=81.66, Steps=81685, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1540, Total reward=88.73, Steps=81744, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1541, Total reward=70.1, Steps=81891, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1542, Total reward=38.66, Steps=81940, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1543, Total reward=71.11, Steps=82033, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1544, Total reward=98.78, Steps=82130, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1545, Total reward=166.57, Steps=82252, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1546, Total reward=153.17, Steps=82361, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1547, Total reward=32.1, Steps=82401, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1548, Total reward=82.51, Steps=82445, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1549, Total reward=48.97, Steps=82483, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1550, Total reward=36.51, Steps=82515, Training iteration=30
Policy training> Surrogate loss=0.0010251719504594803, KL divergence=0.0020023982506245375, Entropy=0.8247929215431213, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06867208331823349, KL divergence=0.025120684877038002, Entropy=0.8169390559196472, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08602675050497055, KL divergence=0.04232313111424446, Entropy=0.8053099513053894, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09640800952911377, KL divergence=0.05254494026303291, Entropy=0.8010156750679016, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09992104768753052, KL divergence=0.05713757872581482, Entropy=0.7993596196174622, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10298255831003189, KL divergence=0.06023197993636131, Entropy=0.8004969954490662, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10046183317899704, KL divergence=0.06241459771990776, Entropy=0.8014187812805176, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10674750804901123, KL divergence=0.06427036970853806, Entropy=0.8043064475059509, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10754725337028503, KL divergence=0.06586163491010666, Entropy=0.8058381080627441, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10882642865180969, KL divergence=0.06710683554410934, Entropy=0.8055808544158936, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/176_Step-82515.ckpt']
Uploaded 3 files for checkpoint 176 in 0.51 seconds
saved intermediate frozen graph: current/model/model_176.pb
Best checkpoint number: 174, Last checkpoint number: 174
Copying the frozen checkpoint from ./frozen_models/agent/model_174.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'164'}
Training> Name=main_level/agent, Worker=0, Episode=1551, Total reward=108.28, Steps=82574, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1552, Total reward=77.89, Steps=82628, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1553, Total reward=33.97, Steps=82651, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1554, Total reward=5.56, Steps=82668, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1555, Total reward=0.0, Steps=82669, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1556, Total reward=93.29, Steps=82749, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1557, Total reward=103.63, Steps=82864, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1558, Total reward=77.48, Steps=82944, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1559, Total reward=25.4, Steps=82988, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1560, Total reward=15.39, Steps=83012, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1561, Total reward=58.47, Steps=83060, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1562, Total reward=53.95, Steps=83139, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1563, Total reward=6.65, Steps=83165, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1564, Total reward=5.46, Steps=83215, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1565, Total reward=55.32, Steps=83303, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1566, Total reward=65.63, Steps=83367, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1567, Total reward=65.37, Steps=83473, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1568, Total reward=73.91, Steps=83518, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1569, Total reward=0.0, Steps=83519, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1570, Total reward=56.06, Steps=83574, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1571, Total reward=11.63, Steps=83591, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1572, Total reward=41.8, Steps=83610, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1573, Total reward=55.16, Steps=83644, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1574, Total reward=31.04, Steps=83668, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1575, Total reward=36.8, Steps=83723, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1576, Total reward=46.4, Steps=83790, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1577, Total reward=70.34, Steps=83847, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1578, Total reward=27.22, Steps=83863, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1579, Total reward=80.69, Steps=83950, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1580, Total reward=60.58, Steps=83993, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1581, Total reward=51.88, Steps=84040, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1582, Total reward=11.12, Steps=84055, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1583, Total reward=15.7, Steps=84101, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1584, Total reward=25.3, Steps=84144, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1585, Total reward=82.09, Steps=84238, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1586, Total reward=108.91, Steps=84354, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1587, Total reward=72.47, Steps=84409, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1588, Total reward=75.63, Steps=84443, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1589, Total reward=55.94, Steps=84492, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1590, Total reward=11.15, Steps=84515, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1591, Total reward=71.24, Steps=84577, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1592, Total reward=89.91, Steps=84625, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1593, Total reward=0.02, Steps=84645, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1594, Total reward=32.65, Steps=84669, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1595, Total reward=56.85, Steps=84757, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1596, Total reward=155.54, Steps=84908, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1597, Total reward=101.36, Steps=84974, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1598, Total reward=38.82, Steps=84998, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1599, Total reward=70.44, Steps=85059, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1600, Total reward=91.53, Steps=85109, Training iteration=31
Policy training> Surrogate loss=-0.0019836563151329756, KL divergence=0.0011476961662992835, Entropy=0.8130753636360168, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.07131826877593994, KL divergence=0.022983869537711143, Entropy=0.8058843612670898, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08949251472949982, KL divergence=0.04175032675266266, Entropy=0.8010103106498718, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09481508284807205, KL divergence=0.05373778194189072, Entropy=0.7953049540519714, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.10011555254459381, KL divergence=0.0605672188103199, Entropy=0.7941350340843201, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10151749849319458, KL divergence=0.06454350054264069, Entropy=0.7945493459701538, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.1053515300154686, KL divergence=0.0671968013048172, Entropy=0.7951577305793762, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10551021248102188, KL divergence=0.06996040046215057, Entropy=0.7954262495040894, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10593296587467194, KL divergence=0.07218403369188309, Entropy=0.7982586026191711, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10844291746616364, KL divergence=0.07405256479978561, Entropy=0.8001699447631836, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/177_Step-85109.ckpt']
Uploaded 3 files for checkpoint 177 in 0.50 seconds
saved intermediate frozen graph: current/model/model_177.pb
Best checkpoint number: 174, Last checkpoint number: 175
Copying the frozen checkpoint from ./frozen_models/agent/model_174.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'173'}
Training> Name=main_level/agent, Worker=0, Episode=1601, Total reward=179.58, Steps=85254, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1602, Total reward=43.59, Steps=85303, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1603, Total reward=9.59, Steps=85329, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1604, Total reward=41.77, Steps=85387, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1605, Total reward=121.5, Steps=85471, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1606, Total reward=135.02, Steps=85553, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1607, Total reward=87.35, Steps=85661, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1608, Total reward=123.11, Steps=85741, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1609, Total reward=96.72, Steps=85813, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1610, Total reward=30.58, Steps=85847, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1611, Total reward=32.5, Steps=85898, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1612, Total reward=93.19, Steps=85951, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1613, Total reward=52.85, Steps=85997, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1614, Total reward=51.72, Steps=86025, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1615, Total reward=75.26, Steps=86079, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1616, Total reward=102.14, Steps=86177, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1617, Total reward=76.98, Steps=86238, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1618, Total reward=121.99, Steps=86333, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1619, Total reward=282.19, Steps=86618, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1620, Total reward=90.97, Steps=86702, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1621, Total reward=227.77, Steps=86880, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1622, Total reward=137.94, Steps=87016, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1623, Total reward=7.67, Steps=87031, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1624, Total reward=6.51, Steps=87048, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1625, Total reward=10.09, Steps=87078, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1626, Total reward=90.21, Steps=87144, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1627, Total reward=78.18, Steps=87219, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1628, Total reward=70.0, Steps=87258, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1629, Total reward=79.54, Steps=87327, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1630, Total reward=37.07, Steps=87376, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1631, Total reward=100.75, Steps=87443, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1632, Total reward=91.29, Steps=87490, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1633, Total reward=3.8, Steps=87511, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1634, Total reward=54.57, Steps=87553, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1635, Total reward=164.95, Steps=87704, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1636, Total reward=30.7, Steps=87745, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1637, Total reward=110.27, Steps=87845, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1638, Total reward=132.92, Steps=87983, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1639, Total reward=112.33, Steps=88090, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1640, Total reward=71.24, Steps=88138, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1641, Total reward=250.58, Steps=88308, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1642, Total reward=28.59, Steps=88326, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1643, Total reward=10.7, Steps=88373, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1644, Total reward=110.55, Steps=88467, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1645, Total reward=11.35, Steps=88496, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1646, Total reward=80.82, Steps=88590, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1647, Total reward=3.24, Steps=88611, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1648, Total reward=102.75, Steps=88656, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1649, Total reward=52.19, Steps=88695, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1650, Total reward=41.49, Steps=88749, Training iteration=32
Policy training> Surrogate loss=0.005921876523643732, KL divergence=0.0021732423920184374, Entropy=0.8421684503555298, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06559063494205475, KL divergence=0.03081480972468853, Entropy=0.8282259702682495, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08281583338975906, KL divergence=0.04664375260472298, Entropy=0.8197070360183716, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09543896466493607, KL divergence=0.05554395914077759, Entropy=0.8171249628067017, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09871117025613785, KL divergence=0.06038108840584755, Entropy=0.8160372972488403, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09634659439325333, KL divergence=0.06364364176988602, Entropy=0.8154975771903992, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10451476275920868, KL divergence=0.06677515059709549, Entropy=0.8157016634941101, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10710866749286652, KL divergence=0.0681728795170784, Entropy=0.817152202129364, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.11148553341627121, KL divergence=0.07111113518476486, Entropy=0.822123110294342, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10907123982906342, KL divergence=0.0728297010064125, Entropy=0.8208985328674316, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/178_Step-88749.ckpt']
Uploaded 3 files for checkpoint 178 in 0.55 seconds
saved intermediate frozen graph: current/model/model_178.pb
Best checkpoint number: 174, Last checkpoint number: 176
Copying the frozen checkpoint from ./frozen_models/agent/model_174.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'175'}
Training> Name=main_level/agent, Worker=0, Episode=1651, Total reward=51.46, Steps=88829, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1652, Total reward=36.18, Steps=88863, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1653, Total reward=40.22, Steps=88888, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1654, Total reward=23.02, Steps=88912, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1655, Total reward=33.64, Steps=88929, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1656, Total reward=22.23, Steps=88953, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1657, Total reward=128.47, Steps=89042, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1658, Total reward=74.7, Steps=89125, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1659, Total reward=63.7, Steps=89230, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1660, Total reward=101.26, Steps=89327, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1661, Total reward=51.32, Steps=89376, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1662, Total reward=48.82, Steps=89418, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1663, Total reward=20.16, Steps=89472, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1664, Total reward=10.3, Steps=89495, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1665, Total reward=73.1, Steps=89587, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1666, Total reward=77.32, Steps=89662, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1667, Total reward=97.82, Steps=89769, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1668, Total reward=182.72, Steps=89868, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1669, Total reward=24.53, Steps=89886, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1670, Total reward=11.24, Steps=89898, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1671, Total reward=33.89, Steps=89936, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1672, Total reward=18.13, Steps=89967, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1673, Total reward=61.47, Steps=90003, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1674, Total reward=21.5, Steps=90024, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1675, Total reward=19.36, Steps=90037, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1676, Total reward=20.8, Steps=90054, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1677, Total reward=66.88, Steps=90111, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1678, Total reward=57.43, Steps=90151, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1679, Total reward=83.53, Steps=90220, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1680, Total reward=77.1, Steps=90287, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1681, Total reward=61.87, Steps=90331, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1682, Total reward=40.48, Steps=90374, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1683, Total reward=13.84, Steps=90412, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1684, Total reward=19.98, Steps=90502, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1685, Total reward=6.36, Steps=90532, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1686, Total reward=145.78, Steps=90646, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1687, Total reward=68.69, Steps=90702, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1688, Total reward=25.32, Steps=90716, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1689, Total reward=39.99, Steps=90771, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1690, Total reward=11.22, Steps=90798, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1691, Total reward=57.27, Steps=90852, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1692, Total reward=87.29, Steps=90901, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1693, Total reward=50.54, Steps=90939, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1694, Total reward=38.67, Steps=90965, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1695, Total reward=38.09, Steps=90997, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1696, Total reward=71.65, Steps=91069, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1697, Total reward=26.51, Steps=91092, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1698, Total reward=197.09, Steps=91255, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1699, Total reward=5.67, Steps=91271, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1700, Total reward=98.85, Steps=91323, Training iteration=33
Policy training> Surrogate loss=-0.0005120579153299332, KL divergence=0.0013503567315638065, Entropy=0.8147555589675903, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06259464472532272, KL divergence=0.02290932461619377, Entropy=0.8006526827812195, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08446753770112991, KL divergence=0.041606366634368896, Entropy=0.7838010191917419, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09014318883419037, KL divergence=0.05280957743525505, Entropy=0.7771814465522766, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09676321595907211, KL divergence=0.05987207219004631, Entropy=0.7761443257331848, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0980658084154129, KL divergence=0.0646735429763794, Entropy=0.7755930423736572, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10292981564998627, KL divergence=0.06816508620977402, Entropy=0.775749683380127, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10300557315349579, KL divergence=0.07051629573106766, Entropy=0.7761507630348206, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10640953481197357, KL divergence=0.07235529273748398, Entropy=0.7773479223251343, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10426031053066254, KL divergence=0.07358520478010178, Entropy=0.7753576636314392, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/179_Step-91323.ckpt']
Uploaded 3 files for checkpoint 179 in 0.50 seconds
saved intermediate frozen graph: current/model/model_179.pb
Best checkpoint number: 174, Last checkpoint number: 177
Copying the frozen checkpoint from ./frozen_models/agent/model_174.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'176'}
Training> Name=main_level/agent, Worker=0, Episode=1701, Total reward=77.22, Steps=91384, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1702, Total reward=37.17, Steps=91413, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1703, Total reward=19.49, Steps=91479, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1704, Total reward=16.81, Steps=91503, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1705, Total reward=104.87, Steps=91587, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1706, Total reward=90.96, Steps=91673, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1707, Total reward=43.01, Steps=91712, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1708, Total reward=60.73, Steps=91758, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1709, Total reward=24.97, Steps=91783, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1710, Total reward=35.14, Steps=91821, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1711, Total reward=35.19, Steps=91854, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1712, Total reward=97.3, Steps=91902, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1713, Total reward=0.02, Steps=91917, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1714, Total reward=25.95, Steps=91944, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1715, Total reward=34.04, Steps=91976, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1716, Total reward=79.68, Steps=92044, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1717, Total reward=20.11, Steps=92065, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1718, Total reward=59.12, Steps=92111, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1719, Total reward=90.9, Steps=92181, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1720, Total reward=92.25, Steps=92244, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1721, Total reward=97.64, Steps=92360, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1722, Total reward=35.05, Steps=92392, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1723, Total reward=120.07, Steps=92500, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1724, Total reward=2.81, Steps=92533, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1725, Total reward=5.91, Steps=92549, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1726, Total reward=162.8, Steps=92671, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1727, Total reward=96.67, Steps=92730, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1728, Total reward=153.36, Steps=92813, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1729, Total reward=41.33, Steps=92847, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1730, Total reward=61.52, Steps=92899, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1731, Total reward=41.65, Steps=92951, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1732, Total reward=72.24, Steps=93003, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1733, Total reward=31.92, Steps=93028, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1734, Total reward=46.47, Steps=93054, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1735, Total reward=73.23, Steps=93133, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1736, Total reward=30.74, Steps=93166, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1737, Total reward=64.19, Steps=93222, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1738, Total reward=46.12, Steps=93251, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1739, Total reward=45.89, Steps=93330, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1740, Total reward=89.53, Steps=93398, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1741, Total reward=52.74, Steps=93433, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1742, Total reward=65.76, Steps=93512, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1743, Total reward=2.99, Steps=93529, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1744, Total reward=19.35, Steps=93575, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1745, Total reward=27.51, Steps=93611, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1746, Total reward=92.29, Steps=93666, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1747, Total reward=128.53, Steps=93777, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1748, Total reward=77.51, Steps=93817, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1749, Total reward=20.74, Steps=93840, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1750, Total reward=30.51, Steps=93895, Training iteration=34
Policy training> Surrogate loss=0.0013112660963088274, KL divergence=0.0011436805361881852, Entropy=0.8189977407455444, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06788519024848938, KL divergence=0.01929723285138607, Entropy=0.8035521507263184, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08447582274675369, KL divergence=0.03761761263012886, Entropy=0.7940478324890137, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09492354094982147, KL divergence=0.05009275674819946, Entropy=0.7915620803833008, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09828977286815643, KL divergence=0.057316362857818604, Entropy=0.7872871160507202, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10205379873514175, KL divergence=0.06228585168719292, Entropy=0.7858579754829407, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10487349331378937, KL divergence=0.06554468721151352, Entropy=0.7843436002731323, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10524673759937286, KL divergence=0.06786717474460602, Entropy=0.7857596278190613, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10848797857761383, KL divergence=0.06933270394802094, Entropy=0.785076916217804, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10832563787698746, KL divergence=0.07121983915567398, Entropy=0.7865190505981445, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/180_Step-93895.ckpt']
Uploaded 3 files for checkpoint 180 in 0.59 seconds
saved intermediate frozen graph: current/model/model_180.pb
Best checkpoint number: 174, Last checkpoint number: 178
Copying the frozen checkpoint from ./frozen_models/agent/model_174.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'177'}
Training> Name=main_level/agent, Worker=0, Episode=1751, Total reward=36.73, Steps=93932, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1752, Total reward=81.92, Steps=93979, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1753, Total reward=16.89, Steps=94007, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1754, Total reward=69.11, Steps=94049, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1755, Total reward=104.09, Steps=94135, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1756, Total reward=140.48, Steps=94267, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1757, Total reward=76.31, Steps=94329, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1758, Total reward=122.91, Steps=94444, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1759, Total reward=248.41, Steps=94654, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1760, Total reward=177.03, Steps=94809, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1761, Total reward=52.03, Steps=94841, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1762, Total reward=18.31, Steps=94857, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1763, Total reward=117.76, Steps=95031, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1764, Total reward=211.72, Steps=95194, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1765, Total reward=98.9, Steps=95279, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1766, Total reward=195.14, Steps=95409, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1767, Total reward=98.91, Steps=95468, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1768, Total reward=68.86, Steps=95559, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1769, Total reward=52.08, Steps=95590, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1770, Total reward=0.0, Steps=95591, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1771, Total reward=66.23, Steps=95622, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1772, Total reward=99.15, Steps=95676, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1773, Total reward=60.46, Steps=95716, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1774, Total reward=91.57, Steps=95798, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1775, Total reward=91.01, Steps=95888, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1776, Total reward=163.52, Steps=96002, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1777, Total reward=48.64, Steps=96058, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1778, Total reward=63.95, Steps=96092, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1779, Total reward=71.05, Steps=96176, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1780, Total reward=41.58, Steps=96210, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1781, Total reward=67.49, Steps=96262, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1782, Total reward=46.81, Steps=96311, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1783, Total reward=117.85, Steps=96441, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1784, Total reward=62.66, Steps=96514, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1785, Total reward=196.45, Steps=96662, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1786, Total reward=18.01, Steps=96679, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1787, Total reward=107.13, Steps=96735, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1788, Total reward=102.49, Steps=96800, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1789, Total reward=36.71, Steps=96822, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1790, Total reward=11.7, Steps=96843, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1791, Total reward=106.51, Steps=96911, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1792, Total reward=97.66, Steps=96963, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1793, Total reward=52.47, Steps=97000, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1794, Total reward=45.96, Steps=97037, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1795, Total reward=35.08, Steps=97067, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1796, Total reward=23.13, Steps=97101, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1797, Total reward=52.43, Steps=97131, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1798, Total reward=27.91, Steps=97172, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1799, Total reward=38.77, Steps=97208, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1800, Total reward=135.78, Steps=97335, Training iteration=35
Policy training> Surrogate loss=0.0026082422118633986, KL divergence=0.0021544781047850847, Entropy=0.7936820387840271, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06854216754436493, KL divergence=0.027468077838420868, Entropy=0.7849238514900208, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0893101692199707, KL divergence=0.044696394354104996, Entropy=0.7707332372665405, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0967157632112503, KL divergence=0.05319347605109215, Entropy=0.7646056413650513, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09800247848033905, KL divergence=0.05864720046520233, Entropy=0.7674898505210876, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09403065592050552, KL divergence=0.06109710782766342, Entropy=0.7664127945899963, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10549663752317429, KL divergence=0.06484833359718323, Entropy=0.7668113112449646, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.1049172431230545, KL divergence=0.06581943482160568, Entropy=0.7736960053443909, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10964401811361313, KL divergence=0.06880497187376022, Entropy=0.7719622254371643, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10859120637178421, KL divergence=0.0706959143280983, Entropy=0.7761754989624023, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/181_Step-97335.ckpt']
Uploaded 3 files for checkpoint 181 in 0.53 seconds
saved intermediate frozen graph: current/model/model_181.pb
Best checkpoint number: 174, Last checkpoint number: 179
Copying the frozen checkpoint from ./frozen_models/agent/model_174.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'178'}
Training> Name=main_level/agent, Worker=0, Episode=1801, Total reward=47.6, Steps=97371, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1802, Total reward=40.83, Steps=97405, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1803, Total reward=14.2, Steps=97448, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1804, Total reward=97.8, Steps=97547, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1805, Total reward=140.7, Steps=97621, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1806, Total reward=83.52, Steps=97699, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1807, Total reward=148.03, Steps=97801, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1808, Total reward=115.18, Steps=97885, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1809, Total reward=50.3, Steps=97948, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1810, Total reward=37.91, Steps=98011, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1811, Total reward=92.49, Steps=98076, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1812, Total reward=83.09, Steps=98124, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1813, Total reward=0.02, Steps=98139, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1814, Total reward=46.99, Steps=98185, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1815, Total reward=109.12, Steps=98271, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1816, Total reward=54.19, Steps=98334, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1817, Total reward=9.56, Steps=98347, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1818, Total reward=37.67, Steps=98372, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1819, Total reward=6.44, Steps=98405, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1820, Total reward=95.84, Steps=98506, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1821, Total reward=75.38, Steps=98576, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1822, Total reward=92.02, Steps=98692, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1823, Total reward=227.45, Steps=98869, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1824, Total reward=107.49, Steps=98977, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1825, Total reward=98.43, Steps=99086, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1826, Total reward=138.67, Steps=99196, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1827, Total reward=109.93, Steps=99320, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1828, Total reward=83.46, Steps=99359, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1829, Total reward=128.83, Steps=99437, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1830, Total reward=84.77, Steps=99500, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1831, Total reward=48.0, Steps=99582, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1832, Total reward=22.03, Steps=99622, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1833, Total reward=46.16, Steps=99665, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1834, Total reward=11.65, Steps=99686, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1835, Total reward=151.95, Steps=99801, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1836, Total reward=10.96, Steps=99818, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1837, Total reward=137.18, Steps=99926, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1838, Total reward=313.27, Steps=100145, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1839, Total reward=18.98, Steps=100184, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1840, Total reward=60.04, Steps=100232, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1841, Total reward=203.79, Steps=100409, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1842, Total reward=69.83, Steps=100481, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1843, Total reward=9.98, Steps=100521, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1844, Total reward=111.39, Steps=100638, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1845, Total reward=21.0, Steps=100694, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1846, Total reward=64.84, Steps=100764, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1847, Total reward=152.99, Steps=100870, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1848, Total reward=21.11, Steps=100884, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1849, Total reward=37.56, Steps=100919, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1850, Total reward=15.06, Steps=100937, Training iteration=36
Policy training> Surrogate loss=0.00032858658232726157, KL divergence=0.004600878339260817, Entropy=0.8107849955558777, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0749598890542984, KL divergence=0.03459522873163223, Entropy=0.7901702523231506, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0903647169470787, KL divergence=0.05131334438920021, Entropy=0.7797046899795532, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09968540817499161, KL divergence=0.060716453939676285, Entropy=0.7753978967666626, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.10323846340179443, KL divergence=0.06531866639852524, Entropy=0.7767195701599121, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10665129125118256, KL divergence=0.06830798834562302, Entropy=0.7773641347885132, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10766441375017166, KL divergence=0.07094008475542068, Entropy=0.7756155133247375, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10893720388412476, KL divergence=0.07247477769851685, Entropy=0.7771537899971008, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.1102328822016716, KL divergence=0.07437161356210709, Entropy=0.7777093052864075, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10997325927019119, KL divergence=0.07635122537612915, Entropy=0.7777960896492004, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/182_Step-100937.ckpt']
Uploaded 3 files for checkpoint 182 in 0.61 seconds
saved intermediate frozen graph: current/model/model_182.pb
Best checkpoint number: 180, Last checkpoint number: 180
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'174'}
Training> Name=main_level/agent, Worker=0, Episode=1851, Total reward=44.0, Steps=100979, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1852, Total reward=83.93, Steps=101032, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1853, Total reward=57.25, Steps=101071, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1854, Total reward=54.91, Steps=101108, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1855, Total reward=165.5, Steps=101246, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1856, Total reward=159.38, Steps=101383, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1857, Total reward=71.33, Steps=101433, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1858, Total reward=76.14, Steps=101491, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1859, Total reward=93.09, Steps=101556, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1860, Total reward=185.0, Steps=101705, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1861, Total reward=37.17, Steps=101725, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1862, Total reward=38.95, Steps=101751, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1863, Total reward=9.97, Steps=101792, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1864, Total reward=97.1, Steps=101883, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1865, Total reward=6.15, Steps=101907, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1866, Total reward=75.5, Steps=101994, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1867, Total reward=248.16, Steps=102205, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1868, Total reward=168.04, Steps=102304, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1869, Total reward=40.94, Steps=102324, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1870, Total reward=141.43, Steps=102453, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1871, Total reward=65.76, Steps=102497, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1872, Total reward=81.56, Steps=102545, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1873, Total reward=58.47, Steps=102583, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1874, Total reward=41.14, Steps=102608, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1875, Total reward=34.49, Steps=102644, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1876, Total reward=26.94, Steps=102680, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1877, Total reward=187.75, Steps=102794, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1878, Total reward=26.5, Steps=102835, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1879, Total reward=59.45, Steps=102905, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1880, Total reward=56.88, Steps=102955, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1881, Total reward=112.41, Steps=103035, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1882, Total reward=47.7, Steps=103057, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1883, Total reward=154.63, Steps=103240, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1884, Total reward=23.3, Steps=103273, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1885, Total reward=13.48, Steps=103307, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1886, Total reward=81.45, Steps=103408, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1887, Total reward=3.7, Steps=103421, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1888, Total reward=88.44, Steps=103464, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1889, Total reward=128.14, Steps=103559, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1890, Total reward=48.7, Steps=103620, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1891, Total reward=95.44, Steps=103686, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1892, Total reward=99.32, Steps=103772, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1893, Total reward=60.56, Steps=103809, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1894, Total reward=24.19, Steps=103833, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1895, Total reward=73.81, Steps=103876, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1896, Total reward=30.62, Steps=103893, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1897, Total reward=148.74, Steps=104019, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1898, Total reward=104.13, Steps=104075, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1899, Total reward=102.07, Steps=104211, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1900, Total reward=119.21, Steps=104302, Training iteration=37
Policy training> Surrogate loss=-0.00044773632544092834, KL divergence=0.001973744248971343, Entropy=0.7707352042198181, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0719749927520752, KL divergence=0.031240804120898247, Entropy=0.7581807374954224, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08858045190572739, KL divergence=0.050592876970767975, Entropy=0.7521994709968567, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0951564684510231, KL divergence=0.059705574065446854, Entropy=0.7490902543067932, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0981941968202591, KL divergence=0.06446731835603714, Entropy=0.7450559139251709, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09776090830564499, KL divergence=0.06764625012874603, Entropy=0.7477983236312866, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.1061611920595169, KL divergence=0.07057198882102966, Entropy=0.7461437582969666, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10468167811632156, KL divergence=0.07261757552623749, Entropy=0.7467816472053528, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10903723537921906, KL divergence=0.07398039847612381, Entropy=0.7509241104125977, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10498134791851044, KL divergence=0.07585328072309494, Entropy=0.7509396076202393, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/183_Step-104302.ckpt']
Uploaded 3 files for checkpoint 183 in 0.55 seconds
saved intermediate frozen graph: current/model/model_183.pb
Best checkpoint number: 180, Last checkpoint number: 181
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'179'}
Training> Name=main_level/agent, Worker=0, Episode=1901, Total reward=61.86, Steps=104347, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1902, Total reward=10.73, Steps=104363, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1903, Total reward=0.02, Steps=104381, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1904, Total reward=19.88, Steps=104421, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1905, Total reward=161.44, Steps=104561, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1906, Total reward=3.65, Steps=104573, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1907, Total reward=93.01, Steps=104625, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1908, Total reward=87.48, Steps=104666, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1909, Total reward=28.75, Steps=104701, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1910, Total reward=43.34, Steps=104737, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1911, Total reward=30.54, Steps=104772, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1912, Total reward=42.98, Steps=104804, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1913, Total reward=60.68, Steps=104841, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1914, Total reward=44.92, Steps=104880, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1915, Total reward=71.23, Steps=104930, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1916, Total reward=34.06, Steps=104957, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1917, Total reward=90.2, Steps=105025, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1918, Total reward=107.25, Steps=105109, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1919, Total reward=77.52, Steps=105194, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1920, Total reward=35.06, Steps=105229, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1921, Total reward=71.45, Steps=105277, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1922, Total reward=39.33, Steps=105308, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1923, Total reward=8.29, Steps=105346, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1924, Total reward=42.31, Steps=105402, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1925, Total reward=112.51, Steps=105482, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1926, Total reward=76.44, Steps=105537, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1927, Total reward=20.36, Steps=105560, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1928, Total reward=129.68, Steps=105661, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1929, Total reward=123.12, Steps=105750, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1930, Total reward=55.1, Steps=105808, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1931, Total reward=47.61, Steps=105851, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1932, Total reward=60.81, Steps=105889, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1933, Total reward=57.43, Steps=105928, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1934, Total reward=62.92, Steps=105972, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1935, Total reward=72.57, Steps=106044, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1936, Total reward=97.04, Steps=106126, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1937, Total reward=178.05, Steps=106245, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1938, Total reward=65.7, Steps=106280, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1939, Total reward=111.06, Steps=106371, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1940, Total reward=85.2, Steps=106414, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1941, Total reward=69.19, Steps=106456, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1942, Total reward=33.22, Steps=106489, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1943, Total reward=367.27, Steps=106791, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1944, Total reward=6.95, Steps=106803, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1945, Total reward=103.29, Steps=106878, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1946, Total reward=21.85, Steps=106902, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1947, Total reward=33.91, Steps=106928, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1948, Total reward=110.58, Steps=106989, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1949, Total reward=61.52, Steps=107049, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1950, Total reward=30.3, Steps=107112, Training iteration=38
Policy training> Surrogate loss=-0.0008637215942144394, KL divergence=0.00111790606752038, Entropy=0.7704116702079773, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06108067184686661, KL divergence=0.021773774176836014, Entropy=0.7620894312858582, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0925523042678833, KL divergence=0.0436372235417366, Entropy=0.7493869662284851, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09895195066928864, KL divergence=0.057206738740205765, Entropy=0.7526108622550964, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.10099244117736816, KL divergence=0.0634884461760521, Entropy=0.7465716600418091, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09861405938863754, KL divergence=0.06950929015874863, Entropy=0.745921790599823, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.11272529512643814, KL divergence=0.07367056608200073, Entropy=0.7414361238479614, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.11150956153869629, KL divergence=0.0751543939113617, Entropy=0.7496741414070129, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10427287966012955, KL divergence=0.07683847844600677, Entropy=0.7599448561668396, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.1145876795053482, KL divergence=0.07943231612443924, Entropy=0.7615145444869995, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/184_Step-107112.ckpt']
Uploaded 3 files for checkpoint 184 in 0.54 seconds
saved intermediate frozen graph: current/model/model_184.pb
Best checkpoint number: 180, Last checkpoint number: 182
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'181'}
Training> Name=main_level/agent, Worker=0, Episode=1951, Total reward=30.37, Steps=107146, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1952, Total reward=44.67, Steps=107187, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1953, Total reward=59.05, Steps=107224, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1954, Total reward=46.83, Steps=107257, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1955, Total reward=103.43, Steps=107345, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1956, Total reward=23.18, Steps=107366, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1957, Total reward=100.77, Steps=107472, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1958, Total reward=102.48, Steps=107576, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1959, Total reward=28.59, Steps=107608, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1960, Total reward=78.27, Steps=107662, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1961, Total reward=76.83, Steps=107737, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1962, Total reward=57.07, Steps=107798, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1963, Total reward=10.21, Steps=107833, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1964, Total reward=9.64, Steps=107860, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1965, Total reward=89.39, Steps=107941, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1966, Total reward=48.57, Steps=107964, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1967, Total reward=47.29, Steps=107998, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1968, Total reward=107.35, Steps=108082, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1969, Total reward=49.13, Steps=108116, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1970, Total reward=19.12, Steps=108134, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1971, Total reward=96.38, Steps=108198, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1972, Total reward=43.11, Steps=108231, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1973, Total reward=66.37, Steps=108269, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1974, Total reward=66.11, Steps=108318, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1975, Total reward=14.84, Steps=108344, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1976, Total reward=143.03, Steps=108500, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1977, Total reward=32.33, Steps=108529, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1978, Total reward=201.98, Steps=108714, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1979, Total reward=94.1, Steps=108790, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1980, Total reward=81.22, Steps=108829, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1981, Total reward=76.86, Steps=108871, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1982, Total reward=128.34, Steps=109002, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1983, Total reward=14.44, Steps=109041, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1984, Total reward=143.84, Steps=109160, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1985, Total reward=99.39, Steps=109241, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1986, Total reward=38.87, Steps=109280, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1987, Total reward=364.66, Steps=109578, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1988, Total reward=107.59, Steps=109643, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1989, Total reward=30.79, Steps=109660, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1990, Total reward=94.73, Steps=109749, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1991, Total reward=20.06, Steps=109781, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1992, Total reward=82.09, Steps=109828, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1993, Total reward=53.32, Steps=109866, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1994, Total reward=109.75, Steps=109995, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1995, Total reward=31.37, Steps=110040, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1996, Total reward=45.17, Steps=110102, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1997, Total reward=157.11, Steps=110206, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1998, Total reward=71.04, Steps=110244, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1999, Total reward=10.54, Steps=110278, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=2000, Total reward=78.9, Steps=110361, Training iteration=39
Policy training> Surrogate loss=-0.0003825094026979059, KL divergence=0.0016845565987750888, Entropy=0.7789086699485779, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.063885398209095, KL divergence=0.02871909737586975, Entropy=0.7766149044036865, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08281143754720688, KL divergence=0.049944281578063965, Entropy=0.7672939896583557, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09699324518442154, KL divergence=0.06055253744125366, Entropy=0.7598496079444885, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09927824139595032, KL divergence=0.06570948660373688, Entropy=0.759070634841919, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10095593333244324, KL divergence=0.07117857038974762, Entropy=0.7615160942077637, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10451778024435043, KL divergence=0.07304974645376205, Entropy=0.7574500441551208, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10062535852193832, KL divergence=0.0743173286318779, Entropy=0.7571544647216797, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10860168933868408, KL divergence=0.07656283676624298, Entropy=0.7586240768432617, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10260263830423355, KL divergence=0.07759055495262146, Entropy=0.7630768418312073, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/185_Step-110361.ckpt']
Uploaded 3 files for checkpoint 185 in 0.51 seconds
saved intermediate frozen graph: current/model/model_185.pb
Best checkpoint number: 180, Last checkpoint number: 183
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'182'}
Training> Name=main_level/agent, Worker=0, Episode=2001, Total reward=40.52, Steps=110385, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2002, Total reward=91.08, Steps=110496, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2003, Total reward=19.74, Steps=110532, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2004, Total reward=13.23, Steps=110563, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2005, Total reward=24.72, Steps=110611, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2006, Total reward=21.0, Steps=110641, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2007, Total reward=98.06, Steps=110716, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2008, Total reward=96.39, Steps=110790, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2009, Total reward=66.04, Steps=110864, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2010, Total reward=77.98, Steps=110917, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2011, Total reward=24.26, Steps=110941, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2012, Total reward=87.69, Steps=110995, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2013, Total reward=42.34, Steps=111020, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2014, Total reward=197.02, Steps=111221, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2015, Total reward=170.91, Steps=111355, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2016, Total reward=22.02, Steps=111387, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2017, Total reward=81.21, Steps=111433, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2018, Total reward=22.5, Steps=111447, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2019, Total reward=46.43, Steps=111509, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2020, Total reward=73.59, Steps=111570, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2021, Total reward=70.96, Steps=111617, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2022, Total reward=166.87, Steps=111734, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2023, Total reward=29.84, Steps=111785, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2024, Total reward=28.45, Steps=111846, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2025, Total reward=152.94, Steps=111962, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2026, Total reward=195.83, Steps=112096, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2027, Total reward=128.82, Steps=112224, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2028, Total reward=67.62, Steps=112277, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2029, Total reward=121.72, Steps=112368, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2030, Total reward=61.9, Steps=112427, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2031, Total reward=59.63, Steps=112460, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2032, Total reward=63.74, Steps=112520, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2033, Total reward=31.26, Steps=112564, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2034, Total reward=58.37, Steps=112601, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2035, Total reward=110.44, Steps=112699, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2036, Total reward=36.94, Steps=112731, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2037, Total reward=60.26, Steps=112769, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2038, Total reward=153.58, Steps=112889, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2039, Total reward=221.68, Steps=113058, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2040, Total reward=94.56, Steps=113152, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2041, Total reward=79.8, Steps=113219, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2042, Total reward=80.96, Steps=113312, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2043, Total reward=0.03, Steps=113339, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2044, Total reward=7.3, Steps=113351, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2045, Total reward=159.36, Steps=113521, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2046, Total reward=96.12, Steps=113593, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2047, Total reward=133.35, Steps=113703, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2048, Total reward=103.88, Steps=113782, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2049, Total reward=61.24, Steps=113859, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2050, Total reward=43.89, Steps=113906, Training iteration=40
Policy training> Surrogate loss=0.005430448334664106, KL divergence=0.0013350489316508174, Entropy=0.7924864888191223, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.062223903834819794, KL divergence=0.03275522217154503, Entropy=0.7713700532913208, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07333174347877502, KL divergence=0.053329985588788986, Entropy=0.768042266368866, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0881972461938858, KL divergence=0.06247983127832413, Entropy=0.7606188654899597, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09242303669452667, KL divergence=0.06530488282442093, Entropy=0.7602627277374268, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09568267315626144, KL divergence=0.06965810805559158, Entropy=0.7622190117835999, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10247238725423813, KL divergence=0.07236608117818832, Entropy=0.7607651352882385, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.1045694649219513, KL divergence=0.07461966574192047, Entropy=0.7626166939735413, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.1073833629488945, KL divergence=0.07636986672878265, Entropy=0.7594295740127563, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10490676760673523, KL divergence=0.07727081328630447, Entropy=0.7581297159194946, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/186_Step-113906.ckpt']
Uploaded 3 files for checkpoint 186 in 0.53 seconds
saved intermediate frozen graph: current/model/model_186.pb
Best checkpoint number: 180, Last checkpoint number: 184
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'183'}
Training> Name=main_level/agent, Worker=0, Episode=2051, Total reward=51.57, Steps=113953, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2052, Total reward=85.37, Steps=114004, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2053, Total reward=25.66, Steps=114027, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2054, Total reward=9.78, Steps=114046, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2055, Total reward=156.29, Steps=114171, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2056, Total reward=85.55, Steps=114242, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2057, Total reward=179.15, Steps=114368, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2058, Total reward=75.54, Steps=114425, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2059, Total reward=27.42, Steps=114459, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2060, Total reward=36.38, Steps=114500, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2061, Total reward=64.32, Steps=114546, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2062, Total reward=140.62, Steps=114679, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2063, Total reward=3.0, Steps=114705, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2064, Total reward=10.33, Steps=114743, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2065, Total reward=117.95, Steps=114871, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2066, Total reward=79.84, Steps=114942, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2067, Total reward=120.8, Steps=115005, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2068, Total reward=90.29, Steps=115043, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2069, Total reward=121.59, Steps=115142, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2070, Total reward=115.07, Steps=115205, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2071, Total reward=45.95, Steps=115249, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2072, Total reward=71.39, Steps=115299, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2073, Total reward=52.87, Steps=115335, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2074, Total reward=50.24, Steps=115373, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2075, Total reward=19.8, Steps=115405, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2076, Total reward=99.96, Steps=115490, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2077, Total reward=48.54, Steps=115532, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2078, Total reward=86.58, Steps=115626, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2079, Total reward=237.78, Steps=115802, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2080, Total reward=63.19, Steps=115854, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2081, Total reward=85.44, Steps=115903, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2082, Total reward=38.72, Steps=115928, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2083, Total reward=121.41, Steps=116076, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2084, Total reward=27.68, Steps=116139, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2085, Total reward=3.22, Steps=116149, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2086, Total reward=62.99, Steps=116207, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2087, Total reward=174.17, Steps=116343, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2088, Total reward=135.77, Steps=116424, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2089, Total reward=45.5, Steps=116468, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2090, Total reward=22.56, Steps=116495, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2091, Total reward=108.84, Steps=116563, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2092, Total reward=85.49, Steps=116616, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2093, Total reward=61.92, Steps=116658, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2094, Total reward=44.99, Steps=116696, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2095, Total reward=28.24, Steps=116723, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2096, Total reward=157.31, Steps=116884, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2097, Total reward=29.28, Steps=116908, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2098, Total reward=298.05, Steps=117166, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2099, Total reward=209.18, Steps=117335, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2100, Total reward=99.62, Steps=117398, Training iteration=41
Policy training> Surrogate loss=-0.002773358253762126, KL divergence=0.0019739705603569746, Entropy=0.7664780020713806, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06432387977838516, KL divergence=0.031870558857917786, Entropy=0.7471997737884521, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08316612243652344, KL divergence=0.04923206567764282, Entropy=0.7508410811424255, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08918052166700363, KL divergence=0.05845528095960617, Entropy=0.7527914047241211, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09517852216959, KL divergence=0.06541883945465088, Entropy=0.7535113096237183, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09261061251163483, KL divergence=0.06865547597408295, Entropy=0.7553090453147888, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10052156448364258, KL divergence=0.07006406784057617, Entropy=0.7578903436660767, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10439334809780121, KL divergence=0.07293935865163803, Entropy=0.7552341818809509, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10428083688020706, KL divergence=0.07542695850133896, Entropy=0.7569897770881653, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10615728795528412, KL divergence=0.07607387751340866, Entropy=0.755433976650238, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/187_Step-117398.ckpt']
Uploaded 3 files for checkpoint 187 in 0.52 seconds
saved intermediate frozen graph: current/model/model_187.pb
Best checkpoint number: 180, Last checkpoint number: 185
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'184'}
Training> Name=main_level/agent, Worker=0, Episode=2101, Total reward=163.8, Steps=117543, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2102, Total reward=56.14, Steps=117617, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2103, Total reward=131.0, Steps=117731, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2104, Total reward=88.92, Steps=117857, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2105, Total reward=40.49, Steps=117922, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2106, Total reward=32.57, Steps=117963, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2107, Total reward=137.46, Steps=118080, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2108, Total reward=127.22, Steps=118136, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2109, Total reward=90.39, Steps=118218, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2110, Total reward=29.97, Steps=118252, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2111, Total reward=58.97, Steps=118300, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2112, Total reward=92.86, Steps=118352, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2113, Total reward=57.85, Steps=118392, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2114, Total reward=51.36, Steps=118418, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2115, Total reward=12.03, Steps=118430, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2116, Total reward=76.93, Steps=118491, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2117, Total reward=53.09, Steps=118527, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2118, Total reward=75.1, Steps=118590, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2119, Total reward=245.73, Steps=118821, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2120, Total reward=218.84, Steps=118969, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2121, Total reward=88.01, Steps=119021, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2122, Total reward=32.39, Steps=119057, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2123, Total reward=102.32, Steps=119161, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2124, Total reward=12.52, Steps=119209, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2125, Total reward=102.11, Steps=119308, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2126, Total reward=32.6, Steps=119349, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2127, Total reward=317.18, Steps=119619, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2128, Total reward=86.38, Steps=119658, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2129, Total reward=31.54, Steps=119703, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2130, Total reward=92.37, Steps=119779, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2131, Total reward=89.34, Steps=119843, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2132, Total reward=22.03, Steps=119864, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2133, Total reward=47.49, Steps=119898, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2134, Total reward=53.91, Steps=119925, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2135, Total reward=210.03, Steps=120119, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2136, Total reward=93.0, Steps=120193, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2137, Total reward=77.54, Steps=120239, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2138, Total reward=58.15, Steps=120281, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2139, Total reward=24.34, Steps=120316, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2140, Total reward=119.43, Steps=120405, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2141, Total reward=25.87, Steps=120427, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2142, Total reward=36.15, Steps=120453, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2143, Total reward=3.28, Steps=120478, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2144, Total reward=9.26, Steps=120500, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2145, Total reward=95.56, Steps=120593, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2146, Total reward=13.64, Steps=120616, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2147, Total reward=11.94, Steps=120667, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2148, Total reward=154.47, Steps=120755, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2149, Total reward=73.72, Steps=120837, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2150, Total reward=43.48, Steps=120867, Training iteration=42
Policy training> Surrogate loss=0.0014299589674919844, KL divergence=0.0017219088040292263, Entropy=0.7735903859138489, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.07138605415821075, KL divergence=0.031508252024650574, Entropy=0.765094518661499, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08818478882312775, KL divergence=0.05446401983499527, Entropy=0.7511945366859436, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09730972349643707, KL divergence=0.0640810951590538, Entropy=0.7504109740257263, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.10108720511198044, KL divergence=0.06944850832223892, Entropy=0.7505817413330078, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10214388370513916, KL divergence=0.07244677096605301, Entropy=0.7553929686546326, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10069335252046585, KL divergence=0.07570076733827591, Entropy=0.7532290816307068, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10321475565433502, KL divergence=0.07773347198963165, Entropy=0.7519298791885376, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10341489315032959, KL divergence=0.07872697710990906, Entropy=0.7559953331947327, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10912950336933136, KL divergence=0.08019652962684631, Entropy=0.7552028894424438, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/188_Step-120867.ckpt']
Uploaded 3 files for checkpoint 188 in 0.54 seconds
saved intermediate frozen graph: current/model/model_188.pb
Best checkpoint number: 180, Last checkpoint number: 186
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'185'}
Training> Name=main_level/agent, Worker=0, Episode=2151, Total reward=49.33, Steps=120918, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2152, Total reward=67.99, Steps=120954, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2153, Total reward=59.05, Steps=120993, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2154, Total reward=44.85, Steps=121019, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2155, Total reward=12.75, Steps=121033, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2156, Total reward=92.95, Steps=121095, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2157, Total reward=48.48, Steps=121141, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2158, Total reward=44.53, Steps=121171, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2159, Total reward=95.8, Steps=121261, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2160, Total reward=0.0, Steps=121262, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2161, Total reward=80.21, Steps=121335, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2162, Total reward=32.41, Steps=121398, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2163, Total reward=71.35, Steps=121473, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2164, Total reward=12.74, Steps=121505, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2165, Total reward=75.16, Steps=121599, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2166, Total reward=80.66, Steps=121687, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2167, Total reward=66.14, Steps=121744, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2168, Total reward=88.32, Steps=121780, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2169, Total reward=23.37, Steps=121821, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2170, Total reward=66.42, Steps=121886, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2171, Total reward=107.36, Steps=121948, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2172, Total reward=65.17, Steps=121985, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2173, Total reward=72.76, Steps=122022, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2174, Total reward=41.55, Steps=122050, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2175, Total reward=57.09, Steps=122097, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2176, Total reward=84.65, Steps=122178, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2177, Total reward=72.74, Steps=122239, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2178, Total reward=71.73, Steps=122290, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2179, Total reward=33.81, Steps=122326, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2180, Total reward=43.73, Steps=122351, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2181, Total reward=83.28, Steps=122432, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2182, Total reward=61.56, Steps=122515, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2183, Total reward=214.67, Steps=122686, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2184, Total reward=9.65, Steps=122731, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2185, Total reward=159.46, Steps=122885, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2186, Total reward=41.55, Steps=122978, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2187, Total reward=159.53, Steps=123086, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2188, Total reward=77.66, Steps=123133, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2189, Total reward=23.56, Steps=123176, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2190, Total reward=56.74, Steps=123237, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2191, Total reward=41.39, Steps=123269, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2192, Total reward=102.57, Steps=123324, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2193, Total reward=58.73, Steps=123361, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2194, Total reward=60.35, Steps=123399, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2195, Total reward=139.58, Steps=123526, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2196, Total reward=25.27, Steps=123564, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2197, Total reward=75.96, Steps=123628, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2198, Total reward=68.87, Steps=123661, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2199, Total reward=95.91, Steps=123743, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2200, Total reward=14.88, Steps=123772, Training iteration=43
Policy training> Surrogate loss=-0.002545642899349332, KL divergence=0.0017836079932749271, Entropy=0.7644533514976501, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06262708455324173, KL divergence=0.028225397691130638, Entropy=0.7472139596939087, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08333145827054977, KL divergence=0.05080094188451767, Entropy=0.7334570288658142, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0959620550274849, KL divergence=0.06415534764528275, Entropy=0.72446608543396, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09639153629541397, KL divergence=0.07197834551334381, Entropy=0.7260227203369141, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.10065910965204239, KL divergence=0.07512405514717102, Entropy=0.7216916084289551, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09705767035484314, KL divergence=0.07694505900144577, Entropy=0.7220343947410583, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.1040387824177742, KL divergence=0.08001241832971573, Entropy=0.7205259799957275, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09885304421186447, KL divergence=0.08045560866594315, Entropy=0.7232915759086609, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10164462774991989, KL divergence=0.08217938989400864, Entropy=0.7229047417640686, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/189_Step-123772.ckpt']
Uploaded 3 files for checkpoint 189 in 0.57 seconds
saved intermediate frozen graph: current/model/model_189.pb
Best checkpoint number: 180, Last checkpoint number: 187
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'186'}
Training> Name=main_level/agent, Worker=0, Episode=2201, Total reward=235.08, Steps=123945, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2202, Total reward=36.36, Steps=123977, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2203, Total reward=0.02, Steps=124001, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2204, Total reward=4.7, Steps=124025, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2205, Total reward=34.56, Steps=124091, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2206, Total reward=108.13, Steps=124160, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2207, Total reward=110.45, Steps=124224, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2208, Total reward=78.44, Steps=124259, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2209, Total reward=24.66, Steps=124287, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2210, Total reward=60.46, Steps=124349, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2211, Total reward=99.41, Steps=124414, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2212, Total reward=23.11, Steps=124444, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2213, Total reward=53.74, Steps=124480, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2214, Total reward=30.63, Steps=124503, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2215, Total reward=42.56, Steps=124554, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2216, Total reward=44.39, Steps=124597, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2217, Total reward=40.37, Steps=124621, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2218, Total reward=64.24, Steps=124671, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2219, Total reward=17.25, Steps=124697, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2220, Total reward=107.05, Steps=124811, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2221, Total reward=57.13, Steps=124904, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2222, Total reward=108.88, Steps=125058, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2223, Total reward=15.4, Steps=125094, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2224, Total reward=31.62, Steps=125132, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2225, Total reward=15.19, Steps=125154, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2226, Total reward=74.66, Steps=125224, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2227, Total reward=88.58, Steps=125288, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2228, Total reward=107.49, Steps=125341, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2229, Total reward=36.7, Steps=125366, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2230, Total reward=102.5, Steps=125425, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2231, Total reward=95.39, Steps=125497, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2232, Total reward=84.8, Steps=125536, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2233, Total reward=60.84, Steps=125575, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2234, Total reward=49.42, Steps=125617, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2235, Total reward=107.51, Steps=125687, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2236, Total reward=34.42, Steps=125708, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2237, Total reward=59.26, Steps=125751, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2238, Total reward=68.41, Steps=125792, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2239, Total reward=61.13, Steps=125852, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2240, Total reward=70.17, Steps=125920, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2241, Total reward=89.38, Steps=125983, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2242, Total reward=34.77, Steps=126028, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2243, Total reward=27.13, Steps=126108, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2244, Total reward=14.65, Steps=126139, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2245, Total reward=39.94, Steps=126182, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2246, Total reward=182.27, Steps=126319, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2247, Total reward=90.95, Steps=126372, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2248, Total reward=27.52, Steps=126390, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2249, Total reward=113.17, Steps=126460, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2250, Total reward=15.15, Steps=126475, Training iteration=44
Policy training> Surrogate loss=-0.002778967609629035, KL divergence=0.0014331822749227285, Entropy=0.7588123679161072, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.07076015323400497, KL divergence=0.02894323505461216, Entropy=0.7419136762619019, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07740356028079987, KL divergence=0.05037084221839905, Entropy=0.7287024259567261, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08629538118839264, KL divergence=0.06196032091975212, Entropy=0.7318510413169861, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0932144895195961, KL divergence=0.0686032772064209, Entropy=0.7231236696243286, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0963667780160904, KL divergence=0.07291177660226822, Entropy=0.7261090278625488, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09678395092487335, KL divergence=0.07443693280220032, Entropy=0.7287066578865051, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10236413776874542, KL divergence=0.07618134468793869, Entropy=0.7267006039619446, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10908385366201401, KL divergence=0.07877612113952637, Entropy=0.726431131362915, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09870744496583939, KL divergence=0.07810793817043304, Entropy=0.7248766422271729, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/190_Step-126475.ckpt']
Uploaded 3 files for checkpoint 190 in 0.63 seconds
saved intermediate frozen graph: current/model/model_190.pb
Best checkpoint number: 180, Last checkpoint number: 188
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'187'}
Training> Name=main_level/agent, Worker=0, Episode=2251, Total reward=103.82, Steps=126540, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2252, Total reward=89.97, Steps=126590, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2253, Total reward=53.07, Steps=126625, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2254, Total reward=49.19, Steps=126652, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2255, Total reward=182.67, Steps=126796, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2256, Total reward=90.56, Steps=126881, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2257, Total reward=120.54, Steps=126973, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2258, Total reward=175.08, Steps=127148, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2259, Total reward=101.55, Steps=127211, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2260, Total reward=104.16, Steps=127265, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2261, Total reward=231.27, Steps=127477, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2262, Total reward=141.75, Steps=127634, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2263, Total reward=37.37, Steps=127747, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2264, Total reward=8.23, Steps=127770, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2265, Total reward=9.34, Steps=127797, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2266, Total reward=126.11, Steps=127921, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2267, Total reward=171.84, Steps=128054, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2268, Total reward=84.83, Steps=128095, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2269, Total reward=55.25, Steps=128149, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2270, Total reward=63.94, Steps=128216, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2271, Total reward=57.73, Steps=128259, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2272, Total reward=17.17, Steps=128299, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2273, Total reward=49.37, Steps=128338, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2274, Total reward=66.92, Steps=128380, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2275, Total reward=57.29, Steps=128431, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2276, Total reward=21.99, Steps=128449, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2277, Total reward=3.85, Steps=128459, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2278, Total reward=154.63, Steps=128550, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2279, Total reward=16.18, Steps=128571, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2280, Total reward=233.67, Steps=128791, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2281, Total reward=84.07, Steps=128917, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2282, Total reward=41.34, Steps=128951, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2283, Total reward=184.89, Steps=129130, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2284, Total reward=217.66, Steps=129304, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2285, Total reward=138.41, Steps=129429, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2286, Total reward=171.46, Steps=129573, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2287, Total reward=16.36, Steps=129592, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2288, Total reward=84.54, Steps=129645, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2289, Total reward=83.97, Steps=129719, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2290, Total reward=18.9, Steps=129733, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2291, Total reward=83.2, Steps=129795, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2292, Total reward=85.99, Steps=129841, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2293, Total reward=55.8, Steps=129880, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2294, Total reward=125.42, Steps=130012, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2295, Total reward=31.06, Steps=130039, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2296, Total reward=131.51, Steps=130178, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2297, Total reward=53.78, Steps=130220, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2298, Total reward=66.32, Steps=130265, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2299, Total reward=101.18, Steps=130363, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2300, Total reward=85.64, Steps=130440, Training iteration=45
Policy training> Surrogate loss=-0.00042715793824754655, KL divergence=0.0028702006675302982, Entropy=0.7417610883712769, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0701209083199501, KL divergence=0.03620591387152672, Entropy=0.719187319278717, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08401534706354141, KL divergence=0.056509580463171005, Entropy=0.7157732844352722, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09168459475040436, KL divergence=0.06462632864713669, Entropy=0.7136055827140808, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0923927053809166, KL divergence=0.06913796067237854, Entropy=0.7100212574005127, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09929291903972626, KL divergence=0.07233917713165283, Entropy=0.7133846879005432, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09997694939374924, KL divergence=0.07391876727342606, Entropy=0.7152807116508484, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09370148926973343, KL divergence=0.07606218755245209, Entropy=0.7130267024040222, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0969460979104042, KL divergence=0.07679995894432068, Entropy=0.718993604183197, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10119102150201797, KL divergence=0.07961579412221909, Entropy=0.716918408870697, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/191_Step-130440.ckpt']
Uploaded 3 files for checkpoint 191 in 0.55 seconds
saved intermediate frozen graph: current/model/model_191.pb
Best checkpoint number: 180, Last checkpoint number: 189
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'188'}
Training> Name=main_level/agent, Worker=0, Episode=2301, Total reward=66.88, Steps=130489, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2302, Total reward=35.3, Steps=130538, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2303, Total reward=15.68, Steps=130571, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2304, Total reward=99.43, Steps=130668, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2305, Total reward=7.32, Steps=130688, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2306, Total reward=39.25, Steps=130736, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2307, Total reward=120.14, Steps=130854, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2308, Total reward=110.72, Steps=130907, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2309, Total reward=15.03, Steps=130921, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2310, Total reward=34.23, Steps=130960, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2311, Total reward=53.67, Steps=131016, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2312, Total reward=87.38, Steps=131069, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2313, Total reward=29.79, Steps=131092, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2314, Total reward=25.95, Steps=131119, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2315, Total reward=81.83, Steps=131172, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2316, Total reward=28.8, Steps=131195, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2317, Total reward=171.13, Steps=131342, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2318, Total reward=132.04, Steps=131462, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2319, Total reward=93.54, Steps=131577, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2320, Total reward=221.08, Steps=131736, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2321, Total reward=138.71, Steps=131886, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2322, Total reward=33.56, Steps=131908, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2323, Total reward=151.3, Steps=132037, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2324, Total reward=35.0, Steps=132090, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2325, Total reward=105.28, Steps=132206, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2326, Total reward=66.16, Steps=132282, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2327, Total reward=8.41, Steps=132312, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2328, Total reward=88.57, Steps=132355, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2329, Total reward=107.92, Steps=132438, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2330, Total reward=122.65, Steps=132516, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2331, Total reward=11.82, Steps=132554, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2332, Total reward=79.48, Steps=132636, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2333, Total reward=57.53, Steps=132674, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2334, Total reward=65.83, Steps=132716, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2335, Total reward=300.36, Steps=133004, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2336, Total reward=111.4, Steps=133137, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2337, Total reward=47.25, Steps=133168, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2338, Total reward=57.75, Steps=133221, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2339, Total reward=96.47, Steps=133306, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2340, Total reward=115.28, Steps=133406, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2341, Total reward=14.7, Steps=133421, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2342, Total reward=55.07, Steps=133487, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2343, Total reward=32.35, Steps=133564, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2344, Total reward=132.99, Steps=133707, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2345, Total reward=18.34, Steps=133732, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2346, Total reward=184.46, Steps=133878, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2347, Total reward=98.33, Steps=133934, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2348, Total reward=122.9, Steps=134018, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2349, Total reward=31.07, Steps=134053, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2350, Total reward=30.16, Steps=134122, Training iteration=46
Policy training> Surrogate loss=-0.0018102031899616122, KL divergence=0.0024107596836984158, Entropy=0.7591075301170349, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06723994761705399, KL divergence=0.03578030318021774, Entropy=0.7396315336227417, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08092107623815536, KL divergence=0.05645374208688736, Entropy=0.7314820289611816, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09332994371652603, KL divergence=0.06470437347888947, Entropy=0.73497074842453, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08926402032375336, KL divergence=0.07032135874032974, Entropy=0.7329303026199341, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09921787679195404, KL divergence=0.0730445608496666, Entropy=0.7340408563613892, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10157809406518936, KL divergence=0.07744646817445755, Entropy=0.7270000576972961, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.10120753943920135, KL divergence=0.07869631052017212, Entropy=0.7293093800544739, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.1011628732085228, KL divergence=0.08057350665330887, Entropy=0.7316887974739075, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.1020418182015419, KL divergence=0.08303891867399216, Entropy=0.7295547723770142, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/192_Step-134122.ckpt']
Uploaded 3 files for checkpoint 192 in 0.50 seconds
saved intermediate frozen graph: current/model/model_192.pb
Best checkpoint number: 180, Last checkpoint number: 190
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'189'}
Training> Name=main_level/agent, Worker=0, Episode=2351, Total reward=44.68, Steps=134160, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2352, Total reward=31.35, Steps=134200, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2353, Total reward=50.73, Steps=134236, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2354, Total reward=27.21, Steps=134260, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2355, Total reward=360.55, Steps=134542, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2356, Total reward=73.41, Steps=134607, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2357, Total reward=21.88, Steps=134641, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2358, Total reward=178.96, Steps=134778, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2359, Total reward=148.46, Steps=134931, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2360, Total reward=109.31, Steps=135035, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2361, Total reward=69.46, Steps=135077, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2362, Total reward=61.66, Steps=135154, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2363, Total reward=195.88, Steps=135340, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2364, Total reward=155.66, Steps=135485, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2365, Total reward=126.2, Steps=135568, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2366, Total reward=22.01, Steps=135609, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2367, Total reward=81.94, Steps=135662, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2368, Total reward=115.53, Steps=135760, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2369, Total reward=116.22, Steps=135833, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2370, Total reward=39.53, Steps=135895, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2371, Total reward=101.2, Steps=135966, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2372, Total reward=56.82, Steps=136000, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2373, Total reward=54.3, Steps=136034, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2374, Total reward=33.47, Steps=136047, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2375, Total reward=5.3, Steps=136062, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2376, Total reward=182.72, Steps=136193, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2377, Total reward=71.67, Steps=136245, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2378, Total reward=81.07, Steps=136329, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2379, Total reward=96.61, Steps=136399, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2380, Total reward=62.95, Steps=136433, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2381, Total reward=30.4, Steps=136451, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2382, Total reward=147.19, Steps=136611, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2383, Total reward=13.24, Steps=136648, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2384, Total reward=30.58, Steps=136685, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2385, Total reward=105.22, Steps=136776, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2386, Total reward=80.93, Steps=136858, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2387, Total reward=64.05, Steps=136902, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2388, Total reward=179.16, Steps=137011, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2389, Total reward=73.14, Steps=137081, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2390, Total reward=162.32, Steps=137240, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2391, Total reward=42.54, Steps=137287, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2392, Total reward=94.8, Steps=137348, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2393, Total reward=50.22, Steps=137385, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2394, Total reward=57.1, Steps=137423, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2395, Total reward=18.07, Steps=137450, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2396, Total reward=37.42, Steps=137483, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2397, Total reward=145.43, Steps=137598, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2398, Total reward=165.79, Steps=137723, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2399, Total reward=73.25, Steps=137816, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2400, Total reward=56.17, Steps=137858, Training iteration=47
Policy training> Surrogate loss=0.007967380806803703, KL divergence=0.002281125169247389, Entropy=0.7312380075454712, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06561772525310516, KL divergence=0.031914569437503815, Entropy=0.7085760831832886, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08216319978237152, KL divergence=0.05297461524605751, Entropy=0.7046743631362915, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.09236613661050797, KL divergence=0.062513068318367, Entropy=0.7010058760643005, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09734483063220978, KL divergence=0.06833142787218094, Entropy=0.7028918862342834, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09632003307342529, KL divergence=0.07186230272054672, Entropy=0.7012726664543152, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.10738887637853622, KL divergence=0.07382979243993759, Entropy=0.700819194316864, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.1028483435511589, KL divergence=0.07565449178218842, Entropy=0.7034271955490112, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10507556051015854, KL divergence=0.07748429477214813, Entropy=0.704903244972229, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10600365698337555, KL divergence=0.07918114215135574, Entropy=0.7036771774291992, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/193_Step-137858.ckpt']
Uploaded 3 files for checkpoint 193 in 0.58 seconds
saved intermediate frozen graph: current/model/model_193.pb
Best checkpoint number: 180, Last checkpoint number: 191
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'190'}
Training> Name=main_level/agent, Worker=0, Episode=2401, Total reward=202.7, Steps=138056, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2402, Total reward=146.99, Steps=138178, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2403, Total reward=13.04, Steps=138257, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2404, Total reward=93.87, Steps=138373, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2405, Total reward=92.84, Steps=138453, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2406, Total reward=203.0, Steps=138590, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2407, Total reward=115.87, Steps=138699, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2408, Total reward=87.3, Steps=138742, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2409, Total reward=34.9, Steps=138777, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2410, Total reward=38.59, Steps=138825, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2411, Total reward=87.76, Steps=138899, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2412, Total reward=40.75, Steps=138932, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2413, Total reward=56.62, Steps=138966, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2414, Total reward=69.97, Steps=139008, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2415, Total reward=97.88, Steps=139076, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2416, Total reward=26.13, Steps=139095, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2417, Total reward=70.98, Steps=139154, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2418, Total reward=306.37, Steps=139406, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2419, Total reward=120.94, Steps=139512, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2420, Total reward=71.36, Steps=139558, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2421, Total reward=79.44, Steps=139613, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2422, Total reward=37.5, Steps=139679, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2423, Total reward=123.38, Steps=139792, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2424, Total reward=90.02, Steps=139886, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2425, Total reward=144.38, Steps=140013, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2426, Total reward=107.4, Steps=140117, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2427, Total reward=72.16, Steps=140175, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2428, Total reward=88.0, Steps=140215, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2429, Total reward=94.44, Steps=140295, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2430, Total reward=15.14, Steps=140309, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2431, Total reward=97.16, Steps=140378, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2432, Total reward=49.49, Steps=140410, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2433, Total reward=2.81, Steps=140434, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2434, Total reward=30.26, Steps=140459, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2435, Total reward=144.44, Steps=140579, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2436, Total reward=86.92, Steps=140646, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2437, Total reward=163.54, Steps=140746, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2438, Total reward=129.63, Steps=140878, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2439, Total reward=115.63, Steps=141001, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2440, Total reward=120.11, Steps=141108, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2441, Total reward=147.25, Steps=141247, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2442, Total reward=149.1, Steps=141383, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2443, Total reward=167.34, Steps=141539, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2444, Total reward=110.26, Steps=141646, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2445, Total reward=13.5, Steps=141672, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2446, Total reward=103.41, Steps=141742, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2447, Total reward=128.46, Steps=141829, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2448, Total reward=105.75, Steps=141929, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2449, Total reward=16.38, Steps=141942, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2450, Total reward=26.53, Steps=141961, Training iteration=48
Policy training> Surrogate loss=0.002209198195487261, KL divergence=0.0038060268852859735, Entropy=0.7255210280418396, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0632750391960144, KL divergence=0.040572281926870346, Entropy=0.7119894027709961, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07970772683620453, KL divergence=0.058272894471883774, Entropy=0.7110257148742676, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08719193935394287, KL divergence=0.0656270831823349, Entropy=0.7080891728401184, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09217026829719543, KL divergence=0.07047112286090851, Entropy=0.7044306993484497, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0948399007320404, KL divergence=0.07439345866441727, Entropy=0.702269434928894, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09701653569936752, KL divergence=0.07752042263746262, Entropy=0.7019256949424744, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09945032000541687, KL divergence=0.07993853092193604, Entropy=0.7021958827972412, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10116245597600937, KL divergence=0.08206748962402344, Entropy=0.7025989294052124, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.10242029279470444, KL divergence=0.08383670449256897, Entropy=0.702069878578186, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/194_Step-141961.ckpt']
Uploaded 3 files for checkpoint 194 in 0.60 seconds
saved intermediate frozen graph: current/model/model_194.pb
Best checkpoint number: 180, Last checkpoint number: 192
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'191'}
Training> Name=main_level/agent, Worker=0, Episode=2451, Total reward=34.23, Steps=142010, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2452, Total reward=27.31, Steps=142045, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2453, Total reward=55.11, Steps=142084, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2454, Total reward=49.22, Steps=142112, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2455, Total reward=195.51, Steps=142311, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2456, Total reward=245.16, Steps=142539, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2457, Total reward=122.32, Steps=142675, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2458, Total reward=139.72, Steps=142775, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2459, Total reward=100.81, Steps=142844, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2460, Total reward=210.71, Steps=143041, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2461, Total reward=29.21, Steps=143065, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2462, Total reward=61.23, Steps=143126, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2463, Total reward=161.05, Steps=143287, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2464, Total reward=0.03, Steps=143313, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2465, Total reward=87.5, Steps=143399, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2466, Total reward=122.26, Steps=143483, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2467, Total reward=120.05, Steps=143576, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2468, Total reward=108.77, Steps=143678, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2469, Total reward=24.35, Steps=143711, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2470, Total reward=113.63, Steps=143792, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2471, Total reward=105.6, Steps=143860, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2472, Total reward=31.41, Steps=143891, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2473, Total reward=14.2, Steps=143915, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2474, Total reward=69.96, Steps=143945, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2475, Total reward=16.11, Steps=143958, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2476, Total reward=93.17, Steps=144033, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2477, Total reward=87.06, Steps=144088, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2478, Total reward=68.84, Steps=144124, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2479, Total reward=177.91, Steps=144308, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2480, Total reward=84.89, Steps=144391, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2481, Total reward=45.81, Steps=144441, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2482, Total reward=166.16, Steps=144582, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2483, Total reward=3.69, Steps=144605, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2484, Total reward=3.57, Steps=144619, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2485, Total reward=91.35, Steps=144720, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2486, Total reward=102.04, Steps=144792, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2487, Total reward=192.44, Steps=144919, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2488, Total reward=94.69, Steps=144958, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2489, Total reward=112.69, Steps=145027, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2490, Total reward=48.57, Steps=145070, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2491, Total reward=43.09, Steps=145106, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2492, Total reward=12.94, Steps=145139, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2493, Total reward=55.94, Steps=145177, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2494, Total reward=54.77, Steps=145223, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2495, Total reward=26.37, Steps=145237, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2496, Total reward=49.65, Steps=145297, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2497, Total reward=159.93, Steps=145397, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2498, Total reward=21.11, Steps=145412, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2499, Total reward=80.34, Steps=145495, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2500, Total reward=93.81, Steps=145556, Training iteration=49
Policy training> Surrogate loss=-0.0016916407039389014, KL divergence=0.002966560423374176, Entropy=0.7147866487503052, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06429044157266617, KL divergence=0.03847001865506172, Entropy=0.692753255367279, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07892502844333649, KL divergence=0.0588916540145874, Entropy=0.6844474077224731, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08691870421171188, KL divergence=0.06707515567541122, Entropy=0.6848694086074829, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0913667306303978, KL divergence=0.07144935429096222, Entropy=0.6854944825172424, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09338117390871048, KL divergence=0.07478872686624527, Entropy=0.6855334043502808, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09687482565641403, KL divergence=0.07694105803966522, Entropy=0.6856295466423035, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09797944873571396, KL divergence=0.07912731915712357, Entropy=0.6852763891220093, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09933574497699738, KL divergence=0.08052075654268265, Entropy=0.6846867203712463, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09973115473985672, KL divergence=0.08185375481843948, Entropy=0.6851005554199219, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/195_Step-145556.ckpt']
Uploaded 3 files for checkpoint 195 in 0.47 seconds
saved intermediate frozen graph: current/model/model_195.pb
Best checkpoint number: 180, Last checkpoint number: 193
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'192'}
Training> Name=main_level/agent, Worker=0, Episode=2501, Total reward=76.97, Steps=145598, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2502, Total reward=24.03, Steps=145622, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2503, Total reward=17.3, Steps=145681, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2504, Total reward=150.43, Steps=145826, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2505, Total reward=130.67, Steps=145960, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2506, Total reward=121.08, Steps=146068, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2507, Total reward=125.0, Steps=146178, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2508, Total reward=69.86, Steps=146219, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2509, Total reward=41.92, Steps=146259, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2510, Total reward=61.32, Steps=146330, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2511, Total reward=56.41, Steps=146376, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2512, Total reward=93.56, Steps=146425, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2513, Total reward=41.4, Steps=146469, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2514, Total reward=57.95, Steps=146515, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2515, Total reward=334.24, Steps=146798, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2516, Total reward=99.75, Steps=146894, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2517, Total reward=79.22, Steps=146945, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2518, Total reward=81.27, Steps=147021, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2519, Total reward=50.96, Steps=147081, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2520, Total reward=88.52, Steps=147184, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2521, Total reward=85.39, Steps=147260, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2522, Total reward=37.84, Steps=147284, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2523, Total reward=0.02, Steps=147301, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2524, Total reward=107.31, Steps=147400, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2525, Total reward=190.91, Steps=147566, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2526, Total reward=101.1, Steps=147635, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2527, Total reward=100.46, Steps=147727, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2528, Total reward=123.17, Steps=147792, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2529, Total reward=75.28, Steps=147869, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2530, Total reward=30.46, Steps=147889, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2531, Total reward=90.04, Steps=147964, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2532, Total reward=100.79, Steps=148022, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2533, Total reward=35.8, Steps=148044, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2534, Total reward=38.14, Steps=148069, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2535, Total reward=100.59, Steps=148152, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2536, Total reward=47.46, Steps=148192, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2537, Total reward=75.56, Steps=148245, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2538, Total reward=74.36, Steps=148280, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2539, Total reward=95.86, Steps=148371, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2540, Total reward=241.7, Steps=148562, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2541, Total reward=10.92, Steps=148579, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2542, Total reward=34.06, Steps=148599, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2543, Total reward=85.71, Steps=148725, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2544, Total reward=107.36, Steps=148820, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2545, Total reward=3.4, Steps=148847, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2546, Total reward=130.98, Steps=148938, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2547, Total reward=204.49, Steps=149054, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2548, Total reward=85.4, Steps=149092, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2549, Total reward=38.98, Steps=149119, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2550, Total reward=19.06, Steps=149150, Training iteration=50
Policy training> Surrogate loss=0.00039383131661452353, KL divergence=0.0031327318865805864, Entropy=0.696043074131012, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.05954703316092491, KL divergence=0.036600448191165924, Entropy=0.6767445802688599, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07318001985549927, KL divergence=0.055316098034381866, Entropy=0.6666321158409119, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0802459865808487, KL divergence=0.06480772793292999, Entropy=0.659724771976471, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08569009602069855, KL divergence=0.0694073960185051, Entropy=0.6569463610649109, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0877452865242958, KL divergence=0.07241910696029663, Entropy=0.6577272415161133, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08868590742349625, KL divergence=0.07490333169698715, Entropy=0.6593765616416931, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09098131209611893, KL divergence=0.07724062353372574, Entropy=0.6604518890380859, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09355286508798599, KL divergence=0.0795271173119545, Entropy=0.6608069539070129, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09512878209352493, KL divergence=0.08152195066213608, Entropy=0.6602640748023987, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/196_Step-149150.ckpt']
Uploaded 3 files for checkpoint 196 in 0.46 seconds
saved intermediate frozen graph: current/model/model_196.pb
Best checkpoint number: 180, Last checkpoint number: 194
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'193'}
Training> Name=main_level/agent, Worker=0, Episode=2551, Total reward=105.21, Steps=149212, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2552, Total reward=99.27, Steps=149292, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2553, Total reward=56.32, Steps=149329, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2554, Total reward=46.21, Steps=149359, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2555, Total reward=401.86, Steps=149654, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2556, Total reward=94.24, Steps=149756, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2557, Total reward=154.16, Steps=149873, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2558, Total reward=136.22, Steps=149960, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2559, Total reward=77.51, Steps=150038, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2560, Total reward=48.04, Steps=150107, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2561, Total reward=57.6, Steps=150153, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2562, Total reward=37.2, Steps=150182, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2563, Total reward=0.02, Steps=150206, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2564, Total reward=106.28, Steps=150364, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2565, Total reward=95.73, Steps=150458, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2566, Total reward=124.53, Steps=150566, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2567, Total reward=177.23, Steps=150682, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2568, Total reward=93.15, Steps=150751, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2569, Total reward=22.69, Steps=150769, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2570, Total reward=38.78, Steps=150802, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2571, Total reward=100.35, Steps=150853, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2572, Total reward=48.41, Steps=150894, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2573, Total reward=61.02, Steps=150934, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2574, Total reward=53.39, Steps=150970, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2575, Total reward=27.13, Steps=150997, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2576, Total reward=153.64, Steps=151121, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2577, Total reward=64.82, Steps=151180, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2578, Total reward=26.28, Steps=151194, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2579, Total reward=101.59, Steps=151261, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2580, Total reward=88.84, Steps=151317, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2581, Total reward=162.19, Steps=151464, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2582, Total reward=38.81, Steps=151499, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2583, Total reward=95.4, Steps=151630, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2584, Total reward=113.02, Steps=151722, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2585, Total reward=27.36, Steps=151754, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2586, Total reward=186.49, Steps=151889, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2587, Total reward=69.23, Steps=151956, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2588, Total reward=108.49, Steps=152008, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2589, Total reward=96.04, Steps=152085, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2590, Total reward=121.82, Steps=152164, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2591, Total reward=3.8, Steps=152176, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2592, Total reward=74.13, Steps=152231, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2593, Total reward=50.71, Steps=152273, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2594, Total reward=56.52, Steps=152313, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2595, Total reward=17.57, Steps=152327, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2596, Total reward=191.52, Steps=152478, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2597, Total reward=130.29, Steps=152608, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2598, Total reward=142.18, Steps=152704, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2599, Total reward=41.87, Steps=152764, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2600, Total reward=107.37, Steps=152817, Training iteration=51
Policy training> Surrogate loss=-0.0023681612219661474, KL divergence=0.002699972363188863, Entropy=0.6748052835464478, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06254284828901291, KL divergence=0.03854694217443466, Entropy=0.6565436720848083, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07945584505796432, KL divergence=0.05743812769651413, Entropy=0.6531341671943665, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0866115540266037, KL divergence=0.0677012950181961, Entropy=0.6520993113517761, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0877716988325119, KL divergence=0.07270019501447678, Entropy=0.650894284248352, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09565725177526474, KL divergence=0.07591794431209564, Entropy=0.6503677368164062, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09460556507110596, KL divergence=0.07905012369155884, Entropy=0.6536430716514587, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0958029255270958, KL divergence=0.08189104497432709, Entropy=0.6533223986625671, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09416703879833221, KL divergence=0.0842442736029625, Entropy=0.655570387840271, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09700758010149002, KL divergence=0.08543180674314499, Entropy=0.6523107290267944, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/197_Step-152817.ckpt']
Uploaded 3 files for checkpoint 197 in 0.50 seconds
saved intermediate frozen graph: current/model/model_197.pb
Best checkpoint number: 180, Last checkpoint number: 195
Copying the frozen checkpoint from ./frozen_models/agent/model_180.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'194'}
Training> Name=main_level/agent, Worker=0, Episode=2601, Total reward=239.19, Steps=153012, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2602, Total reward=109.77, Steps=153137, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2603, Total reward=12.58, Steps=153184, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2604, Total reward=393.83, Steps=153488, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2605, Total reward=76.8, Steps=153586, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2606, Total reward=7.19, Steps=153610, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2607, Total reward=140.24, Steps=153706, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2608, Total reward=99.19, Steps=153745, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2609, Total reward=91.73, Steps=153827, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2610, Total reward=87.43, Steps=153897, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2611, Total reward=79.05, Steps=153947, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2612, Total reward=80.28, Steps=154001, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2613, Total reward=0.02, Steps=154017, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2614, Total reward=51.87, Steps=154056, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2615, Total reward=181.31, Steps=154203, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2616, Total reward=361.15, Steps=154485, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2617, Total reward=87.4, Steps=154552, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2618, Total reward=69.55, Steps=154632, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2619, Total reward=10.13, Steps=154646, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2620, Total reward=211.34, Steps=154807, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2621, Total reward=74.34, Steps=154885, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2622, Total reward=246.86, Steps=155082, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2623, Total reward=96.18, Steps=155214, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2624, Total reward=10.73, Steps=155257, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2625, Total reward=44.92, Steps=155307, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2626, Total reward=109.94, Steps=155379, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2627, Total reward=29.49, Steps=155415, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2628, Total reward=120.18, Steps=155502, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2629, Total reward=36.86, Steps=155525, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2630, Total reward=114.65, Steps=155605, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2631, Total reward=95.77, Steps=155663, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2632, Total reward=97.75, Steps=155716, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2633, Total reward=37.21, Steps=155741, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2634, Total reward=31.8, Steps=155785, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2635, Total reward=81.18, Steps=155850, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2636, Total reward=33.48, Steps=155881, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2637, Total reward=92.32, Steps=155934, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2638, Total reward=27.35, Steps=155949, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2639, Total reward=20.42, Steps=155971, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2640, Total reward=112.45, Steps=156086, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2641, Total reward=140.87, Steps=156213, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2642, Total reward=155.28, Steps=156330, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2643, Total reward=3.54, Steps=156406, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2644, Total reward=10.93, Steps=156436, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2645, Total reward=122.58, Steps=156544, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2646, Total reward=85.99, Steps=156642, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2647, Total reward=132.79, Steps=156751, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2648, Total reward=69.06, Steps=156795, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2649, Total reward=84.01, Steps=156879, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2650, Total reward=118.63, Steps=157002, Training iteration=52
Policy training> Surrogate loss=0.0033236604649573565, KL divergence=0.004988478496670723, Entropy=0.6627812385559082, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.05846303328871727, KL divergence=0.04609299451112747, Entropy=0.6387934684753418, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0754084661602974, KL divergence=0.06372787058353424, Entropy=0.6335521936416626, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08238102495670319, KL divergence=0.07188352942466736, Entropy=0.6299518942832947, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08412706851959229, KL divergence=0.07614503055810928, Entropy=0.6292508840560913, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0864030197262764, KL divergence=0.08007605373859406, Entropy=0.6304787397384644, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08920914679765701, KL divergence=0.0823930948972702, Entropy=0.6304701566696167, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09065204113721848, KL divergence=0.08459572494029999, Entropy=0.6293498277664185, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09489274770021439, KL divergence=0.08608704060316086, Entropy=0.6318926811218262, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09327942878007889, KL divergence=0.08803859353065491, Entropy=0.6304540634155273, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/198_Step-157002.ckpt']
Uploaded 3 files for checkpoint 198 in 0.48 seconds
saved intermediate frozen graph: current/model/model_198.pb
Best checkpoint number: 196, Last checkpoint number: 196
Copying the frozen checkpoint from ./frozen_models/agent/model_196.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'195'}
Training> Name=main_level/agent, Worker=0, Episode=2651, Total reward=123.43, Steps=157098, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2652, Total reward=84.53, Steps=157191, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2653, Total reward=16.16, Steps=157199, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2654, Total reward=48.05, Steps=157238, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2655, Total reward=203.31, Steps=157375, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2656, Total reward=89.55, Steps=157448, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2657, Total reward=142.32, Steps=157568, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2658, Total reward=48.05, Steps=157593, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2659, Total reward=29.66, Steps=157625, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2660, Total reward=63.06, Steps=157659, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2661, Total reward=130.34, Steps=157763, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2662, Total reward=52.51, Steps=157836, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2663, Total reward=7.15, Steps=157901, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2664, Total reward=152.4, Steps=158067, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2665, Total reward=110.04, Steps=158153, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2666, Total reward=214.75, Steps=158291, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2667, Total reward=96.41, Steps=158342, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2668, Total reward=83.63, Steps=158382, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2669, Total reward=62.67, Steps=158450, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2670, Total reward=78.66, Steps=158536, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2671, Total reward=28.05, Steps=158573, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2672, Total reward=80.87, Steps=158610, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2673, Total reward=39.46, Steps=158657, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2674, Total reward=136.7, Steps=158754, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2675, Total reward=141.57, Steps=158904, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2676, Total reward=84.62, Steps=158991, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2677, Total reward=114.37, Steps=159069, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2678, Total reward=70.71, Steps=159120, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2679, Total reward=57.2, Steps=159180, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2680, Total reward=209.22, Steps=159334, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2681, Total reward=94.77, Steps=159425, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2682, Total reward=41.86, Steps=159455, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2683, Total reward=17.08, Steps=159489, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2684, Total reward=5.81, Steps=159513, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2685, Total reward=197.29, Steps=159667, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2686, Total reward=142.8, Steps=159784, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2687, Total reward=161.0, Steps=159919, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2688, Total reward=124.37, Steps=160003, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2689, Total reward=88.54, Steps=160085, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2690, Total reward=72.79, Steps=160158, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2691, Total reward=52.86, Steps=160206, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2692, Total reward=85.9, Steps=160290, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2693, Total reward=50.69, Steps=160333, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2694, Total reward=58.31, Steps=160359, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2695, Total reward=94.9, Steps=160467, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2696, Total reward=20.43, Steps=160508, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2697, Total reward=51.46, Steps=160544, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2698, Total reward=35.78, Steps=160567, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2699, Total reward=11.54, Steps=160595, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2700, Total reward=61.9, Steps=160645, Training iteration=53
Policy training> Surrogate loss=0.0005064851720817387, KL divergence=0.0036126954946666956, Entropy=0.6315340399742126, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06305500119924545, KL divergence=0.04640994593501091, Entropy=0.5996767282485962, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07726937532424927, KL divergence=0.06817332655191422, Entropy=0.5913028120994568, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08510036021471024, KL divergence=0.07714633643627167, Entropy=0.5858615636825562, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08597853034734726, KL divergence=0.08465640991926193, Entropy=0.5829963684082031, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.09530802816152573, KL divergence=0.08705795556306839, Entropy=0.5843686461448669, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09077391773462296, KL divergence=0.09109417349100113, Entropy=0.5841339826583862, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09407349675893784, KL divergence=0.09372600167989731, Entropy=0.5838928818702698, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.10243742167949677, KL divergence=0.09564170986413956, Entropy=0.5827580690383911, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09691574424505234, KL divergence=0.09778338670730591, Entropy=0.5847506523132324, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/199_Step-160645.ckpt']
Uploaded 3 files for checkpoint 199 in 0.47 seconds
saved intermediate frozen graph: current/model/model_199.pb
Best checkpoint number: 196, Last checkpoint number: 197
Copying the frozen checkpoint from ./frozen_models/agent/model_196.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'180'}
Training> Name=main_level/agent, Worker=0, Episode=2701, Total reward=93.94, Steps=160747, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2702, Total reward=64.48, Steps=160817, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2703, Total reward=131.51, Steps=160970, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2704, Total reward=40.84, Steps=161021, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2705, Total reward=22.93, Steps=161057, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2706, Total reward=111.77, Steps=161122, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2707, Total reward=136.81, Steps=161235, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2708, Total reward=116.77, Steps=161303, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2709, Total reward=141.67, Steps=161394, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2710, Total reward=93.4, Steps=161481, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2711, Total reward=87.77, Steps=161555, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2712, Total reward=15.09, Steps=161567, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2713, Total reward=57.87, Steps=161606, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2714, Total reward=37.45, Steps=161632, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2715, Total reward=320.7, Steps=161862, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2716, Total reward=120.91, Steps=161959, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2717, Total reward=82.86, Steps=162008, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2718, Total reward=73.31, Steps=162067, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2719, Total reward=71.05, Steps=162145, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2720, Total reward=95.75, Steps=162199, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2721, Total reward=90.35, Steps=162288, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2722, Total reward=16.43, Steps=162298, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2723, Total reward=5.58, Steps=162335, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2724, Total reward=21.61, Steps=162369, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2725, Total reward=228.47, Steps=162531, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2726, Total reward=42.79, Steps=162593, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2727, Total reward=113.46, Steps=162661, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2728, Total reward=72.7, Steps=162692, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2729, Total reward=74.22, Steps=162753, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2730, Total reward=90.23, Steps=162836, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2731, Total reward=120.26, Steps=162900, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2732, Total reward=73.08, Steps=162995, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2733, Total reward=33.83, Steps=163019, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2734, Total reward=52.2, Steps=163059, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2735, Total reward=170.48, Steps=163167, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2736, Total reward=105.87, Steps=163308, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2737, Total reward=7.12, Steps=163331, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2738, Total reward=90.56, Steps=163407, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2739, Total reward=77.25, Steps=163475, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2740, Total reward=223.91, Steps=163654, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2741, Total reward=98.01, Steps=163728, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2742, Total reward=47.97, Steps=163761, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2743, Total reward=171.15, Steps=163937, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2744, Total reward=0.02, Steps=163960, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2745, Total reward=104.69, Steps=164046, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2746, Total reward=215.03, Steps=164176, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2747, Total reward=233.88, Steps=164371, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2748, Total reward=186.04, Steps=164514, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2749, Total reward=39.55, Steps=164544, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2750, Total reward=60.93, Steps=164604, Training iteration=54
Policy training> Surrogate loss=-0.003372076665982604, KL divergence=0.0038836589083075523, Entropy=0.6353935599327087, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.060505662113428116, KL divergence=0.03777451068162918, Entropy=0.6137113571166992, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07269367575645447, KL divergence=0.05966493859887123, Entropy=0.6050137281417847, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07924152165651321, KL divergence=0.07254232466220856, Entropy=0.6025702357292175, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08647023141384125, KL divergence=0.07690370827913284, Entropy=0.6015229821205139, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0862775519490242, KL divergence=0.0819174274802208, Entropy=0.6044624447822571, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09026658535003662, KL divergence=0.08467519283294678, Entropy=0.6021749973297119, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09959693253040314, KL divergence=0.08479999005794525, Entropy=0.5994532108306885, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09178247302770615, KL divergence=0.08827780932188034, Entropy=0.603206217288971, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09849095344543457, KL divergence=0.0891747996211052, Entropy=0.6017822623252869, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/200_Step-164604.ckpt']
Uploaded 3 files for checkpoint 200 in 0.56 seconds
saved intermediate frozen graph: current/model/model_200.pb
Best checkpoint number: 196, Last checkpoint number: 198
Copying the frozen checkpoint from ./frozen_models/agent/model_196.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'197'}
Training> Name=main_level/agent, Worker=0, Episode=2751, Total reward=99.84, Steps=164676, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2752, Total reward=51.47, Steps=164712, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2753, Total reward=2.8, Steps=164733, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2754, Total reward=39.14, Steps=164763, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2755, Total reward=325.49, Steps=165058, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2756, Total reward=82.25, Steps=165160, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2757, Total reward=40.08, Steps=165198, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2758, Total reward=52.83, Steps=165227, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2759, Total reward=183.24, Steps=165402, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2760, Total reward=103.81, Steps=165449, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2761, Total reward=65.46, Steps=165492, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2762, Total reward=168.92, Steps=165641, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2763, Total reward=109.87, Steps=165818, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2764, Total reward=4.02, Steps=165843, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2765, Total reward=128.96, Steps=165946, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2766, Total reward=140.97, Steps=166074, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2767, Total reward=175.55, Steps=166187, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2768, Total reward=104.24, Steps=166236, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2769, Total reward=22.75, Steps=166259, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2770, Total reward=42.89, Steps=166289, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2771, Total reward=29.23, Steps=166324, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2772, Total reward=84.95, Steps=166379, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2773, Total reward=2.8, Steps=166400, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2774, Total reward=298.51, Steps=166666, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2775, Total reward=19.37, Steps=166680, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2776, Total reward=91.66, Steps=166774, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2777, Total reward=76.21, Steps=166833, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2778, Total reward=134.19, Steps=166922, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2779, Total reward=111.94, Steps=167032, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2780, Total reward=102.44, Steps=167079, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2781, Total reward=70.19, Steps=167126, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2782, Total reward=16.45, Steps=167141, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2783, Total reward=3.08, Steps=167166, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2784, Total reward=72.28, Steps=167277, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2785, Total reward=202.56, Steps=167426, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2786, Total reward=109.79, Steps=167498, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2787, Total reward=90.01, Steps=167556, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2788, Total reward=109.76, Steps=167662, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2789, Total reward=57.84, Steps=167703, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2790, Total reward=132.25, Steps=167782, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2791, Total reward=56.95, Steps=167834, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2792, Total reward=95.69, Steps=167886, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2793, Total reward=29.74, Steps=167929, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2794, Total reward=63.73, Steps=167966, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2795, Total reward=40.9, Steps=168019, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2796, Total reward=95.32, Steps=168087, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2797, Total reward=70.23, Steps=168136, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2798, Total reward=20.7, Steps=168152, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2799, Total reward=80.95, Steps=168257, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2800, Total reward=109.82, Steps=168318, Training iteration=55
Policy training> Surrogate loss=0.0052301702089607716, KL divergence=0.003533504204824567, Entropy=0.6309686899185181, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.061354707926511765, KL divergence=0.04929409548640251, Entropy=0.6158944964408875, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07644724100828171, KL divergence=0.07659976184368134, Entropy=0.607081413269043, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08471046388149261, KL divergence=0.08781325072050095, Entropy=0.6024429202079773, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09177778661251068, KL divergence=0.0929265096783638, Entropy=0.6038022041320801, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08638814836740494, KL divergence=0.09605227410793304, Entropy=0.6055629849433899, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09237506240606308, KL divergence=0.0999140739440918, Entropy=0.6067841649055481, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09686176478862762, KL divergence=0.10228866338729858, Entropy=0.6063088774681091, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09295517206192017, KL divergence=0.1026710793375969, Entropy=0.6070705652236938, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09927620738744736, KL divergence=0.10438031703233719, Entropy=0.6065545082092285, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/201_Step-168318.ckpt']
Uploaded 3 files for checkpoint 201 in 0.54 seconds
saved intermediate frozen graph: current/model/model_201.pb
Best checkpoint number: 196, Last checkpoint number: 199
Copying the frozen checkpoint from ./frozen_models/agent/model_196.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'198'}
Training> Name=main_level/agent, Worker=0, Episode=2801, Total reward=52.34, Steps=168350, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2802, Total reward=56.9, Steps=168430, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2803, Total reward=15.2, Steps=168452, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2804, Total reward=50.59, Steps=168520, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2805, Total reward=110.39, Steps=168652, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2806, Total reward=209.58, Steps=168792, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2807, Total reward=156.02, Steps=168928, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2808, Total reward=132.73, Steps=169007, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2809, Total reward=125.23, Steps=169101, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2810, Total reward=54.35, Steps=169156, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2811, Total reward=46.12, Steps=169207, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2812, Total reward=25.14, Steps=169248, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2813, Total reward=52.02, Steps=169287, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2814, Total reward=312.09, Steps=169544, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2815, Total reward=203.93, Steps=169740, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2816, Total reward=277.96, Steps=169975, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2817, Total reward=216.76, Steps=170188, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2818, Total reward=31.65, Steps=170205, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2819, Total reward=6.19, Steps=170223, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2820, Total reward=105.82, Steps=170277, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2821, Total reward=180.45, Steps=170415, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2822, Total reward=43.17, Steps=170460, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2823, Total reward=208.3, Steps=170633, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2824, Total reward=58.02, Steps=170721, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2825, Total reward=171.31, Steps=170886, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2826, Total reward=14.24, Steps=170914, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2827, Total reward=107.82, Steps=170985, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2828, Total reward=85.94, Steps=171032, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2829, Total reward=125.8, Steps=171125, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2830, Total reward=99.46, Steps=171196, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2831, Total reward=84.05, Steps=171246, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2832, Total reward=19.28, Steps=171257, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2833, Total reward=27.87, Steps=171280, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2834, Total reward=66.34, Steps=171366, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2835, Total reward=125.76, Steps=171447, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2836, Total reward=273.54, Steps=171673, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2837, Total reward=158.9, Steps=171810, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2838, Total reward=169.39, Steps=171935, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2839, Total reward=89.02, Steps=172053, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2840, Total reward=112.46, Steps=172142, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2841, Total reward=85.51, Steps=172203, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2842, Total reward=170.33, Steps=172398, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2843, Total reward=14.5, Steps=172450, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2844, Total reward=10.67, Steps=172472, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2845, Total reward=117.01, Steps=172567, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2846, Total reward=92.47, Steps=172635, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2847, Total reward=152.21, Steps=172768, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2848, Total reward=136.09, Steps=172859, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2849, Total reward=47.5, Steps=172917, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2850, Total reward=125.35, Steps=172996, Training iteration=56
Policy training> Surrogate loss=0.004167551640421152, KL divergence=0.004409525543451309, Entropy=0.6431642174720764, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.06308085471391678, KL divergence=0.04760550707578659, Entropy=0.6216611862182617, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07520086318254471, KL divergence=0.06722808629274368, Entropy=0.6176153421401978, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08165087550878525, KL divergence=0.07454106956720352, Entropy=0.6138512492179871, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08723184466362, KL divergence=0.07879988849163055, Entropy=0.6109699010848999, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0904892086982727, KL divergence=0.08067522197961807, Entropy=0.6087687015533447, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09386853128671646, KL divergence=0.08396531641483307, Entropy=0.6091521382331848, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09640749543905258, KL divergence=0.08575854450464249, Entropy=0.6103001236915588, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09139159321784973, KL divergence=0.08746305108070374, Entropy=0.6123926639556885, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09485933184623718, KL divergence=0.09000757336616516, Entropy=0.6133589148521423, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/202_Step-172996.ckpt']
Uploaded 3 files for checkpoint 202 in 0.53 seconds
saved intermediate frozen graph: current/model/model_202.pb
Best checkpoint number: 196, Last checkpoint number: 200
Copying the frozen checkpoint from ./frozen_models/agent/model_196.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'199'}
Training> Name=main_level/agent, Worker=0, Episode=2851, Total reward=62.17, Steps=173044, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2852, Total reward=107.42, Steps=173101, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2853, Total reward=56.36, Steps=173144, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2854, Total reward=48.41, Steps=173171, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2855, Total reward=21.49, Steps=173186, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2856, Total reward=21.9, Steps=173227, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2857, Total reward=104.82, Steps=173299, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2858, Total reward=111.39, Steps=173388, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2859, Total reward=109.73, Steps=173485, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2860, Total reward=255.26, Steps=173706, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2861, Total reward=55.99, Steps=173764, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2862, Total reward=54.24, Steps=173846, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2863, Total reward=281.35, Steps=174104, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2864, Total reward=7.58, Steps=174136, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2865, Total reward=106.74, Steps=174230, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2866, Total reward=93.21, Steps=174307, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2867, Total reward=9.3, Steps=174330, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2868, Total reward=113.46, Steps=174420, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2869, Total reward=26.19, Steps=174459, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2870, Total reward=124.46, Steps=174534, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2871, Total reward=68.86, Steps=174591, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2872, Total reward=115.3, Steps=174648, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2873, Total reward=57.8, Steps=174683, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2874, Total reward=0.0, Steps=174684, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2875, Total reward=116.62, Steps=174764, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2876, Total reward=262.9, Steps=174996, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2877, Total reward=237.72, Steps=175223, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2878, Total reward=127.4, Steps=175351, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2879, Total reward=7.88, Steps=175380, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2880, Total reward=107.55, Steps=175476, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2881, Total reward=71.13, Steps=175515, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2882, Total reward=48.13, Steps=175550, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2883, Total reward=137.4, Steps=175687, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2884, Total reward=124.33, Steps=175838, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2885, Total reward=108.83, Steps=175945, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2886, Total reward=18.75, Steps=175971, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2887, Total reward=94.13, Steps=176024, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2888, Total reward=174.92, Steps=176137, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2889, Total reward=24.12, Steps=176157, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2890, Total reward=115.04, Steps=176242, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2891, Total reward=100.93, Steps=176307, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2892, Total reward=91.91, Steps=176364, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2893, Total reward=32.39, Steps=176393, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2894, Total reward=49.6, Steps=176432, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2895, Total reward=21.91, Steps=176457, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2896, Total reward=156.56, Steps=176619, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2897, Total reward=66.16, Steps=176676, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2898, Total reward=144.84, Steps=176821, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2899, Total reward=133.42, Steps=176959, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2900, Total reward=192.98, Steps=177147, Training iteration=57
Policy training> Surrogate loss=0.001187439076602459, KL divergence=0.004160651005804539, Entropy=0.6190429925918579, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0625428780913353, KL divergence=0.049197614192962646, Entropy=0.6007992625236511, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07619106024503708, KL divergence=0.07196085900068283, Entropy=0.5970652103424072, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08071242272853851, KL divergence=0.08033647388219833, Entropy=0.591977059841156, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08301816135644913, KL divergence=0.08439408242702484, Entropy=0.5933895111083984, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08846664428710938, KL divergence=0.08782655000686646, Entropy=0.5929551124572754, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.09007146209478378, KL divergence=0.08961258828639984, Entropy=0.5932645797729492, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09336696565151215, KL divergence=0.09123571217060089, Entropy=0.5924909710884094, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09320184588432312, KL divergence=0.09253216534852982, Entropy=0.5929213762283325, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09433741122484207, KL divergence=0.09412790089845657, Entropy=0.5933831930160522, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/203_Step-177147.ckpt']
Uploaded 3 files for checkpoint 203 in 0.56 seconds
saved intermediate frozen graph: current/model/model_203.pb
Best checkpoint number: 196, Last checkpoint number: 201
Copying the frozen checkpoint from ./frozen_models/agent/model_196.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'200'}
Training> Name=main_level/agent, Worker=0, Episode=2901, Total reward=81.79, Steps=177231, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2902, Total reward=64.53, Steps=177300, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2903, Total reward=9.58, Steps=177347, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2904, Total reward=144.2, Steps=177486, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2905, Total reward=23.37, Steps=177526, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2906, Total reward=193.95, Steps=177664, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2907, Total reward=118.35, Steps=177752, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2908, Total reward=132.33, Steps=177830, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2909, Total reward=68.36, Steps=177877, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2910, Total reward=117.04, Steps=177963, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2911, Total reward=28.4, Steps=178002, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2912, Total reward=15.87, Steps=178014, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2913, Total reward=48.98, Steps=178054, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2914, Total reward=59.75, Steps=178093, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2915, Total reward=32.82, Steps=178123, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2916, Total reward=312.07, Steps=178400, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2917, Total reward=187.89, Steps=178616, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2918, Total reward=114.58, Steps=178714, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2919, Total reward=86.06, Steps=178820, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2920, Total reward=261.12, Steps=179060, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2921, Total reward=83.73, Steps=179135, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2922, Total reward=46.1, Steps=179183, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2923, Total reward=34.14, Steps=179243, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2924, Total reward=32.6, Steps=179295, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2925, Total reward=12.43, Steps=179332, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2926, Total reward=155.35, Steps=179458, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2927, Total reward=75.22, Steps=179509, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2928, Total reward=151.12, Steps=179602, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2929, Total reward=114.13, Steps=179681, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2930, Total reward=10.4, Steps=179708, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2931, Total reward=119.81, Steps=179779, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2932, Total reward=89.46, Steps=179831, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2933, Total reward=65.64, Steps=179871, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2934, Total reward=66.34, Steps=179912, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2935, Total reward=212.65, Steps=180041, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2936, Total reward=124.7, Steps=180164, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2937, Total reward=197.71, Steps=180342, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2938, Total reward=109.66, Steps=180437, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2939, Total reward=203.58, Steps=180625, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2940, Total reward=101.53, Steps=180682, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2941, Total reward=181.17, Steps=180838, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2942, Total reward=34.83, Steps=180874, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2943, Total reward=176.33, Steps=181064, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2944, Total reward=14.58, Steps=181108, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2945, Total reward=23.08, Steps=181139, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2946, Total reward=37.84, Steps=181182, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2947, Total reward=85.54, Steps=181236, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2948, Total reward=72.67, Steps=181279, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2949, Total reward=115.27, Steps=181378, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2950, Total reward=100.82, Steps=181458, Training iteration=58
Policy training> Surrogate loss=0.00019523128867149353, KL divergence=0.00511604268103838, Entropy=0.613653838634491, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.05811139568686485, KL divergence=0.041041482239961624, Entropy=0.598253607749939, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0728418380022049, KL divergence=0.06619543582201004, Entropy=0.5826215744018555, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08522690087556839, KL divergence=0.07707488536834717, Entropy=0.5749309062957764, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.09421152621507645, KL divergence=0.08280621469020844, Entropy=0.5725129842758179, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0889744982123375, KL divergence=0.08684514462947845, Entropy=0.5731890201568604, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08733637630939484, KL divergence=0.08988992869853973, Entropy=0.5731353759765625, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09113795310258865, KL divergence=0.09169991314411163, Entropy=0.5799006223678589, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09342596679925919, KL divergence=0.09443992376327515, Entropy=0.5769849419593811, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09819916635751724, KL divergence=0.09670447558164597, Entropy=0.5728523135185242, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/204_Step-181458.ckpt']
Uploaded 3 files for checkpoint 204 in 0.57 seconds
saved intermediate frozen graph: current/model/model_204.pb
Best checkpoint number: 202, Last checkpoint number: 202
Copying the frozen checkpoint from ./frozen_models/agent/model_202.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'201'}
Training> Name=main_level/agent, Worker=0, Episode=2951, Total reward=91.53, Steps=181528, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2952, Total reward=86.67, Steps=181579, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2953, Total reward=47.64, Steps=181620, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2954, Total reward=41.1, Steps=181644, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2955, Total reward=184.66, Steps=181822, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2956, Total reward=38.75, Steps=181861, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2957, Total reward=234.5, Steps=182072, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2958, Total reward=232.32, Steps=182233, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2959, Total reward=114.42, Steps=182299, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2960, Total reward=179.37, Steps=182481, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2961, Total reward=190.99, Steps=182628, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2962, Total reward=133.75, Steps=182753, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2963, Total reward=173.0, Steps=182938, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2964, Total reward=9.32, Steps=182988, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2965, Total reward=69.29, Steps=183057, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2966, Total reward=152.89, Steps=183185, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2967, Total reward=88.06, Steps=183239, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2968, Total reward=182.18, Steps=183331, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2969, Total reward=27.4, Steps=183350, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2970, Total reward=115.55, Steps=183416, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2971, Total reward=106.03, Steps=183490, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2972, Total reward=79.23, Steps=183542, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2973, Total reward=47.45, Steps=183585, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2974, Total reward=43.06, Steps=183611, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2975, Total reward=211.69, Steps=183780, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2976, Total reward=155.97, Steps=183901, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2977, Total reward=62.51, Steps=183969, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2978, Total reward=101.68, Steps=184063, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2979, Total reward=117.16, Steps=184193, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2980, Total reward=60.9, Steps=184262, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2981, Total reward=115.08, Steps=184379, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2982, Total reward=61.26, Steps=184470, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2983, Total reward=104.35, Steps=184581, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2984, Total reward=110.68, Steps=184683, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2985, Total reward=100.15, Steps=184765, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2986, Total reward=139.75, Steps=184895, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2987, Total reward=92.54, Steps=184948, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2988, Total reward=75.31, Steps=184997, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2989, Total reward=38.14, Steps=185033, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2990, Total reward=62.78, Steps=185108, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2991, Total reward=41.23, Steps=185148, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2992, Total reward=51.45, Steps=185181, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2993, Total reward=60.5, Steps=185220, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2994, Total reward=48.48, Steps=185250, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2995, Total reward=9.62, Steps=185263, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2996, Total reward=16.51, Steps=185283, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2997, Total reward=179.18, Steps=185480, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2998, Total reward=131.04, Steps=185618, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2999, Total reward=267.13, Steps=185850, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=3000, Total reward=84.09, Steps=185907, Training iteration=59
Policy training> Surrogate loss=0.0028069126419723034, KL divergence=0.004076912999153137, Entropy=0.6150622963905334, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.056627057492733, KL divergence=0.05064623802900314, Entropy=0.5909363627433777, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06877451390028, KL divergence=0.07107164710760117, Entropy=0.5868388414382935, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0758044496178627, KL divergence=0.07813874632120132, Entropy=0.5872615575790405, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08277992159128189, KL divergence=0.08197322487831116, Entropy=0.5839426517486572, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08421012759208679, KL divergence=0.08549284189939499, Entropy=0.5845382213592529, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07955856621265411, KL divergence=0.08674394339323044, Entropy=0.5819973945617676, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08329366892576218, KL divergence=0.09028346836566925, Entropy=0.5826098322868347, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08786815404891968, KL divergence=0.09366589039564133, Entropy=0.5832551121711731, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08757289499044418, KL divergence=0.09421208500862122, Entropy=0.5835195183753967, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/205_Step-185907.ckpt']
Uploaded 3 files for checkpoint 205 in 0.48 seconds
saved intermediate frozen graph: current/model/model_205.pb
Best checkpoint number: 202, Last checkpoint number: 203
Copying the frozen checkpoint from ./frozen_models/agent/model_202.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'203'}
Training> Name=main_level/agent, Worker=0, Episode=3001, Total reward=192.1, Steps=186036, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3002, Total reward=66.65, Steps=186109, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3003, Total reward=7.51, Steps=186132, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3004, Total reward=106.36, Steps=186246, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3005, Total reward=222.65, Steps=186409, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3006, Total reward=14.73, Steps=186434, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3007, Total reward=137.56, Steps=186559, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3008, Total reward=108.12, Steps=186634, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3009, Total reward=48.01, Steps=186682, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3010, Total reward=49.6, Steps=186746, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3011, Total reward=22.57, Steps=186772, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3012, Total reward=100.37, Steps=186825, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3013, Total reward=38.48, Steps=186869, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3014, Total reward=60.4, Steps=186910, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3015, Total reward=110.91, Steps=186994, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3016, Total reward=169.12, Steps=187187, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3017, Total reward=49.86, Steps=187219, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3018, Total reward=233.61, Steps=187408, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3019, Total reward=87.66, Steps=187496, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3020, Total reward=206.49, Steps=187732, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3021, Total reward=187.1, Steps=187923, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3022, Total reward=114.66, Steps=188046, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3023, Total reward=0.01, Steps=188060, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3024, Total reward=176.46, Steps=188244, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3025, Total reward=23.43, Steps=188290, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3026, Total reward=246.3, Steps=188413, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3027, Total reward=150.22, Steps=188525, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3028, Total reward=195.46, Steps=188638, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3029, Total reward=51.42, Steps=188715, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3030, Total reward=104.55, Steps=188795, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3031, Total reward=107.63, Steps=188858, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3032, Total reward=81.48, Steps=188905, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3033, Total reward=31.13, Steps=188928, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3034, Total reward=64.48, Steps=188989, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3035, Total reward=146.62, Steps=189097, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3036, Total reward=169.11, Steps=189219, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3037, Total reward=124.44, Steps=189330, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3038, Total reward=97.83, Steps=189400, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3039, Total reward=89.57, Steps=189516, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3040, Total reward=254.53, Steps=189715, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3041, Total reward=91.99, Steps=189816, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3042, Total reward=37.48, Steps=189868, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3043, Total reward=7.28, Steps=189893, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3044, Total reward=13.35, Steps=189927, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3045, Total reward=155.22, Steps=190025, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3046, Total reward=71.5, Steps=190099, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3047, Total reward=139.87, Steps=190208, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3048, Total reward=101.05, Steps=190256, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3049, Total reward=131.07, Steps=190357, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3050, Total reward=39.71, Steps=190387, Training iteration=60
Policy training> Surrogate loss=0.0010610565077513456, KL divergence=0.0045008426532149315, Entropy=0.5794124603271484, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.05352019891142845, KL divergence=0.049716223031282425, Entropy=0.5664170384407043, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06720688939094543, KL divergence=0.07089056074619293, Entropy=0.5511525869369507, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07703310251235962, KL divergence=0.0808282271027565, Entropy=0.5497028827667236, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08019374310970306, KL divergence=0.08389211446046829, Entropy=0.5474516749382019, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08196868747472763, KL divergence=0.08721786737442017, Entropy=0.5448708534240723, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08445277810096741, KL divergence=0.09040527045726776, Entropy=0.5471005439758301, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08658217638731003, KL divergence=0.09102107584476471, Entropy=0.5431922674179077, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08914146572351456, KL divergence=0.09483437240123749, Entropy=0.5455084443092346, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08648055791854858, KL divergence=0.09586658328771591, Entropy=0.5443593263626099, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/206_Step-190387.ckpt']
Uploaded 3 files for checkpoint 206 in 0.61 seconds
saved intermediate frozen graph: current/model/model_206.pb
Best checkpoint number: 202, Last checkpoint number: 204
Copying the frozen checkpoint from ./frozen_models/agent/model_202.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'196'}
Training> Name=main_level/agent, Worker=0, Episode=3051, Total reward=36.26, Steps=190413, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3052, Total reward=81.49, Steps=190464, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3053, Total reward=47.67, Steps=190502, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3054, Total reward=64.98, Steps=190539, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3055, Total reward=23.74, Steps=190572, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3056, Total reward=83.31, Steps=190640, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3057, Total reward=127.59, Steps=190733, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3058, Total reward=117.77, Steps=190887, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3059, Total reward=11.54, Steps=190919, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3060, Total reward=105.54, Steps=191005, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3061, Total reward=216.54, Steps=191233, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3062, Total reward=186.66, Steps=191426, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3063, Total reward=0.02, Steps=191444, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3064, Total reward=0.04, Steps=191484, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3065, Total reward=151.16, Steps=191615, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3066, Total reward=147.89, Steps=191758, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3067, Total reward=150.17, Steps=191888, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3068, Total reward=134.56, Steps=191978, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3069, Total reward=32.94, Steps=191997, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3070, Total reward=36.49, Steps=192026, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3071, Total reward=101.25, Steps=192097, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3072, Total reward=22.04, Steps=192110, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3073, Total reward=54.53, Steps=192150, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3074, Total reward=110.43, Steps=192256, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3075, Total reward=175.22, Steps=192377, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3076, Total reward=31.18, Steps=192424, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3077, Total reward=36.33, Steps=192454, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3078, Total reward=99.68, Steps=192572, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3079, Total reward=289.62, Steps=192809, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3080, Total reward=108.24, Steps=192918, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3081, Total reward=66.9, Steps=192959, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3082, Total reward=49.17, Steps=193022, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3083, Total reward=74.0, Steps=193144, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3084, Total reward=62.94, Steps=193231, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3085, Total reward=212.82, Steps=193402, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3086, Total reward=169.3, Steps=193533, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3087, Total reward=121.12, Steps=193618, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3088, Total reward=85.65, Steps=193658, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3089, Total reward=64.6, Steps=193697, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3090, Total reward=112.47, Steps=193778, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3091, Total reward=86.94, Steps=193854, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3092, Total reward=47.21, Steps=193892, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3093, Total reward=70.92, Steps=193929, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3094, Total reward=44.35, Steps=193970, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3095, Total reward=39.43, Steps=193988, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3096, Total reward=38.95, Steps=194041, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3097, Total reward=72.62, Steps=194111, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3098, Total reward=187.98, Steps=194313, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3099, Total reward=25.79, Steps=194351, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3100, Total reward=93.73, Steps=194423, Training iteration=61
Policy training> Surrogate loss=-0.002532662358134985, KL divergence=0.0036799677181988955, Entropy=0.5812655091285706, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04529327154159546, KL divergence=0.04993026331067085, Entropy=0.5505756139755249, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06470105797052383, KL divergence=0.07424748688936234, Entropy=0.5383716821670532, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08030064404010773, KL divergence=0.08493749052286148, Entropy=0.5352977514266968, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.07679352164268494, KL divergence=0.09139667451381683, Entropy=0.5327860116958618, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08130659908056259, KL divergence=0.09519972652196884, Entropy=0.5315635204315186, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08965946733951569, KL divergence=0.09745997935533524, Entropy=0.5288122892379761, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08841538429260254, KL divergence=0.10084307193756104, Entropy=0.5309082865715027, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08822495490312576, KL divergence=0.10277260839939117, Entropy=0.5337823033332825, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08587761223316193, KL divergence=0.10560059547424316, Entropy=0.5312578082084656, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/207_Step-194423.ckpt']
Uploaded 3 files for checkpoint 207 in 0.53 seconds
saved intermediate frozen graph: current/model/model_207.pb
Best checkpoint number: 202, Last checkpoint number: 205
Copying the frozen checkpoint from ./frozen_models/agent/model_202.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'204'}
Training> Name=main_level/agent, Worker=0, Episode=3101, Total reward=57.88, Steps=194466, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3102, Total reward=141.73, Steps=194602, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3103, Total reward=16.25, Steps=194650, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3104, Total reward=13.63, Steps=194677, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3105, Total reward=163.82, Steps=194822, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3106, Total reward=160.83, Steps=194961, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3107, Total reward=174.86, Steps=195069, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3108, Total reward=107.65, Steps=195130, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3109, Total reward=125.72, Steps=195206, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3110, Total reward=81.28, Steps=195268, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3111, Total reward=128.01, Steps=195331, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3112, Total reward=91.65, Steps=195388, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3113, Total reward=51.44, Steps=195430, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3114, Total reward=120.93, Steps=195529, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3115, Total reward=19.86, Steps=195558, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3116, Total reward=60.01, Steps=195612, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3117, Total reward=121.55, Steps=195739, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3118, Total reward=51.82, Steps=195801, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3119, Total reward=80.44, Steps=195850, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3120, Total reward=305.38, Steps=196078, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3121, Total reward=68.06, Steps=196123, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3122, Total reward=230.69, Steps=196318, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3123, Total reward=14.05, Steps=196386, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3124, Total reward=10.21, Steps=196408, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3125, Total reward=76.64, Steps=196468, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3126, Total reward=138.37, Steps=196594, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3127, Total reward=140.48, Steps=196716, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3128, Total reward=89.41, Steps=196759, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3129, Total reward=97.66, Steps=196838, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3130, Total reward=22.12, Steps=196873, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3131, Total reward=49.4, Steps=196917, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3132, Total reward=100.13, Steps=196975, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3133, Total reward=30.32, Steps=196996, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3134, Total reward=215.19, Steps=197217, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3135, Total reward=162.14, Steps=197345, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3136, Total reward=126.9, Steps=197459, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3137, Total reward=68.31, Steps=197511, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3138, Total reward=162.41, Steps=197700, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3139, Total reward=6.41, Steps=197716, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3140, Total reward=99.95, Steps=197773, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3141, Total reward=262.09, Steps=198007, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3142, Total reward=208.99, Steps=198207, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3143, Total reward=23.92, Steps=198262, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3144, Total reward=24.69, Steps=198318, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3145, Total reward=351.3, Steps=198628, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3146, Total reward=18.23, Steps=198649, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3147, Total reward=114.92, Steps=198724, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3148, Total reward=58.72, Steps=198758, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3149, Total reward=25.82, Steps=198778, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3150, Total reward=58.79, Steps=198842, Training iteration=62
Policy training> Surrogate loss=0.0009773584315553308, KL divergence=0.004291805438697338, Entropy=0.5678391456604004, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.053460072726011276, KL divergence=0.05396411940455437, Entropy=0.5466091632843018, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.07328097522258759, KL divergence=0.07594235241413116, Entropy=0.538628876209259, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08096297085285187, KL divergence=0.08503085374832153, Entropy=0.5388185977935791, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08148013055324554, KL divergence=0.08857026696205139, Entropy=0.5343510508537292, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08515230566263199, KL divergence=0.09291438013315201, Entropy=0.5362176895141602, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0863235741853714, KL divergence=0.0973023772239685, Entropy=0.5365498661994934, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09105519950389862, KL divergence=0.09947608411312103, Entropy=0.5373442769050598, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.09020974487066269, KL divergence=0.10205211490392685, Entropy=0.5363304018974304, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.09353502094745636, KL divergence=0.10363278537988663, Entropy=0.5356944799423218, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/208_Step-198842.ckpt']
Uploaded 3 files for checkpoint 208 in 0.57 seconds
saved intermediate frozen graph: current/model/model_208.pb
Best checkpoint number: 206, Last checkpoint number: 206
Copying the frozen checkpoint from ./frozen_models/agent/model_206.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'205'}
Training> Name=main_level/agent, Worker=0, Episode=3151, Total reward=54.84, Steps=198898, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3152, Total reward=71.24, Steps=198935, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3153, Total reward=60.21, Steps=198973, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3154, Total reward=66.3, Steps=199015, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3155, Total reward=101.75, Steps=199089, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3156, Total reward=127.87, Steps=199207, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3157, Total reward=95.63, Steps=199271, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3158, Total reward=65.82, Steps=199307, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3159, Total reward=251.32, Steps=199553, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3160, Total reward=7.84, Steps=199566, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3161, Total reward=61.91, Steps=199624, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3162, Total reward=157.54, Steps=199772, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3163, Total reward=94.29, Steps=199898, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3164, Total reward=213.54, Steps=200054, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3165, Total reward=69.7, Steps=200120, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3166, Total reward=11.16, Steps=200161, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3167, Total reward=204.75, Steps=200286, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3168, Total reward=117.16, Steps=200362, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3169, Total reward=83.9, Steps=200434, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3170, Total reward=78.07, Steps=200504, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3171, Total reward=48.58, Steps=200542, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3172, Total reward=92.63, Steps=200596, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3173, Total reward=36.42, Steps=200619, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3174, Total reward=101.1, Steps=200721, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3175, Total reward=40.82, Steps=200738, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3176, Total reward=121.52, Steps=200882, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3177, Total reward=77.67, Steps=200939, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3178, Total reward=64.44, Steps=200989, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3179, Total reward=244.5, Steps=201177, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3180, Total reward=75.33, Steps=201272, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3181, Total reward=54.98, Steps=201315, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3182, Total reward=48.87, Steps=201385, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3183, Total reward=173.0, Steps=201550, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3184, Total reward=164.87, Steps=201742, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3185, Total reward=16.72, Steps=201782, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3186, Total reward=18.26, Steps=201811, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3187, Total reward=119.72, Steps=201927, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3188, Total reward=109.54, Steps=202023, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3189, Total reward=30.36, Steps=202042, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3190, Total reward=51.81, Steps=202078, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3191, Total reward=95.87, Steps=202151, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3192, Total reward=68.95, Steps=202193, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3193, Total reward=9.37, Steps=202218, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3194, Total reward=34.79, Steps=202248, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3195, Total reward=25.32, Steps=202272, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3196, Total reward=155.09, Steps=202389, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3197, Total reward=156.92, Steps=202517, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3198, Total reward=158.77, Steps=202628, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3199, Total reward=202.75, Steps=202860, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3200, Total reward=69.01, Steps=202932, Training iteration=63
Policy training> Surrogate loss=0.003940990660339594, KL divergence=0.003748305607587099, Entropy=0.5616858005523682, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.048676349222660065, KL divergence=0.051503363996744156, Entropy=0.5433989763259888, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06712029129266739, KL divergence=0.07831135392189026, Entropy=0.5414892435073853, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.08056797087192535, KL divergence=0.08884864300489426, Entropy=0.5292190909385681, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08122777193784714, KL divergence=0.0954800471663475, Entropy=0.531205952167511, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07663848251104355, KL divergence=0.09798919409513474, Entropy=0.5297536849975586, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08329243212938309, KL divergence=0.10113761574029922, Entropy=0.5309962034225464, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.09033635258674622, KL divergence=0.1028953343629837, Entropy=0.5302793979644775, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08319274336099625, KL divergence=0.10547372698783875, Entropy=0.5336105823516846, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08326351642608643, KL divergence=0.10532288253307343, Entropy=0.5344017148017883, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/209_Step-202932.ckpt']
Uploaded 3 files for checkpoint 209 in 0.60 seconds
saved intermediate frozen graph: current/model/model_209.pb
Best checkpoint number: 206, Last checkpoint number: 207
Copying the frozen checkpoint from ./frozen_models/agent/model_206.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'202'}
Training> Name=main_level/agent, Worker=0, Episode=3201, Total reward=63.03, Steps=203012, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3202, Total reward=19.46, Steps=203029, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3203, Total reward=4.05, Steps=203086, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3204, Total reward=6.34, Steps=203110, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3205, Total reward=34.95, Steps=203160, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3206, Total reward=80.83, Steps=203231, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3207, Total reward=0.01, Steps=203240, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3208, Total reward=198.04, Steps=203348, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3209, Total reward=53.08, Steps=203390, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3210, Total reward=114.5, Steps=203461, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3211, Total reward=70.65, Steps=203515, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3212, Total reward=68.75, Steps=203552, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3213, Total reward=54.48, Steps=203590, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3214, Total reward=47.75, Steps=203629, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3215, Total reward=253.38, Steps=203869, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3216, Total reward=41.81, Steps=203916, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3217, Total reward=25.93, Steps=203955, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3218, Total reward=75.7, Steps=204001, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3219, Total reward=217.67, Steps=204231, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3220, Total reward=22.93, Steps=204247, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3221, Total reward=67.78, Steps=204291, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3222, Total reward=40.09, Steps=204341, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3223, Total reward=106.56, Steps=204459, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3224, Total reward=218.36, Steps=204628, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3225, Total reward=177.03, Steps=204800, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3226, Total reward=101.78, Steps=204937, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3227, Total reward=84.2, Steps=205008, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3228, Total reward=75.85, Steps=205048, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3229, Total reward=44.95, Steps=205081, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3230, Total reward=62.66, Steps=205149, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3231, Total reward=30.62, Steps=205175, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3232, Total reward=103.06, Steps=205227, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3233, Total reward=0.02, Steps=205245, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3234, Total reward=166.84, Steps=205402, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3235, Total reward=187.37, Steps=205552, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3236, Total reward=11.25, Steps=205569, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3237, Total reward=283.42, Steps=205821, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3238, Total reward=58.14, Steps=205855, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3239, Total reward=317.1, Steps=206092, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3240, Total reward=228.23, Steps=206318, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3241, Total reward=204.62, Steps=206455, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3242, Total reward=128.12, Steps=206588, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3243, Total reward=101.43, Steps=206693, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3244, Total reward=138.13, Steps=206819, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3245, Total reward=122.29, Steps=206898, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3246, Total reward=237.92, Steps=207076, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3247, Total reward=111.09, Steps=207194, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3248, Total reward=89.96, Steps=207232, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3249, Total reward=93.9, Steps=207306, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3250, Total reward=53.92, Steps=207399, Training iteration=64
Policy training> Surrogate loss=-0.00030511923250742257, KL divergence=0.004734800197184086, Entropy=0.5622850060462952, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.053172558546066284, KL divergence=0.05646814405918121, Entropy=0.5355778932571411, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06846507638692856, KL divergence=0.08001033216714859, Entropy=0.5334244966506958, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07475882023572922, KL divergence=0.08846568316221237, Entropy=0.5318987965583801, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.07804346084594727, KL divergence=0.09206066280603409, Entropy=0.5301771759986877, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07892708480358124, KL divergence=0.09578447788953781, Entropy=0.5318989753723145, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0813954621553421, KL divergence=0.099957674741745, Entropy=0.5310020446777344, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0874643549323082, KL divergence=0.10304027050733566, Entropy=0.5294070243835449, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08508187532424927, KL divergence=0.10506448149681091, Entropy=0.5312130451202393, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08931488543748856, KL divergence=0.10415700823068619, Entropy=0.5319144129753113, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/210_Step-207399.ckpt']
Uploaded 3 files for checkpoint 210 in 0.50 seconds
saved intermediate frozen graph: current/model/model_210.pb
Best checkpoint number: 208, Last checkpoint number: 208
Copying the frozen checkpoint from ./frozen_models/agent/model_208.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'206'}
Training> Name=main_level/agent, Worker=0, Episode=3251, Total reward=81.19, Steps=207465, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3252, Total reward=49.03, Steps=207497, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3253, Total reward=0.02, Steps=207512, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3254, Total reward=41.62, Steps=207564, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3255, Total reward=65.62, Steps=207644, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3256, Total reward=17.97, Steps=207665, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3257, Total reward=137.93, Steps=207769, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3258, Total reward=110.19, Steps=207893, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3259, Total reward=177.83, Steps=208081, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3260, Total reward=97.6, Steps=208161, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3261, Total reward=75.35, Steps=208258, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3262, Total reward=41.45, Steps=208316, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3263, Total reward=7.86, Steps=208345, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3264, Total reward=14.41, Steps=208367, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3265, Total reward=29.96, Steps=208405, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3266, Total reward=119.48, Steps=208472, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3267, Total reward=88.0, Steps=208524, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3268, Total reward=73.89, Steps=208562, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3269, Total reward=106.35, Steps=208643, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3270, Total reward=11.21, Steps=208658, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3271, Total reward=19.3, Steps=208714, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3272, Total reward=91.42, Steps=208768, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3273, Total reward=55.25, Steps=208811, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3274, Total reward=45.96, Steps=208853, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3275, Total reward=203.64, Steps=209016, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3276, Total reward=45.9, Steps=209069, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3277, Total reward=139.37, Steps=209175, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3278, Total reward=106.99, Steps=209315, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3279, Total reward=92.87, Steps=209428, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3280, Total reward=67.8, Steps=209464, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3281, Total reward=86.1, Steps=209539, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3282, Total reward=33.15, Steps=209561, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3283, Total reward=0.02, Steps=209576, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3284, Total reward=199.63, Steps=209740, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3285, Total reward=210.18, Steps=209904, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3286, Total reward=181.98, Steps=210030, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3287, Total reward=85.83, Steps=210121, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3288, Total reward=170.46, Steps=210232, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3289, Total reward=32.02, Steps=210251, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3290, Total reward=43.56, Steps=210314, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3291, Total reward=106.86, Steps=210385, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3292, Total reward=103.5, Steps=210442, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3293, Total reward=39.49, Steps=210468, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3294, Total reward=63.38, Steps=210510, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3295, Total reward=129.12, Steps=210624, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3296, Total reward=99.45, Steps=210722, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3297, Total reward=317.08, Steps=211004, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3298, Total reward=130.17, Steps=211111, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3299, Total reward=89.81, Steps=211197, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3300, Total reward=403.07, Steps=211509, Training iteration=65
Policy training> Surrogate loss=-0.0002946030581369996, KL divergence=0.004931306466460228, Entropy=0.5361454486846924, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.057182423770427704, KL divergence=0.05664851516485214, Entropy=0.5133645534515381, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06879683583974838, KL divergence=0.07858790457248688, Entropy=0.504249095916748, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07713182270526886, KL divergence=0.08823105692863464, Entropy=0.5001627802848816, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.08072242885828018, KL divergence=0.09279520064592361, Entropy=0.49792608618736267, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08204443752765656, KL divergence=0.09544044733047485, Entropy=0.4979482889175415, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08518142998218536, KL divergence=0.09769390523433685, Entropy=0.49763524532318115, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08535835146903992, KL divergence=0.10071100294589996, Entropy=0.498584121465683, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08553354442119598, KL divergence=0.10195687413215637, Entropy=0.49744555354118347, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08926373720169067, KL divergence=0.10371296107769012, Entropy=0.49735772609710693, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/211_Step-211509.ckpt']
Uploaded 3 files for checkpoint 211 in 0.66 seconds
saved intermediate frozen graph: current/model/model_211.pb
Best checkpoint number: 208, Last checkpoint number: 209
Copying the frozen checkpoint from ./frozen_models/agent/model_208.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'207'}
Training> Name=main_level/agent, Worker=0, Episode=3301, Total reward=80.91, Steps=211563, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3302, Total reward=390.16, Steps=211891, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3303, Total reward=113.37, Steps=212024, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3304, Total reward=6.35, Steps=212058, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3305, Total reward=64.87, Steps=212127, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3306, Total reward=113.33, Steps=212193, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3307, Total reward=193.99, Steps=212311, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3308, Total reward=197.78, Steps=212428, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3309, Total reward=62.95, Steps=212498, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3310, Total reward=113.34, Steps=212582, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3311, Total reward=115.36, Steps=212648, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3312, Total reward=83.51, Steps=212707, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3313, Total reward=42.22, Steps=212744, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3314, Total reward=34.23, Steps=212768, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3315, Total reward=19.17, Steps=212783, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3316, Total reward=49.23, Steps=212843, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3317, Total reward=151.11, Steps=212961, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3318, Total reward=268.82, Steps=213217, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3319, Total reward=97.74, Steps=213323, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3320, Total reward=233.52, Steps=213527, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3321, Total reward=74.51, Steps=213571, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3322, Total reward=162.0, Steps=213702, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3323, Total reward=7.21, Steps=213729, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3324, Total reward=78.77, Steps=213854, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3325, Total reward=84.5, Steps=213948, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3326, Total reward=25.61, Steps=213991, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3327, Total reward=37.24, Steps=214030, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3328, Total reward=185.05, Steps=214137, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3329, Total reward=68.12, Steps=214198, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3330, Total reward=75.13, Steps=214266, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3331, Total reward=94.19, Steps=214323, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3332, Total reward=56.06, Steps=214357, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3333, Total reward=63.35, Steps=214397, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3334, Total reward=53.76, Steps=214425, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3335, Total reward=15.19, Steps=214438, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3336, Total reward=167.27, Steps=214557, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3337, Total reward=176.03, Steps=214701, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3338, Total reward=140.66, Steps=214794, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3339, Total reward=10.53, Steps=214808, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3340, Total reward=268.19, Steps=215030, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3341, Total reward=193.27, Steps=215220, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3342, Total reward=44.17, Steps=215269, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3343, Total reward=160.42, Steps=215427, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3344, Total reward=3.85, Steps=215439, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3345, Total reward=18.26, Steps=215473, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3346, Total reward=143.31, Steps=215592, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3347, Total reward=88.19, Steps=215669, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3348, Total reward=212.81, Steps=215775, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3349, Total reward=28.39, Steps=215825, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3350, Total reward=29.7, Steps=215864, Training iteration=66
Policy training> Surrogate loss=0.0031606717966496944, KL divergence=0.005613952875137329, Entropy=0.5441663265228271, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.05329317972064018, KL divergence=0.057690706104040146, Entropy=0.5176618695259094, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06675191968679428, KL divergence=0.0810897946357727, Entropy=0.5123405456542969, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.07342401146888733, KL divergence=0.09006484597921371, Entropy=0.5107055902481079, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.07627803832292557, KL divergence=0.09494282305240631, Entropy=0.5092748403549194, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.078715980052948, KL divergence=0.09816242009401321, Entropy=0.5089898109436035, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.08045140653848648, KL divergence=0.10010448098182678, Entropy=0.5102708339691162, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.08267003297805786, KL divergence=0.10247952491044998, Entropy=0.5117747187614441, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08393732458353043, KL divergence=0.10494257509708405, Entropy=0.5125088691711426, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.085405133664608, KL divergence=0.1070496141910553, Entropy=0.5140156745910645, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/212_Step-215864.ckpt']
Uploaded 3 files for checkpoint 212 in 0.51 seconds
saved intermediate frozen graph: current/model/model_212.pb
Best checkpoint number: 208, Last checkpoint number: 210
Copying the frozen checkpoint from ./frozen_models/agent/model_208.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'209'}
Training> Name=main_level/agent, Worker=0, Episode=3351, Total reward=111.86, Steps=215932, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3352, Total reward=87.75, Steps=215988, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3353, Total reward=52.85, Steps=216029, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3354, Total reward=60.02, Steps=216069, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3355, Total reward=24.74, Steps=216101, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3356, Total reward=23.17, Steps=216139, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3357, Total reward=16.39, Steps=216160, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3358, Total reward=285.71, Steps=216426, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3359, Total reward=233.34, Steps=216654, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3360, Total reward=253.51, Steps=216870, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3361, Total reward=192.23, Steps=217049, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3362, Total reward=43.78, Steps=217116, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3363, Total reward=99.62, Steps=217226, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3364, Total reward=190.76, Steps=217383, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3365, Total reward=96.08, Steps=217463, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3366, Total reward=118.62, Steps=217574, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3367, Total reward=180.49, Steps=217770, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3368, Total reward=138.58, Steps=217862, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3369, Total reward=145.23, Steps=217950, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3370, Total reward=125.6, Steps=218034, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3371, Total reward=87.28, Steps=218106, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3372, Total reward=93.99, Steps=218161, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3373, Total reward=57.72, Steps=218201, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3374, Total reward=54.53, Steps=218230, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3375, Total reward=234.82, Steps=218383, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3376, Total reward=178.78, Steps=218549, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3377, Total reward=230.14, Steps=218801, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3378, Total reward=239.77, Steps=219022, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3379, Total reward=92.21, Steps=219128, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3380, Total reward=85.7, Steps=219188, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3381, Total reward=27.83, Steps=219212, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3382, Total reward=171.71, Steps=219386, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3383, Total reward=17.47, Steps=219438, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3384, Total reward=116.37, Steps=219535, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3385, Total reward=117.74, Steps=219617, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3386, Total reward=142.72, Steps=219736, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3387, Total reward=123.11, Steps=219859, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3388, Total reward=199.33, Steps=219965, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3389, Total reward=40.21, Steps=220011, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3390, Total reward=34.38, Steps=220082, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3391, Total reward=45.31, Steps=220129, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3392, Total reward=74.55, Steps=220166, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3393, Total reward=52.01, Steps=220203, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3394, Total reward=62.66, Steps=220245, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3395, Total reward=389.02, Steps=220540, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3396, Total reward=243.13, Steps=220783, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3397, Total reward=110.88, Steps=220877, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3398, Total reward=278.55, Steps=221087, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3399, Total reward=67.83, Steps=221167, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3400, Total reward=70.73, Steps=221247, Training iteration=67
Policy training> Surrogate loss=0.002262620721012354, KL divergence=0.0073129464872181416, Entropy=0.545488715171814, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.05174265429377556, KL divergence=0.062261708080768585, Entropy=0.5199548006057739, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06497728824615479, KL divergence=0.08213256299495697, Entropy=0.5107764005661011, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.070614293217659, KL divergence=0.08841796964406967, Entropy=0.5072019696235657, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.07349998503923416, KL divergence=0.09345290809869766, Entropy=0.5074309706687927, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07647491991519928, KL divergence=0.09752614051103592, Entropy=0.5057194828987122, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07941985130310059, KL divergence=0.1013040766119957, Entropy=0.5039989948272705, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07946349680423737, KL divergence=0.10517905652523041, Entropy=0.5042939186096191, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08072005957365036, KL divergence=0.10795392096042633, Entropy=0.5048809051513672, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08146769553422928, KL divergence=0.10976351052522659, Entropy=0.5051383972167969, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/213_Step-221247.ckpt']
Uploaded 3 files for checkpoint 213 in 0.51 seconds
saved intermediate frozen graph: current/model/model_213.pb
Best checkpoint number: 208, Last checkpoint number: 211
Copying the frozen checkpoint from ./frozen_models/agent/model_208.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'210'}
Training> Name=main_level/agent, Worker=0, Episode=3401, Total reward=54.83, Steps=221283, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3402, Total reward=52.42, Steps=221344, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3403, Total reward=143.55, Steps=221515, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3404, Total reward=105.64, Steps=221614, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3405, Total reward=19.83, Steps=221645, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3406, Total reward=18.17, Steps=221678, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3407, Total reward=205.72, Steps=221825, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3408, Total reward=93.2, Steps=221875, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3409, Total reward=115.47, Steps=221948, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3410, Total reward=27.69, Steps=221995, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3411, Total reward=49.78, Steps=222045, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3412, Total reward=87.12, Steps=222097, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3413, Total reward=28.08, Steps=222142, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3414, Total reward=247.08, Steps=222335, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3415, Total reward=204.77, Steps=222469, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3416, Total reward=258.26, Steps=222712, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3417, Total reward=165.33, Steps=222860, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3418, Total reward=65.07, Steps=222901, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3419, Total reward=9.15, Steps=222942, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3420, Total reward=60.98, Steps=223014, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3421, Total reward=69.2, Steps=223071, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3422, Total reward=46.84, Steps=223096, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3423, Total reward=21.09, Steps=223144, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3424, Total reward=170.2, Steps=223297, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3425, Total reward=3.42, Steps=223311, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3426, Total reward=191.22, Steps=223434, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3427, Total reward=108.39, Steps=223488, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3428, Total reward=111.99, Steps=223585, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3429, Total reward=143.48, Steps=223677, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3430, Total reward=20.99, Steps=223707, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3431, Total reward=76.9, Steps=223762, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3432, Total reward=97.87, Steps=223824, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3433, Total reward=55.82, Steps=223861, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3434, Total reward=51.15, Steps=223905, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3435, Total reward=14.79, Steps=223919, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3436, Total reward=125.46, Steps=224038, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3437, Total reward=91.14, Steps=224102, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3438, Total reward=0.0, Steps=224103, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3439, Total reward=92.72, Steps=224178, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3440, Total reward=88.09, Steps=224243, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3441, Total reward=81.73, Steps=224285, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3442, Total reward=273.35, Steps=224482, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3443, Total reward=153.19, Steps=224655, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3444, Total reward=2.43, Steps=224690, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3445, Total reward=332.41, Steps=224988, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3446, Total reward=207.36, Steps=225126, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3447, Total reward=51.38, Steps=225162, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3448, Total reward=81.83, Steps=225200, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3449, Total reward=59.22, Steps=225244, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3450, Total reward=63.59, Steps=225300, Training iteration=68
Policy training> Surrogate loss=0.007103488314896822, KL divergence=0.004983085207641125, Entropy=0.511105477809906, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.05308087542653084, KL divergence=0.051573287695646286, Entropy=0.4879158139228821, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06564254313707352, KL divergence=0.08058056235313416, Entropy=0.4775962233543396, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0702345222234726, KL divergence=0.09110303968191147, Entropy=0.47250303626060486, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0797881931066513, KL divergence=0.0982576385140419, Entropy=0.47151532769203186, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07725884765386581, KL divergence=0.10198139399290085, Entropy=0.4739680290222168, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07321949303150177, KL divergence=0.10482067614793777, Entropy=0.47305387258529663, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07839535921812057, KL divergence=0.10726810246706009, Entropy=0.47369179129600525, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07965657860040665, KL divergence=0.10829048603773117, Entropy=0.47431886196136475, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07860167324542999, KL divergence=0.1080632135272026, Entropy=0.4714573919773102, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/214_Step-225300.ckpt']
Uploaded 3 files for checkpoint 214 in 0.62 seconds
saved intermediate frozen graph: current/model/model_214.pb
Best checkpoint number: 212, Last checkpoint number: 212
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'211'}
Training> Name=main_level/agent, Worker=0, Episode=3451, Total reward=90.18, Steps=225377, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3452, Total reward=97.65, Steps=225436, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3453, Total reward=48.06, Steps=225477, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3454, Total reward=26.28, Steps=225489, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3455, Total reward=309.62, Steps=225756, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3456, Total reward=7.54, Steps=225773, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3457, Total reward=143.45, Steps=225882, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3458, Total reward=101.89, Steps=225961, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3459, Total reward=75.31, Steps=226039, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3460, Total reward=270.85, Steps=226271, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3461, Total reward=225.42, Steps=226477, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3462, Total reward=50.16, Steps=226518, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3463, Total reward=3.74, Steps=226543, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3464, Total reward=10.57, Steps=226586, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3465, Total reward=95.91, Steps=226686, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3466, Total reward=118.03, Steps=226815, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3467, Total reward=202.5, Steps=226942, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3468, Total reward=165.61, Steps=227034, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3469, Total reward=23.1, Steps=227058, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3470, Total reward=127.61, Steps=227127, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3471, Total reward=78.07, Steps=227170, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3472, Total reward=49.7, Steps=227204, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3473, Total reward=0.02, Steps=227222, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3474, Total reward=33.8, Steps=227235, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3475, Total reward=80.66, Steps=227299, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3476, Total reward=336.41, Steps=227592, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3477, Total reward=128.13, Steps=227682, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3478, Total reward=63.74, Steps=227717, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3479, Total reward=237.59, Steps=227951, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3480, Total reward=67.53, Steps=227996, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3481, Total reward=306.76, Steps=228214, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3482, Total reward=28.49, Steps=228235, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3483, Total reward=158.43, Steps=228398, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3484, Total reward=18.51, Steps=228441, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3485, Total reward=189.06, Steps=228594, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3486, Total reward=208.72, Steps=228742, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3487, Total reward=127.02, Steps=228855, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3488, Total reward=97.96, Steps=228959, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3489, Total reward=42.59, Steps=229002, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3490, Total reward=18.73, Steps=229043, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3491, Total reward=44.6, Steps=229065, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3492, Total reward=74.99, Steps=229117, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3493, Total reward=0.02, Steps=229134, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3494, Total reward=322.96, Steps=229456, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3495, Total reward=73.74, Steps=229517, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3496, Total reward=168.45, Steps=229641, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3497, Total reward=112.27, Steps=229762, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3498, Total reward=58.94, Steps=229815, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3499, Total reward=14.16, Steps=229833, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3500, Total reward=245.36, Steps=229997, Training iteration=69
Policy training> Surrogate loss=0.004723000805824995, KL divergence=0.005060525145381689, Entropy=0.5009608864784241, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04795167222619057, KL divergence=0.06010888144373894, Entropy=0.47819021344184875, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.061935439705848694, KL divergence=0.08019424974918365, Entropy=0.4726461172103882, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06849686056375504, KL divergence=0.0884670615196228, Entropy=0.4742228090763092, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06981358677148819, KL divergence=0.0956762507557869, Entropy=0.47587326169013977, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07831696420907974, KL divergence=0.09764444082975388, Entropy=0.47611284255981445, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07579921931028366, KL divergence=0.09896304458379745, Entropy=0.47574201226234436, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07530948519706726, KL divergence=0.10108424723148346, Entropy=0.477129191160202, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07353955507278442, KL divergence=0.10314185917377472, Entropy=0.47678348422050476, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07991567254066467, KL divergence=0.10472050309181213, Entropy=0.4781688153743744, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/215_Step-229997.ckpt']
Uploaded 3 files for checkpoint 215 in 0.47 seconds
saved intermediate frozen graph: current/model/model_215.pb
Best checkpoint number: 212, Last checkpoint number: 213
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'208'}
Training> Name=main_level/agent, Worker=0, Episode=3501, Total reward=221.22, Steps=230185, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3502, Total reward=13.14, Steps=230199, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3503, Total reward=139.5, Steps=230373, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3504, Total reward=132.43, Steps=230499, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3505, Total reward=88.63, Steps=230596, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3506, Total reward=216.24, Steps=230730, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3507, Total reward=100.53, Steps=230812, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3508, Total reward=127.56, Steps=230916, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3509, Total reward=37.27, Steps=230966, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3510, Total reward=14.84, Steps=230982, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3511, Total reward=100.96, Steps=231051, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3512, Total reward=92.43, Steps=231105, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3513, Total reward=64.45, Steps=231144, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3514, Total reward=316.44, Steps=231482, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3515, Total reward=14.37, Steps=231494, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3516, Total reward=79.46, Steps=231561, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3517, Total reward=271.54, Steps=231783, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3518, Total reward=127.35, Steps=231921, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3519, Total reward=60.73, Steps=231982, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3520, Total reward=193.89, Steps=232152, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3521, Total reward=80.03, Steps=232199, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3522, Total reward=147.88, Steps=232331, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3523, Total reward=221.82, Steps=232499, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3524, Total reward=153.26, Steps=232636, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3525, Total reward=10.04, Steps=232663, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3526, Total reward=11.34, Steps=232681, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3527, Total reward=165.99, Steps=232791, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3528, Total reward=147.57, Steps=232891, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3529, Total reward=38.17, Steps=232914, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3530, Total reward=33.09, Steps=232959, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3531, Total reward=48.41, Steps=233012, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3532, Total reward=48.18, Steps=233044, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3533, Total reward=53.57, Steps=233084, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3534, Total reward=33.64, Steps=233110, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3535, Total reward=36.64, Steps=233127, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3536, Total reward=79.44, Steps=233209, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3537, Total reward=177.65, Steps=233365, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3538, Total reward=67.67, Steps=233397, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3539, Total reward=207.99, Steps=233592, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3540, Total reward=89.78, Steps=233681, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3541, Total reward=81.31, Steps=233730, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3542, Total reward=43.57, Steps=233777, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3543, Total reward=116.03, Steps=233912, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3544, Total reward=201.15, Steps=234071, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3545, Total reward=92.86, Steps=234178, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3546, Total reward=182.8, Steps=234315, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3547, Total reward=104.17, Steps=234410, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3548, Total reward=95.65, Steps=234471, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3549, Total reward=136.43, Steps=234573, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3550, Total reward=38.52, Steps=234628, Training iteration=70
Policy training> Surrogate loss=0.004172137007117271, KL divergence=0.005761891603469849, Entropy=0.5282774567604065, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.050015032291412354, KL divergence=0.06083836033940315, Entropy=0.506310224533081, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06214817985892296, KL divergence=0.08296185731887817, Entropy=0.49823176860809326, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06965996325016022, KL divergence=0.09351158887147903, Entropy=0.497456431388855, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.07439804822206497, KL divergence=0.10064689069986343, Entropy=0.4936075210571289, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07591715455055237, KL divergence=0.10244031995534897, Entropy=0.49531206488609314, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0770130306482315, KL divergence=0.10453256964683533, Entropy=0.4945336878299713, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07863049954175949, KL divergence=0.10664661973714828, Entropy=0.49635982513427734, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08098538964986801, KL divergence=0.1085105836391449, Entropy=0.49777331948280334, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0826791301369667, KL divergence=0.1101565882563591, Entropy=0.4974098801612854, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/216_Step-234628.ckpt']
Uploaded 3 files for checkpoint 216 in 0.59 seconds
saved intermediate frozen graph: current/model/model_216.pb
Best checkpoint number: 212, Last checkpoint number: 214
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'213'}
Training> Name=main_level/agent, Worker=0, Episode=3551, Total reward=127.11, Steps=234697, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3552, Total reward=90.51, Steps=234747, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3553, Total reward=63.69, Steps=234788, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3554, Total reward=55.43, Steps=234816, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3555, Total reward=129.15, Steps=234922, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3556, Total reward=83.12, Steps=234995, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3557, Total reward=282.96, Steps=235213, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3558, Total reward=147.84, Steps=235323, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3559, Total reward=27.74, Steps=235357, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3560, Total reward=148.96, Steps=235523, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3561, Total reward=74.51, Steps=235566, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3562, Total reward=123.63, Steps=235695, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3563, Total reward=155.37, Steps=235879, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3564, Total reward=0.02, Steps=235898, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3565, Total reward=140.86, Steps=236008, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3566, Total reward=107.8, Steps=236089, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3567, Total reward=184.37, Steps=236196, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3568, Total reward=90.92, Steps=236249, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3569, Total reward=121.99, Steps=236346, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3570, Total reward=46.61, Steps=236410, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3571, Total reward=105.02, Steps=236477, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3572, Total reward=69.52, Steps=236519, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3573, Total reward=55.37, Steps=236553, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3574, Total reward=52.94, Steps=236582, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3575, Total reward=0.0, Steps=236583, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3576, Total reward=78.27, Steps=236654, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3577, Total reward=151.92, Steps=236760, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3578, Total reward=71.62, Steps=236808, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3579, Total reward=268.61, Steps=237027, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3580, Total reward=267.26, Steps=237255, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3581, Total reward=191.95, Steps=237465, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3582, Total reward=20.24, Steps=237486, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3583, Total reward=106.15, Steps=237614, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3584, Total reward=166.69, Steps=237742, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3585, Total reward=35.99, Steps=237795, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3586, Total reward=157.48, Steps=237929, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3587, Total reward=135.76, Steps=238041, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3588, Total reward=133.43, Steps=238137, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3589, Total reward=78.87, Steps=238210, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3590, Total reward=116.38, Steps=238287, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3591, Total reward=56.16, Steps=238337, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3592, Total reward=79.57, Steps=238388, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3593, Total reward=287.06, Steps=238690, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3594, Total reward=243.66, Steps=238855, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3595, Total reward=197.95, Steps=239045, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3596, Total reward=263.96, Steps=239296, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3597, Total reward=267.54, Steps=239513, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3598, Total reward=75.91, Steps=239552, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3599, Total reward=113.35, Steps=239673, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3600, Total reward=114.63, Steps=239784, Training iteration=71
Policy training> Surrogate loss=-0.00022452585108112544, KL divergence=0.008164764381945133, Entropy=0.5169309377670288, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04784149304032326, KL divergence=0.06686258316040039, Entropy=0.49899959564208984, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05991653352975845, KL divergence=0.08371372520923615, Entropy=0.49634209275245667, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06775869429111481, KL divergence=0.09014956653118134, Entropy=0.4956128001213074, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06971027702093124, KL divergence=0.09456995874643326, Entropy=0.4929064214229584, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07143458724021912, KL divergence=0.09627965092658997, Entropy=0.492562472820282, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07517268508672714, KL divergence=0.09980826079845428, Entropy=0.4910062849521637, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07756300270557404, KL divergence=0.10272730886936188, Entropy=0.4911068081855774, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07880546152591705, KL divergence=0.1043834462761879, Entropy=0.49128779768943787, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07977424561977386, KL divergence=0.1068350300192833, Entropy=0.49049288034439087, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/217_Step-239784.ckpt']
Uploaded 3 files for checkpoint 217 in 0.57 seconds
saved intermediate frozen graph: current/model/model_217.pb
Best checkpoint number: 212, Last checkpoint number: 215
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'214'}
Training> Name=main_level/agent, Worker=0, Episode=3601, Total reward=248.34, Steps=239985, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3602, Total reward=11.96, Steps=240001, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3603, Total reward=175.2, Steps=240161, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3604, Total reward=117.21, Steps=240269, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3605, Total reward=216.81, Steps=240412, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3606, Total reward=195.66, Steps=240569, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3607, Total reward=117.78, Steps=240656, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3608, Total reward=146.66, Steps=240755, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3609, Total reward=149.51, Steps=240849, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3610, Total reward=106.12, Steps=240933, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3611, Total reward=103.72, Steps=241005, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3612, Total reward=83.9, Steps=241064, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3613, Total reward=71.16, Steps=241106, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3614, Total reward=60.67, Steps=241135, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3615, Total reward=132.11, Steps=241218, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3616, Total reward=33.41, Steps=241247, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3617, Total reward=153.61, Steps=241373, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3618, Total reward=142.34, Steps=241462, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3619, Total reward=29.86, Steps=241500, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3620, Total reward=37.98, Steps=241531, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3621, Total reward=207.93, Steps=241668, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3622, Total reward=266.34, Steps=241863, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3623, Total reward=210.86, Steps=242048, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3624, Total reward=38.69, Steps=242134, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3625, Total reward=90.89, Steps=242224, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3626, Total reward=60.48, Steps=242288, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3627, Total reward=144.48, Steps=242429, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3628, Total reward=160.62, Steps=242523, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3629, Total reward=124.35, Steps=242618, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3630, Total reward=112.94, Steps=242698, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3631, Total reward=110.84, Steps=242767, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3632, Total reward=78.49, Steps=242812, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3633, Total reward=69.66, Steps=242850, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3634, Total reward=68.48, Steps=242893, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3635, Total reward=20.5, Steps=242908, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3636, Total reward=170.76, Steps=243066, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3637, Total reward=290.08, Steps=243342, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3638, Total reward=129.05, Steps=243451, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3639, Total reward=99.34, Steps=243535, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3640, Total reward=285.71, Steps=243751, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3641, Total reward=89.85, Steps=243799, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3642, Total reward=152.34, Steps=243994, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3643, Total reward=12.61, Steps=244064, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3644, Total reward=93.64, Steps=244163, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3645, Total reward=223.84, Steps=244315, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3646, Total reward=156.85, Steps=244448, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3647, Total reward=110.6, Steps=244537, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3648, Total reward=132.74, Steps=244632, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3649, Total reward=95.27, Steps=244704, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3650, Total reward=100.19, Steps=244782, Training iteration=72
Policy training> Surrogate loss=0.003259472781792283, KL divergence=0.007253148127347231, Entropy=0.49519672989845276, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04563113674521446, KL divergence=0.06393507122993469, Entropy=0.4834511876106262, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.056346189230680466, KL divergence=0.08517775684595108, Entropy=0.4698878228664398, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0596863217651844, KL divergence=0.08961065113544464, Entropy=0.46380555629730225, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.07100913673639297, KL divergence=0.09449683874845505, Entropy=0.4637107253074646, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07116523385047913, KL divergence=0.09917876869440079, Entropy=0.46139711141586304, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07169406116008759, KL divergence=0.1017887219786644, Entropy=0.4615360200405121, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07233110070228577, KL divergence=0.10383079200983047, Entropy=0.46229082345962524, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07515415549278259, KL divergence=0.10548850148916245, Entropy=0.4626089036464691, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07268726080656052, KL divergence=0.10783747583627701, Entropy=0.46437153220176697, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/218_Step-244782.ckpt']
Uploaded 3 files for checkpoint 218 in 0.53 seconds
saved intermediate frozen graph: current/model/model_218.pb
Best checkpoint number: 212, Last checkpoint number: 216
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'215'}
Training> Name=main_level/agent, Worker=0, Episode=3651, Total reward=19.41, Steps=244797, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3652, Total reward=116.46, Steps=244875, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3653, Total reward=51.84, Steps=244909, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3654, Total reward=52.5, Steps=244947, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3655, Total reward=293.8, Steps=245249, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3656, Total reward=94.47, Steps=245334, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3657, Total reward=86.56, Steps=245383, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3658, Total reward=233.54, Steps=245581, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3659, Total reward=116.14, Steps=245706, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3660, Total reward=104.89, Steps=245795, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3661, Total reward=264.7, Steps=245995, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3662, Total reward=26.33, Steps=246007, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3663, Total reward=19.09, Steps=246067, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3664, Total reward=90.86, Steps=246167, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3665, Total reward=98.18, Steps=246277, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3666, Total reward=166.67, Steps=246410, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3667, Total reward=124.59, Steps=246515, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3668, Total reward=162.52, Steps=246604, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3669, Total reward=25.46, Steps=246626, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3670, Total reward=116.71, Steps=246706, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3671, Total reward=64.12, Steps=246763, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3672, Total reward=104.03, Steps=246822, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3673, Total reward=60.6, Steps=246862, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3674, Total reward=61.09, Steps=246909, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3675, Total reward=359.68, Steps=247178, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3676, Total reward=165.42, Steps=247307, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3677, Total reward=45.75, Steps=247350, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3678, Total reward=293.15, Steps=247582, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3679, Total reward=266.47, Steps=247783, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3680, Total reward=73.93, Steps=247839, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3681, Total reward=47.36, Steps=247862, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3682, Total reward=263.15, Steps=248053, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3683, Total reward=36.94, Steps=248104, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3684, Total reward=180.96, Steps=248253, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3685, Total reward=13.76, Steps=248291, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3686, Total reward=117.07, Steps=248366, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3687, Total reward=322.61, Steps=248617, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3688, Total reward=87.88, Steps=248655, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3689, Total reward=23.37, Steps=248677, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3690, Total reward=305.33, Steps=248943, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3691, Total reward=113.37, Steps=249013, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3692, Total reward=68.16, Steps=249061, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3693, Total reward=48.15, Steps=249087, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3694, Total reward=65.66, Steps=249129, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3695, Total reward=193.21, Steps=249271, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3696, Total reward=53.36, Steps=249323, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3697, Total reward=264.45, Steps=249563, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3698, Total reward=237.43, Steps=249764, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3699, Total reward=229.74, Steps=249982, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3700, Total reward=116.84, Steps=250089, Training iteration=73
Policy training> Surrogate loss=0.0043610152788460255, KL divergence=0.008189784362912178, Entropy=0.49989739060401917, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04367592930793762, KL divergence=0.05720653384923935, Entropy=0.4820471704006195, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0571574866771698, KL divergence=0.08134156465530396, Entropy=0.48178163170814514, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06790824234485626, KL divergence=0.08713977038860321, Entropy=0.47736042737960815, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06787212193012238, KL divergence=0.09223116934299469, Entropy=0.4768170714378357, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07075329124927521, KL divergence=0.09430968761444092, Entropy=0.475452184677124, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07667310535907745, KL divergence=0.09678621590137482, Entropy=0.47789129614830017, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07161536067724228, KL divergence=0.09601473808288574, Entropy=0.4777867794036865, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07598225027322769, KL divergence=0.10016702115535736, Entropy=0.47759905457496643, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08151467144489288, KL divergence=0.10202374309301376, Entropy=0.4791490435600281, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/219_Step-250089.ckpt']
Uploaded 3 files for checkpoint 219 in 0.54 seconds
saved intermediate frozen graph: current/model/model_219.pb
Best checkpoint number: 212, Last checkpoint number: 217
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'216'}
Training> Name=main_level/agent, Worker=0, Episode=3701, Total reward=7.69, Steps=250103, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3702, Total reward=142.82, Steps=250234, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3703, Total reward=161.58, Steps=250405, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3704, Total reward=178.24, Steps=250557, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3705, Total reward=103.11, Steps=250645, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3706, Total reward=235.89, Steps=250781, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3707, Total reward=78.92, Steps=250834, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3708, Total reward=181.45, Steps=250941, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3709, Total reward=84.83, Steps=251003, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3710, Total reward=108.66, Steps=251081, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3711, Total reward=58.31, Steps=251130, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3712, Total reward=108.17, Steps=251206, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3713, Total reward=78.79, Steps=251248, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3714, Total reward=61.9, Steps=251292, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3715, Total reward=15.23, Steps=251304, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3716, Total reward=165.22, Steps=251461, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3717, Total reward=325.33, Steps=251732, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3718, Total reward=69.12, Steps=251787, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3719, Total reward=342.99, Steps=252015, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3720, Total reward=115.72, Steps=252112, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3721, Total reward=84.18, Steps=252153, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3722, Total reward=39.46, Steps=252195, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3723, Total reward=16.06, Steps=252260, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3724, Total reward=15.71, Steps=252323, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3725, Total reward=114.13, Steps=252419, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3726, Total reward=89.84, Steps=252542, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3727, Total reward=69.0, Steps=252574, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3728, Total reward=114.55, Steps=252616, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3729, Total reward=107.24, Steps=252698, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3730, Total reward=132.57, Steps=252785, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3731, Total reward=58.62, Steps=252836, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3732, Total reward=100.0, Steps=252888, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3733, Total reward=48.94, Steps=252928, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3734, Total reward=66.45, Steps=252965, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3735, Total reward=376.61, Steps=253258, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3736, Total reward=349.01, Steps=253563, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3737, Total reward=306.93, Steps=253825, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3738, Total reward=65.65, Steps=253863, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3739, Total reward=17.01, Steps=253884, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3740, Total reward=80.98, Steps=253982, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3741, Total reward=275.14, Steps=254189, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3742, Total reward=135.33, Steps=254317, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3743, Total reward=156.52, Steps=254524, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3744, Total reward=67.74, Steps=254649, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3745, Total reward=43.29, Steps=254685, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3746, Total reward=7.57, Steps=254720, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3747, Total reward=85.85, Steps=254779, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3748, Total reward=86.38, Steps=254817, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3749, Total reward=42.37, Steps=254862, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3750, Total reward=64.11, Steps=254920, Training iteration=74
Policy training> Surrogate loss=0.0007737779524177313, KL divergence=0.00547954672947526, Entropy=0.4989975392818451, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04701331630349159, KL divergence=0.059480853378772736, Entropy=0.4683168828487396, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06012226641178131, KL divergence=0.07873693108558655, Entropy=0.45469221472740173, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06552521139383316, KL divergence=0.08846147358417511, Entropy=0.454032301902771, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06803765892982483, KL divergence=0.09188897907733917, Entropy=0.4543130695819855, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07124713063240051, KL divergence=0.09315034747123718, Entropy=0.4543607532978058, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06969007849693298, KL divergence=0.09656579792499542, Entropy=0.4532245993614197, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0782410129904747, KL divergence=0.09857465326786041, Entropy=0.456400990486145, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07587897032499313, KL divergence=0.10131768882274628, Entropy=0.45436251163482666, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07778458297252655, KL divergence=0.10384701192378998, Entropy=0.45705896615982056, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/220_Step-254920.ckpt']
Uploaded 3 files for checkpoint 220 in 0.50 seconds
saved intermediate frozen graph: current/model/model_220.pb
Best checkpoint number: 212, Last checkpoint number: 218
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'217'}
Training> Name=main_level/agent, Worker=0, Episode=3751, Total reward=26.62, Steps=254939, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3752, Total reward=98.37, Steps=254996, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3753, Total reward=55.07, Steps=255039, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3754, Total reward=227.22, Steps=255198, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3755, Total reward=138.69, Steps=255324, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3756, Total reward=195.16, Steps=255525, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3757, Total reward=180.78, Steps=255644, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3758, Total reward=148.58, Steps=255745, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3759, Total reward=103.53, Steps=255822, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3760, Total reward=77.31, Steps=255915, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3761, Total reward=74.9, Steps=255991, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3762, Total reward=163.25, Steps=256138, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3763, Total reward=16.7, Steps=256203, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3764, Total reward=51.36, Steps=256286, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3765, Total reward=189.73, Steps=256438, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3766, Total reward=102.29, Steps=256522, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3767, Total reward=216.66, Steps=256672, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3768, Total reward=207.8, Steps=256791, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3769, Total reward=112.85, Steps=256889, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3770, Total reward=244.14, Steps=257091, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3771, Total reward=87.34, Steps=257146, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3772, Total reward=72.32, Steps=257205, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3773, Total reward=65.8, Steps=257244, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3774, Total reward=41.25, Steps=257288, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3775, Total reward=22.13, Steps=257301, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3776, Total reward=31.11, Steps=257336, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3777, Total reward=99.44, Steps=257433, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3778, Total reward=173.48, Steps=257588, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3779, Total reward=323.34, Steps=257910, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3780, Total reward=296.3, Steps=258137, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3781, Total reward=68.68, Steps=258181, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3782, Total reward=60.3, Steps=258244, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3783, Total reward=188.84, Steps=258433, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3784, Total reward=174.62, Steps=258610, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3785, Total reward=174.54, Steps=258771, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3786, Total reward=102.18, Steps=258877, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3787, Total reward=145.99, Steps=258985, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3788, Total reward=92.04, Steps=259025, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3789, Total reward=94.32, Steps=259100, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3790, Total reward=44.53, Steps=259131, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3791, Total reward=109.71, Steps=259202, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3792, Total reward=103.6, Steps=259259, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3793, Total reward=51.9, Steps=259300, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3794, Total reward=59.44, Steps=259342, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3795, Total reward=163.62, Steps=259468, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3796, Total reward=171.99, Steps=259600, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3797, Total reward=315.42, Steps=259871, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3798, Total reward=130.92, Steps=259955, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3799, Total reward=45.32, Steps=260015, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3800, Total reward=301.42, Steps=260218, Training iteration=75
Policy training> Surrogate loss=0.00046649930300191045, KL divergence=0.006418888457119465, Entropy=0.4800468385219574, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04896990954875946, KL divergence=0.06912152469158173, Entropy=0.45669764280319214, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06418509781360626, KL divergence=0.09526024013757706, Entropy=0.4554864466190338, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06666398793458939, KL divergence=0.10322507470846176, Entropy=0.4567517340183258, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06897642463445663, KL divergence=0.10729378461837769, Entropy=0.45609450340270996, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07761166244745255, KL divergence=0.11232098191976547, Entropy=0.4547635614871979, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07188665121793747, KL divergence=0.11411094665527344, Entropy=0.4564264416694641, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07608840614557266, KL divergence=0.1152716726064682, Entropy=0.4601210951805115, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08306568115949631, KL divergence=0.11781499534845352, Entropy=0.45741862058639526, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.08038706332445145, KL divergence=0.11990537494421005, Entropy=0.45843490958213806, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/221_Step-260218.ckpt']
Uploaded 3 files for checkpoint 221 in 0.55 seconds
saved intermediate frozen graph: current/model/model_221.pb
Best checkpoint number: 212, Last checkpoint number: 219
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'218'}
Training> Name=main_level/agent, Worker=0, Episode=3801, Total reward=232.39, Steps=260409, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3802, Total reward=53.1, Steps=260462, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3803, Total reward=76.75, Steps=260589, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3804, Total reward=109.28, Steps=260682, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3805, Total reward=189.1, Steps=260836, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3806, Total reward=59.31, Steps=260920, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3807, Total reward=163.79, Steps=261053, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3808, Total reward=156.16, Steps=261146, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3809, Total reward=35.81, Steps=261169, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3810, Total reward=11.35, Steps=261183, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3811, Total reward=108.39, Steps=261253, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3812, Total reward=59.61, Steps=261288, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3813, Total reward=60.62, Steps=261326, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3814, Total reward=0.0, Steps=261327, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3815, Total reward=298.3, Steps=261625, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3816, Total reward=82.3, Steps=261698, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3817, Total reward=89.48, Steps=261748, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3818, Total reward=273.24, Steps=261962, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3819, Total reward=184.72, Steps=262138, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3820, Total reward=245.48, Steps=262342, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3821, Total reward=219.63, Steps=262529, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3822, Total reward=37.92, Steps=262565, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3823, Total reward=0.02, Steps=262588, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3824, Total reward=1.42, Steps=262607, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3825, Total reward=101.82, Steps=262705, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3826, Total reward=218.5, Steps=262871, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3827, Total reward=212.56, Steps=262991, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3828, Total reward=122.82, Steps=263073, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3829, Total reward=51.75, Steps=263119, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3830, Total reward=64.72, Steps=263183, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3831, Total reward=55.26, Steps=263228, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3832, Total reward=86.28, Steps=263266, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3833, Total reward=33.47, Steps=263308, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3834, Total reward=398.43, Steps=263597, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3835, Total reward=21.82, Steps=263610, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3836, Total reward=153.16, Steps=263765, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3837, Total reward=406.15, Steps=264026, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3838, Total reward=273.76, Steps=264253, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3839, Total reward=148.05, Steps=264370, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3840, Total reward=109.69, Steps=264433, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3841, Total reward=243.96, Steps=264645, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3842, Total reward=99.54, Steps=264774, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3843, Total reward=210.06, Steps=264968, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3844, Total reward=149.58, Steps=265116, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3845, Total reward=196.18, Steps=265282, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3846, Total reward=120.56, Steps=265355, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3847, Total reward=142.89, Steps=265467, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3848, Total reward=126.17, Steps=265548, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3849, Total reward=55.94, Steps=265638, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3850, Total reward=106.72, Steps=265712, Training iteration=76
Policy training> Surrogate loss=0.0038935698103159666, KL divergence=0.006307917647063732, Entropy=0.47867438197135925, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04240646958351135, KL divergence=0.06557586789131165, Entropy=0.45523351430892944, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05840824916958809, KL divergence=0.0892191082239151, Entropy=0.44548267126083374, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.057251207530498505, KL divergence=0.09869271516799927, Entropy=0.4438149929046631, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06506556272506714, KL divergence=0.10057767480611801, Entropy=0.44579607248306274, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0681794285774231, KL divergence=0.1038583442568779, Entropy=0.44638532400131226, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07105118036270142, KL divergence=0.10564917325973511, Entropy=0.44752296805381775, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06906560063362122, KL divergence=0.10814513266086578, Entropy=0.4499795138835907, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07094486057758331, KL divergence=0.10922637581825256, Entropy=0.448081374168396, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07331021875143051, KL divergence=0.11232035607099533, Entropy=0.4494810104370117, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/222_Step-265712.ckpt']
Uploaded 3 files for checkpoint 222 in 0.57 seconds
saved intermediate frozen graph: current/model/model_222.pb
Best checkpoint number: 212, Last checkpoint number: 220
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'219'}
Training> Name=main_level/agent, Worker=0, Episode=3851, Total reward=131.85, Steps=265810, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3852, Total reward=68.16, Steps=265869, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3853, Total reward=54.12, Steps=265910, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3854, Total reward=41.5, Steps=265952, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3855, Total reward=19.02, Steps=265964, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3856, Total reward=148.36, Steps=266089, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3857, Total reward=63.9, Steps=266142, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3858, Total reward=315.44, Steps=266411, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3859, Total reward=115.62, Steps=266524, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3860, Total reward=87.48, Steps=266587, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3861, Total reward=191.72, Steps=266750, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3862, Total reward=124.4, Steps=266874, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3863, Total reward=136.16, Steps=267009, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3864, Total reward=9.58, Steps=267037, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3865, Total reward=137.21, Steps=267179, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3866, Total reward=167.62, Steps=267297, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3867, Total reward=88.77, Steps=267350, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3868, Total reward=90.34, Steps=267406, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3869, Total reward=145.29, Steps=267496, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3870, Total reward=125.09, Steps=267571, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3871, Total reward=103.0, Steps=267637, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3872, Total reward=101.11, Steps=267693, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3873, Total reward=63.74, Steps=267730, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3874, Total reward=361.61, Steps=268037, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3875, Total reward=214.88, Steps=268174, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3876, Total reward=47.21, Steps=268240, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3877, Total reward=253.15, Steps=268474, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3878, Total reward=110.54, Steps=268570, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3879, Total reward=19.79, Steps=268603, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3880, Total reward=266.38, Steps=268819, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3881, Total reward=216.3, Steps=269028, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3882, Total reward=271.18, Steps=269231, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3883, Total reward=30.03, Steps=269318, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3884, Total reward=133.07, Steps=269439, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3885, Total reward=182.11, Steps=269573, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3886, Total reward=7.35, Steps=269586, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3887, Total reward=38.62, Steps=269616, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3888, Total reward=142.68, Steps=269716, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3889, Total reward=51.37, Steps=269766, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3890, Total reward=155.83, Steps=269853, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3891, Total reward=117.11, Steps=269919, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3892, Total reward=62.58, Steps=269955, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3893, Total reward=59.55, Steps=269996, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3894, Total reward=63.01, Steps=270047, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3895, Total reward=16.44, Steps=270059, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3896, Total reward=185.81, Steps=270212, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3897, Total reward=74.3, Steps=270266, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3898, Total reward=97.74, Steps=270339, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3899, Total reward=117.66, Steps=270483, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3900, Total reward=226.1, Steps=270706, Training iteration=77
Policy training> Surrogate loss=0.002245750278234482, KL divergence=0.007125943899154663, Entropy=0.4695819020271301, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03989511355757713, KL divergence=0.06326612830162048, Entropy=0.4537665843963623, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05689339339733124, KL divergence=0.08786699175834656, Entropy=0.44629910588264465, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06450813263654709, KL divergence=0.09771472960710526, Entropy=0.4436088800430298, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0661318376660347, KL divergence=0.10226204991340637, Entropy=0.43929997086524963, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.07026619464159012, KL divergence=0.10437972843647003, Entropy=0.4387001693248749, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06606271117925644, KL divergence=0.1067490428686142, Entropy=0.43610483407974243, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07275112718343735, KL divergence=0.10982883721590042, Entropy=0.4362504780292511, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07096963375806808, KL divergence=0.11284834146499634, Entropy=0.43982353806495667, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07562852650880814, KL divergence=0.11410896480083466, Entropy=0.4403902292251587, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/223_Step-270706.ckpt']
Uploaded 3 files for checkpoint 223 in 0.53 seconds
saved intermediate frozen graph: current/model/model_223.pb
Best checkpoint number: 212, Last checkpoint number: 221
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'220'}
Training> Name=main_level/agent, Worker=0, Episode=3901, Total reward=166.07, Steps=270879, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3902, Total reward=38.21, Steps=270902, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3903, Total reward=11.31, Steps=270951, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3904, Total reward=15.05, Steps=270985, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3905, Total reward=100.82, Steps=271066, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3906, Total reward=127.37, Steps=271164, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3907, Total reward=3.32, Steps=271177, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3908, Total reward=82.42, Steps=271216, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3909, Total reward=23.59, Steps=271239, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3910, Total reward=23.03, Steps=271266, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3911, Total reward=132.96, Steps=271340, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3912, Total reward=101.5, Steps=271396, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3913, Total reward=61.24, Steps=271436, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3914, Total reward=367.65, Steps=271720, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3915, Total reward=45.36, Steps=271763, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3916, Total reward=328.19, Steps=272052, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3917, Total reward=248.17, Steps=272262, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3918, Total reward=299.53, Steps=272512, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3919, Total reward=276.71, Steps=272754, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3920, Total reward=226.53, Steps=272931, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3921, Total reward=170.18, Steps=273077, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3922, Total reward=135.33, Steps=273209, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3923, Total reward=159.45, Steps=273341, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3924, Total reward=169.04, Steps=273511, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3925, Total reward=209.34, Steps=273672, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3926, Total reward=65.6, Steps=273743, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3927, Total reward=73.42, Steps=273777, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3928, Total reward=178.22, Steps=273897, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3929, Total reward=84.86, Steps=273979, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3930, Total reward=61.97, Steps=274040, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3931, Total reward=95.82, Steps=274110, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3932, Total reward=59.61, Steps=274161, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3933, Total reward=0.02, Steps=274179, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3934, Total reward=62.94, Steps=274208, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3935, Total reward=426.14, Steps=274522, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3936, Total reward=177.9, Steps=274710, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3937, Total reward=185.21, Steps=274860, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3938, Total reward=337.12, Steps=275121, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3939, Total reward=115.34, Steps=275191, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3940, Total reward=87.53, Steps=275263, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3941, Total reward=250.28, Steps=275477, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3942, Total reward=225.05, Steps=275650, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3943, Total reward=11.79, Steps=275670, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3944, Total reward=60.48, Steps=275762, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3945, Total reward=139.89, Steps=275877, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3946, Total reward=165.59, Steps=276016, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3947, Total reward=90.43, Steps=276091, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3948, Total reward=158.06, Steps=276181, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3949, Total reward=102.9, Steps=276270, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3950, Total reward=54.97, Steps=276339, Training iteration=78
Policy training> Surrogate loss=0.004114135634154081, KL divergence=0.0068357884883880615, Entropy=0.47791245579719543, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.043214138597249985, KL divergence=0.06815362721681595, Entropy=0.4541715383529663, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05561135709285736, KL divergence=0.08780115842819214, Entropy=0.44882479310035706, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06140546873211861, KL divergence=0.09137532860040665, Entropy=0.44795989990234375, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06493902951478958, KL divergence=0.09328004717826843, Entropy=0.4472143352031708, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06703248620033264, KL divergence=0.09532750397920609, Entropy=0.447206050157547, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06897097080945969, KL divergence=0.09867791831493378, Entropy=0.447348952293396, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07003312557935715, KL divergence=0.10104436427354813, Entropy=0.4469928741455078, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0714888721704483, KL divergence=0.10270173847675323, Entropy=0.4473353624343872, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07230610400438309, KL divergence=0.10456247627735138, Entropy=0.4472459554672241, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/224_Step-276339.ckpt']
Uploaded 3 files for checkpoint 224 in 0.60 seconds
saved intermediate frozen graph: current/model/model_224.pb
Best checkpoint number: 212, Last checkpoint number: 222
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'221'}
Training> Name=main_level/agent, Worker=0, Episode=3951, Total reward=11.76, Steps=276358, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3952, Total reward=111.58, Steps=276409, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3953, Total reward=238.08, Steps=276639, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3954, Total reward=311.34, Steps=276936, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3955, Total reward=102.41, Steps=277016, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3956, Total reward=350.34, Steps=277320, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3957, Total reward=25.03, Steps=277342, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3958, Total reward=170.39, Steps=277463, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3959, Total reward=104.52, Steps=277536, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3960, Total reward=87.0, Steps=277594, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3961, Total reward=292.41, Steps=277845, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3962, Total reward=37.96, Steps=277872, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3963, Total reward=148.37, Steps=278048, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3964, Total reward=112.17, Steps=278158, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3965, Total reward=84.45, Steps=278261, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3966, Total reward=143.89, Steps=278346, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3967, Total reward=151.64, Steps=278449, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3968, Total reward=364.78, Steps=278693, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3969, Total reward=103.74, Steps=278785, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3970, Total reward=46.72, Steps=278840, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3971, Total reward=107.81, Steps=278906, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3972, Total reward=11.48, Steps=278917, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3973, Total reward=61.94, Steps=278953, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3974, Total reward=61.85, Steps=278991, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3975, Total reward=107.16, Steps=279074, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3976, Total reward=26.64, Steps=279125, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3977, Total reward=106.32, Steps=279242, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3978, Total reward=146.7, Steps=279397, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3979, Total reward=223.77, Steps=279608, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3980, Total reward=189.77, Steps=279838, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3981, Total reward=91.52, Steps=279947, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3982, Total reward=232.25, Steps=280147, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3983, Total reward=216.78, Steps=280331, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3984, Total reward=162.83, Steps=280511, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3985, Total reward=27.29, Steps=280561, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3986, Total reward=182.97, Steps=280710, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3987, Total reward=181.1, Steps=280837, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3988, Total reward=101.34, Steps=280895, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3989, Total reward=154.37, Steps=280990, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3990, Total reward=80.82, Steps=281047, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3991, Total reward=115.53, Steps=281118, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3992, Total reward=40.93, Steps=281151, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3993, Total reward=62.99, Steps=281189, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3994, Total reward=59.73, Steps=281230, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3995, Total reward=40.82, Steps=281248, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3996, Total reward=160.37, Steps=281381, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3997, Total reward=279.22, Steps=281593, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3998, Total reward=150.66, Steps=281704, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3999, Total reward=123.04, Steps=281824, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=4000, Total reward=179.07, Steps=281976, Training iteration=79
Policy training> Surrogate loss=0.004123789258301258, KL divergence=0.008010082878172398, Entropy=0.4656783938407898, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0458669438958168, KL divergence=0.07115518301725388, Entropy=0.44717875123023987, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.055668510496616364, KL divergence=0.0911816656589508, Entropy=0.4410262405872345, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0613020621240139, KL divergence=0.09752661734819412, Entropy=0.4390735626220703, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06377848982810974, KL divergence=0.10057637840509415, Entropy=0.43891775608062744, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0663181021809578, KL divergence=0.10406229645013809, Entropy=0.43726804852485657, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0680345892906189, KL divergence=0.10574328154325485, Entropy=0.43657323718070984, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06812193989753723, KL divergence=0.10783349722623825, Entropy=0.4375241994857788, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0705990344285965, KL divergence=0.10968602448701859, Entropy=0.4369193911552429, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07066133618354797, KL divergence=0.11257979273796082, Entropy=0.4357464909553528, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/225_Step-281976.ckpt']
Uploaded 3 files for checkpoint 225 in 0.60 seconds
saved intermediate frozen graph: current/model/model_225.pb
Best checkpoint number: 212, Last checkpoint number: 223
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'222'}
Training> Name=main_level/agent, Worker=0, Episode=4001, Total reward=179.81, Steps=282197, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4002, Total reward=162.13, Steps=282394, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4003, Total reward=182.78, Steps=282561, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4004, Total reward=138.74, Steps=282682, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4005, Total reward=168.55, Steps=282804, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4006, Total reward=148.59, Steps=282936, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4007, Total reward=103.59, Steps=282992, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4008, Total reward=91.04, Steps=283031, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4009, Total reward=115.0, Steps=283147, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4010, Total reward=99.34, Steps=283236, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4011, Total reward=71.64, Steps=283290, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4012, Total reward=87.56, Steps=283335, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4013, Total reward=71.09, Steps=283380, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4014, Total reward=59.0, Steps=283408, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4015, Total reward=113.01, Steps=283510, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4016, Total reward=148.89, Steps=283639, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4017, Total reward=136.61, Steps=283754, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4018, Total reward=23.48, Steps=283770, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4019, Total reward=199.84, Steps=283944, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4020, Total reward=193.86, Steps=284112, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4021, Total reward=177.86, Steps=284338, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4022, Total reward=47.16, Steps=284379, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4023, Total reward=368.49, Steps=284709, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4024, Total reward=156.41, Steps=284848, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4025, Total reward=211.98, Steps=284998, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4026, Total reward=197.16, Steps=285139, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4027, Total reward=147.82, Steps=285251, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4028, Total reward=89.84, Steps=285288, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4029, Total reward=30.66, Steps=285305, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4030, Total reward=66.07, Steps=285362, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4031, Total reward=114.2, Steps=285431, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4032, Total reward=127.21, Steps=285499, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4033, Total reward=69.14, Steps=285538, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4034, Total reward=59.15, Steps=285569, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4035, Total reward=34.92, Steps=285601, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4036, Total reward=260.24, Steps=285827, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4037, Total reward=24.72, Steps=285849, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4038, Total reward=150.14, Steps=285939, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4039, Total reward=99.13, Steps=286044, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4040, Total reward=195.75, Steps=286194, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4041, Total reward=92.86, Steps=286312, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4042, Total reward=43.05, Steps=286336, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4043, Total reward=187.22, Steps=286504, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4044, Total reward=209.38, Steps=286678, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4045, Total reward=91.62, Steps=286762, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4046, Total reward=170.17, Steps=286891, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4047, Total reward=88.43, Steps=286947, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4048, Total reward=74.47, Steps=286985, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4049, Total reward=154.76, Steps=287097, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4050, Total reward=101.82, Steps=287176, Training iteration=80
Policy training> Surrogate loss=0.0044769542291760445, KL divergence=0.007829385809600353, Entropy=0.46026116609573364, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04511623829603195, KL divergence=0.06315706670284271, Entropy=0.44440025091171265, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05519304424524307, KL divergence=0.08703764528036118, Entropy=0.4380076825618744, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05982772633433342, KL divergence=0.0931851863861084, Entropy=0.4384394586086273, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06332209706306458, KL divergence=0.09781681001186371, Entropy=0.43680286407470703, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06716535240411758, KL divergence=0.10156083106994629, Entropy=0.4370817244052887, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06896334886550903, KL divergence=0.10487258434295654, Entropy=0.43616780638694763, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06532718241214752, KL divergence=0.10504083335399628, Entropy=0.4360034465789795, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06960582733154297, KL divergence=0.10806254297494888, Entropy=0.4359024465084076, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0726400688290596, KL divergence=0.10848013311624527, Entropy=0.4368714690208435, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/226_Step-287176.ckpt']
Uploaded 3 files for checkpoint 226 in 0.49 seconds
saved intermediate frozen graph: current/model/model_226.pb
Best checkpoint number: 212, Last checkpoint number: 224
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'223'}
Training> Name=main_level/agent, Worker=0, Episode=4051, Total reward=105.43, Steps=287244, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4052, Total reward=118.91, Steps=287319, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4053, Total reward=25.82, Steps=287331, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4054, Total reward=53.88, Steps=287377, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4055, Total reward=293.86, Steps=287677, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4056, Total reward=108.94, Steps=287749, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4057, Total reward=268.95, Steps=288040, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4058, Total reward=139.84, Steps=288119, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4059, Total reward=319.75, Steps=288375, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4060, Total reward=87.59, Steps=288480, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4061, Total reward=57.0, Steps=288519, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4062, Total reward=29.57, Steps=288561, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4063, Total reward=3.31, Steps=288587, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4064, Total reward=107.92, Steps=288699, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4065, Total reward=194.26, Steps=288877, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4066, Total reward=7.32, Steps=288902, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4067, Total reward=101.94, Steps=289002, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4068, Total reward=96.6, Steps=289039, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4069, Total reward=90.88, Steps=289130, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4070, Total reward=48.3, Steps=289190, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4071, Total reward=72.9, Steps=289228, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4072, Total reward=67.73, Steps=289279, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4073, Total reward=7.46, Steps=289301, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4074, Total reward=44.72, Steps=289331, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4075, Total reward=33.11, Steps=289348, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4076, Total reward=214.75, Steps=289545, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4077, Total reward=306.11, Steps=289819, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4078, Total reward=238.07, Steps=290036, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4079, Total reward=128.02, Steps=290152, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4080, Total reward=210.66, Steps=290354, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4081, Total reward=20.05, Steps=290371, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4082, Total reward=40.09, Steps=290404, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4083, Total reward=190.27, Steps=290607, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4084, Total reward=132.46, Steps=290731, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4085, Total reward=9.92, Steps=290757, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4086, Total reward=99.19, Steps=290820, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4087, Total reward=150.27, Steps=290927, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4088, Total reward=145.44, Steps=291015, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4089, Total reward=35.87, Steps=291068, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4090, Total reward=54.87, Steps=291133, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4091, Total reward=101.85, Steps=291201, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4092, Total reward=106.08, Steps=291282, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4093, Total reward=244.85, Steps=291471, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4094, Total reward=53.35, Steps=291517, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4095, Total reward=20.15, Steps=291532, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4096, Total reward=154.27, Steps=291685, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4097, Total reward=351.59, Steps=291953, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4098, Total reward=310.2, Steps=292200, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4099, Total reward=49.48, Steps=292252, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4100, Total reward=93.28, Steps=292306, Training iteration=81
Policy training> Surrogate loss=0.007228607777506113, KL divergence=0.008212875574827194, Entropy=0.45350146293640137, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04369502142071724, KL divergence=0.06519647687673569, Entropy=0.42931145429611206, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05589468404650688, KL divergence=0.0899338498711586, Entropy=0.4197596609592438, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06165493652224541, KL divergence=0.09881219267845154, Entropy=0.41799402236938477, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06435218453407288, KL divergence=0.10469768196344376, Entropy=0.4158186912536621, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06781625747680664, KL divergence=0.1068958267569542, Entropy=0.41365981101989746, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06927251070737839, KL divergence=0.10866831243038177, Entropy=0.41473498940467834, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07015349715948105, KL divergence=0.11104737222194672, Entropy=0.415010541677475, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07095148414373398, KL divergence=0.11245934665203094, Entropy=0.4144117832183838, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07246772199869156, KL divergence=0.11413655430078506, Entropy=0.41458749771118164, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/227_Step-292306.ckpt']
Uploaded 3 files for checkpoint 227 in 0.50 seconds
saved intermediate frozen graph: current/model/model_227.pb
Best checkpoint number: 212, Last checkpoint number: 225
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'224'}
Training> Name=main_level/agent, Worker=0, Episode=4101, Total reward=93.96, Steps=292358, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4102, Total reward=56.19, Steps=292389, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4103, Total reward=251.74, Steps=292559, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4104, Total reward=3.54, Steps=292582, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4105, Total reward=192.79, Steps=292738, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4106, Total reward=152.13, Steps=292867, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4107, Total reward=181.73, Steps=292986, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4108, Total reward=197.37, Steps=293090, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4109, Total reward=143.06, Steps=293181, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4110, Total reward=111.21, Steps=293256, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4111, Total reward=114.03, Steps=293328, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4112, Total reward=110.61, Steps=293386, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4113, Total reward=40.84, Steps=293430, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4114, Total reward=46.9, Steps=293468, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4115, Total reward=144.29, Steps=293554, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4116, Total reward=51.43, Steps=293605, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4117, Total reward=170.93, Steps=293714, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4118, Total reward=140.54, Steps=293876, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4119, Total reward=214.64, Steps=294069, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4120, Total reward=97.96, Steps=294175, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4121, Total reward=310.75, Steps=294382, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4122, Total reward=229.1, Steps=294555, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4123, Total reward=207.61, Steps=294741, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4124, Total reward=139.92, Steps=294909, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4125, Total reward=174.89, Steps=295064, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4126, Total reward=106.69, Steps=295175, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4127, Total reward=118.33, Steps=295261, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4128, Total reward=257.11, Steps=295482, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4129, Total reward=138.36, Steps=295577, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4130, Total reward=90.6, Steps=295643, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4131, Total reward=119.87, Steps=295715, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4132, Total reward=50.14, Steps=295749, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4133, Total reward=63.65, Steps=295789, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4134, Total reward=25.43, Steps=295801, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4135, Total reward=66.58, Steps=295848, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4136, Total reward=222.54, Steps=296093, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4137, Total reward=171.32, Steps=296197, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4138, Total reward=136.57, Steps=296313, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4139, Total reward=21.8, Steps=296337, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4140, Total reward=198.0, Steps=296563, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4141, Total reward=198.76, Steps=296775, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4142, Total reward=213.38, Steps=296959, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4143, Total reward=114.18, Steps=297150, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4144, Total reward=248.71, Steps=297339, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4145, Total reward=11.05, Steps=297370, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4146, Total reward=205.2, Steps=297513, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4147, Total reward=106.59, Steps=297597, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4148, Total reward=173.81, Steps=297700, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4149, Total reward=86.81, Steps=297780, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4150, Total reward=134.57, Steps=297866, Training iteration=82
Policy training> Surrogate loss=0.0011611495865508914, KL divergence=0.0077018821612000465, Entropy=0.435812771320343, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04343586787581444, KL divergence=0.07015660405158997, Entropy=0.4118725061416626, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.048760682344436646, KL divergence=0.0868135467171669, Entropy=0.4052603244781494, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05950837954878807, KL divergence=0.09457952529191971, Entropy=0.40359029173851013, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06196396052837372, KL divergence=0.09960850328207016, Entropy=0.40576788783073425, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06302228569984436, KL divergence=0.10270098596811295, Entropy=0.4048839807510376, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06605616956949234, KL divergence=0.10716573148965836, Entropy=0.40338870882987976, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06835883110761642, KL divergence=0.1094207763671875, Entropy=0.4032976031303406, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07029169797897339, KL divergence=0.11181748658418655, Entropy=0.4043542742729187, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06569980084896088, KL divergence=0.11367145925760269, Entropy=0.40546971559524536, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/228_Step-297866.ckpt']
Uploaded 3 files for checkpoint 228 in 0.58 seconds
saved intermediate frozen graph: current/model/model_228.pb
Best checkpoint number: 212, Last checkpoint number: 226
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'225'}
Training> Name=main_level/agent, Worker=0, Episode=4151, Total reward=101.56, Steps=297932, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4152, Total reward=6.89, Steps=297944, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4153, Total reward=65.49, Steps=297983, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4154, Total reward=60.85, Steps=298012, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4155, Total reward=285.07, Steps=298272, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4156, Total reward=188.22, Steps=298453, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4157, Total reward=355.83, Steps=298715, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4158, Total reward=142.88, Steps=298805, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4159, Total reward=88.03, Steps=298893, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4160, Total reward=82.39, Steps=298942, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4161, Total reward=460.98, Steps=299241, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4162, Total reward=241.95, Steps=299445, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4163, Total reward=7.59, Steps=299485, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4164, Total reward=10.59, Steps=299519, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4165, Total reward=164.54, Steps=299668, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4166, Total reward=99.81, Steps=299737, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4167, Total reward=87.09, Steps=299815, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4168, Total reward=123.75, Steps=299911, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4169, Total reward=137.76, Steps=300005, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4170, Total reward=22.66, Steps=300037, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4171, Total reward=93.18, Steps=300105, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4172, Total reward=16.38, Steps=300117, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4173, Total reward=59.15, Steps=300156, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4174, Total reward=153.4, Steps=300287, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4175, Total reward=195.29, Steps=300435, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4176, Total reward=25.91, Steps=300453, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4177, Total reward=299.01, Steps=300702, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4178, Total reward=123.36, Steps=300840, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4179, Total reward=101.54, Steps=300936, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4180, Total reward=268.74, Steps=301164, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4181, Total reward=310.63, Steps=301379, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4182, Total reward=31.23, Steps=301417, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4183, Total reward=22.06, Steps=301464, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4184, Total reward=133.14, Steps=301559, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4185, Total reward=196.04, Steps=301675, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4186, Total reward=162.16, Steps=301783, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4187, Total reward=80.19, Steps=301829, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4188, Total reward=178.09, Steps=301937, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4189, Total reward=146.31, Steps=302050, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4190, Total reward=107.68, Steps=302115, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4191, Total reward=52.75, Steps=302158, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4192, Total reward=80.18, Steps=302196, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4193, Total reward=21.23, Steps=302223, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4194, Total reward=178.46, Steps=302388, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4195, Total reward=280.38, Steps=302619, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4196, Total reward=150.88, Steps=302816, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4197, Total reward=290.77, Steps=303108, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4198, Total reward=97.99, Steps=303183, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4199, Total reward=16.99, Steps=303203, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4200, Total reward=82.75, Steps=303310, Training iteration=83
Policy training> Surrogate loss=0.005428709555417299, KL divergence=0.009437721222639084, Entropy=0.4247438311576843, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03784359246492386, KL divergence=0.08395559340715408, Entropy=0.40552154183387756, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.054665401577949524, KL divergence=0.10139031708240509, Entropy=0.3998619616031647, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.057955820113420486, KL divergence=0.10859619081020355, Entropy=0.3967970907688141, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06355756521224976, KL divergence=0.11353219300508499, Entropy=0.3958298861980438, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06509164720773697, KL divergence=0.11539453268051147, Entropy=0.394573837518692, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06796590238809586, KL divergence=0.11868567764759064, Entropy=0.39542701840400696, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06936042755842209, KL divergence=0.12091118842363358, Entropy=0.39643844962120056, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07053433358669281, KL divergence=0.12329979985952377, Entropy=0.3974358141422272, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06984182447195053, KL divergence=0.12513957917690277, Entropy=0.39756444096565247, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/229_Step-303310.ckpt']
Uploaded 3 files for checkpoint 229 in 0.61 seconds
saved intermediate frozen graph: current/model/model_229.pb
Best checkpoint number: 212, Last checkpoint number: 227
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'226'}
Training> Name=main_level/agent, Worker=0, Episode=4201, Total reward=231.54, Steps=303518, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4202, Total reward=38.0, Steps=303549, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4203, Total reward=172.42, Steps=303708, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4204, Total reward=114.25, Steps=303801, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4205, Total reward=109.57, Steps=303909, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4206, Total reward=211.76, Steps=304055, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4207, Total reward=178.2, Steps=304158, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4208, Total reward=136.09, Steps=304278, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4209, Total reward=44.08, Steps=304322, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4210, Total reward=124.91, Steps=304406, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4211, Total reward=89.35, Steps=304460, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4212, Total reward=281.69, Steps=304658, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4213, Total reward=51.51, Steps=304694, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4214, Total reward=46.94, Steps=304730, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4215, Total reward=400.42, Steps=305027, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4216, Total reward=91.67, Steps=305099, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4217, Total reward=300.89, Steps=305339, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4218, Total reward=64.56, Steps=305396, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4219, Total reward=226.01, Steps=305609, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4220, Total reward=333.67, Steps=305835, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4221, Total reward=64.93, Steps=305882, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4222, Total reward=184.41, Steps=306071, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4223, Total reward=145.59, Steps=306256, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4224, Total reward=69.8, Steps=306374, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4225, Total reward=13.76, Steps=306406, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4226, Total reward=136.73, Steps=306523, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4227, Total reward=104.91, Steps=306617, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4228, Total reward=206.39, Steps=306722, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4229, Total reward=38.91, Steps=306760, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4230, Total reward=104.58, Steps=306842, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4231, Total reward=11.72, Steps=306852, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4232, Total reward=101.45, Steps=306900, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4233, Total reward=55.74, Steps=306935, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4234, Total reward=56.84, Steps=306976, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4235, Total reward=37.97, Steps=306993, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4236, Total reward=134.63, Steps=307121, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4237, Total reward=264.34, Steps=307343, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4238, Total reward=255.64, Steps=307539, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4239, Total reward=204.56, Steps=307800, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4240, Total reward=253.92, Steps=308031, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4241, Total reward=181.28, Steps=308175, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4242, Total reward=58.53, Steps=308233, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4243, Total reward=189.86, Steps=308390, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4244, Total reward=7.63, Steps=308410, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4245, Total reward=158.06, Steps=308563, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4246, Total reward=247.52, Steps=308698, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4247, Total reward=128.85, Steps=308811, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4248, Total reward=197.02, Steps=308919, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4249, Total reward=177.53, Steps=309008, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4250, Total reward=22.71, Steps=309025, Training iteration=84
Policy training> Surrogate loss=0.004182454664260149, KL divergence=0.008366475813090801, Entropy=0.42397376894950867, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03725011646747589, KL divergence=0.06938374042510986, Entropy=0.3974538743495941, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05141814053058624, KL divergence=0.08759091049432755, Entropy=0.3938296139240265, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0590088926255703, KL divergence=0.09197009354829788, Entropy=0.3939726650714874, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06159098073840141, KL divergence=0.09743935614824295, Entropy=0.3939984440803528, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.065309077501297, KL divergence=0.101871557533741, Entropy=0.3954477310180664, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06364649534225464, KL divergence=0.10431041568517685, Entropy=0.3961861729621887, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06352616101503372, KL divergence=0.10466418415307999, Entropy=0.3956385552883148, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06962600350379944, KL divergence=0.10767780989408493, Entropy=0.39561164379119873, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06898282468318939, KL divergence=0.10826366394758224, Entropy=0.39564064145088196, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/230_Step-309025.ckpt']
Uploaded 3 files for checkpoint 230 in 0.50 seconds
saved intermediate frozen graph: current/model/model_230.pb
Best checkpoint number: 212, Last checkpoint number: 228
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'227'}
Training> Name=main_level/agent, Worker=0, Episode=4251, Total reward=113.73, Steps=309091, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4252, Total reward=42.95, Steps=309124, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4253, Total reward=365.74, Steps=309436, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4254, Total reward=236.55, Steps=309641, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4255, Total reward=153.64, Steps=309840, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4256, Total reward=254.33, Steps=310073, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4257, Total reward=239.89, Steps=310302, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4258, Total reward=248.51, Steps=310527, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4259, Total reward=183.17, Steps=310702, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4260, Total reward=171.78, Steps=310891, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4261, Total reward=86.25, Steps=310958, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4262, Total reward=32.21, Steps=310986, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4263, Total reward=6.91, Steps=311040, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4264, Total reward=179.24, Steps=311190, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4265, Total reward=193.95, Steps=311367, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4266, Total reward=64.65, Steps=311420, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4267, Total reward=93.01, Steps=311504, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4268, Total reward=78.38, Steps=311542, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4269, Total reward=50.92, Steps=311598, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4270, Total reward=66.09, Steps=311667, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4271, Total reward=113.43, Steps=311731, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4272, Total reward=94.51, Steps=311786, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4273, Total reward=49.92, Steps=311828, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4274, Total reward=61.86, Steps=311871, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4275, Total reward=72.11, Steps=311930, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4276, Total reward=324.0, Steps=312218, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4277, Total reward=299.02, Steps=312515, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4278, Total reward=386.5, Steps=312845, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4279, Total reward=277.96, Steps=313078, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4280, Total reward=10.82, Steps=313105, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4281, Total reward=171.24, Steps=313300, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4282, Total reward=43.01, Steps=313359, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4283, Total reward=195.13, Steps=313526, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4284, Total reward=158.0, Steps=313715, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4285, Total reward=129.72, Steps=313864, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4286, Total reward=199.47, Steps=314003, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4287, Total reward=140.09, Steps=314118, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4288, Total reward=138.61, Steps=314216, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4289, Total reward=22.31, Steps=314246, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4290, Total reward=3.81, Steps=314260, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4291, Total reward=49.8, Steps=314316, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4292, Total reward=43.84, Steps=314357, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4293, Total reward=23.91, Steps=314368, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4294, Total reward=56.52, Steps=314410, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4295, Total reward=420.8, Steps=314715, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4296, Total reward=335.4, Steps=315004, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4297, Total reward=67.72, Steps=315070, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4298, Total reward=153.96, Steps=315217, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4299, Total reward=280.13, Steps=315458, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4300, Total reward=219.36, Steps=315664, Training iteration=85
Policy training> Surrogate loss=0.007621958386152983, KL divergence=0.012386750429868698, Entropy=0.4119141399860382, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03458736836910248, KL divergence=0.0794191062450409, Entropy=0.3882615268230438, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.045621108263731, KL divergence=0.0951426774263382, Entropy=0.38673004508018494, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05194012075662613, KL divergence=0.09804810583591461, Entropy=0.38723650574684143, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05331225320696831, KL divergence=0.10429929941892624, Entropy=0.38862720131874084, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05736389011144638, KL divergence=0.10611774772405624, Entropy=0.3889938294887543, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05516516789793968, KL divergence=0.1080217957496643, Entropy=0.39104893803596497, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.060035429894924164, KL divergence=0.1087067723274231, Entropy=0.39551854133605957, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.061104126274585724, KL divergence=0.11277522891759872, Entropy=0.39440813660621643, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06074367091059685, KL divergence=0.1154438778758049, Entropy=0.396579772233963, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/231_Step-315664.ckpt']
Uploaded 3 files for checkpoint 231 in 0.55 seconds
saved intermediate frozen graph: current/model/model_231.pb
Best checkpoint number: 212, Last checkpoint number: 229
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'228'}
Training> Name=main_level/agent, Worker=0, Episode=4301, Total reward=282.49, Steps=315869, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4302, Total reward=251.46, Steps=316065, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4303, Total reward=24.69, Steps=316129, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4304, Total reward=133.05, Steps=316219, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4305, Total reward=10.03, Steps=316239, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4306, Total reward=220.23, Steps=316371, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4307, Total reward=132.05, Steps=316485, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4308, Total reward=167.25, Steps=316575, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4309, Total reward=128.82, Steps=316690, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4310, Total reward=140.35, Steps=316762, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4311, Total reward=66.28, Steps=316814, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4312, Total reward=116.01, Steps=316902, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4313, Total reward=69.69, Steps=316943, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4314, Total reward=225.58, Steps=317084, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4315, Total reward=68.05, Steps=317168, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4316, Total reward=162.61, Steps=317311, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4317, Total reward=373.75, Steps=317597, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4318, Total reward=271.09, Steps=317836, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4319, Total reward=243.13, Steps=318037, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4320, Total reward=114.69, Steps=318153, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4321, Total reward=149.28, Steps=318297, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4322, Total reward=261.69, Steps=318495, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4323, Total reward=232.46, Steps=318665, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4324, Total reward=15.95, Steps=318709, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4325, Total reward=18.9, Steps=318731, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4326, Total reward=243.98, Steps=318869, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4327, Total reward=102.18, Steps=318938, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4328, Total reward=133.41, Steps=319030, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4329, Total reward=0.0, Steps=319031, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4330, Total reward=35.75, Steps=319069, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4331, Total reward=63.97, Steps=319120, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4332, Total reward=10.64, Steps=319132, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4333, Total reward=44.6, Steps=319157, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4334, Total reward=113.78, Steps=319252, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4335, Total reward=142.85, Steps=319344, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4336, Total reward=216.77, Steps=319512, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4337, Total reward=360.76, Steps=319772, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4338, Total reward=194.04, Steps=319991, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4339, Total reward=96.89, Steps=320076, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4340, Total reward=213.35, Steps=320259, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4341, Total reward=72.4, Steps=320346, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4342, Total reward=194.38, Steps=320524, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4343, Total reward=14.64, Steps=320571, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4344, Total reward=9.76, Steps=320596, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4345, Total reward=186.93, Steps=320733, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4346, Total reward=83.42, Steps=320806, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4347, Total reward=61.35, Steps=320886, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4348, Total reward=164.81, Steps=320989, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4349, Total reward=130.36, Steps=321082, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4350, Total reward=51.49, Steps=321146, Training iteration=86
Policy training> Surrogate loss=0.0007735404069535434, KL divergence=0.008871238678693771, Entropy=0.4261009097099304, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03842572122812271, KL divergence=0.060651518404483795, Entropy=0.41060152649879456, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04810239002108574, KL divergence=0.07697249948978424, Entropy=0.4046531319618225, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05600236728787422, KL divergence=0.08845022320747375, Entropy=0.40379706025123596, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05913601070642471, KL divergence=0.09088142961263657, Entropy=0.4016294479370117, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06221922114491463, KL divergence=0.0937027782201767, Entropy=0.40248826146125793, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.062029384076595306, KL divergence=0.09710405766963959, Entropy=0.4022495746612549, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06272488832473755, KL divergence=0.09959972649812698, Entropy=0.404654324054718, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0635250136256218, KL divergence=0.10178563743829727, Entropy=0.40261197090148926, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06569605320692062, KL divergence=0.10496347397565842, Entropy=0.40513625741004944, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/232_Step-321146.ckpt']
Uploaded 3 files for checkpoint 232 in 0.47 seconds
saved intermediate frozen graph: current/model/model_232.pb
Best checkpoint number: 212, Last checkpoint number: 230
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'229'}
Training> Name=main_level/agent, Worker=0, Episode=4351, Total reward=83.94, Steps=321201, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4352, Total reward=97.43, Steps=321253, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4353, Total reward=55.8, Steps=321292, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4354, Total reward=58.76, Steps=321333, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4355, Total reward=181.47, Steps=321476, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4356, Total reward=276.8, Steps=321710, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4357, Total reward=166.05, Steps=321845, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4358, Total reward=277.85, Steps=322093, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4359, Total reward=109.16, Steps=322227, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4360, Total reward=226.76, Steps=322454, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4361, Total reward=179.92, Steps=322629, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4362, Total reward=9.44, Steps=322647, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4363, Total reward=104.51, Steps=322766, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4364, Total reward=198.31, Steps=322902, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4365, Total reward=140.06, Steps=322990, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4366, Total reward=140.77, Steps=323096, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4367, Total reward=18.01, Steps=323125, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4368, Total reward=81.13, Steps=323160, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4369, Total reward=113.78, Steps=323238, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4370, Total reward=113.4, Steps=323313, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4371, Total reward=23.99, Steps=323337, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4372, Total reward=95.41, Steps=323386, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4373, Total reward=83.24, Steps=323428, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4374, Total reward=331.33, Steps=323725, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4375, Total reward=291.4, Steps=323996, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4376, Total reward=18.91, Steps=324017, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4377, Total reward=190.98, Steps=324172, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4378, Total reward=338.14, Steps=324440, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4379, Total reward=49.69, Steps=324509, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4380, Total reward=171.16, Steps=324672, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4381, Total reward=196.73, Steps=324885, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4382, Total reward=182.32, Steps=325088, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4383, Total reward=6.8, Steps=325105, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4384, Total reward=7.59, Steps=325139, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4385, Total reward=54.7, Steps=325212, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4386, Total reward=216.4, Steps=325354, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4387, Total reward=104.66, Steps=325404, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4388, Total reward=145.54, Steps=325498, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4389, Total reward=76.66, Steps=325589, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4390, Total reward=55.32, Steps=325663, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4391, Total reward=90.84, Steps=325726, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4392, Total reward=94.33, Steps=325781, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4393, Total reward=58.16, Steps=325823, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4394, Total reward=72.98, Steps=325863, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4395, Total reward=204.67, Steps=326046, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4396, Total reward=216.91, Steps=326218, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4397, Total reward=145.05, Steps=326327, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4398, Total reward=148.0, Steps=326416, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4399, Total reward=83.76, Steps=326497, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4400, Total reward=100.44, Steps=326592, Training iteration=87
Policy training> Surrogate loss=0.003059223759919405, KL divergence=0.0067573911510407925, Entropy=0.42653337121009827, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03884550556540489, KL divergence=0.07480565458536148, Entropy=0.41142264008522034, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05060241371393204, KL divergence=0.0888570100069046, Entropy=0.40492403507232666, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.057165853679180145, KL divergence=0.09720074385404587, Entropy=0.40125715732574463, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05935279652476311, KL divergence=0.10291090607643127, Entropy=0.40087515115737915, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06093085929751396, KL divergence=0.10517289489507675, Entropy=0.39973321557044983, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06333908438682556, KL divergence=0.10698632150888443, Entropy=0.3993056118488312, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06652010977268219, KL divergence=0.10876477509737015, Entropy=0.3999585211277008, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0672035813331604, KL divergence=0.11089376360177994, Entropy=0.4007677733898163, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06561600416898727, KL divergence=0.11235393583774567, Entropy=0.4012034237384796, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/233_Step-326592.ckpt']
Uploaded 3 files for checkpoint 233 in 0.56 seconds
saved intermediate frozen graph: current/model/model_233.pb
Best checkpoint number: 212, Last checkpoint number: 231
Copying the frozen checkpoint from ./frozen_models/agent/model_212.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'230'}
Training> Name=main_level/agent, Worker=0, Episode=4401, Total reward=61.02, Steps=326698, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4402, Total reward=264.03, Steps=326898, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4403, Total reward=116.63, Steps=327076, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4404, Total reward=167.41, Steps=327226, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4405, Total reward=13.35, Steps=327256, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4406, Total reward=190.27, Steps=327374, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4407, Total reward=62.84, Steps=327440, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4408, Total reward=359.08, Steps=327698, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4409, Total reward=43.29, Steps=327745, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4410, Total reward=280.92, Steps=327981, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4411, Total reward=105.4, Steps=328044, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4412, Total reward=78.2, Steps=328104, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4413, Total reward=68.61, Steps=328164, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4414, Total reward=292.93, Steps=328403, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4415, Total reward=268.02, Steps=328653, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4416, Total reward=311.45, Steps=328927, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4417, Total reward=140.24, Steps=329069, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4418, Total reward=76.11, Steps=329107, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4419, Total reward=125.27, Steps=329210, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4420, Total reward=114.51, Steps=329301, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4421, Total reward=92.32, Steps=329374, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4422, Total reward=36.13, Steps=329413, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4423, Total reward=103.7, Steps=329530, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4424, Total reward=123.46, Steps=329685, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4425, Total reward=138.31, Steps=329783, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4426, Total reward=195.02, Steps=329930, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4427, Total reward=211.23, Steps=330088, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4428, Total reward=118.58, Steps=330142, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4429, Total reward=22.9, Steps=330163, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4430, Total reward=60.88, Steps=330226, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4431, Total reward=107.77, Steps=330319, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4432, Total reward=262.7, Steps=330574, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4433, Total reward=95.95, Steps=330638, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4434, Total reward=30.63, Steps=330677, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4435, Total reward=157.87, Steps=330852, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4436, Total reward=27.91, Steps=330888, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4437, Total reward=165.07, Steps=331024, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4438, Total reward=64.12, Steps=331080, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4439, Total reward=216.81, Steps=331284, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4440, Total reward=114.64, Steps=331387, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4441, Total reward=81.22, Steps=331442, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4442, Total reward=266.28, Steps=331637, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4443, Total reward=108.12, Steps=331767, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4444, Total reward=214.66, Steps=331930, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4445, Total reward=10.89, Steps=331968, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4446, Total reward=216.68, Steps=332108, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4447, Total reward=156.98, Steps=332219, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4448, Total reward=208.03, Steps=332319, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4449, Total reward=27.64, Steps=332340, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4450, Total reward=108.62, Steps=332417, Training iteration=88
Policy training> Surrogate loss=0.0082149812951684, KL divergence=0.008754326961934566, Entropy=0.4207198917865753, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.035622287541627884, KL divergence=0.07518616318702698, Entropy=0.39392736554145813, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05516009032726288, KL divergence=0.0910341814160347, Entropy=0.39037325978279114, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06040908396244049, KL divergence=0.09932105243206024, Entropy=0.39258089661598206, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05849292129278183, KL divergence=0.10357404500246048, Entropy=0.39031124114990234, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.059087123721838, KL divergence=0.10776839405298233, Entropy=0.39286690950393677, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06414609402418137, KL divergence=0.107453353703022, Entropy=0.39373522996902466, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06673231720924377, KL divergence=0.1111011952161789, Entropy=0.394875168800354, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06976663321256638, KL divergence=0.1133633553981781, Entropy=0.3932704031467438, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06972663849592209, KL divergence=0.11580240726470947, Entropy=0.39602944254875183, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/234_Step-332417.ckpt']
Uploaded 3 files for checkpoint 234 in 0.49 seconds
saved intermediate frozen graph: current/model/model_234.pb
Best checkpoint number: 232, Last checkpoint number: 232
Copying the frozen checkpoint from ./frozen_models/agent/model_232.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'231'}
Training> Name=main_level/agent, Worker=0, Episode=4451, Total reward=398.13, Steps=332745, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4452, Total reward=45.73, Steps=332783, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4453, Total reward=56.32, Steps=332821, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4454, Total reward=61.97, Steps=332868, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4455, Total reward=232.51, Steps=333055, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4456, Total reward=185.17, Steps=333307, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4457, Total reward=157.83, Steps=333433, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4458, Total reward=317.65, Steps=333702, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4459, Total reward=297.91, Steps=333939, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4460, Total reward=96.61, Steps=334000, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4461, Total reward=99.62, Steps=334095, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4462, Total reward=181.05, Steps=334299, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4463, Total reward=3.3, Steps=334326, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4464, Total reward=169.13, Steps=334461, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4465, Total reward=185.23, Steps=334611, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4466, Total reward=213.66, Steps=334747, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4467, Total reward=174.61, Steps=334875, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4468, Total reward=167.81, Steps=334984, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4469, Total reward=88.36, Steps=335070, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4470, Total reward=51.46, Steps=335119, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4471, Total reward=114.79, Steps=335188, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4472, Total reward=94.38, Steps=335255, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4473, Total reward=70.29, Steps=335297, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4474, Total reward=55.87, Steps=335339, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4475, Total reward=196.67, Steps=335515, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4476, Total reward=207.59, Steps=335694, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4477, Total reward=250.93, Steps=335921, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4478, Total reward=231.5, Steps=336134, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4479, Total reward=65.61, Steps=336198, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4480, Total reward=69.28, Steps=336264, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4481, Total reward=179.49, Steps=336422, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4482, Total reward=78.24, Steps=336493, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4483, Total reward=12.58, Steps=336549, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4484, Total reward=148.4, Steps=336695, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4485, Total reward=182.0, Steps=336861, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4486, Total reward=134.04, Steps=336993, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4487, Total reward=27.15, Steps=337023, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4488, Total reward=152.95, Steps=337118, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4489, Total reward=35.23, Steps=337138, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4490, Total reward=115.85, Steps=337210, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4491, Total reward=353.63, Steps=337531, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4492, Total reward=93.35, Steps=337580, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4493, Total reward=31.98, Steps=337606, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4494, Total reward=433.4, Steps=337914, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4495, Total reward=286.92, Steps=338149, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4496, Total reward=333.39, Steps=338393, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4497, Total reward=373.65, Steps=338666, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4498, Total reward=128.57, Steps=338810, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4499, Total reward=290.27, Steps=339058, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4500, Total reward=90.75, Steps=339115, Training iteration=89
Policy training> Surrogate loss=0.0062065026722848415, KL divergence=0.009824003092944622, Entropy=0.4238148033618927, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.039126262068748474, KL divergence=0.07050676643848419, Entropy=0.402567982673645, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05150936543941498, KL divergence=0.0881115272641182, Entropy=0.4000392556190491, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05625845864415169, KL divergence=0.09458768367767334, Entropy=0.4035365879535675, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05863780155777931, KL divergence=0.09824507683515549, Entropy=0.404694139957428, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06084034964442253, KL divergence=0.10210904479026794, Entropy=0.40571561455726624, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.061578650027513504, KL divergence=0.10451087355613708, Entropy=0.40606626868247986, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06341499835252762, KL divergence=0.10575433075428009, Entropy=0.4062587320804596, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06383757293224335, KL divergence=0.10732327401638031, Entropy=0.40789249539375305, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06620167940855026, KL divergence=0.11010892689228058, Entropy=0.40930211544036865, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/235_Step-339115.ckpt']
Uploaded 3 files for checkpoint 235 in 0.56 seconds
saved intermediate frozen graph: current/model/model_235.pb
Best checkpoint number: 232, Last checkpoint number: 233
Copying the frozen checkpoint from ./frozen_models/agent/model_232.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'212'}
Training> Name=main_level/agent, Worker=0, Episode=4501, Total reward=275.87, Steps=339343, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4502, Total reward=21.93, Steps=339362, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4503, Total reward=222.17, Steps=339572, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4504, Total reward=109.03, Steps=339689, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4505, Total reward=206.93, Steps=339841, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4506, Total reward=194.51, Steps=339986, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4507, Total reward=121.69, Steps=340049, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4508, Total reward=116.89, Steps=340135, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4509, Total reward=108.32, Steps=340230, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4510, Total reward=68.19, Steps=340287, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4511, Total reward=115.73, Steps=340357, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4512, Total reward=276.91, Steps=340556, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4513, Total reward=62.1, Steps=340596, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4514, Total reward=32.89, Steps=340608, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4515, Total reward=2.82, Steps=340622, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4516, Total reward=359.6, Steps=340932, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4517, Total reward=200.87, Steps=341069, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4518, Total reward=229.78, Steps=341264, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4519, Total reward=290.41, Steps=341504, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4520, Total reward=313.41, Steps=341750, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4521, Total reward=178.87, Steps=341886, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4522, Total reward=161.36, Steps=342036, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4523, Total reward=12.29, Steps=342052, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4524, Total reward=24.22, Steps=342093, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4525, Total reward=194.59, Steps=342237, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4526, Total reward=76.86, Steps=342292, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4527, Total reward=223.38, Steps=342409, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4528, Total reward=228.25, Steps=342513, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4529, Total reward=26.67, Steps=342555, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4530, Total reward=295.02, Steps=342777, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4531, Total reward=111.37, Steps=342846, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4532, Total reward=106.32, Steps=342927, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4533, Total reward=54.56, Steps=342963, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4534, Total reward=67.19, Steps=343006, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4535, Total reward=391.67, Steps=343326, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4536, Total reward=24.13, Steps=343355, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4537, Total reward=134.28, Steps=343458, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4538, Total reward=151.91, Steps=343555, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4539, Total reward=300.84, Steps=343793, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4540, Total reward=37.67, Steps=343840, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4541, Total reward=67.03, Steps=343904, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4542, Total reward=151.21, Steps=344089, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4543, Total reward=108.6, Steps=344208, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4544, Total reward=161.93, Steps=344336, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4545, Total reward=197.31, Steps=344479, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4546, Total reward=105.86, Steps=344566, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4547, Total reward=117.33, Steps=344649, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4548, Total reward=173.63, Steps=344747, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4549, Total reward=37.86, Steps=344805, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4550, Total reward=403.44, Steps=345110, Training iteration=90
Policy training> Surrogate loss=0.004142881836742163, KL divergence=0.008146736770868301, Entropy=0.4212806820869446, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.043017130345106125, KL divergence=0.06910181045532227, Entropy=0.40407952666282654, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05199813470244408, KL divergence=0.08684725314378738, Entropy=0.40033677220344543, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05503988638520241, KL divergence=0.0953882560133934, Entropy=0.4033917486667633, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05708162114024162, KL divergence=0.10373133420944214, Entropy=0.40374457836151123, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06506672501564026, KL divergence=0.10458924621343613, Entropy=0.403613805770874, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06454546749591827, KL divergence=0.10807617008686066, Entropy=0.4054160714149475, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06324142962694168, KL divergence=0.10824164003133774, Entropy=0.40469807386398315, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06252514570951462, KL divergence=0.11310182511806488, Entropy=0.4063311517238617, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06637837737798691, KL divergence=0.11536312103271484, Entropy=0.40655404329299927, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/236_Step-345110.ckpt']
Uploaded 3 files for checkpoint 236 in 0.50 seconds
saved intermediate frozen graph: current/model/model_236.pb
Best checkpoint number: 234, Last checkpoint number: 234
Copying the frozen checkpoint from ./frozen_models/agent/model_234.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'232'}
Training> Name=main_level/agent, Worker=0, Episode=4551, Total reward=69.76, Steps=345163, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4552, Total reward=29.46, Steps=345201, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4553, Total reward=95.93, Steps=345271, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4554, Total reward=420.15, Steps=345576, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4555, Total reward=329.03, Steps=345825, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4556, Total reward=95.56, Steps=345898, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4557, Total reward=276.8, Steps=346170, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4558, Total reward=110.09, Steps=346264, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4559, Total reward=211.94, Steps=346507, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4560, Total reward=353.25, Steps=346733, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4561, Total reward=150.6, Steps=346896, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4562, Total reward=34.75, Steps=346937, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4563, Total reward=104.81, Steps=347068, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4564, Total reward=152.99, Steps=347242, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4565, Total reward=213.08, Steps=347389, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4566, Total reward=235.68, Steps=347528, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4567, Total reward=174.82, Steps=347631, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4568, Total reward=92.94, Steps=347688, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4569, Total reward=120.81, Steps=347786, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4570, Total reward=104.92, Steps=347870, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4571, Total reward=114.78, Steps=347939, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4572, Total reward=29.64, Steps=347962, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4573, Total reward=29.12, Steps=347987, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4574, Total reward=194.55, Steps=348174, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4575, Total reward=171.62, Steps=348315, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4576, Total reward=350.03, Steps=348633, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4577, Total reward=196.93, Steps=348760, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4578, Total reward=140.02, Steps=348877, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4579, Total reward=15.35, Steps=348911, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4580, Total reward=82.96, Steps=349010, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4581, Total reward=223.39, Steps=349151, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4582, Total reward=112.62, Steps=349312, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4583, Total reward=133.22, Steps=349454, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4584, Total reward=4.69, Steps=349478, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4585, Total reward=129.11, Steps=349634, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4586, Total reward=191.05, Steps=349773, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4587, Total reward=145.91, Steps=349878, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4588, Total reward=186.02, Steps=349969, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4589, Total reward=156.48, Steps=350068, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4590, Total reward=75.75, Steps=350130, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4591, Total reward=92.49, Steps=350201, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4592, Total reward=118.2, Steps=350269, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4593, Total reward=412.96, Steps=350587, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4594, Total reward=48.74, Steps=350635, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4595, Total reward=391.67, Steps=350968, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4596, Total reward=25.34, Steps=350990, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4597, Total reward=52.46, Steps=351031, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4598, Total reward=135.32, Steps=351131, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4599, Total reward=47.21, Steps=351191, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4600, Total reward=0.0, Steps=351192, Training iteration=91
Policy training> Surrogate loss=0.005801450926810503, KL divergence=0.009482644498348236, Entropy=0.4058939516544342, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.034558605402708054, KL divergence=0.06305550783872604, Entropy=0.38482293486595154, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.047725800424814224, KL divergence=0.08308964222669601, Entropy=0.38105109333992004, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05089806020259857, KL divergence=0.09070441126823425, Entropy=0.3796831965446472, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05783385783433914, KL divergence=0.09425687789916992, Entropy=0.38119474053382874, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05952955409884453, KL divergence=0.09701039642095566, Entropy=0.3831159472465515, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.056171298027038574, KL divergence=0.09819949418306351, Entropy=0.382096529006958, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.057647570967674255, KL divergence=0.10174234211444855, Entropy=0.38170570135116577, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05731711536645889, KL divergence=0.10340569168329239, Entropy=0.3845177888870239, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06270293891429901, KL divergence=0.10520432144403458, Entropy=0.3849729597568512, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/237_Step-351192.ckpt']
Uploaded 3 files for checkpoint 237 in 0.54 seconds
saved intermediate frozen graph: current/model/model_237.pb
Best checkpoint number: 234, Last checkpoint number: 235
Copying the frozen checkpoint from ./frozen_models/agent/model_234.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'233'}
Training> Name=main_level/agent, Worker=0, Episode=4601, Total reward=289.89, Steps=351405, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4602, Total reward=41.1, Steps=351435, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4603, Total reward=169.55, Steps=351603, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4604, Total reward=148.84, Steps=351765, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4605, Total reward=373.83, Steps=352055, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4606, Total reward=127.66, Steps=352147, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4607, Total reward=243.9, Steps=352349, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4608, Total reward=187.42, Steps=352439, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4609, Total reward=47.96, Steps=352513, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4610, Total reward=11.37, Steps=352527, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4611, Total reward=95.23, Steps=352573, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4612, Total reward=265.1, Steps=352779, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4613, Total reward=58.34, Steps=352818, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4614, Total reward=215.89, Steps=352989, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4615, Total reward=331.28, Steps=353279, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4616, Total reward=48.51, Steps=353340, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4617, Total reward=192.96, Steps=353491, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4618, Total reward=370.48, Steps=353747, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4619, Total reward=15.12, Steps=353764, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4620, Total reward=215.9, Steps=353914, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4621, Total reward=251.8, Steps=354138, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4622, Total reward=22.63, Steps=354159, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4623, Total reward=159.85, Steps=354364, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4624, Total reward=121.15, Steps=354472, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4625, Total reward=210.5, Steps=354633, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4626, Total reward=146.58, Steps=354735, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4627, Total reward=200.86, Steps=354857, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4628, Total reward=88.6, Steps=354897, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4629, Total reward=28.84, Steps=354944, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4630, Total reward=53.37, Steps=354978, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4631, Total reward=104.08, Steps=355044, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4632, Total reward=84.86, Steps=355099, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4633, Total reward=85.2, Steps=355158, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4634, Total reward=215.06, Steps=355341, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4635, Total reward=204.2, Steps=355476, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4636, Total reward=412.53, Steps=355770, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4637, Total reward=83.08, Steps=355840, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4638, Total reward=365.18, Steps=356095, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4639, Total reward=201.03, Steps=356361, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4640, Total reward=265.18, Steps=356602, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4641, Total reward=281.47, Steps=356829, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4642, Total reward=124.4, Steps=356937, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4643, Total reward=10.57, Steps=356975, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4644, Total reward=177.3, Steps=357149, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4645, Total reward=164.54, Steps=357289, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4646, Total reward=110.89, Steps=357380, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4647, Total reward=94.83, Steps=357435, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4648, Total reward=135.28, Steps=357501, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4649, Total reward=91.64, Steps=357582, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4650, Total reward=134.35, Steps=357666, Training iteration=92
Policy training> Surrogate loss=0.0049231755547225475, KL divergence=0.006811941973865032, Entropy=0.4158165752887726, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.037849828600883484, KL divergence=0.06487331539392471, Entropy=0.3946221172809601, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04830532893538475, KL divergence=0.0863165557384491, Entropy=0.3917158544063568, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05059829354286194, KL divergence=0.09202326089143753, Entropy=0.39071694016456604, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.056357383728027344, KL divergence=0.09355463087558746, Entropy=0.39377865195274353, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06157270073890686, KL divergence=0.09651762992143631, Entropy=0.39328253269195557, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06002751737833023, KL divergence=0.09896398335695267, Entropy=0.39425280690193176, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06184782460331917, KL divergence=0.1007641851902008, Entropy=0.3952610492706299, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06359720975160599, KL divergence=0.10270930081605911, Entropy=0.39527085423469543, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06308824568986893, KL divergence=0.10477180778980255, Entropy=0.3957952558994293, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/238_Step-357666.ckpt']
Uploaded 3 files for checkpoint 238 in 0.57 seconds
saved intermediate frozen graph: current/model/model_238.pb
Best checkpoint number: 234, Last checkpoint number: 236
Copying the frozen checkpoint from ./frozen_models/agent/model_234.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'235'}
Training> Name=main_level/agent, Worker=0, Episode=4651, Total reward=76.85, Steps=357714, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4652, Total reward=87.41, Steps=357775, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4653, Total reward=59.65, Steps=357812, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4654, Total reward=218.1, Steps=357966, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4655, Total reward=413.16, Steps=358295, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4656, Total reward=312.63, Steps=358608, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4657, Total reward=398.81, Steps=358908, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4658, Total reward=257.29, Steps=359100, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4659, Total reward=221.63, Steps=359347, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4660, Total reward=57.52, Steps=359406, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4661, Total reward=93.29, Steps=359483, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4662, Total reward=51.22, Steps=359514, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4663, Total reward=80.71, Steps=359616, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4664, Total reward=138.02, Steps=359714, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4665, Total reward=95.36, Steps=359804, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4666, Total reward=470.06, Steps=360068, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4667, Total reward=150.01, Steps=360178, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4668, Total reward=108.3, Steps=360235, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4669, Total reward=76.92, Steps=360321, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4670, Total reward=62.59, Steps=360388, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4671, Total reward=113.82, Steps=360457, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4672, Total reward=88.44, Steps=360513, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4673, Total reward=55.67, Steps=360554, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4674, Total reward=107.05, Steps=360663, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4675, Total reward=22.66, Steps=360678, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4676, Total reward=82.06, Steps=360769, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4677, Total reward=323.65, Steps=361031, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4678, Total reward=0.0, Steps=361032, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4679, Total reward=282.23, Steps=361273, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4680, Total reward=311.17, Steps=361491, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4681, Total reward=38.26, Steps=361529, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4682, Total reward=48.95, Steps=361560, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4683, Total reward=169.66, Steps=361725, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4684, Total reward=3.02, Steps=361751, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4685, Total reward=107.89, Steps=361833, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4686, Total reward=187.81, Steps=361976, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4687, Total reward=168.46, Steps=362085, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4688, Total reward=186.91, Steps=362198, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4689, Total reward=152.98, Steps=362286, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4690, Total reward=11.78, Steps=362299, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4691, Total reward=81.96, Steps=362353, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4692, Total reward=89.72, Steps=362407, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4693, Total reward=69.82, Steps=362450, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4694, Total reward=66.11, Steps=362502, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4695, Total reward=15.28, Steps=362516, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4696, Total reward=396.29, Steps=362817, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4697, Total reward=343.46, Steps=363112, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4698, Total reward=270.91, Steps=363331, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4699, Total reward=105.97, Steps=363416, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4700, Total reward=275.27, Steps=363680, Training iteration=93
Policy training> Surrogate loss=0.004602247383445501, KL divergence=0.008941339328885078, Entropy=0.407496839761734, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03999723866581917, KL divergence=0.07153331488370895, Entropy=0.3872121274471283, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.047632522881031036, KL divergence=0.09053664654493332, Entropy=0.38346248865127563, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.055483754724264145, KL divergence=0.09378298372030258, Entropy=0.3804824650287628, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05655984207987785, KL divergence=0.09910788387060165, Entropy=0.3831593990325928, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.061103738844394684, KL divergence=0.1002853512763977, Entropy=0.38243064284324646, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.060900259763002396, KL divergence=0.10230576246976852, Entropy=0.38159844279289246, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06217743456363678, KL divergence=0.10343037545681, Entropy=0.38098832964897156, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06087563931941986, KL divergence=0.10513798147439957, Entropy=0.38246527314186096, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06391074508428574, KL divergence=0.10605438798666, Entropy=0.3845514953136444, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/239_Step-363680.ckpt']
Uploaded 3 files for checkpoint 239 in 0.51 seconds
saved intermediate frozen graph: current/model/model_239.pb
Best checkpoint number: 234, Last checkpoint number: 237
Copying the frozen checkpoint from ./frozen_models/agent/model_234.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'236'}
Training> Name=main_level/agent, Worker=0, Episode=4701, Total reward=268.08, Steps=363924, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4702, Total reward=167.71, Steps=364114, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4703, Total reward=115.57, Steps=364232, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4704, Total reward=24.49, Steps=364269, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4705, Total reward=206.55, Steps=364403, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4706, Total reward=98.71, Steps=364490, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4707, Total reward=83.43, Steps=364553, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4708, Total reward=172.2, Steps=364641, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4709, Total reward=32.28, Steps=364669, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4710, Total reward=88.13, Steps=364727, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4711, Total reward=134.64, Steps=364815, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4712, Total reward=35.09, Steps=364860, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4713, Total reward=55.61, Steps=364905, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4714, Total reward=51.33, Steps=364953, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4715, Total reward=386.45, Steps=365274, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4716, Total reward=77.46, Steps=365364, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4717, Total reward=234.24, Steps=365602, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4718, Total reward=324.45, Steps=365858, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4719, Total reward=92.8, Steps=365931, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4720, Total reward=218.97, Steps=366081, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4721, Total reward=298.58, Steps=366324, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4722, Total reward=27.67, Steps=366344, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4723, Total reward=28.77, Steps=366419, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4724, Total reward=171.72, Steps=366577, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4725, Total reward=222.51, Steps=366745, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4726, Total reward=224.14, Steps=366873, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4727, Total reward=117.08, Steps=366958, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4728, Total reward=137.11, Steps=367058, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4729, Total reward=59.83, Steps=367146, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4730, Total reward=19.26, Steps=367166, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4731, Total reward=41.73, Steps=367212, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4732, Total reward=273.3, Steps=367418, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4733, Total reward=82.69, Steps=367458, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4734, Total reward=233.58, Steps=367666, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4735, Total reward=191.17, Steps=367818, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4736, Total reward=399.28, Steps=368104, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4737, Total reward=232.13, Steps=368317, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4738, Total reward=333.61, Steps=368592, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4739, Total reward=86.7, Steps=368660, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4740, Total reward=87.3, Steps=368728, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4741, Total reward=75.85, Steps=368799, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4742, Total reward=223.51, Steps=368982, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4743, Total reward=10.97, Steps=369006, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4744, Total reward=182.13, Steps=369183, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4745, Total reward=117.08, Steps=369278, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4746, Total reward=147.74, Steps=369385, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4747, Total reward=222.48, Steps=369515, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4748, Total reward=326.27, Steps=369717, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4749, Total reward=55.17, Steps=369787, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4750, Total reward=72.06, Steps=369866, Training iteration=94
Policy training> Surrogate loss=0.0086739556863904, KL divergence=0.01465222705155611, Entropy=0.41774916648864746, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03454669192433357, KL divergence=0.08138412237167358, Entropy=0.40045297145843506, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.047314587980508804, KL divergence=0.10379578918218613, Entropy=0.3969210088253021, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05446278676390648, KL divergence=0.10747764259576797, Entropy=0.3978852927684784, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05801763758063316, KL divergence=0.11128175258636475, Entropy=0.3993013799190521, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06124855577945709, KL divergence=0.1143326386809349, Entropy=0.3996971547603607, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06335195153951645, KL divergence=0.11743050813674927, Entropy=0.4006112515926361, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06388215720653534, KL divergence=0.11781946569681168, Entropy=0.40165841579437256, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06630527973175049, KL divergence=0.11959121376276016, Entropy=0.40190446376800537, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06572913378477097, KL divergence=0.12229243665933609, Entropy=0.40276864171028137, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/240_Step-369866.ckpt']
Uploaded 3 files for checkpoint 240 in 0.60 seconds
saved intermediate frozen graph: current/model/model_240.pb
Best checkpoint number: 234, Last checkpoint number: 238
Copying the frozen checkpoint from ./frozen_models/agent/model_234.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'237'}
Training> Name=main_level/agent, Worker=0, Episode=4751, Total reward=83.16, Steps=369940, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4752, Total reward=166.57, Steps=370046, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4753, Total reward=49.45, Steps=370090, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4754, Total reward=32.3, Steps=370103, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4755, Total reward=138.89, Steps=370242, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4756, Total reward=317.96, Steps=370535, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4757, Total reward=345.73, Steps=370846, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4758, Total reward=291.91, Steps=371087, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4759, Total reward=97.29, Steps=371161, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4760, Total reward=103.55, Steps=371217, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4761, Total reward=186.96, Steps=371362, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4762, Total reward=354.47, Steps=371643, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4763, Total reward=18.46, Steps=371693, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4764, Total reward=107.64, Steps=371841, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4765, Total reward=70.97, Steps=371921, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4766, Total reward=173.21, Steps=372043, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4767, Total reward=119.12, Steps=372159, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4768, Total reward=94.48, Steps=372198, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4769, Total reward=141.66, Steps=372291, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4770, Total reward=28.5, Steps=372319, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4771, Total reward=41.57, Steps=372372, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4772, Total reward=66.0, Steps=372410, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4773, Total reward=65.48, Steps=372451, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4774, Total reward=74.73, Steps=372506, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4775, Total reward=126.38, Steps=372603, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4776, Total reward=316.19, Steps=372812, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4777, Total reward=276.75, Steps=373019, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4778, Total reward=246.1, Steps=373224, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4779, Total reward=100.88, Steps=373380, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4780, Total reward=274.52, Steps=373581, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4781, Total reward=71.72, Steps=373626, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4782, Total reward=166.23, Steps=373819, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4783, Total reward=162.96, Steps=373978, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4784, Total reward=371.83, Steps=374298, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4785, Total reward=277.63, Steps=374476, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4786, Total reward=196.66, Steps=374624, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4787, Total reward=130.6, Steps=374715, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4788, Total reward=160.82, Steps=374801, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4789, Total reward=131.82, Steps=374905, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4790, Total reward=113.11, Steps=374981, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4791, Total reward=154.55, Steps=375075, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4792, Total reward=66.65, Steps=375111, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4793, Total reward=89.83, Steps=375171, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4794, Total reward=111.57, Steps=375274, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4795, Total reward=58.46, Steps=375322, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4796, Total reward=392.88, Steps=375624, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4797, Total reward=173.46, Steps=375764, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4798, Total reward=355.48, Steps=376020, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4799, Total reward=58.81, Steps=376083, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4800, Total reward=319.2, Steps=376297, Training iteration=95
Policy training> Surrogate loss=0.007001712918281555, KL divergence=0.007656675763428211, Entropy=0.402048796415329, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03683414310216904, KL divergence=0.07506830990314484, Entropy=0.382832795381546, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04955997318029404, KL divergence=0.09419707208871841, Entropy=0.38275402784347534, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05526988208293915, KL divergence=0.09819381684064865, Entropy=0.3804483115673065, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05875011906027794, KL divergence=0.10375701636075974, Entropy=0.38059142231941223, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05996149033308029, KL divergence=0.10714258998632431, Entropy=0.3800808787345886, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.062057528644800186, KL divergence=0.11064648628234863, Entropy=0.3807032108306885, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06389986723661423, KL divergence=0.11219435930252075, Entropy=0.38138023018836975, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06454199552536011, KL divergence=0.11398393660783768, Entropy=0.3818070888519287, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0655728280544281, KL divergence=0.1152837872505188, Entropy=0.3828572928905487, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/241_Step-376297.ckpt']
Uploaded 3 files for checkpoint 241 in 0.71 seconds
saved intermediate frozen graph: current/model/model_241.pb
Best checkpoint number: 234, Last checkpoint number: 239
Copying the frozen checkpoint from ./frozen_models/agent/model_234.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'238'}
Training> Name=main_level/agent, Worker=0, Episode=4801, Total reward=71.02, Steps=376341, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4802, Total reward=47.77, Steps=376387, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4803, Total reward=13.43, Steps=376441, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4804, Total reward=0.03, Steps=376467, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4805, Total reward=197.21, Steps=376604, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4806, Total reward=428.57, Steps=376918, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4807, Total reward=192.77, Steps=377051, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4808, Total reward=84.91, Steps=377090, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4809, Total reward=145.86, Steps=377175, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4810, Total reward=56.39, Steps=377233, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4811, Total reward=40.99, Steps=377286, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4812, Total reward=73.73, Steps=377338, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4813, Total reward=49.75, Steps=377376, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4814, Total reward=222.62, Steps=377521, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4815, Total reward=160.58, Steps=377671, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4816, Total reward=178.14, Steps=377852, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4817, Total reward=121.85, Steps=377956, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4818, Total reward=232.65, Steps=378187, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4819, Total reward=108.79, Steps=378304, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4820, Total reward=318.16, Steps=378535, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4821, Total reward=166.8, Steps=378685, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4822, Total reward=47.63, Steps=378723, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4823, Total reward=219.07, Steps=378940, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4824, Total reward=17.67, Steps=378981, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4825, Total reward=230.96, Steps=379135, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4826, Total reward=131.51, Steps=379234, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4827, Total reward=148.36, Steps=379341, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4828, Total reward=188.14, Steps=379461, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4829, Total reward=131.65, Steps=379541, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4830, Total reward=28.51, Steps=379607, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4831, Total reward=120.25, Steps=379679, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4832, Total reward=84.85, Steps=379739, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4833, Total reward=51.36, Steps=379779, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4834, Total reward=60.07, Steps=379820, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4835, Total reward=95.5, Steps=379911, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4836, Total reward=165.49, Steps=380093, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4837, Total reward=80.06, Steps=380139, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4838, Total reward=326.12, Steps=380366, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4839, Total reward=227.18, Steps=380596, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4840, Total reward=323.55, Steps=380821, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4841, Total reward=292.33, Steps=381032, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4842, Total reward=62.48, Steps=381096, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4843, Total reward=173.31, Steps=381267, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4844, Total reward=213.57, Steps=381408, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4845, Total reward=214.14, Steps=381568, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4846, Total reward=45.56, Steps=381591, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4847, Total reward=172.33, Steps=381716, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4848, Total reward=217.81, Steps=381832, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4849, Total reward=110.94, Steps=381943, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4850, Total reward=87.05, Steps=382006, Training iteration=96
Policy training> Surrogate loss=0.005090147722512484, KL divergence=0.01033325120806694, Entropy=0.4076091945171356, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03953327611088753, KL divergence=0.07549643516540527, Entropy=0.37980711460113525, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05329242721199989, KL divergence=0.09651611000299454, Entropy=0.37368181347846985, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05717555060982704, KL divergence=0.1055799201130867, Entropy=0.37066105008125305, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06252607703208923, KL divergence=0.10768943279981613, Entropy=0.3705386221408844, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06318251043558121, KL divergence=0.11096623539924622, Entropy=0.36961767077445984, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06725262105464935, KL divergence=0.11376407742500305, Entropy=0.368801474571228, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06592042744159698, KL divergence=0.11462762951850891, Entropy=0.36767569184303284, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06800846755504608, KL divergence=0.11708772927522659, Entropy=0.368871808052063, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0683252215385437, KL divergence=0.11993894726037979, Entropy=0.36873188614845276, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/242_Step-382006.ckpt']
Uploaded 3 files for checkpoint 242 in 0.58 seconds
saved intermediate frozen graph: current/model/model_242.pb
Best checkpoint number: 234, Last checkpoint number: 240
Copying the frozen checkpoint from ./frozen_models/agent/model_234.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'239'}
Training> Name=main_level/agent, Worker=0, Episode=4851, Total reward=395.39, Steps=382312, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4852, Total reward=114.21, Steps=382366, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4853, Total reward=44.99, Steps=382391, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4854, Total reward=130.14, Steps=382471, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4855, Total reward=198.26, Steps=382626, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4856, Total reward=142.18, Steps=382744, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4857, Total reward=160.53, Steps=382882, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4858, Total reward=346.35, Steps=383152, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4859, Total reward=278.55, Steps=383386, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4860, Total reward=107.79, Steps=383472, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4861, Total reward=252.32, Steps=383659, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4862, Total reward=44.48, Steps=383738, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4863, Total reward=128.11, Steps=383873, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4864, Total reward=141.02, Steps=384017, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4865, Total reward=144.65, Steps=384148, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4866, Total reward=211.4, Steps=384285, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4867, Total reward=171.14, Steps=384399, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4868, Total reward=176.84, Steps=384512, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4869, Total reward=36.36, Steps=384554, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4870, Total reward=89.69, Steps=384612, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4871, Total reward=80.56, Steps=384685, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4872, Total reward=93.29, Steps=384736, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4873, Total reward=0.02, Steps=384753, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4874, Total reward=221.84, Steps=384928, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4875, Total reward=201.67, Steps=385099, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4876, Total reward=347.72, Steps=385405, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4877, Total reward=150.21, Steps=385519, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4878, Total reward=152.27, Steps=385611, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4879, Total reward=232.99, Steps=385772, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4880, Total reward=70.21, Steps=385827, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4881, Total reward=67.88, Steps=385862, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4882, Total reward=287.51, Steps=386067, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4883, Total reward=215.05, Steps=386251, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4884, Total reward=143.74, Steps=386372, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4885, Total reward=138.32, Steps=386521, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4886, Total reward=431.63, Steps=386840, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4887, Total reward=94.89, Steps=386896, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4888, Total reward=213.46, Steps=387005, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4889, Total reward=8.54, Steps=387016, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4890, Total reward=121.69, Steps=387093, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4891, Total reward=71.38, Steps=387151, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4892, Total reward=84.47, Steps=387207, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4893, Total reward=41.05, Steps=387250, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4894, Total reward=66.51, Steps=387291, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4895, Total reward=110.05, Steps=387374, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4896, Total reward=280.51, Steps=387640, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4897, Total reward=87.35, Steps=387693, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4898, Total reward=328.1, Steps=387962, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4899, Total reward=106.17, Steps=388034, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4900, Total reward=39.56, Steps=388071, Training iteration=97
Policy training> Surrogate loss=-0.0006764770951122046, KL divergence=0.01004383061081171, Entropy=0.39507344365119934, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03729360178112984, KL divergence=0.06977253407239914, Entropy=0.37550437450408936, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04493498429656029, KL divergence=0.08432305604219437, Entropy=0.3691047132015228, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.053071554750204086, KL divergence=0.09568171203136444, Entropy=0.3725108206272125, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.058088093996047974, KL divergence=0.09944456815719604, Entropy=0.37174052000045776, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.056405942887067795, KL divergence=0.10160920768976212, Entropy=0.36967021226882935, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.059568025171756744, KL divergence=0.10477842390537262, Entropy=0.3678215742111206, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06333036720752716, KL divergence=0.10617833584547043, Entropy=0.3665861487388611, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.060506001114845276, KL divergence=0.10833589732646942, Entropy=0.3688991367816925, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06225001811981201, KL divergence=0.11037103086709976, Entropy=0.3691410720348358, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/243_Step-388071.ckpt']
Uploaded 3 files for checkpoint 243 in 0.56 seconds
saved intermediate frozen graph: current/model/model_243.pb
Best checkpoint number: 234, Last checkpoint number: 241
Copying the frozen checkpoint from ./frozen_models/agent/model_234.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'240'}
Training> Name=main_level/agent, Worker=0, Episode=4901, Total reward=237.51, Steps=388282, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4902, Total reward=167.67, Steps=388431, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4903, Total reward=177.7, Steps=388606, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4904, Total reward=76.01, Steps=388683, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4905, Total reward=106.33, Steps=388793, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4906, Total reward=220.11, Steps=388925, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4907, Total reward=183.13, Steps=389045, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4908, Total reward=130.12, Steps=389145, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4909, Total reward=50.31, Steps=389193, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4910, Total reward=87.28, Steps=389255, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4911, Total reward=101.53, Steps=389322, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4912, Total reward=95.99, Steps=389380, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4913, Total reward=199.61, Steps=389555, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4914, Total reward=178.46, Steps=389695, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4915, Total reward=440.41, Steps=390014, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4916, Total reward=179.43, Steps=390134, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4917, Total reward=144.73, Steps=390271, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4918, Total reward=333.03, Steps=390535, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4919, Total reward=75.69, Steps=390607, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4920, Total reward=112.5, Steps=390721, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4921, Total reward=204.78, Steps=390911, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4922, Total reward=211.93, Steps=391082, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4923, Total reward=151.21, Steps=391251, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4924, Total reward=155.6, Steps=391394, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4925, Total reward=11.68, Steps=391413, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4926, Total reward=93.15, Steps=391492, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4927, Total reward=247.52, Steps=391634, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4928, Total reward=164.72, Steps=391723, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4929, Total reward=121.11, Steps=391822, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4930, Total reward=41.77, Steps=391875, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4931, Total reward=68.97, Steps=391912, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4932, Total reward=54.54, Steps=391936, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4933, Total reward=55.56, Steps=391977, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4934, Total reward=203.53, Steps=392130, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4935, Total reward=472.07, Steps=392424, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4936, Total reward=87.52, Steps=392498, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4937, Total reward=322.01, Steps=392756, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4938, Total reward=31.82, Steps=392780, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4939, Total reward=203.41, Steps=392982, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4940, Total reward=220.58, Steps=393213, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4941, Total reward=133.39, Steps=393347, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4942, Total reward=240.45, Steps=393617, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4943, Total reward=13.78, Steps=393656, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4944, Total reward=4.65, Steps=393681, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4945, Total reward=201.1, Steps=393843, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4946, Total reward=220.72, Steps=393985, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4947, Total reward=105.58, Steps=394040, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4948, Total reward=82.81, Steps=394078, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4949, Total reward=26.34, Steps=394097, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4950, Total reward=91.83, Steps=394187, Training iteration=98
Policy training> Surrogate loss=0.005233130883425474, KL divergence=0.008241168223321438, Entropy=0.39526692032814026, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03613464906811714, KL divergence=0.07360779494047165, Entropy=0.3751622140407562, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04723252356052399, KL divergence=0.09436289966106415, Entropy=0.37718403339385986, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.050025202333927155, KL divergence=0.10154756158590317, Entropy=0.37784233689308167, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06471379101276398, KL divergence=0.10900791734457016, Entropy=0.3791024684906006, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05980369448661804, KL divergence=0.11219923198223114, Entropy=0.3783937990665436, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06461648643016815, KL divergence=0.11381321400403976, Entropy=0.3770439326763153, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0620977059006691, KL divergence=0.11583299189805984, Entropy=0.37686461210250854, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06291037052869797, KL divergence=0.11914827674627304, Entropy=0.37883397936820984, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06584049016237259, KL divergence=0.12057431042194366, Entropy=0.37885135412216187, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/244_Step-394187.ckpt']
Uploaded 3 files for checkpoint 244 in 0.55 seconds
saved intermediate frozen graph: current/model/model_244.pb
Best checkpoint number: 234, Last checkpoint number: 242
Copying the frozen checkpoint from ./frozen_models/agent/model_234.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'241'}
Training> Name=main_level/agent, Worker=0, Episode=4951, Total reward=84.43, Steps=394235, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4952, Total reward=121.08, Steps=394318, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4953, Total reward=70.2, Steps=394360, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4954, Total reward=52.35, Steps=394405, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4955, Total reward=440.43, Steps=394708, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4956, Total reward=149.74, Steps=394839, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4957, Total reward=82.64, Steps=394896, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4958, Total reward=68.33, Steps=394938, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4959, Total reward=61.74, Steps=395017, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4960, Total reward=97.31, Steps=395102, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4961, Total reward=76.28, Steps=395170, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4962, Total reward=146.12, Steps=395307, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4963, Total reward=3.6, Steps=395331, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4964, Total reward=2.58, Steps=395349, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4965, Total reward=174.2, Steps=395479, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4966, Total reward=95.91, Steps=395550, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4967, Total reward=165.32, Steps=395684, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4968, Total reward=114.78, Steps=395742, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4969, Total reward=165.97, Steps=395842, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4970, Total reward=72.91, Steps=395891, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4971, Total reward=84.01, Steps=395963, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4972, Total reward=13.15, Steps=395975, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4973, Total reward=64.41, Steps=396016, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4974, Total reward=223.94, Steps=396164, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4975, Total reward=115.41, Steps=396264, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4976, Total reward=14.74, Steps=396286, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4977, Total reward=333.22, Steps=396570, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4978, Total reward=89.67, Steps=396682, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4979, Total reward=331.4, Steps=396910, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4980, Total reward=402.18, Steps=397221, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4981, Total reward=355.62, Steps=397557, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4982, Total reward=227.92, Steps=397775, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4983, Total reward=67.04, Steps=397884, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4984, Total reward=211.57, Steps=398044, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4985, Total reward=144.03, Steps=398185, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4986, Total reward=131.19, Steps=398304, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4987, Total reward=178.01, Steps=398433, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4988, Total reward=212.93, Steps=398540, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4989, Total reward=153.11, Steps=398638, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4990, Total reward=139.6, Steps=398714, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4991, Total reward=69.49, Steps=398769, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4992, Total reward=185.55, Steps=398885, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4993, Total reward=45.92, Steps=398926, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4994, Total reward=68.13, Steps=398969, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4995, Total reward=138.5, Steps=399096, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4996, Total reward=258.49, Steps=399331, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4997, Total reward=289.48, Steps=399542, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4998, Total reward=141.16, Steps=399643, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4999, Total reward=122.28, Steps=399796, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=5000, Total reward=81.09, Steps=399921, Training iteration=99
Policy training> Surrogate loss=0.00035060298978351057, KL divergence=0.010490232147276402, Entropy=0.39658185839653015, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03827982768416405, KL divergence=0.08195532113313675, Entropy=0.37671443819999695, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.048762157559394836, KL divergence=0.10488347709178925, Entropy=0.37216559052467346, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.056872423738241196, KL divergence=0.11526019126176834, Entropy=0.3693297207355499, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06140829250216484, KL divergence=0.11969621479511261, Entropy=0.37023353576660156, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06133318319916725, KL divergence=0.12218404561281204, Entropy=0.3711300194263458, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06462843716144562, KL divergence=0.12488310039043427, Entropy=0.3719511032104492, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06522925198078156, KL divergence=0.1279144436120987, Entropy=0.37093478441238403, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06661652773618698, KL divergence=0.12724709510803223, Entropy=0.37023821473121643, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06759011745452881, KL divergence=0.13134008646011353, Entropy=0.37208545207977295, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/245_Step-399921.ckpt']
Uploaded 3 files for checkpoint 245 in 0.60 seconds
saved intermediate frozen graph: current/model/model_245.pb
Best checkpoint number: 234, Last checkpoint number: 243
Copying the frozen checkpoint from ./frozen_models/agent/model_234.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'242'}
Training> Name=main_level/agent, Worker=0, Episode=5001, Total reward=405.84, Steps=400233, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5002, Total reward=76.24, Steps=400313, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5003, Total reward=7.89, Steps=400340, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5004, Total reward=170.96, Steps=400472, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5005, Total reward=242.85, Steps=400622, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5006, Total reward=138.99, Steps=400752, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5007, Total reward=5.14, Steps=400777, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5008, Total reward=163.93, Steps=400890, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5009, Total reward=111.59, Steps=400974, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5010, Total reward=81.8, Steps=401039, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5011, Total reward=87.88, Steps=401096, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5012, Total reward=304.19, Steps=401345, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5013, Total reward=54.06, Steps=401382, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5014, Total reward=54.83, Steps=401421, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5015, Total reward=68.01, Steps=401475, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5016, Total reward=168.64, Steps=401613, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5017, Total reward=145.18, Steps=401731, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5018, Total reward=141.0, Steps=401825, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5019, Total reward=102.44, Steps=401932, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5020, Total reward=82.79, Steps=402021, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5021, Total reward=84.9, Steps=402142, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5022, Total reward=53.1, Steps=402213, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5023, Total reward=18.75, Steps=402256, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5024, Total reward=25.68, Steps=402305, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5025, Total reward=20.75, Steps=402343, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5026, Total reward=146.01, Steps=402456, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5027, Total reward=194.24, Steps=402584, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5028, Total reward=169.24, Steps=402688, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5029, Total reward=21.7, Steps=402705, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5030, Total reward=75.24, Steps=402755, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5031, Total reward=85.3, Steps=402801, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5032, Total reward=94.43, Steps=402860, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5033, Total reward=168.31, Steps=403066, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5034, Total reward=66.38, Steps=403111, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5035, Total reward=222.86, Steps=403317, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5036, Total reward=21.06, Steps=403337, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5037, Total reward=282.4, Steps=403553, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5038, Total reward=231.21, Steps=403772, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5039, Total reward=229.45, Steps=403977, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5040, Total reward=266.05, Steps=404206, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5041, Total reward=78.85, Steps=404266, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5042, Total reward=49.5, Steps=404309, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5043, Total reward=208.53, Steps=404484, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5044, Total reward=189.82, Steps=404643, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5045, Total reward=125.86, Steps=404740, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5046, Total reward=154.28, Steps=404863, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5047, Total reward=137.53, Steps=404979, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5048, Total reward=202.58, Steps=405089, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5049, Total reward=48.24, Steps=405129, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5050, Total reward=165.62, Steps=405229, Training iteration=100
Policy training> Surrogate loss=0.004534978419542313, KL divergence=0.00964661780744791, Entropy=0.404862642288208, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0376940593123436, KL divergence=0.07600297778844833, Entropy=0.381310373544693, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04563922435045242, KL divergence=0.10555437952280045, Entropy=0.3780848979949951, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05560062453150749, KL divergence=0.11741992086172104, Entropy=0.37190258502960205, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06373517215251923, KL divergence=0.121699258685112, Entropy=0.36756378412246704, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06383566558361053, KL divergence=0.1260564923286438, Entropy=0.3670860826969147, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06636448204517365, KL divergence=0.1291033923625946, Entropy=0.36825865507125854, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06479555368423462, KL divergence=0.1290796548128128, Entropy=0.36795729398727417, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06881869584321976, KL divergence=0.13344763219356537, Entropy=0.367986798286438, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07101308554410934, KL divergence=0.13250675797462463, Entropy=0.3667300343513489, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/246_Step-405229.ckpt']
Uploaded 3 files for checkpoint 246 in 0.57 seconds
saved intermediate frozen graph: current/model/model_246.pb
Best checkpoint number: 234, Last checkpoint number: 244
Copying the frozen checkpoint from ./frozen_models/agent/model_234.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'243'}
Training> Name=main_level/agent, Worker=0, Episode=5051, Total reward=62.67, Steps=405276, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5052, Total reward=63.06, Steps=405320, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5053, Total reward=52.72, Steps=405360, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5054, Total reward=30.32, Steps=405383, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5055, Total reward=296.95, Steps=405644, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5056, Total reward=145.2, Steps=405779, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5057, Total reward=137.1, Steps=405895, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5058, Total reward=74.55, Steps=405934, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5059, Total reward=17.91, Steps=405966, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5060, Total reward=296.93, Steps=406195, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5061, Total reward=74.91, Steps=406271, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5062, Total reward=130.35, Steps=406409, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5063, Total reward=130.67, Steps=406554, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5064, Total reward=212.94, Steps=406726, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5065, Total reward=122.39, Steps=406835, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5066, Total reward=82.42, Steps=406927, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5067, Total reward=6.57, Steps=406947, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5068, Total reward=87.26, Steps=406983, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5069, Total reward=221.1, Steps=407149, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5070, Total reward=73.39, Steps=407220, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5071, Total reward=29.88, Steps=407274, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5072, Total reward=117.77, Steps=407333, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5073, Total reward=72.56, Steps=407392, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5074, Total reward=416.42, Steps=407704, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5075, Total reward=19.23, Steps=407719, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5076, Total reward=329.88, Steps=407991, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5077, Total reward=162.61, Steps=408118, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5078, Total reward=217.95, Steps=408324, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5079, Total reward=307.48, Steps=408565, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5080, Total reward=285.03, Steps=408801, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5081, Total reward=439.16, Steps=409097, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5082, Total reward=175.8, Steps=409248, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5083, Total reward=15.42, Steps=409283, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5084, Total reward=216.73, Steps=409453, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5085, Total reward=100.69, Steps=409562, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5086, Total reward=118.16, Steps=409642, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5087, Total reward=104.54, Steps=409698, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5088, Total reward=197.58, Steps=409817, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5089, Total reward=95.09, Steps=409894, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5090, Total reward=66.12, Steps=409959, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5091, Total reward=84.49, Steps=410033, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5092, Total reward=51.76, Steps=410060, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5093, Total reward=63.37, Steps=410099, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5094, Total reward=359.84, Steps=410396, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5095, Total reward=305.96, Steps=410672, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5096, Total reward=179.38, Steps=410836, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5097, Total reward=256.28, Steps=411034, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5098, Total reward=155.46, Steps=411167, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5099, Total reward=89.96, Steps=411244, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5100, Total reward=382.15, Steps=411551, Training iteration=101
Policy training> Surrogate loss=0.00258462387137115, KL divergence=0.010341881774365902, Entropy=0.3925769031047821, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.037998516112565994, KL divergence=0.07604338973760605, Entropy=0.3718161880970001, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04370236396789551, KL divergence=0.09545302391052246, Entropy=0.3689331114292145, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04994170740246773, KL divergence=0.10268942266702652, Entropy=0.3681745231151581, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05634937807917595, KL divergence=0.10616936534643173, Entropy=0.3695882558822632, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05581476911902428, KL divergence=0.10686548799276352, Entropy=0.37063518166542053, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06028883159160614, KL divergence=0.11104436963796616, Entropy=0.3727285861968994, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06244036555290222, KL divergence=0.11137626320123672, Entropy=0.3726668059825897, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06543507426977158, KL divergence=0.11364743113517761, Entropy=0.3715468943119049, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06178070604801178, KL divergence=0.11719908565282822, Entropy=0.3717333972454071, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/247_Step-411551.ckpt']
Uploaded 3 files for checkpoint 247 in 0.60 seconds
saved intermediate frozen graph: current/model/model_247.pb
Best checkpoint number: 234, Last checkpoint number: 245
Copying the frozen checkpoint from ./frozen_models/agent/model_234.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'244'}
Training> Name=main_level/agent, Worker=0, Episode=5101, Total reward=151.31, Steps=411700, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5102, Total reward=158.97, Steps=411844, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5103, Total reward=348.41, Steps=412154, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5104, Total reward=130.25, Steps=412258, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5105, Total reward=106.53, Steps=412395, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5106, Total reward=175.02, Steps=412542, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5107, Total reward=121.68, Steps=412669, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5108, Total reward=116.46, Steps=412751, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5109, Total reward=160.34, Steps=412845, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5110, Total reward=105.0, Steps=412932, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5111, Total reward=119.52, Steps=413000, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5112, Total reward=16.78, Steps=413012, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5113, Total reward=37.32, Steps=413048, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5114, Total reward=240.15, Steps=413202, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5115, Total reward=36.27, Steps=413219, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5116, Total reward=126.66, Steps=413323, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5117, Total reward=28.95, Steps=413346, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5118, Total reward=135.72, Steps=413452, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5119, Total reward=110.39, Steps=413551, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5120, Total reward=320.09, Steps=413776, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5121, Total reward=276.89, Steps=413992, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5122, Total reward=41.14, Steps=414031, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5123, Total reward=6.5, Steps=414050, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5124, Total reward=115.5, Steps=414142, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5125, Total reward=218.03, Steps=414292, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5126, Total reward=196.47, Steps=414436, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5127, Total reward=184.0, Steps=414560, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5128, Total reward=128.76, Steps=414661, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5129, Total reward=15.91, Steps=414677, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5130, Total reward=103.97, Steps=414759, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5131, Total reward=117.44, Steps=414820, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5132, Total reward=9.98, Steps=414833, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5133, Total reward=66.03, Steps=414875, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5134, Total reward=345.11, Steps=415196, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5135, Total reward=167.82, Steps=415329, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5136, Total reward=274.21, Steps=415575, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5137, Total reward=70.58, Steps=415619, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5138, Total reward=151.52, Steps=415725, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5139, Total reward=281.23, Steps=415964, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5140, Total reward=104.78, Steps=416057, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5141, Total reward=250.84, Steps=416244, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5142, Total reward=199.33, Steps=416415, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5143, Total reward=209.95, Steps=416571, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5144, Total reward=25.61, Steps=416623, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5145, Total reward=125.93, Steps=416708, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5146, Total reward=38.49, Steps=416765, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5147, Total reward=131.91, Steps=416846, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5148, Total reward=95.42, Steps=416903, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5149, Total reward=41.93, Steps=416946, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5150, Total reward=80.51, Steps=416999, Training iteration=102
Policy training> Surrogate loss=0.00652450742200017, KL divergence=0.008528449572622776, Entropy=0.3920260965824127, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03821888938546181, KL divergence=0.065890371799469, Entropy=0.3789583742618561, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04827655479311943, KL divergence=0.0927390307188034, Entropy=0.3678845763206482, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0548858568072319, KL divergence=0.10299030691385269, Entropy=0.3637785017490387, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.058059897273778915, KL divergence=0.10681045800447464, Entropy=0.3635782301425934, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.060577280819416046, KL divergence=0.11069932579994202, Entropy=0.36320775747299194, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0626557469367981, KL divergence=0.11313103139400482, Entropy=0.3641076683998108, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.061259910464286804, KL divergence=0.11491749435663223, Entropy=0.36611390113830566, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06296131759881973, KL divergence=0.12016699463129044, Entropy=0.3659283220767975, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0679149180650711, KL divergence=0.12304964661598206, Entropy=0.36646732687950134, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/248_Step-416999.ckpt']
Uploaded 3 files for checkpoint 248 in 0.59 seconds
saved intermediate frozen graph: current/model/model_248.pb
Best checkpoint number: 246, Last checkpoint number: 246
Copying the frozen checkpoint from ./frozen_models/agent/model_246.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'245'}
Training> Name=main_level/agent, Worker=0, Episode=5151, Total reward=165.05, Steps=417142, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5152, Total reward=136.46, Steps=417223, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5153, Total reward=83.39, Steps=417285, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5154, Total reward=35.73, Steps=417334, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5155, Total reward=25.35, Steps=417350, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5156, Total reward=179.74, Steps=417478, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5157, Total reward=139.98, Steps=417578, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5158, Total reward=316.99, Steps=417850, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5159, Total reward=218.57, Steps=418021, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5160, Total reward=200.39, Steps=418180, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5161, Total reward=75.43, Steps=418251, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5162, Total reward=165.17, Steps=418390, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5163, Total reward=106.85, Steps=418507, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5164, Total reward=374.39, Steps=418839, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5165, Total reward=135.53, Steps=418984, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5166, Total reward=160.11, Steps=419126, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5167, Total reward=104.23, Steps=419196, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5168, Total reward=212.87, Steps=419323, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5169, Total reward=27.53, Steps=419387, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5170, Total reward=109.58, Steps=419466, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5171, Total reward=49.9, Steps=419520, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5172, Total reward=73.86, Steps=419563, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5173, Total reward=15.92, Steps=419588, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5174, Total reward=55.49, Steps=419622, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5175, Total reward=19.16, Steps=419635, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5176, Total reward=272.03, Steps=419847, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5177, Total reward=60.29, Steps=419889, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5178, Total reward=134.11, Steps=419971, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5179, Total reward=278.42, Steps=420207, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5180, Total reward=208.7, Steps=420387, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5181, Total reward=122.28, Steps=420513, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5182, Total reward=150.46, Steps=420681, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5183, Total reward=230.02, Steps=420870, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5184, Total reward=179.94, Steps=421059, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5185, Total reward=190.29, Steps=421205, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5186, Total reward=3.7, Steps=421218, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5187, Total reward=164.51, Steps=421331, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5188, Total reward=143.57, Steps=421425, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5189, Total reward=24.95, Steps=421457, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5190, Total reward=140.94, Steps=421558, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5191, Total reward=11.72, Steps=421571, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5192, Total reward=19.54, Steps=421595, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5193, Total reward=82.39, Steps=421663, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5194, Total reward=55.96, Steps=421692, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5195, Total reward=326.15, Steps=421953, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5196, Total reward=93.06, Steps=422022, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5197, Total reward=3.88, Steps=422038, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5198, Total reward=270.83, Steps=422325, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5199, Total reward=72.42, Steps=422408, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5200, Total reward=55.66, Steps=422449, Training iteration=103
Policy training> Surrogate loss=-0.0008823261596262455, KL divergence=0.006156510673463345, Entropy=0.3799791634082794, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.034967344254255295, KL divergence=0.07462666183710098, Entropy=0.35995274782180786, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04819177836179733, KL divergence=0.10115170478820801, Entropy=0.3547784090042114, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05283175781369209, KL divergence=0.10769618302583694, Entropy=0.3542811870574951, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05756901949644089, KL divergence=0.11222276091575623, Entropy=0.35634830594062805, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05965841934084892, KL divergence=0.11722434312105179, Entropy=0.3561929166316986, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0610029399394989, KL divergence=0.11921824514865875, Entropy=0.3566136956214905, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06270188838243484, KL divergence=0.12306414544582367, Entropy=0.3576321005821228, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06634332984685898, KL divergence=0.12327131628990173, Entropy=0.35732153058052063, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06474748253822327, KL divergence=0.1281953901052475, Entropy=0.35781124234199524, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/249_Step-422449.ckpt']
Uploaded 3 files for checkpoint 249 in 0.50 seconds
saved intermediate frozen graph: current/model/model_249.pb
Best checkpoint number: 246, Last checkpoint number: 247
Copying the frozen checkpoint from ./frozen_models/agent/model_246.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'234'}
Training> Name=main_level/agent, Worker=0, Episode=5201, Total reward=241.97, Steps=422650, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5202, Total reward=17.68, Steps=422669, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5203, Total reward=3.58, Steps=422696, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5204, Total reward=239.1, Steps=422848, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5205, Total reward=117.99, Steps=422936, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5206, Total reward=70.34, Steps=422979, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5207, Total reward=91.78, Steps=423046, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5208, Total reward=163.4, Steps=423135, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5209, Total reward=62.63, Steps=423223, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5210, Total reward=41.87, Steps=423299, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5211, Total reward=415.66, Steps=423605, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5212, Total reward=87.34, Steps=423665, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5213, Total reward=61.07, Steps=423705, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5214, Total reward=73.51, Steps=423733, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5215, Total reward=53.04, Steps=423781, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5216, Total reward=322.98, Steps=424095, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5217, Total reward=59.42, Steps=424168, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5218, Total reward=342.27, Steps=424508, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5219, Total reward=187.38, Steps=424738, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5220, Total reward=91.09, Steps=424816, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5221, Total reward=258.52, Steps=425037, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5222, Total reward=35.52, Steps=425062, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5223, Total reward=146.05, Steps=425223, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5224, Total reward=36.9, Steps=425284, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5225, Total reward=223.57, Steps=425438, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5226, Total reward=183.12, Steps=425566, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5227, Total reward=164.86, Steps=425694, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5228, Total reward=208.13, Steps=425806, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5229, Total reward=116.81, Steps=425905, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5230, Total reward=112.25, Steps=425984, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5231, Total reward=122.99, Steps=426055, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5232, Total reward=47.26, Steps=426091, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5233, Total reward=385.08, Steps=426405, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5234, Total reward=159.69, Steps=426519, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5235, Total reward=393.23, Steps=426810, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5236, Total reward=61.18, Steps=426876, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5237, Total reward=353.99, Steps=427137, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5238, Total reward=341.43, Steps=427408, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5239, Total reward=54.13, Steps=427481, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5240, Total reward=409.81, Steps=427779, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5241, Total reward=64.64, Steps=427818, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5242, Total reward=40.86, Steps=427864, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5243, Total reward=94.9, Steps=427999, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5244, Total reward=220.95, Steps=428143, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5245, Total reward=104.92, Steps=428214, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5246, Total reward=382.41, Steps=428467, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5247, Total reward=420.07, Steps=428765, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5248, Total reward=206.49, Steps=428882, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5249, Total reward=29.91, Steps=428901, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5250, Total reward=64.9, Steps=428964, Training iteration=104
Policy training> Surrogate loss=0.008410719223320484, KL divergence=0.015525003895163536, Entropy=0.37553706765174866, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.031893689185380936, KL divergence=0.08115621656179428, Entropy=0.3567523956298828, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.044560160487890244, KL divergence=0.10033034533262253, Entropy=0.35737791657447815, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05002494156360626, KL divergence=0.10995537042617798, Entropy=0.3591081500053406, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04957761615514755, KL divergence=0.11387065798044205, Entropy=0.3602416217327118, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05521773919463158, KL divergence=0.11932496726512909, Entropy=0.36150696873664856, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05715685337781906, KL divergence=0.12070271372795105, Entropy=0.36274784803390503, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05635380372405052, KL divergence=0.12421081215143204, Entropy=0.3649429380893707, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06258673220872879, KL divergence=0.1250445693731308, Entropy=0.36637672781944275, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.061426449567079544, KL divergence=0.12791524827480316, Entropy=0.36701539158821106, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/250_Step-428964.ckpt']
Uploaded 3 files for checkpoint 250 in 0.53 seconds
saved intermediate frozen graph: current/model/model_250.pb
Best checkpoint number: 246, Last checkpoint number: 248
Copying the frozen checkpoint from ./frozen_models/agent/model_246.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'247'}
Training> Name=main_level/agent, Worker=0, Episode=5251, Total reward=57.45, Steps=429014, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5252, Total reward=49.53, Steps=429056, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5253, Total reward=87.09, Steps=429097, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5254, Total reward=144.66, Steps=429195, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5255, Total reward=19.17, Steps=429209, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5256, Total reward=272.35, Steps=429469, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5257, Total reward=370.55, Steps=429748, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5258, Total reward=40.69, Steps=429773, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5259, Total reward=98.66, Steps=429845, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5260, Total reward=249.51, Steps=430069, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5261, Total reward=305.44, Steps=430291, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5262, Total reward=229.84, Steps=430490, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5263, Total reward=19.53, Steps=430546, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5264, Total reward=360.02, Steps=430869, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5265, Total reward=192.64, Steps=431019, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5266, Total reward=234.1, Steps=431179, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5267, Total reward=189.93, Steps=431311, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5268, Total reward=206.31, Steps=431423, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5269, Total reward=101.92, Steps=431511, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5270, Total reward=61.71, Steps=431573, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5271, Total reward=98.85, Steps=431639, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5272, Total reward=236.65, Steps=431804, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5273, Total reward=40.72, Steps=431850, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5274, Total reward=28.33, Steps=431862, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5275, Total reward=168.48, Steps=431998, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5276, Total reward=264.66, Steps=432235, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5277, Total reward=353.67, Steps=432524, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5278, Total reward=345.46, Steps=432788, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5279, Total reward=343.77, Steps=433024, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5280, Total reward=85.45, Steps=433101, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5281, Total reward=290.95, Steps=433307, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5282, Total reward=47.91, Steps=433350, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5283, Total reward=184.23, Steps=433523, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5284, Total reward=121.03, Steps=433636, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5285, Total reward=23.79, Steps=433670, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5286, Total reward=210.58, Steps=433812, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5287, Total reward=203.19, Steps=433934, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5288, Total reward=362.49, Steps=434227, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5289, Total reward=133.76, Steps=434323, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5290, Total reward=38.4, Steps=434362, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5291, Total reward=85.35, Steps=434408, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5292, Total reward=79.85, Steps=434464, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5293, Total reward=83.64, Steps=434509, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5294, Total reward=182.89, Steps=434687, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5295, Total reward=123.16, Steps=434776, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5296, Total reward=202.98, Steps=434920, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5297, Total reward=361.53, Steps=435209, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5298, Total reward=25.74, Steps=435225, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5299, Total reward=315.81, Steps=435470, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5300, Total reward=256.15, Steps=435715, Training iteration=105
Policy training> Surrogate loss=0.005845857784152031, KL divergence=0.013087606988847256, Entropy=0.3805365264415741, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03644528612494469, KL divergence=0.07399279624223709, Entropy=0.36562612652778625, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.046964045614004135, KL divergence=0.09065502136945724, Entropy=0.3607625663280487, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.049525562673807144, KL divergence=0.09897087514400482, Entropy=0.36003461480140686, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.056876081973314285, KL divergence=0.10156885534524918, Entropy=0.3605596423149109, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05733444169163704, KL divergence=0.1062815859913826, Entropy=0.3599433898925781, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06102653220295906, KL divergence=0.10740364342927933, Entropy=0.3608047664165497, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0603795200586319, KL divergence=0.1127624437212944, Entropy=0.36172547936439514, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06160112842917442, KL divergence=0.1140412986278534, Entropy=0.36430251598358154, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06270366162061691, KL divergence=0.1159118264913559, Entropy=0.3633406460285187, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/251_Step-435715.ckpt']
Uploaded 3 files for checkpoint 251 in 0.60 seconds
saved intermediate frozen graph: current/model/model_251.pb
Best checkpoint number: 246, Last checkpoint number: 249
Copying the frozen checkpoint from ./frozen_models/agent/model_246.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'248'}
Training> Name=main_level/agent, Worker=0, Episode=5301, Total reward=244.19, Steps=435939, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5302, Total reward=158.69, Steps=436140, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5303, Total reward=214.78, Steps=436323, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5304, Total reward=109.99, Steps=436450, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5305, Total reward=218.45, Steps=436600, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5306, Total reward=406.68, Steps=436911, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5307, Total reward=186.04, Steps=437042, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5308, Total reward=141.16, Steps=437142, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5309, Total reward=69.2, Steps=437196, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5310, Total reward=69.01, Steps=437261, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5311, Total reward=54.69, Steps=437319, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5312, Total reward=393.03, Steps=437644, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5313, Total reward=53.1, Steps=437686, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5314, Total reward=360.18, Steps=437998, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5315, Total reward=3.59, Steps=438015, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5316, Total reward=427.88, Steps=438317, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5317, Total reward=41.02, Steps=438367, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5318, Total reward=84.99, Steps=438447, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5319, Total reward=87.21, Steps=438578, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5320, Total reward=114.84, Steps=438685, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5321, Total reward=68.88, Steps=438755, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5322, Total reward=45.35, Steps=438786, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5323, Total reward=203.63, Steps=438947, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5324, Total reward=125.03, Steps=439117, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5325, Total reward=160.12, Steps=439294, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5326, Total reward=43.43, Steps=439350, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5327, Total reward=218.16, Steps=439480, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5328, Total reward=91.86, Steps=439522, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5329, Total reward=140.91, Steps=439619, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5330, Total reward=77.57, Steps=439668, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5331, Total reward=117.33, Steps=439735, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5332, Total reward=124.83, Steps=439789, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5333, Total reward=64.67, Steps=439827, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5334, Total reward=216.5, Steps=439979, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5335, Total reward=195.06, Steps=440104, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5336, Total reward=179.37, Steps=440237, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5337, Total reward=321.53, Steps=440517, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5338, Total reward=377.58, Steps=440852, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5339, Total reward=123.75, Steps=440954, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5340, Total reward=183.9, Steps=441114, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5341, Total reward=80.84, Steps=441155, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5342, Total reward=61.66, Steps=441237, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5343, Total reward=197.82, Steps=441425, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5344, Total reward=119.4, Steps=441588, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5345, Total reward=17.13, Steps=441616, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5346, Total reward=118.28, Steps=441686, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5347, Total reward=224.73, Steps=441834, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5348, Total reward=196.33, Steps=441948, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5349, Total reward=102.81, Steps=442022, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5350, Total reward=114.35, Steps=442104, Training iteration=106
Policy training> Surrogate loss=0.00933923851698637, KL divergence=0.010433420538902283, Entropy=0.37020137906074524, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03233315423130989, KL divergence=0.06818924099206924, Entropy=0.3501551151275635, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.044652100652456284, KL divergence=0.08835136890411377, Entropy=0.34345945715904236, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0525529682636261, KL divergence=0.09496656060218811, Entropy=0.34367677569389343, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04989608749747276, KL divergence=0.0982227697968483, Entropy=0.3423050343990326, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0567023865878582, KL divergence=0.10324728488922119, Entropy=0.34172940254211426, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.054984692484140396, KL divergence=0.1054399311542511, Entropy=0.3433469235897064, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05641758069396019, KL divergence=0.10935714095830917, Entropy=0.34544578194618225, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.060108885169029236, KL divergence=0.11120489984750748, Entropy=0.3457578420639038, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05933811143040657, KL divergence=0.11148232966661453, Entropy=0.34602871537208557, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/252_Step-442104.ckpt']
Uploaded 3 files for checkpoint 252 in 0.58 seconds
saved intermediate frozen graph: current/model/model_252.pb
Best checkpoint number: 246, Last checkpoint number: 250
Copying the frozen checkpoint from ./frozen_models/agent/model_246.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'249'}
Training> Name=main_level/agent, Worker=0, Episode=5351, Total reward=28.24, Steps=442145, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5352, Total reward=224.69, Steps=442309, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5353, Total reward=56.65, Steps=442351, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5354, Total reward=391.72, Steps=442648, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5355, Total reward=371.23, Steps=442972, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5356, Total reward=184.78, Steps=443102, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5357, Total reward=156.94, Steps=443230, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5358, Total reward=368.72, Steps=443501, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5359, Total reward=87.28, Steps=443614, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5360, Total reward=287.09, Steps=443853, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5361, Total reward=170.52, Steps=444014, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5362, Total reward=205.59, Steps=444204, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5363, Total reward=13.29, Steps=444243, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5364, Total reward=169.71, Steps=444394, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5365, Total reward=23.74, Steps=444431, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5366, Total reward=138.45, Steps=444543, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5367, Total reward=23.97, Steps=444576, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5368, Total reward=392.26, Steps=444875, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5369, Total reward=104.61, Steps=444959, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5370, Total reward=32.36, Steps=444996, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5371, Total reward=100.1, Steps=445061, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5372, Total reward=119.45, Steps=445120, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5373, Total reward=49.37, Steps=445161, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5374, Total reward=437.65, Steps=445484, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5375, Total reward=407.01, Steps=445803, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5376, Total reward=431.39, Steps=446083, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5377, Total reward=397.45, Steps=446365, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5378, Total reward=59.88, Steps=446422, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5379, Total reward=317.79, Steps=446673, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5380, Total reward=185.01, Steps=446881, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5381, Total reward=1.99, Steps=446896, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5382, Total reward=295.14, Steps=447098, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5383, Total reward=211.0, Steps=447279, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5384, Total reward=138.71, Steps=447424, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5385, Total reward=195.14, Steps=447578, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5386, Total reward=191.12, Steps=447726, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5387, Total reward=0.01, Steps=447738, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5388, Total reward=167.8, Steps=447857, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5389, Total reward=104.45, Steps=447935, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5390, Total reward=145.7, Steps=448014, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5391, Total reward=139.76, Steps=448078, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5392, Total reward=83.18, Steps=448136, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5393, Total reward=58.96, Steps=448177, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5394, Total reward=57.67, Steps=448227, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5395, Total reward=23.77, Steps=448264, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5396, Total reward=90.28, Steps=448340, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5397, Total reward=193.69, Steps=448497, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5398, Total reward=259.78, Steps=448683, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5399, Total reward=109.6, Steps=448760, Training iteration=107
Training> Name=main_level/agent, Worker=0, Episode=5400, Total reward=325.45, Steps=448993, Training iteration=107
Policy training> Surrogate loss=0.0005817579803988338, KL divergence=0.01178229320794344, Entropy=0.38628435134887695, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.027081526815891266, KL divergence=0.06872305274009705, Entropy=0.3691701292991638, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04207902401685715, KL divergence=0.08278262615203857, Entropy=0.365325391292572, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04933037981390953, KL divergence=0.08994030207395554, Entropy=0.36331552267074585, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05160742998123169, KL divergence=0.09717155247926712, Entropy=0.3623099625110626, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.051581867039203644, KL divergence=0.09928084164857864, Entropy=0.36237379908561707, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05358987674117088, KL divergence=0.10204656422138214, Entropy=0.36287596821784973, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05879563093185425, KL divergence=0.10328833013772964, Entropy=0.3609767258167267, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05617484822869301, KL divergence=0.10652630031108856, Entropy=0.35966742038726807, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05523955449461937, KL divergence=0.10927372425794601, Entropy=0.36123421788215637, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/253_Step-448993.ckpt']
Uploaded 3 files for checkpoint 253 in 0.54 seconds
saved intermediate frozen graph: current/model/model_253.pb
Best checkpoint number: 246, Last checkpoint number: 251
Copying the frozen checkpoint from ./frozen_models/agent/model_246.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'250'}
Training> Name=main_level/agent, Worker=0, Episode=5401, Total reward=301.62, Steps=449211, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5402, Total reward=186.02, Steps=449381, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5403, Total reward=26.84, Steps=449436, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5404, Total reward=184.83, Steps=449609, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5405, Total reward=16.75, Steps=449644, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5406, Total reward=165.89, Steps=449784, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5407, Total reward=154.6, Steps=449897, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5408, Total reward=242.53, Steps=450007, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5409, Total reward=41.58, Steps=450103, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5410, Total reward=119.55, Steps=450187, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5411, Total reward=39.81, Steps=450239, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5412, Total reward=82.53, Steps=450298, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5413, Total reward=97.28, Steps=450353, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5414, Total reward=58.78, Steps=450410, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5415, Total reward=419.64, Steps=450713, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5416, Total reward=191.02, Steps=450883, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5417, Total reward=70.59, Steps=450937, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5418, Total reward=289.09, Steps=451233, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5419, Total reward=214.48, Steps=451401, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5420, Total reward=178.12, Steps=451578, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5421, Total reward=54.89, Steps=451606, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5422, Total reward=255.71, Steps=451806, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5423, Total reward=31.19, Steps=451918, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5424, Total reward=388.67, Steps=452237, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5425, Total reward=206.83, Steps=452393, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5426, Total reward=140.89, Steps=452498, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5427, Total reward=230.67, Steps=452619, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5428, Total reward=92.71, Steps=452660, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5429, Total reward=341.96, Steps=452987, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5430, Total reward=113.62, Steps=453094, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5431, Total reward=57.38, Steps=453168, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5432, Total reward=223.1, Steps=453318, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5433, Total reward=68.47, Steps=453392, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5434, Total reward=55.84, Steps=453444, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5435, Total reward=385.41, Steps=453749, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5436, Total reward=188.59, Steps=453921, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5437, Total reward=270.54, Steps=454130, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5438, Total reward=330.18, Steps=454437, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5439, Total reward=74.81, Steps=454524, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5440, Total reward=328.64, Steps=454749, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5441, Total reward=280.15, Steps=454973, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5442, Total reward=135.95, Steps=455108, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5443, Total reward=123.34, Steps=455229, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5444, Total reward=200.12, Steps=455383, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5445, Total reward=7.02, Steps=455409, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5446, Total reward=158.66, Steps=455567, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5447, Total reward=184.47, Steps=455698, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5448, Total reward=129.36, Steps=455798, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5449, Total reward=19.53, Steps=455826, Training iteration=108
Training> Name=main_level/agent, Worker=0, Episode=5450, Total reward=121.43, Steps=455905, Training iteration=108
Policy training> Surrogate loss=0.003315621055662632, KL divergence=0.011958204209804535, Entropy=0.36751583218574524, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.032507434487342834, KL divergence=0.08209528028964996, Entropy=0.3455081284046173, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04400638863444328, KL divergence=0.09626663476228714, Entropy=0.3407890200614929, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04923263192176819, KL divergence=0.1011107787489891, Entropy=0.34165099263191223, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.052040956914424896, KL divergence=0.10412406176328659, Entropy=0.34039685130119324, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05402754247188568, KL divergence=0.10590627789497375, Entropy=0.34056180715560913, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05556222423911095, KL divergence=0.10832016915082932, Entropy=0.3413730561733246, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.056897446513175964, KL divergence=0.10991433262825012, Entropy=0.3415130376815796, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.057961996644735336, KL divergence=0.11179428547620773, Entropy=0.3422035574913025, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.058885566890239716, KL divergence=0.11341249197721481, Entropy=0.3423457443714142, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/254_Step-455905.ckpt']
Uploaded 3 files for checkpoint 254 in 0.59 seconds
saved intermediate frozen graph: current/model/model_254.pb
Best checkpoint number: 246, Last checkpoint number: 252
Copying the frozen checkpoint from ./frozen_models/agent/model_246.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'251'}
Training> Name=main_level/agent, Worker=0, Episode=5451, Total reward=126.56, Steps=455977, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5452, Total reward=14.34, Steps=456000, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5453, Total reward=316.1, Steps=456322, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5454, Total reward=213.97, Steps=456498, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5455, Total reward=187.72, Steps=456641, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5456, Total reward=165.3, Steps=456761, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5457, Total reward=338.05, Steps=457032, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5458, Total reward=141.38, Steps=457157, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5459, Total reward=279.92, Steps=457405, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5460, Total reward=96.53, Steps=457495, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5461, Total reward=162.1, Steps=457637, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5462, Total reward=244.13, Steps=457846, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5463, Total reward=85.61, Steps=457951, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5464, Total reward=236.24, Steps=458117, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5465, Total reward=122.13, Steps=458213, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5466, Total reward=242.79, Steps=458346, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5467, Total reward=190.53, Steps=458442, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5468, Total reward=199.99, Steps=458562, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5469, Total reward=100.62, Steps=458661, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5470, Total reward=89.96, Steps=458750, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5471, Total reward=117.0, Steps=458820, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5472, Total reward=366.12, Steps=459135, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5473, Total reward=52.73, Steps=459176, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5474, Total reward=68.41, Steps=459222, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5475, Total reward=394.81, Steps=459527, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5476, Total reward=15.87, Steps=459546, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5477, Total reward=304.22, Steps=459818, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5478, Total reward=350.46, Steps=460080, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5479, Total reward=93.12, Steps=460183, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5480, Total reward=299.34, Steps=460393, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5481, Total reward=7.72, Steps=460405, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5482, Total reward=260.72, Steps=460607, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5483, Total reward=215.2, Steps=460782, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5484, Total reward=94.41, Steps=460885, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5485, Total reward=227.34, Steps=461039, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5486, Total reward=213.55, Steps=461218, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5487, Total reward=39.37, Steps=461251, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5488, Total reward=194.15, Steps=461361, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5489, Total reward=19.92, Steps=461380, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5490, Total reward=117.04, Steps=461453, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5491, Total reward=65.23, Steps=461495, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5492, Total reward=31.14, Steps=461538, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5493, Total reward=57.47, Steps=461578, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5494, Total reward=39.21, Steps=461591, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5495, Total reward=356.8, Steps=461880, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5496, Total reward=172.96, Steps=462023, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5497, Total reward=205.72, Steps=462270, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5498, Total reward=69.74, Steps=462312, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5499, Total reward=87.21, Steps=462440, Training iteration=109
Training> Name=main_level/agent, Worker=0, Episode=5500, Total reward=304.49, Steps=462673, Training iteration=109
Policy training> Surrogate loss=0.010473182424902916, KL divergence=0.012659264728426933, Entropy=0.3762025833129883, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03299259394407272, KL divergence=0.08665961772203445, Entropy=0.3501458466053009, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.043554700911045074, KL divergence=0.10303099453449249, Entropy=0.347190260887146, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04790911450982094, KL divergence=0.10663631558418274, Entropy=0.3467506766319275, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05249207466840744, KL divergence=0.11079259216785431, Entropy=0.34651538729667664, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05234019458293915, KL divergence=0.11371691524982452, Entropy=0.3470458984375, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05362043157219887, KL divergence=0.11732398718595505, Entropy=0.346836119890213, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05651120841503143, KL divergence=0.12005781382322311, Entropy=0.3449489176273346, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05695261061191559, KL divergence=0.121309794485569, Entropy=0.3462923765182495, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05736818537116051, KL divergence=0.12528298795223236, Entropy=0.34554150700569153, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/255_Step-462673.ckpt']
Uploaded 3 files for checkpoint 255 in 0.52 seconds
saved intermediate frozen graph: current/model/model_255.pb
Best checkpoint number: 246, Last checkpoint number: 253
Copying the frozen checkpoint from ./frozen_models/agent/model_246.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'252'}
Training> Name=main_level/agent, Worker=0, Episode=5501, Total reward=223.29, Steps=462883, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5502, Total reward=245.44, Steps=463097, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5503, Total reward=137.39, Steps=463227, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5504, Total reward=133.5, Steps=463391, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5505, Total reward=198.54, Steps=463547, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5506, Total reward=107.38, Steps=463645, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5507, Total reward=102.87, Steps=463750, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5508, Total reward=178.87, Steps=463834, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5509, Total reward=37.26, Steps=463858, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5510, Total reward=118.19, Steps=463936, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5511, Total reward=379.67, Steps=464260, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5512, Total reward=211.72, Steps=464411, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5513, Total reward=69.04, Steps=464452, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5514, Total reward=67.99, Steps=464503, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5515, Total reward=16.68, Steps=464518, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5516, Total reward=314.3, Steps=464811, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5517, Total reward=201.67, Steps=464963, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5518, Total reward=356.49, Steps=465235, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5519, Total reward=192.76, Steps=465430, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5520, Total reward=275.21, Steps=465647, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5521, Total reward=285.14, Steps=465860, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5522, Total reward=258.32, Steps=466067, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5523, Total reward=27.54, Steps=466126, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5524, Total reward=23.29, Steps=466170, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5525, Total reward=100.25, Steps=466252, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5526, Total reward=117.01, Steps=466328, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5527, Total reward=192.49, Steps=466453, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5528, Total reward=212.68, Steps=466573, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5529, Total reward=163.38, Steps=466663, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5530, Total reward=128.59, Steps=466741, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5531, Total reward=86.41, Steps=466791, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5532, Total reward=12.59, Steps=466803, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5533, Total reward=55.62, Steps=466844, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5534, Total reward=47.07, Steps=466876, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5535, Total reward=101.55, Steps=466976, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5536, Total reward=188.11, Steps=467105, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5537, Total reward=69.43, Steps=467159, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5538, Total reward=74.11, Steps=467249, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5539, Total reward=235.39, Steps=467473, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5540, Total reward=105.47, Steps=467568, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5541, Total reward=57.79, Steps=467630, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5542, Total reward=207.12, Steps=467809, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5543, Total reward=179.3, Steps=467995, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5544, Total reward=30.09, Steps=468056, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5545, Total reward=23.83, Steps=468091, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5546, Total reward=218.3, Steps=468220, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5547, Total reward=141.02, Steps=468333, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5548, Total reward=171.55, Steps=468455, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5549, Total reward=139.3, Steps=468551, Training iteration=110
Training> Name=main_level/agent, Worker=0, Episode=5550, Total reward=90.56, Steps=468640, Training iteration=110
Policy training> Surrogate loss=0.004313954152166843, KL divergence=0.009808097966015339, Entropy=0.3612058162689209, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.031420666724443436, KL divergence=0.07793880254030228, Entropy=0.3381671905517578, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04265290126204491, KL divergence=0.09878750145435333, Entropy=0.33294668793678284, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04634139686822891, KL divergence=0.10833586752414703, Entropy=0.33066222071647644, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05226597934961319, KL divergence=0.11109417676925659, Entropy=0.3288719654083252, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05500013381242752, KL divergence=0.11472148448228836, Entropy=0.32908087968826294, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05520228296518326, KL divergence=0.11739560961723328, Entropy=0.3312196433544159, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05688934773206711, KL divergence=0.11954434216022491, Entropy=0.3321584165096283, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.057623278349637985, KL divergence=0.12318704277276993, Entropy=0.32990479469299316, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06038479134440422, KL divergence=0.12343911826610565, Entropy=0.33032187819480896, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/256_Step-468640.ckpt']
Uploaded 3 files for checkpoint 256 in 0.52 seconds
saved intermediate frozen graph: current/model/model_256.pb
Best checkpoint number: 246, Last checkpoint number: 254
Copying the frozen checkpoint from ./frozen_models/agent/model_246.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'253'}
Training> Name=main_level/agent, Worker=0, Episode=5551, Total reward=113.09, Steps=468704, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5552, Total reward=94.46, Steps=468761, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5553, Total reward=52.43, Steps=468804, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5554, Total reward=389.42, Steps=469106, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5555, Total reward=76.63, Steps=469172, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5556, Total reward=91.88, Steps=469247, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5557, Total reward=377.93, Steps=469558, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5558, Total reward=286.34, Steps=469806, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5559, Total reward=271.9, Steps=470059, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5560, Total reward=104.44, Steps=470116, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5561, Total reward=288.04, Steps=470332, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5562, Total reward=58.79, Steps=470403, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5563, Total reward=170.2, Steps=470564, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5564, Total reward=4.21, Steps=470591, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5565, Total reward=149.74, Steps=470735, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5566, Total reward=262.45, Steps=470911, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5567, Total reward=200.63, Steps=471026, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5568, Total reward=190.61, Steps=471137, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5569, Total reward=31.52, Steps=471168, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5570, Total reward=127.32, Steps=471248, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5571, Total reward=65.26, Steps=471292, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5572, Total reward=103.55, Steps=471345, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5573, Total reward=151.94, Steps=471470, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5574, Total reward=448.87, Steps=471775, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5575, Total reward=125.03, Steps=471859, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5576, Total reward=79.72, Steps=471937, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5577, Total reward=436.49, Steps=472254, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5578, Total reward=174.27, Steps=472408, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5579, Total reward=57.83, Steps=472481, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5580, Total reward=247.21, Steps=472692, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5581, Total reward=75.93, Steps=472732, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5582, Total reward=0.0, Steps=472733, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5583, Total reward=21.79, Steps=472794, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5584, Total reward=204.52, Steps=472970, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5585, Total reward=357.88, Steps=473284, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5586, Total reward=212.66, Steps=473428, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5587, Total reward=228.02, Steps=473630, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5588, Total reward=234.32, Steps=473768, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5589, Total reward=96.08, Steps=473833, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5590, Total reward=99.19, Steps=473918, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5591, Total reward=113.26, Steps=473988, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5592, Total reward=394.03, Steps=474304, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5593, Total reward=54.28, Steps=474348, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5594, Total reward=372.07, Steps=474638, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5595, Total reward=199.04, Steps=474770, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5596, Total reward=178.61, Steps=474938, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5597, Total reward=391.86, Steps=475238, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5598, Total reward=337.72, Steps=475540, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5599, Total reward=93.49, Steps=475636, Training iteration=111
Training> Name=main_level/agent, Worker=0, Episode=5600, Total reward=312.07, Steps=475886, Training iteration=111
Policy training> Surrogate loss=0.006557866930961609, KL divergence=0.019092457368969917, Entropy=0.34820815920829773, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02896537072956562, KL divergence=0.08496755361557007, Entropy=0.3285358250141144, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03920355066657066, KL divergence=0.09884432703256607, Entropy=0.3265822231769562, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.044797446578741074, KL divergence=0.10655267536640167, Entropy=0.3290686309337616, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04914009943604469, KL divergence=0.1099279522895813, Entropy=0.33001086115837097, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05034249275922775, KL divergence=0.11120253801345825, Entropy=0.3325066566467285, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05131686106324196, KL divergence=0.11363620311021805, Entropy=0.33260464668273926, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05266476422548294, KL divergence=0.11656153947114944, Entropy=0.3335871696472168, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.054536767303943634, KL divergence=0.11729645729064941, Entropy=0.334183007478714, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.054238367825746536, KL divergence=0.11836174875497818, Entropy=0.3341662883758545, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/257_Step-475886.ckpt']
Uploaded 3 files for checkpoint 257 in 0.54 seconds
saved intermediate frozen graph: current/model/model_257.pb
Best checkpoint number: 246, Last checkpoint number: 255
Copying the frozen checkpoint from ./frozen_models/agent/model_246.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'254'}
Training> Name=main_level/agent, Worker=0, Episode=5601, Total reward=87.32, Steps=475967, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5602, Total reward=53.33, Steps=476013, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5603, Total reward=221.01, Steps=476193, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5604, Total reward=27.67, Steps=476237, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5605, Total reward=192.84, Steps=476358, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5606, Total reward=203.35, Steps=476468, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5607, Total reward=0.01, Steps=476481, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5608, Total reward=127.59, Steps=476558, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5609, Total reward=117.04, Steps=476639, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5610, Total reward=134.64, Steps=476714, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5611, Total reward=70.16, Steps=476761, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5612, Total reward=86.52, Steps=476812, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5613, Total reward=53.2, Steps=476852, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5614, Total reward=376.11, Steps=477161, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5615, Total reward=17.54, Steps=477192, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5616, Total reward=91.24, Steps=477262, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5617, Total reward=177.97, Steps=477401, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5618, Total reward=366.4, Steps=477667, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5619, Total reward=89.61, Steps=477753, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5620, Total reward=192.45, Steps=477935, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5621, Total reward=59.94, Steps=477974, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5622, Total reward=183.26, Steps=478130, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5623, Total reward=159.31, Steps=478300, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5624, Total reward=246.25, Steps=478455, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5625, Total reward=213.23, Steps=478600, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5626, Total reward=201.84, Steps=478721, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5627, Total reward=237.73, Steps=478851, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5628, Total reward=114.24, Steps=478951, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5629, Total reward=116.43, Steps=479053, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5630, Total reward=129.43, Steps=479134, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5631, Total reward=47.15, Steps=479171, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5632, Total reward=144.26, Steps=479280, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5633, Total reward=24.83, Steps=479292, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5634, Total reward=66.72, Steps=479342, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5635, Total reward=14.89, Steps=479374, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5636, Total reward=143.02, Steps=479495, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5637, Total reward=355.93, Steps=479759, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5638, Total reward=327.51, Steps=480011, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5639, Total reward=291.36, Steps=480269, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5640, Total reward=217.11, Steps=480427, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5641, Total reward=263.32, Steps=480632, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5642, Total reward=195.34, Steps=480813, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5643, Total reward=3.88, Steps=480829, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5644, Total reward=212.55, Steps=481000, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5645, Total reward=146.59, Steps=481149, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5646, Total reward=121.8, Steps=481276, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5647, Total reward=99.39, Steps=481330, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5648, Total reward=210.5, Steps=481437, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5649, Total reward=71.0, Steps=481525, Training iteration=112
Training> Name=main_level/agent, Worker=0, Episode=5650, Total reward=70.64, Steps=481580, Training iteration=112
Policy training> Surrogate loss=0.005433684214949608, KL divergence=0.009938227012753487, Entropy=0.36605703830718994, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.034251175820827484, KL divergence=0.08623965084552765, Entropy=0.34527787566185, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04525332897901535, KL divergence=0.11088614910840988, Entropy=0.3326321840286255, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.052936967462301254, KL divergence=0.11839916557073593, Entropy=0.33035123348236084, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.054556895047426224, KL divergence=0.12301623821258545, Entropy=0.33043041825294495, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05516117811203003, KL divergence=0.12623083591461182, Entropy=0.3294004201889038, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05621606484055519, KL divergence=0.13016439974308014, Entropy=0.3298324644565582, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.059964217245578766, KL divergence=0.1319839507341385, Entropy=0.3300270736217499, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.061313387006521225, KL divergence=0.13474532961845398, Entropy=0.3291376829147339, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06024862453341484, KL divergence=0.13703452050685883, Entropy=0.3300183415412903, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/258_Step-481580.ckpt']
Uploaded 3 files for checkpoint 258 in 0.50 seconds
saved intermediate frozen graph: current/model/model_258.pb
Best checkpoint number: 246, Last checkpoint number: 256
Copying the frozen checkpoint from ./frozen_models/agent/model_246.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'255'}
Training> Name=main_level/agent, Worker=0, Episode=5651, Total reward=434.74, Steps=481880, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5652, Total reward=194.4, Steps=482002, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5653, Total reward=81.26, Steps=482047, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5654, Total reward=67.44, Steps=482096, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5655, Total reward=373.66, Steps=482379, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5656, Total reward=320.12, Steps=482648, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5657, Total reward=134.61, Steps=482797, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5658, Total reward=142.79, Steps=482918, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5659, Total reward=353.95, Steps=483144, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5660, Total reward=7.95, Steps=483159, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5661, Total reward=261.42, Steps=483369, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5662, Total reward=47.64, Steps=483434, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5663, Total reward=107.15, Steps=483549, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5664, Total reward=230.85, Steps=483696, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5665, Total reward=176.85, Steps=483825, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5666, Total reward=192.55, Steps=483961, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5667, Total reward=78.21, Steps=484011, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5668, Total reward=96.27, Steps=484054, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5669, Total reward=16.61, Steps=484069, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5670, Total reward=133.5, Steps=484146, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5671, Total reward=87.09, Steps=484219, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5672, Total reward=236.37, Steps=484370, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5673, Total reward=56.5, Steps=484412, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5674, Total reward=318.41, Steps=484679, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5675, Total reward=353.18, Steps=484973, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5676, Total reward=97.04, Steps=485046, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5677, Total reward=92.56, Steps=485094, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5678, Total reward=336.56, Steps=485372, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5679, Total reward=251.14, Steps=485610, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5680, Total reward=7.83, Steps=485625, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5681, Total reward=83.58, Steps=485674, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5682, Total reward=156.64, Steps=485850, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5683, Total reward=232.37, Steps=486037, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5684, Total reward=20.52, Steps=486067, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5685, Total reward=208.57, Steps=486223, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5686, Total reward=208.47, Steps=486363, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5687, Total reward=143.48, Steps=486471, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5688, Total reward=163.63, Steps=486571, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5689, Total reward=30.27, Steps=486618, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5690, Total reward=49.63, Steps=486683, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5691, Total reward=48.48, Steps=486738, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5692, Total reward=410.35, Steps=487043, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5693, Total reward=55.6, Steps=487084, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5694, Total reward=405.55, Steps=487406, Training iteration=113
Training> Name=main_level/agent, Worker=0, Episode=5695, Total reward=431.45, Steps=487715, Training iteration=113
