21:C 18 Nov 2020 07:08:23.345 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
21:C 18 Nov 2020 07:08:23.345 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=21, just started
21:C 18 Nov 2020 07:08:23.345 # Configuration loaded
                _._                                                  
           _.-``__ ''-._                                             
      _.-``    `.  `_.  ''-._           Redis 5.0.8 (00000000/0) 64 bit
  .-`` .-```.  ```\/    _.,_ ''-._                                   
 (    '      ,       .-`  | `,    )     Running in standalone mode
 |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
 |    `-._   `._    /     _.-'    |     PID: 21
  `-._    `-._  `-./  _.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |           http://redis.io        
  `-._    `-._`-.__.-'_.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |                                  
  `-._    `-._`-.__.-'_.-'    _.-'                                   
      `-._    `-.__.-'    _.-'                                       
          `-._        _.-'                                           
              `-.__.-'                                               

21:M 18 Nov 2020 07:08:23.347 # WARNING: The TCP backlog setting of 512 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
21:M 18 Nov 2020 07:08:23.347 # Server initialized
21:M 18 Nov 2020 07:08:23.347 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
21:M 18 Nov 2020 07:08:23.347 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
21:M 18 Nov 2020 07:08:23.347 * Ready to accept connections
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2020-11-18 07:08:25,575 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training
2020-11-18 07:08:25,845 sagemaker-containers INFO     Invoking user script

Training Env:

{
    "additional_framework_parameters": {
        "sagemaker_estimator": "RLEstimator"
    },
    "channel_input_dirs": {},
    "current_host": "algo-1-e446d",
    "framework_module": "sagemaker_tensorflow_container.training:main",
    "hosts": [
        "algo-1-e446d"
    ],
    "hyperparameters": {
        "s3_bucket": "bucket",
        "s3_prefix": "current",
        "aws_region": "us-east-1",
        "model_metadata_s3_key": "s3://bucket/custom_files/model_metadata.json",
        "RLCOACH_PRESET": "deepracer",
        "batch_size": 512,
        "beta_entropy": 0.01,
        "discount_factor": 0.9995,
        "e_greedy_value": 0.05,
        "epsilon_steps": 10000,
        "exploration_type": "categorical",
        "loss_type": "huber",
        "lr": 5e-05,
        "num_episodes_between_training": 50,
        "num_epochs": 10,
        "stack_size": 1,
        "term_cond_avg_score": 100000.0,
        "term_cond_max_episodes": 10000,
        "pretrained_s3_bucket": "bucket",
        "pretrained_s3_prefix": "rl-deepracer-pretrained"
    },
    "input_config_dir": "/opt/ml/input/config",
    "input_data_config": {},
    "input_dir": "/opt/ml/input",
    "is_master": true,
    "job_name": "current",
    "log_level": 20,
    "master_hostname": "algo-1-e446d",
    "model_dir": "/opt/ml/model",
    "module_dir": "s3://bucket/current/source/sourcedir.tar.gz",
    "module_name": "training_worker",
    "network_interface_name": "eth0",
    "num_cpus": 16,
    "num_gpus": 3,
    "output_data_dir": "/opt/ml/output/data",
    "output_dir": "/opt/ml/output",
    "output_intermediate_dir": "/opt/ml/output/intermediate",
    "resource_config": {
        "current_host": "algo-1-e446d",
        "hosts": [
            "algo-1-e446d"
        ]
    },
    "user_entry_point": "training_worker.py"
}

Environment variables:

SM_HOSTS=["algo-1-e446d"]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":512,"beta_entropy":0.01,"discount_factor":0.9995,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"huber","lr":5e-05,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":50,"num_epochs":10,"pretrained_s3_bucket":"bucket","pretrained_s3_prefix":"rl-deepracer-pretrained","s3_bucket":"bucket","s3_prefix":"current","stack_size":1,"term_cond_avg_score":100000.0,"term_cond_max_episodes":10000}
SM_USER_ENTRY_POINT=training_worker.py
SM_FRAMEWORK_PARAMS={"sagemaker_estimator":"RLEstimator"}
SM_RESOURCE_CONFIG={"current_host":"algo-1-e446d","hosts":["algo-1-e446d"]}
SM_INPUT_DATA_CONFIG={}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=[]
SM_CURRENT_HOST=algo-1-e446d
SM_MODULE_NAME=training_worker
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=16
SM_NUM_GPUS=3
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://bucket/current/source/sourcedir.tar.gz
SM_TRAINING_ENV={"additional_framework_parameters":{"sagemaker_estimator":"RLEstimator"},"channel_input_dirs":{},"current_host":"algo-1-e446d","framework_module":"sagemaker_tensorflow_container.training:main","hosts":["algo-1-e446d"],"hyperparameters":{"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":512,"beta_entropy":0.01,"discount_factor":0.9995,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"huber","lr":5e-05,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":50,"num_epochs":10,"pretrained_s3_bucket":"bucket","pretrained_s3_prefix":"rl-deepracer-pretrained","s3_bucket":"bucket","s3_prefix":"current","stack_size":1,"term_cond_avg_score":100000.0,"term_cond_max_episodes":10000},"input_config_dir":"/opt/ml/input/config","input_data_config":{},"input_dir":"/opt/ml/input","is_master":true,"job_name":"current","log_level":20,"master_hostname":"algo-1-e446d","model_dir":"/opt/ml/model","module_dir":"s3://bucket/current/source/sourcedir.tar.gz","module_name":"training_worker","network_interface_name":"eth0","num_cpus":16,"num_gpus":3,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1-e446d","hosts":["algo-1-e446d"]},"user_entry_point":"training_worker.py"}
SM_USER_ARGS=["--RLCOACH_PRESET","deepracer","--aws_region","us-east-1","--batch_size","512","--beta_entropy","0.01","--discount_factor","0.9995","--e_greedy_value","0.05","--epsilon_steps","10000","--exploration_type","categorical","--loss_type","huber","--lr","5e-05","--model_metadata_s3_key","s3://bucket/custom_files/model_metadata.json","--num_episodes_between_training","50","--num_epochs","10","--pretrained_s3_bucket","bucket","--pretrained_s3_prefix","rl-deepracer-pretrained","--s3_bucket","bucket","--s3_prefix","current","--stack_size","1","--term_cond_avg_score","100000.0","--term_cond_max_episodes","10000"]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_HP_S3_BUCKET=bucket
SM_HP_S3_PREFIX=current
SM_HP_AWS_REGION=us-east-1
SM_HP_MODEL_METADATA_S3_KEY=s3://bucket/custom_files/model_metadata.json
SM_HP_RLCOACH_PRESET=deepracer
SM_HP_BATCH_SIZE=512
SM_HP_BETA_ENTROPY=0.01
SM_HP_DISCOUNT_FACTOR=0.9995
SM_HP_E_GREEDY_VALUE=0.05
SM_HP_EPSILON_STEPS=10000
SM_HP_EXPLORATION_TYPE=categorical
SM_HP_LOSS_TYPE=huber
SM_HP_LR=5e-05
SM_HP_NUM_EPISODES_BETWEEN_TRAINING=50
SM_HP_NUM_EPOCHS=10
SM_HP_STACK_SIZE=1
SM_HP_TERM_COND_AVG_SCORE=100000.0
SM_HP_TERM_COND_MAX_EPISODES=10000
SM_HP_PRETRAINED_S3_BUCKET=bucket
SM_HP_PRETRAINED_S3_PREFIX=rl-deepracer-pretrained
PYTHONPATH=/usr/local/bin:/opt/amazon:/opt/ml/code:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages

Invoking script with the following command:

/usr/bin/python training_worker.py --RLCOACH_PRESET deepracer --aws_region us-east-1 --batch_size 512 --beta_entropy 0.01 --discount_factor 0.9995 --e_greedy_value 0.05 --epsilon_steps 10000 --exploration_type categorical --loss_type huber --lr 5e-05 --model_metadata_s3_key s3://bucket/custom_files/model_metadata.json --num_episodes_between_training 50 --num_epochs 10 --pretrained_s3_bucket bucket --pretrained_s3_prefix rl-deepracer-pretrained --s3_bucket bucket --s3_prefix current --stack_size 1 --term_cond_avg_score 100000.0 --term_cond_max_episodes 10000


S3 bucket: bucket 
 S3 prefix: current 
 S3 endpoint URL: http://minio:9000
Initializing SageS3Client...
Successfully downloaded model metadata from custom_files/model_metadata.json.
Sensor list ['STEREO_CAMERAS', 'SECTOR_LIDAR'], network DEEP_CONVOLUTIONAL_NETWORK_SHALLOW, simapp_version 2.0
Loaded action space from file: [{'steering_angle': -30.0, 'speed': 2.0, 'index': 0}, {'steering_angle': -11.5842, 'speed': 2.5479, 'index': 1}, {'steering_angle': -5.649, 'speed': 3.8431, 'index': 2}, {'steering_angle': -0.054, 'speed': 4.7192, 'index': 3}, {'steering_angle': 0.7294, 'speed': 5.8319, 'index': 4}, {'steering_angle': 2.3994, 'speed': 2.376, 'index': 5}, {'steering_angle': 4.9649, 'speed': 3.0723, 'index': 6}, {'steering_angle': 8.8131, 'speed': 3.8434, 'index': 7}, {'steering_angle': 16.2503, 'speed': 2.3931, 'index': 8}, {'steering_angle': 30.0, 'speed': 2.0, 'index': 9}]
Using the following hyper-parameters
{
  "batch_size": 512,
  "beta_entropy": 0.01,
  "discount_factor": 0.9995,
  "e_greedy_value": 0.05,
  "epsilon_steps": 10000,
  "exploration_type": "categorical",
  "loss_type": "huber",
  "lr": 5e-05,
  "num_episodes_between_training": 50,
  "num_epochs": 10,
  "stack_size": 1,
  "term_cond_avg_score": 100000.0,
  "term_cond_max_episodes": 10000
}
Uploaded hyperparameters.json to S3
Uploaded IP address information to S3: 172.18.0.5
Sensor list ['STEREO_CAMERAS', 'SECTOR_LIDAR'], network DEEP_CONVOLUTIONAL_NETWORK_SHALLOW, simapp_version 2.0
Unable to find best model data, using last model
## Creating graph - name: MultiAgentGraphManager
## Start physics before creating graph
## Create graph
## Creating agent - name: agent
## Created agent: agent
## Stop physics after creating graph
## Creating session
Checkpoint> Restoring from path=./pretrained_checkpoint/291_Step-716500.ckpt
Checkpoint> Saving in path=['./checkpoint/292_Step-0.ckpt']
Uploaded 3 files for checkpoint 292 in 0.46 seconds
saved intermediate frozen graph: current/model/model_292.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Uploaded 3 files for checkpoint 292 in 0.63 seconds
saved intermediate frozen graph: current/model/model_292.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
DoorMan: installing SIGINT, SIGTERM
Training> Name=main_level/agent, Worker=0, Episode=1, Total reward=22.22, Steps=21, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=2, Total reward=45.25, Steps=63, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=3, Total reward=1.3, Steps=90, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=4, Total reward=11.93, Steps=104, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=5, Total reward=18.48, Steps=128, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=6, Total reward=68.65, Steps=180, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=7, Total reward=48.07, Steps=220, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=8, Total reward=63.1, Steps=276, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=9, Total reward=19.4, Steps=295, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=10, Total reward=57.43, Steps=343, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=11, Total reward=33.46, Steps=378, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=12, Total reward=15.85, Steps=401, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=13, Total reward=52.91, Steps=431, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=14, Total reward=40.9, Steps=453, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=15, Total reward=22.59, Steps=495, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=16, Total reward=19.14, Steps=513, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=17, Total reward=34.91, Steps=550, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=18, Total reward=42.05, Steps=571, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=19, Total reward=16.18, Steps=591, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=20, Total reward=75.84, Steps=625, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=21, Total reward=58.06, Steps=655, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=22, Total reward=28.49, Steps=676, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=23, Total reward=0.02, Steps=691, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=24, Total reward=6.57, Steps=720, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=25, Total reward=45.92, Steps=799, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=26, Total reward=59.25, Steps=872, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=27, Total reward=6.54, Steps=888, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=28, Total reward=53.54, Steps=914, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=29, Total reward=25.15, Steps=943, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=30, Total reward=35.34, Steps=994, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=31, Total reward=25.43, Steps=1023, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=32, Total reward=5.65, Steps=1034, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=33, Total reward=0.02, Steps=1052, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=34, Total reward=35.59, Steps=1071, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=35, Total reward=34.08, Steps=1100, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=36, Total reward=17.97, Steps=1131, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=37, Total reward=39.41, Steps=1163, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=38, Total reward=30.73, Steps=1183, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=39, Total reward=6.5, Steps=1205, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=40, Total reward=85.29, Steps=1244, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=41, Total reward=47.98, Steps=1270, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=42, Total reward=40.96, Steps=1298, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=43, Total reward=41.89, Steps=1359, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=44, Total reward=7.41, Steps=1383, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=45, Total reward=14.86, Steps=1407, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=46, Total reward=50.72, Steps=1447, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=47, Total reward=16.28, Steps=1466, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=48, Total reward=61.86, Steps=1510, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=49, Total reward=22.69, Steps=1531, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=50, Total reward=71.57, Steps=1567, Training iteration=0
Policy training> Surrogate loss=0.0031735326629132032, KL divergence=0.0003150120028294623, Entropy=0.28396400809288025, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04060187190771103, KL divergence=0.010764852166175842, Entropy=0.2795100808143616, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.042236194014549255, KL divergence=0.02359442226588726, Entropy=0.2743164598941803, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04903234541416168, KL divergence=0.0374242402613163, Entropy=0.27153950929641724, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05981403589248657, KL divergence=0.05004400014877319, Entropy=0.26978734135627747, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0648772343993187, KL divergence=0.06073083356022835, Entropy=0.2655197083950043, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06244410201907158, KL divergence=0.07333839684724808, Entropy=0.26671668887138367, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06530485302209854, KL divergence=0.08332286030054092, Entropy=0.2675231695175171, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.057693708688020706, KL divergence=0.09242319315671921, Entropy=0.2660599648952484, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06654481589794159, KL divergence=0.10055341571569443, Entropy=0.26910069584846497, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/293_Step-1567.ckpt']
Uploaded 3 files for checkpoint 293 in 0.57 seconds
saved intermediate frozen graph: current/model/model_293.pb
Best checkpoint number: 292, Last checkpoint number: 292
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=51, Total reward=70.18, Steps=1608, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=52, Total reward=8.76, Steps=1628, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=53, Total reward=44.93, Steps=1658, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=54, Total reward=42.11, Steps=1680, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=55, Total reward=34.16, Steps=1712, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=56, Total reward=21.97, Steps=1736, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=57, Total reward=25.16, Steps=1754, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=58, Total reward=26.21, Steps=1773, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=59, Total reward=50.86, Steps=1819, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=60, Total reward=22.87, Steps=1855, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=61, Total reward=35.77, Steps=1879, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=62, Total reward=30.77, Steps=1903, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=63, Total reward=3.4, Steps=1923, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=64, Total reward=17.97, Steps=1955, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=65, Total reward=41.03, Steps=2037, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=66, Total reward=50.01, Steps=2082, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=67, Total reward=9.37, Steps=2105, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=68, Total reward=50.1, Steps=2132, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=69, Total reward=85.05, Steps=2192, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=70, Total reward=67.46, Steps=2239, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=71, Total reward=36.26, Steps=2266, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=72, Total reward=9.46, Steps=2287, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=73, Total reward=46.2, Steps=2317, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=74, Total reward=27.08, Steps=2328, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=75, Total reward=21.67, Steps=2376, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=76, Total reward=24.94, Steps=2395, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=77, Total reward=39.19, Steps=2429, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=78, Total reward=32.99, Steps=2450, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=79, Total reward=43.33, Steps=2489, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=80, Total reward=23.92, Steps=2519, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=81, Total reward=38.43, Steps=2543, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=82, Total reward=32.77, Steps=2566, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=83, Total reward=5.2, Steps=2604, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=84, Total reward=17.08, Steps=2628, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=85, Total reward=19.8, Steps=2652, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=86, Total reward=7.29, Steps=2675, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=87, Total reward=28.97, Steps=2708, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=88, Total reward=43.62, Steps=2733, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=89, Total reward=19.59, Steps=2762, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=90, Total reward=45.01, Steps=2799, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=91, Total reward=20.96, Steps=2830, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=92, Total reward=3.75, Steps=2841, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=93, Total reward=36.3, Steps=2862, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=94, Total reward=23.06, Steps=2883, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=95, Total reward=28.42, Steps=2913, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=96, Total reward=19.75, Steps=2930, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=97, Total reward=1.87, Steps=2946, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=98, Total reward=34.71, Steps=2972, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=99, Total reward=24.84, Steps=2990, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=100, Total reward=68.89, Steps=3025, Training iteration=1
Policy training> Surrogate loss=-0.032706119120121, KL divergence=2.674377719813492e-05, Entropy=0.28475743532180786, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.010500905103981495, KL divergence=0.0014691652031615376, Entropy=0.28922173380851746, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.030588679015636444, KL divergence=0.006142395548522472, Entropy=0.2907344102859497, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0432087779045105, KL divergence=0.012952066026628017, Entropy=0.29058605432510376, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.02730157971382141, KL divergence=0.02280844748020172, Entropy=0.2961695194244385, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.03637174144387245, KL divergence=0.033950574696063995, Entropy=0.28408974409103394, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04177025705575943, KL divergence=0.043571583926677704, Entropy=0.28539836406707764, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.050457462668418884, KL divergence=0.04909100383520126, Entropy=0.2729465961456299, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06266260892152786, KL divergence=0.06276252865791321, Entropy=0.27951961755752563, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.04947624355554581, KL divergence=0.06912808865308762, Entropy=0.2723492980003357, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/294_Step-3025.ckpt']
Uploaded 3 files for checkpoint 294 in 0.62 seconds
saved intermediate frozen graph: current/model/model_294.pb
Best checkpoint number: 292, Last checkpoint number: 292
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=101, Total reward=44.39, Steps=3051, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=102, Total reward=32.47, Steps=3074, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=103, Total reward=7.23, Steps=3107, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=104, Total reward=12.54, Steps=3123, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=105, Total reward=19.19, Steps=3139, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=106, Total reward=3.6, Steps=3151, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=107, Total reward=48.71, Steps=3190, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=108, Total reward=65.95, Steps=3221, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=109, Total reward=62.48, Steps=3268, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=110, Total reward=58.12, Steps=3313, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=111, Total reward=15.62, Steps=3346, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=112, Total reward=59.22, Steps=3380, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=113, Total reward=46.63, Steps=3411, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=114, Total reward=39.37, Steps=3432, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=115, Total reward=27.84, Steps=3462, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=116, Total reward=22.66, Steps=3491, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=117, Total reward=50.15, Steps=3530, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=118, Total reward=29.14, Steps=3555, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=119, Total reward=12.15, Steps=3583, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=120, Total reward=73.33, Steps=3617, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=121, Total reward=30.19, Steps=3642, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=122, Total reward=38.58, Steps=3666, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=123, Total reward=62.83, Steps=3741, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=124, Total reward=0.02, Steps=3756, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=125, Total reward=8.27, Steps=3773, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=126, Total reward=44.66, Steps=3803, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=127, Total reward=53.5, Steps=3854, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=128, Total reward=57.76, Steps=3884, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=129, Total reward=31.6, Steps=3912, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=130, Total reward=54.02, Steps=3958, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=131, Total reward=30.86, Steps=3987, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=132, Total reward=7.54, Steps=3998, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=133, Total reward=34.35, Steps=4024, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=134, Total reward=29.7, Steps=4035, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=135, Total reward=22.29, Steps=4071, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=136, Total reward=17.42, Steps=4105, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=137, Total reward=42.52, Steps=4139, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=138, Total reward=34.41, Steps=4162, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=139, Total reward=3.81, Steps=4181, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=140, Total reward=47.99, Steps=4223, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=141, Total reward=54.62, Steps=4252, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=142, Total reward=25.65, Steps=4291, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=143, Total reward=3.89, Steps=4304, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=144, Total reward=15.34, Steps=4343, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=145, Total reward=20.53, Steps=4361, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=146, Total reward=61.97, Steps=4405, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=147, Total reward=26.18, Steps=4436, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=148, Total reward=55.1, Steps=4463, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=149, Total reward=16.32, Steps=4489, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=150, Total reward=10.98, Steps=4503, Training iteration=2
Policy training> Surrogate loss=0.012003034353256226, KL divergence=1.924300886457786e-05, Entropy=0.3058459162712097, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.027665603905916214, KL divergence=0.001089209457859397, Entropy=0.30881696939468384, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.040684014558792114, KL divergence=0.0056195384822785854, Entropy=0.28553780913352966, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03723921626806259, KL divergence=0.011956711299717426, Entropy=0.2898433208465576, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05315903201699257, KL divergence=0.022891107946634293, Entropy=0.29650020599365234, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.02103351429104805, KL divergence=0.029886826872825623, Entropy=0.2804988622665405, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0529000386595726, KL divergence=0.04200195148587227, Entropy=0.29477399587631226, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.040765225887298584, KL divergence=0.046622905880212784, Entropy=0.28305840492248535, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04425901919603348, KL divergence=0.060222938656806946, Entropy=0.28724002838134766, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06227943301200867, KL divergence=0.06563892215490341, Entropy=0.2869398593902588, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/295_Step-4503.ckpt']
Uploaded 3 files for checkpoint 295 in 0.64 seconds
saved intermediate frozen graph: current/model/model_295.pb
Best checkpoint number: 292, Last checkpoint number: 293
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'293'}
Training> Name=main_level/agent, Worker=0, Episode=151, Total reward=49.71, Steps=4543, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=152, Total reward=21.52, Steps=4570, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=153, Total reward=46.38, Steps=4599, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=154, Total reward=50.73, Steps=4622, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=155, Total reward=16.21, Steps=4652, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=156, Total reward=23.22, Steps=4672, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=157, Total reward=38.25, Steps=4706, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=158, Total reward=33.56, Steps=4728, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=159, Total reward=65.61, Steps=4775, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=160, Total reward=62.78, Steps=4807, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=161, Total reward=56.8, Steps=4840, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=162, Total reward=35.05, Steps=4862, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=163, Total reward=0.02, Steps=4884, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=164, Total reward=12.85, Steps=4903, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=165, Total reward=52.58, Steps=4956, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=166, Total reward=29.53, Steps=5009, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=167, Total reward=9.8, Steps=5026, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=168, Total reward=51.17, Steps=5054, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=169, Total reward=25.78, Steps=5074, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=170, Total reward=28.75, Steps=5117, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=171, Total reward=35.02, Steps=5144, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=172, Total reward=45.83, Steps=5176, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=173, Total reward=25.33, Steps=5197, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=174, Total reward=19.75, Steps=5220, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=175, Total reward=38.68, Steps=5253, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=176, Total reward=15.61, Steps=5269, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=177, Total reward=38.51, Steps=5306, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=178, Total reward=35.55, Steps=5331, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=179, Total reward=0.01, Steps=5345, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=180, Total reward=64.52, Steps=5386, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=181, Total reward=28.6, Steps=5410, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=182, Total reward=32.05, Steps=5434, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=183, Total reward=3.3, Steps=5458, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=184, Total reward=14.32, Steps=5483, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=185, Total reward=49.5, Steps=5537, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=186, Total reward=15.0, Steps=5554, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=187, Total reward=37.66, Steps=5589, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=188, Total reward=59.62, Steps=5617, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=189, Total reward=35.61, Steps=5641, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=190, Total reward=21.54, Steps=5666, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=191, Total reward=19.94, Steps=5698, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=192, Total reward=55.41, Steps=5732, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=193, Total reward=47.13, Steps=5754, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=194, Total reward=33.41, Steps=5775, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=195, Total reward=16.58, Steps=5821, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=196, Total reward=20.61, Steps=5842, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=197, Total reward=19.17, Steps=5865, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=198, Total reward=39.45, Steps=5889, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=199, Total reward=8.04, Steps=5906, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=200, Total reward=54.84, Steps=5945, Training iteration=3
Policy training> Surrogate loss=0.0020366841927170753, KL divergence=1.6003185010049492e-05, Entropy=0.276117742061615, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0403071753680706, KL divergence=0.0012448860798031092, Entropy=0.2737114727497101, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.028051121160387993, KL divergence=0.005118913017213345, Entropy=0.26566940546035767, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03651542216539383, KL divergence=0.012212889268994331, Entropy=0.2773165702819824, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.01598924957215786, KL divergence=0.019672323018312454, Entropy=0.27785027027130127, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04410892352461815, KL divergence=0.030050130560994148, Entropy=0.2662714421749115, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=0.01105134841054678, KL divergence=0.03558163717389107, Entropy=0.27095162868499756, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07548409700393677, KL divergence=0.05124502629041672, Entropy=0.26339301466941833, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.028877034783363342, KL divergence=0.060411080718040466, Entropy=0.26807570457458496, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.034810297191143036, KL divergence=0.06731459498405457, Entropy=0.2611204981803894, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/296_Step-5945.ckpt']
Uploaded 3 files for checkpoint 296 in 0.56 seconds
saved intermediate frozen graph: current/model/model_296.pb
Best checkpoint number: 292, Last checkpoint number: 294
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'294'}
Training> Name=main_level/agent, Worker=0, Episode=201, Total reward=34.72, Steps=5972, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=202, Total reward=31.49, Steps=5996, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=203, Total reward=20.33, Steps=6026, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=204, Total reward=19.27, Steps=6061, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=205, Total reward=20.18, Steps=6081, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=206, Total reward=55.6, Steps=6129, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=207, Total reward=3.3, Steps=6152, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=208, Total reward=60.43, Steps=6195, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=209, Total reward=27.58, Steps=6224, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=210, Total reward=24.21, Steps=6257, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=211, Total reward=14.25, Steps=6278, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=212, Total reward=26.42, Steps=6306, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=213, Total reward=34.96, Steps=6326, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=214, Total reward=33.03, Steps=6348, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=215, Total reward=38.24, Steps=6379, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=216, Total reward=17.13, Steps=6411, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=217, Total reward=150.71, Steps=6559, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=218, Total reward=48.01, Steps=6581, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=219, Total reward=4.42, Steps=6602, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=220, Total reward=77.41, Steps=6640, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=221, Total reward=55.53, Steps=6673, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=222, Total reward=37.99, Steps=6699, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=223, Total reward=0.02, Steps=6719, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=224, Total reward=12.27, Steps=6753, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=225, Total reward=16.67, Steps=6779, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=226, Total reward=66.09, Steps=6831, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=227, Total reward=56.93, Steps=6868, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=228, Total reward=41.12, Steps=6893, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=229, Total reward=53.82, Steps=6954, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=230, Total reward=62.1, Steps=6991, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=231, Total reward=44.16, Steps=7030, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=232, Total reward=47.77, Steps=7061, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=233, Total reward=38.14, Steps=7090, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=234, Total reward=22.01, Steps=7100, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=235, Total reward=24.38, Steps=7129, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=236, Total reward=16.63, Steps=7153, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=237, Total reward=35.95, Steps=7175, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=238, Total reward=44.01, Steps=7197, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=239, Total reward=82.59, Steps=7278, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=240, Total reward=57.75, Steps=7314, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=241, Total reward=48.09, Steps=7341, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=242, Total reward=39.03, Steps=7385, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=243, Total reward=0.02, Steps=7407, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=244, Total reward=21.91, Steps=7441, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=245, Total reward=11.33, Steps=7458, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=246, Total reward=60.21, Steps=7501, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=247, Total reward=0.01, Steps=7512, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=248, Total reward=50.81, Steps=7538, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=249, Total reward=26.35, Steps=7554, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=250, Total reward=73.8, Steps=7601, Training iteration=4
Policy training> Surrogate loss=-0.009890936315059662, KL divergence=4.5520351704908535e-05, Entropy=0.3111403286457062, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.022290190681815147, KL divergence=0.004367294255644083, Entropy=0.3127310574054718, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04700290784239769, KL divergence=0.016066689044237137, Entropy=0.3019709289073944, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04637088254094124, KL divergence=0.031162044033408165, Entropy=0.29993727803230286, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04098004475235939, KL divergence=0.046854857355356216, Entropy=0.2972118556499481, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.057475995272397995, KL divergence=0.0636904314160347, Entropy=0.2950601875782013, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.055764708667993546, KL divergence=0.07460976392030716, Entropy=0.2901240289211273, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05680355057120323, KL divergence=0.08532338589429855, Entropy=0.28490138053894043, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05178605392575264, KL divergence=0.09575995802879333, Entropy=0.28838613629341125, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05138079449534416, KL divergence=0.10389088839292526, Entropy=0.2881235182285309, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/297_Step-7601.ckpt']
Uploaded 3 files for checkpoint 297 in 0.59 seconds
saved intermediate frozen graph: current/model/model_297.pb
Best checkpoint number: 292, Last checkpoint number: 295
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'295'}
Training> Name=main_level/agent, Worker=0, Episode=251, Total reward=55.98, Steps=7641, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=252, Total reward=51.1, Steps=7683, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=253, Total reward=27.49, Steps=7703, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=254, Total reward=47.69, Steps=7727, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=255, Total reward=20.48, Steps=7756, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=256, Total reward=22.21, Steps=7790, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=257, Total reward=43.09, Steps=7818, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=258, Total reward=23.33, Steps=7830, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=259, Total reward=4.7, Steps=7844, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=260, Total reward=39.14, Steps=7875, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=261, Total reward=41.95, Steps=7900, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=262, Total reward=34.11, Steps=7936, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=263, Total reward=3.26, Steps=7957, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=264, Total reward=12.44, Steps=7979, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=265, Total reward=68.1, Steps=8035, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=266, Total reward=18.23, Steps=8062, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=267, Total reward=50.97, Steps=8099, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=268, Total reward=38.09, Steps=8124, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=269, Total reward=28.6, Steps=8145, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=270, Total reward=11.61, Steps=8175, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=271, Total reward=51.21, Steps=8218, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=272, Total reward=53.26, Steps=8251, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=273, Total reward=29.39, Steps=8272, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=274, Total reward=32.57, Steps=8293, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=275, Total reward=25.69, Steps=8324, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=276, Total reward=17.24, Steps=8356, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=277, Total reward=92.38, Steps=8442, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=278, Total reward=35.67, Steps=8468, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=279, Total reward=19.77, Steps=8500, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=280, Total reward=55.17, Steps=8535, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=281, Total reward=39.24, Steps=8563, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=282, Total reward=39.11, Steps=8596, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=283, Total reward=66.45, Steps=8678, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=284, Total reward=58.85, Steps=8746, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=285, Total reward=14.09, Steps=8768, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=286, Total reward=27.1, Steps=8799, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=287, Total reward=56.87, Steps=8836, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=288, Total reward=38.46, Steps=8860, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=289, Total reward=20.61, Steps=8887, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=290, Total reward=39.87, Steps=8923, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=291, Total reward=25.6, Steps=8962, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=292, Total reward=15.93, Steps=8990, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=293, Total reward=33.46, Steps=9014, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=294, Total reward=25.22, Steps=9024, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=295, Total reward=11.19, Steps=9054, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=296, Total reward=44.12, Steps=9100, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=297, Total reward=53.74, Steps=9136, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=298, Total reward=29.93, Steps=9160, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=299, Total reward=21.55, Steps=9184, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=300, Total reward=21.64, Steps=9203, Training iteration=5
Policy training> Surrogate loss=0.011800989508628845, KL divergence=7.247160829138011e-05, Entropy=0.3052745759487152, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.018830204382538795, KL divergence=0.005232608411461115, Entropy=0.3003368675708771, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03458402678370476, KL divergence=0.017048001289367676, Entropy=0.2967725098133087, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04395603761076927, KL divergence=0.03425710275769234, Entropy=0.2920416593551636, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.049620043486356735, KL divergence=0.050983916968107224, Entropy=0.2891647517681122, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05860162898898125, KL divergence=0.06593554466962814, Entropy=0.28430551290512085, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.053575921803712845, KL divergence=0.0807323232293129, Entropy=0.28447726368904114, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05998006463050842, KL divergence=0.09364240616559982, Entropy=0.2825327515602112, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06328720599412918, KL divergence=0.10096568614244461, Entropy=0.28117993474006653, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.059220362454652786, KL divergence=0.11016645282506943, Entropy=0.28103211522102356, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/298_Step-9203.ckpt']
Uploaded 3 files for checkpoint 298 in 0.52 seconds
saved intermediate frozen graph: current/model/model_298.pb
Best checkpoint number: 292, Last checkpoint number: 296
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'296'}
Training> Name=main_level/agent, Worker=0, Episode=301, Total reward=34.3, Steps=9229, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=302, Total reward=39.48, Steps=9273, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=303, Total reward=40.9, Steps=9347, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=304, Total reward=3.58, Steps=9376, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=305, Total reward=11.06, Steps=9416, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=306, Total reward=49.93, Steps=9473, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=307, Total reward=7.56, Steps=9499, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=308, Total reward=44.31, Steps=9522, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=309, Total reward=74.66, Steps=9574, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=310, Total reward=75.91, Steps=9625, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=311, Total reward=34.29, Steps=9656, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=312, Total reward=47.54, Steps=9688, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=313, Total reward=46.39, Steps=9717, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=314, Total reward=45.33, Steps=9740, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=315, Total reward=19.17, Steps=9773, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=316, Total reward=12.86, Steps=9805, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=317, Total reward=65.0, Steps=9845, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=318, Total reward=38.93, Steps=9868, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=319, Total reward=7.51, Steps=9885, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=320, Total reward=60.83, Steps=9924, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=321, Total reward=50.72, Steps=9954, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=322, Total reward=33.17, Steps=9974, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=323, Total reward=14.66, Steps=10009, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=324, Total reward=1.18, Steps=10036, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=325, Total reward=14.91, Steps=10072, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=326, Total reward=70.04, Steps=10114, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=327, Total reward=3.29, Steps=10142, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=328, Total reward=65.8, Steps=10173, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=329, Total reward=19.9, Steps=10190, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=330, Total reward=41.97, Steps=10226, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=331, Total reward=28.76, Steps=10268, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=332, Total reward=9.35, Steps=10290, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=333, Total reward=47.27, Steps=10321, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=334, Total reward=46.53, Steps=10344, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=335, Total reward=31.16, Steps=10373, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=336, Total reward=24.36, Steps=10394, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=337, Total reward=45.03, Steps=10433, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=338, Total reward=32.9, Steps=10456, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=339, Total reward=70.84, Steps=10523, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=340, Total reward=53.42, Steps=10552, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=341, Total reward=24.13, Steps=10580, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=342, Total reward=31.87, Steps=10600, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=343, Total reward=3.54, Steps=10619, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=344, Total reward=7.66, Steps=10646, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=345, Total reward=32.55, Steps=10698, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=346, Total reward=18.16, Steps=10728, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=347, Total reward=53.84, Steps=10762, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=348, Total reward=43.02, Steps=10787, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=349, Total reward=20.45, Steps=10802, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=350, Total reward=63.41, Steps=10850, Training iteration=6
Policy training> Surrogate loss=0.0017196949338540435, KL divergence=0.00010939787171082571, Entropy=0.3165510594844818, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.016129055991768837, KL divergence=0.00590972974896431, Entropy=0.31328490376472473, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04778946936130524, KL divergence=0.01841125637292862, Entropy=0.3097088038921356, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05774465203285217, KL divergence=0.03557109460234642, Entropy=0.30220237374305725, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05100502446293831, KL divergence=0.05176876485347748, Entropy=0.29751160740852356, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05972588062286377, KL divergence=0.06591083854436874, Entropy=0.2895607650279999, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06117257475852966, KL divergence=0.08472337573766708, Entropy=0.29333314299583435, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.057253655046224594, KL divergence=0.09510939568281174, Entropy=0.28720101714134216, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05742144584655762, KL divergence=0.10453899949789047, Entropy=0.2867804169654846, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05693623796105385, KL divergence=0.11516179889440536, Entropy=0.2860659062862396, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/299_Step-10850.ckpt']
Uploaded 3 files for checkpoint 299 in 0.62 seconds
saved intermediate frozen graph: current/model/model_299.pb
Best checkpoint number: 292, Last checkpoint number: 297
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'297'}
Training> Name=main_level/agent, Worker=0, Episode=351, Total reward=60.02, Steps=10887, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=352, Total reward=10.97, Steps=10910, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=353, Total reward=44.17, Steps=10940, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=354, Total reward=32.29, Steps=10961, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=355, Total reward=32.7, Steps=10990, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=356, Total reward=12.59, Steps=11017, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=357, Total reward=25.74, Steps=11057, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=358, Total reward=32.35, Steps=11080, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=359, Total reward=6.05, Steps=11093, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=360, Total reward=50.46, Steps=11125, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=361, Total reward=32.21, Steps=11149, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=362, Total reward=34.0, Steps=11172, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=363, Total reward=3.43, Steps=11191, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=364, Total reward=19.7, Steps=11215, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=365, Total reward=14.78, Steps=11240, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=366, Total reward=14.78, Steps=11255, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=367, Total reward=47.73, Steps=11295, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=368, Total reward=62.08, Steps=11323, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=369, Total reward=27.51, Steps=11343, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=370, Total reward=17.56, Steps=11368, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=371, Total reward=41.2, Steps=11407, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=372, Total reward=59.65, Steps=11451, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=373, Total reward=25.29, Steps=11472, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=374, Total reward=40.81, Steps=11494, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=375, Total reward=17.66, Steps=11521, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=376, Total reward=11.51, Steps=11554, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=377, Total reward=41.95, Steps=11590, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=378, Total reward=31.17, Steps=11612, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=379, Total reward=71.87, Steps=11663, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=380, Total reward=47.76, Steps=11694, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=381, Total reward=31.85, Steps=11722, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=382, Total reward=30.69, Steps=11746, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=383, Total reward=6.63, Steps=11768, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=384, Total reward=11.52, Steps=11791, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=385, Total reward=6.05, Steps=11808, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=386, Total reward=34.7, Steps=11854, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=387, Total reward=6.65, Steps=11887, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=388, Total reward=65.07, Steps=11933, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=389, Total reward=18.88, Steps=11947, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=390, Total reward=47.1, Steps=11994, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=391, Total reward=35.12, Steps=12029, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=392, Total reward=46.23, Steps=12061, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=393, Total reward=45.71, Steps=12090, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=394, Total reward=43.95, Steps=12113, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=395, Total reward=30.94, Steps=12144, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=396, Total reward=15.76, Steps=12163, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=397, Total reward=12.89, Steps=12183, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=398, Total reward=28.86, Steps=12200, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=399, Total reward=16.2, Steps=12219, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=400, Total reward=83.47, Steps=12255, Training iteration=7
Policy training> Surrogate loss=0.00925234705209732, KL divergence=1.5834095393074676e-05, Entropy=0.3031160235404968, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.016305873170495033, KL divergence=0.0021239861380308867, Entropy=0.3014490306377411, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.06088647246360779, KL divergence=0.009373411536216736, Entropy=0.29957228899002075, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04018884897232056, KL divergence=0.021352704614400864, Entropy=0.2937507927417755, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04587842896580696, KL divergence=0.03428874909877777, Entropy=0.28853756189346313, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04924584552645683, KL divergence=0.05066453665494919, Entropy=0.28360897302627563, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.03251189738512039, KL divergence=0.06439661979675293, Entropy=0.280272901058197, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06809228658676147, KL divergence=0.07791313529014587, Entropy=0.27810442447662354, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04588812217116356, KL divergence=0.09206487238407135, Entropy=0.28590691089630127, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.018833409994840622, KL divergence=0.10433395206928253, Entropy=0.28680604696273804, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/300_Step-12255.ckpt']
Uploaded 3 files for checkpoint 300 in 0.54 seconds
saved intermediate frozen graph: current/model/model_300.pb
Best checkpoint number: 292, Last checkpoint number: 298
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'298'}
Training> Name=main_level/agent, Worker=0, Episode=401, Total reward=59.98, Steps=12284, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=402, Total reward=29.29, Steps=12314, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=403, Total reward=14.2, Steps=12349, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=404, Total reward=83.97, Steps=12417, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=405, Total reward=22.59, Steps=12433, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=406, Total reward=9.57, Steps=12458, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=407, Total reward=47.92, Steps=12497, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=408, Total reward=50.84, Steps=12523, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=409, Total reward=28.24, Steps=12550, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=410, Total reward=43.6, Steps=12600, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=411, Total reward=46.07, Steps=12638, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=412, Total reward=56.97, Steps=12670, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=413, Total reward=37.94, Steps=12692, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=414, Total reward=44.41, Steps=12714, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=415, Total reward=11.39, Steps=12739, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=416, Total reward=17.16, Steps=12771, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=417, Total reward=17.59, Steps=12791, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=418, Total reward=34.3, Steps=12815, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=419, Total reward=67.73, Steps=12863, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=420, Total reward=35.42, Steps=12894, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=421, Total reward=34.24, Steps=12921, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=422, Total reward=26.81, Steps=12941, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=423, Total reward=7.49, Steps=12979, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=424, Total reward=2.85, Steps=12993, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=425, Total reward=17.33, Steps=13009, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=426, Total reward=57.94, Steps=13057, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=427, Total reward=54.56, Steps=13095, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=428, Total reward=71.48, Steps=13138, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=429, Total reward=63.93, Steps=13195, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=430, Total reward=44.72, Steps=13240, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=431, Total reward=11.9, Steps=13263, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=432, Total reward=51.89, Steps=13296, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=433, Total reward=0.02, Steps=13311, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=434, Total reward=29.16, Steps=13329, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=435, Total reward=34.88, Steps=13359, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=436, Total reward=22.36, Steps=13389, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=437, Total reward=24.15, Steps=13416, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=438, Total reward=40.17, Steps=13453, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=439, Total reward=60.04, Steps=13519, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=440, Total reward=63.87, Steps=13554, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=441, Total reward=32.88, Steps=13580, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=442, Total reward=24.59, Steps=13599, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=443, Total reward=0.03, Steps=13626, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=444, Total reward=54.11, Steps=13691, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=445, Total reward=41.89, Steps=13748, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=446, Total reward=64.48, Steps=13794, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=447, Total reward=62.87, Steps=13834, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=448, Total reward=20.4, Steps=13848, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=449, Total reward=85.03, Steps=13909, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=450, Total reward=38.48, Steps=13947, Training iteration=8
Policy training> Surrogate loss=-0.0014118632534518838, KL divergence=0.00013425452925730497, Entropy=0.30762913823127747, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.023611219599843025, KL divergence=0.005682843271642923, Entropy=0.30936357378959656, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04253971204161644, KL divergence=0.01955396682024002, Entropy=0.30750396847724915, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05069729685783386, KL divergence=0.034732360392808914, Entropy=0.29911842942237854, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05946063995361328, KL divergence=0.05331679806113243, Entropy=0.2880348861217499, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06680601835250854, KL divergence=0.06947837024927139, Entropy=0.2926619350910187, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06191181018948555, KL divergence=0.07871336489915848, Entropy=0.2827279269695282, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0679122731089592, KL divergence=0.09099764376878738, Entropy=0.28652501106262207, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.058573734015226364, KL divergence=0.10247009247541428, Entropy=0.28871414065361023, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06588398665189743, KL divergence=0.10763087123632431, Entropy=0.27462729811668396, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/301_Step-13947.ckpt']
Uploaded 3 files for checkpoint 301 in 0.60 seconds
saved intermediate frozen graph: current/model/model_301.pb
Best checkpoint number: 292, Last checkpoint number: 299
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'299'}
Training> Name=main_level/agent, Worker=0, Episode=451, Total reward=52.46, Steps=13984, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=452, Total reward=49.3, Steps=14016, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=453, Total reward=45.1, Steps=14043, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=454, Total reward=26.38, Steps=14054, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=455, Total reward=32.33, Steps=14083, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=456, Total reward=17.7, Steps=14115, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=457, Total reward=42.98, Steps=14154, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=458, Total reward=22.9, Steps=14174, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=459, Total reward=60.81, Steps=14238, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=460, Total reward=55.68, Steps=14270, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=461, Total reward=36.64, Steps=14293, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=462, Total reward=33.6, Steps=14314, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=463, Total reward=28.44, Steps=14391, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=464, Total reward=12.73, Steps=14416, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=465, Total reward=51.53, Steps=14470, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=466, Total reward=49.15, Steps=14566, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=467, Total reward=6.56, Steps=14584, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=468, Total reward=55.38, Steps=14611, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=469, Total reward=27.95, Steps=14631, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=470, Total reward=14.76, Steps=14644, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=471, Total reward=42.0, Steps=14671, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=472, Total reward=16.42, Steps=14697, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=473, Total reward=11.71, Steps=14715, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=474, Total reward=31.98, Steps=14737, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=475, Total reward=23.48, Steps=14782, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=476, Total reward=21.68, Steps=14816, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=477, Total reward=42.28, Steps=14850, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=478, Total reward=29.68, Steps=14873, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=479, Total reward=55.02, Steps=14915, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=480, Total reward=36.69, Steps=14948, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=481, Total reward=31.19, Steps=14975, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=482, Total reward=35.19, Steps=14997, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=483, Total reward=9.92, Steps=15030, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=484, Total reward=6.62, Steps=15047, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=485, Total reward=55.76, Steps=15100, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=486, Total reward=29.64, Steps=15133, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=487, Total reward=48.54, Steps=15187, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=488, Total reward=71.07, Steps=15227, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=489, Total reward=62.75, Steps=15284, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=490, Total reward=11.21, Steps=15311, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=491, Total reward=18.47, Steps=15341, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=492, Total reward=3.63, Steps=15352, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=493, Total reward=43.88, Steps=15380, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=494, Total reward=34.38, Steps=15402, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=495, Total reward=22.5, Steps=15433, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=496, Total reward=14.29, Steps=15467, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=497, Total reward=52.87, Steps=15503, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=498, Total reward=82.78, Steps=15573, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=499, Total reward=31.82, Steps=15617, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=500, Total reward=36.04, Steps=15651, Training iteration=9
Policy training> Surrogate loss=-0.004861585330218077, KL divergence=0.00012432811490725726, Entropy=0.31015077233314514, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.021245120093226433, KL divergence=0.007782701402902603, Entropy=0.3044460117816925, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03488064929842949, KL divergence=0.02495230734348297, Entropy=0.29152074456214905, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04362768307328224, KL divergence=0.046097636222839355, Entropy=0.2940821051597595, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06724655628204346, KL divergence=0.06593495607376099, Entropy=0.2818109393119812, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.053803328424692154, KL divergence=0.08365374803543091, Entropy=0.2806358337402344, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06359262019395828, KL divergence=0.10037198662757874, Entropy=0.27831581234931946, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.051902804523706436, KL divergence=0.11076771467924118, Entropy=0.27736523747444153, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06787485629320145, KL divergence=0.12496846914291382, Entropy=0.282527357339859, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.04766270890831947, KL divergence=0.1336912363767624, Entropy=0.28189408779144287, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/302_Step-15651.ckpt']
Uploaded 3 files for checkpoint 302 in 0.55 seconds
saved intermediate frozen graph: current/model/model_302.pb
Best checkpoint number: 292, Last checkpoint number: 300
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'300'}
Training> Name=main_level/agent, Worker=0, Episode=501, Total reward=31.26, Steps=15678, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=502, Total reward=29.71, Steps=15701, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=503, Total reward=0.02, Steps=15718, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=504, Total reward=21.71, Steps=15746, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=505, Total reward=34.89, Steps=15803, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=506, Total reward=21.91, Steps=15828, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=507, Total reward=22.43, Steps=15871, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=508, Total reward=49.38, Steps=15896, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=509, Total reward=28.25, Steps=15916, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=510, Total reward=62.05, Steps=15964, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=511, Total reward=63.11, Steps=16001, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=512, Total reward=41.06, Steps=16033, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=513, Total reward=40.26, Steps=16059, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=514, Total reward=39.56, Steps=16080, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=515, Total reward=17.45, Steps=16112, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=516, Total reward=17.32, Steps=16132, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=517, Total reward=13.87, Steps=16153, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=518, Total reward=40.95, Steps=16177, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=519, Total reward=77.27, Steps=16225, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=520, Total reward=59.69, Steps=16262, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=521, Total reward=34.41, Steps=16286, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=522, Total reward=27.47, Steps=16304, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=523, Total reward=2.92, Steps=16331, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=524, Total reward=18.21, Steps=16376, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=525, Total reward=85.5, Steps=16429, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=526, Total reward=38.56, Steps=16467, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=527, Total reward=9.47, Steps=16500, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=528, Total reward=55.62, Steps=16525, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=529, Total reward=20.89, Steps=16545, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=530, Total reward=50.77, Steps=16593, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=531, Total reward=38.16, Steps=16636, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=532, Total reward=36.71, Steps=16664, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=533, Total reward=5.73, Steps=16682, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=534, Total reward=42.05, Steps=16704, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=535, Total reward=27.63, Steps=16735, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=536, Total reward=23.85, Steps=16760, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=537, Total reward=35.13, Steps=16791, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=538, Total reward=20.21, Steps=16804, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=539, Total reward=10.99, Steps=16829, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=540, Total reward=76.64, Steps=16867, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=541, Total reward=33.38, Steps=16890, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=542, Total reward=28.92, Steps=16912, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=543, Total reward=9.3, Steps=16979, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=544, Total reward=64.58, Steps=17050, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=545, Total reward=19.47, Steps=17112, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=546, Total reward=55.67, Steps=17158, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=547, Total reward=47.02, Steps=17196, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=548, Total reward=51.56, Steps=17224, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=549, Total reward=40.06, Steps=17249, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=550, Total reward=68.15, Steps=17296, Training iteration=10
Policy training> Surrogate loss=-0.013476462103426456, KL divergence=0.0001238988625118509, Entropy=0.3068620264530182, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.033420857042074203, KL divergence=0.006029909942299128, Entropy=0.3012736141681671, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04099961370229721, KL divergence=0.02245922200381756, Entropy=0.29424047470092773, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04644341394305229, KL divergence=0.03979166969656944, Entropy=0.29050031304359436, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05670243874192238, KL divergence=0.06035435572266579, Entropy=0.28413793444633484, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06385068595409393, KL divergence=0.07689680904150009, Entropy=0.2842091917991638, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.056191336363554, KL divergence=0.0862346962094307, Entropy=0.28090164065361023, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05876316502690315, KL divergence=0.10776706784963608, Entropy=0.2804587781429291, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05770564079284668, KL divergence=0.12011528015136719, Entropy=0.27611228823661804, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06853631883859634, KL divergence=0.12590292096138, Entropy=0.27973055839538574, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/303_Step-17296.ckpt']
Uploaded 3 files for checkpoint 303 in 0.57 seconds
saved intermediate frozen graph: current/model/model_303.pb
Best checkpoint number: 292, Last checkpoint number: 301
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'301'}
Training> Name=main_level/agent, Worker=0, Episode=551, Total reward=42.33, Steps=17337, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=552, Total reward=28.13, Steps=17368, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=553, Total reward=0.02, Steps=17384, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=554, Total reward=44.14, Steps=17407, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=555, Total reward=15.36, Steps=17432, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=556, Total reward=14.39, Steps=17465, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=557, Total reward=51.05, Steps=17502, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=558, Total reward=10.73, Steps=17514, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=559, Total reward=65.61, Steps=17569, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=560, Total reward=33.49, Steps=17612, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=561, Total reward=48.48, Steps=17641, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=562, Total reward=61.32, Steps=17702, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=563, Total reward=10.04, Steps=17739, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=564, Total reward=15.47, Steps=17766, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=565, Total reward=12.89, Steps=17824, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=566, Total reward=18.47, Steps=17840, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=567, Total reward=61.18, Steps=17882, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=568, Total reward=65.63, Steps=17938, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=569, Total reward=28.36, Steps=17958, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=570, Total reward=85.26, Steps=18003, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=571, Total reward=52.16, Steps=18038, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=572, Total reward=53.97, Steps=18070, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=573, Total reward=36.07, Steps=18092, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=574, Total reward=20.74, Steps=18115, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=575, Total reward=16.74, Steps=18146, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=576, Total reward=13.07, Steps=18167, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=577, Total reward=22.66, Steps=18213, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=578, Total reward=49.08, Steps=18250, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=579, Total reward=10.92, Steps=18270, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=580, Total reward=67.68, Steps=18315, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=581, Total reward=58.06, Steps=18344, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=582, Total reward=34.79, Steps=18371, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=583, Total reward=13.26, Steps=18406, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=584, Total reward=48.08, Steps=18474, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=585, Total reward=7.78, Steps=18491, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=586, Total reward=11.86, Steps=18523, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=587, Total reward=56.07, Steps=18562, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=588, Total reward=67.41, Steps=18591, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=589, Total reward=24.65, Steps=18606, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=590, Total reward=32.37, Steps=18639, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=591, Total reward=37.21, Steps=18674, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=592, Total reward=13.87, Steps=18692, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=593, Total reward=31.28, Steps=18711, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=594, Total reward=36.9, Steps=18734, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=595, Total reward=40.37, Steps=18763, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=596, Total reward=17.16, Steps=18782, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=597, Total reward=40.38, Steps=18834, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=598, Total reward=38.97, Steps=18859, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=599, Total reward=71.34, Steps=18901, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=600, Total reward=65.07, Steps=18939, Training iteration=11
Policy training> Surrogate loss=0.012183033861219883, KL divergence=0.00012661040818784386, Entropy=0.3116931915283203, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02047101967036724, KL divergence=0.008648736402392387, Entropy=0.3045564889907837, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04328969493508339, KL divergence=0.024192124605178833, Entropy=0.2970148026943207, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05723293498158455, KL divergence=0.042830366641283035, Entropy=0.29289230704307556, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.056619416922330856, KL divergence=0.05895140767097473, Entropy=0.29413965344429016, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05539074167609215, KL divergence=0.0767950564622879, Entropy=0.2841213047504425, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06005774065852165, KL divergence=0.09208086133003235, Entropy=0.28572455048561096, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06118728592991829, KL divergence=0.10379233211278915, Entropy=0.28397753834724426, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06479226797819138, KL divergence=0.11074421554803848, Entropy=0.2813986837863922, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06494105607271194, KL divergence=0.12066233903169632, Entropy=0.28694531321525574, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/304_Step-18939.ckpt']
Uploaded 3 files for checkpoint 304 in 0.57 seconds
saved intermediate frozen graph: current/model/model_304.pb
Best checkpoint number: 292, Last checkpoint number: 302
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'302'}
Training> Name=main_level/agent, Worker=0, Episode=601, Total reward=32.89, Steps=18967, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=602, Total reward=35.22, Steps=19013, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=603, Total reward=6.27, Steps=19037, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=604, Total reward=8.82, Steps=19079, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=605, Total reward=14.99, Steps=19100, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=606, Total reward=32.11, Steps=19132, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=607, Total reward=40.68, Steps=19167, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=608, Total reward=50.3, Steps=19193, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=609, Total reward=98.77, Steps=19257, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=610, Total reward=44.58, Steps=19292, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=611, Total reward=44.85, Steps=19335, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=612, Total reward=2.98, Steps=19355, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=613, Total reward=58.59, Steps=19386, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=614, Total reward=32.46, Steps=19407, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=615, Total reward=29.97, Steps=19438, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=616, Total reward=17.39, Steps=19469, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=617, Total reward=36.48, Steps=19503, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=618, Total reward=42.92, Steps=19528, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=619, Total reward=47.77, Steps=19576, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=620, Total reward=54.75, Steps=19617, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=621, Total reward=54.52, Steps=19649, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=622, Total reward=38.28, Steps=19668, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=623, Total reward=6.61, Steps=19688, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=624, Total reward=24.77, Steps=19729, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=625, Total reward=1.34, Steps=19743, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=626, Total reward=79.46, Steps=19791, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=627, Total reward=12.99, Steps=19809, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=628, Total reward=63.53, Steps=19837, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=629, Total reward=25.22, Steps=19870, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=630, Total reward=40.6, Steps=19910, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=631, Total reward=21.26, Steps=19941, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=632, Total reward=55.57, Steps=19974, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=633, Total reward=44.77, Steps=20004, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=634, Total reward=32.47, Steps=20025, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=635, Total reward=33.18, Steps=20054, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=636, Total reward=23.39, Steps=20082, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=637, Total reward=56.87, Steps=20117, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=638, Total reward=36.11, Steps=20165, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=639, Total reward=58.6, Steps=20215, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=640, Total reward=55.04, Steps=20249, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=641, Total reward=60.72, Steps=20279, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=642, Total reward=34.42, Steps=20303, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=643, Total reward=6.22, Steps=20330, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=644, Total reward=9.52, Steps=20355, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=645, Total reward=66.11, Steps=20413, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=646, Total reward=78.97, Steps=20461, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=647, Total reward=26.92, Steps=20492, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=648, Total reward=59.15, Steps=20520, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=649, Total reward=31.69, Steps=20553, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=650, Total reward=38.18, Steps=20590, Training iteration=12
Policy training> Surrogate loss=0.010497051291167736, KL divergence=0.00020207786292303354, Entropy=0.3170308470726013, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.023965617641806602, KL divergence=0.007458576466888189, Entropy=0.31439751386642456, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.044703159481287, KL divergence=0.024201825261116028, Entropy=0.3013605773448944, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05269423499703407, KL divergence=0.04370101913809776, Entropy=0.29157915711402893, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06344173103570938, KL divergence=0.06211736425757408, Entropy=0.2945672273635864, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06392208486795425, KL divergence=0.07545005530118942, Entropy=0.2816661298274994, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06021382287144661, KL divergence=0.0913207158446312, Entropy=0.2845178544521332, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07063735276460648, KL divergence=0.10440021753311157, Entropy=0.28907644748687744, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0727534219622612, KL divergence=0.1139073371887207, Entropy=0.2761949300765991, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06275110691785812, KL divergence=0.12741278111934662, Entropy=0.2830415666103363, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/305_Step-20590.ckpt']
Uploaded 3 files for checkpoint 305 in 0.57 seconds
saved intermediate frozen graph: current/model/model_305.pb
Best checkpoint number: 292, Last checkpoint number: 303
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'303'}
Training> Name=main_level/agent, Worker=0, Episode=651, Total reward=76.55, Steps=20633, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=652, Total reward=47.99, Steps=20667, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=653, Total reward=0.02, Steps=20684, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=654, Total reward=34.88, Steps=20707, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=655, Total reward=19.39, Steps=20751, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=656, Total reward=22.19, Steps=20789, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=657, Total reward=32.11, Steps=20826, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=658, Total reward=34.29, Steps=20849, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=659, Total reward=70.16, Steps=20906, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=660, Total reward=73.72, Steps=20947, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=661, Total reward=51.92, Steps=20977, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=662, Total reward=31.15, Steps=20997, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=663, Total reward=7.81, Steps=21030, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=664, Total reward=5.92, Steps=21063, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=665, Total reward=54.32, Steps=21114, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=666, Total reward=3.65, Steps=21141, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=667, Total reward=71.45, Steps=21204, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=668, Total reward=74.82, Steps=21255, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=669, Total reward=25.18, Steps=21276, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=670, Total reward=54.45, Steps=21322, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=671, Total reward=44.62, Steps=21362, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=672, Total reward=69.54, Steps=21405, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=673, Total reward=44.29, Steps=21433, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=674, Total reward=31.33, Steps=21456, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=675, Total reward=45.77, Steps=21486, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=676, Total reward=6.09, Steps=21501, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=677, Total reward=12.26, Steps=21513, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=678, Total reward=45.51, Steps=21537, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=679, Total reward=47.94, Steps=21598, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=680, Total reward=76.6, Steps=21633, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=681, Total reward=49.61, Steps=21661, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=682, Total reward=35.84, Steps=21684, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=683, Total reward=36.52, Steps=21746, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=684, Total reward=13.48, Steps=21774, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=685, Total reward=7.93, Steps=21807, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=686, Total reward=40.96, Steps=21851, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=687, Total reward=6.62, Steps=21866, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=688, Total reward=85.61, Steps=21941, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=689, Total reward=71.22, Steps=21999, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=690, Total reward=55.96, Steps=22048, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=691, Total reward=61.47, Steps=22089, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=692, Total reward=41.09, Steps=22119, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=693, Total reward=52.69, Steps=22149, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=694, Total reward=39.44, Steps=22171, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=695, Total reward=12.67, Steps=22194, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=696, Total reward=18.86, Steps=22227, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=697, Total reward=92.5, Steps=22305, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=698, Total reward=42.32, Steps=22330, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=699, Total reward=9.59, Steps=22349, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=700, Total reward=86.65, Steps=22391, Training iteration=13
Policy training> Surrogate loss=0.0023818339686840773, KL divergence=6.155864684842527e-05, Entropy=0.3025766909122467, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02680465765297413, KL divergence=0.005821657832711935, Entropy=0.3031969666481018, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03633471950888634, KL divergence=0.0189231988042593, Entropy=0.28835147619247437, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04410533979535103, KL divergence=0.03235911205410957, Entropy=0.28264859318733215, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.056026339530944824, KL divergence=0.05079849436879158, Entropy=0.2780754268169403, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.056700557470321655, KL divergence=0.06683077663183212, Entropy=0.27855184674263, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.047891005873680115, KL divergence=0.08217654377222061, Entropy=0.26938527822494507, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.044543009251356125, KL divergence=0.09286316484212875, Entropy=0.2708653509616852, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0684657171368599, KL divergence=0.1042383685708046, Entropy=0.27275148034095764, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0692165270447731, KL divergence=0.11064285039901733, Entropy=0.2687515318393707, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/306_Step-22391.ckpt']
Uploaded 3 files for checkpoint 306 in 0.53 seconds
saved intermediate frozen graph: current/model/model_306.pb
Best checkpoint number: 292, Last checkpoint number: 304
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'304'}
Training> Name=main_level/agent, Worker=0, Episode=701, Total reward=42.56, Steps=22417, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=702, Total reward=38.68, Steps=22442, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=703, Total reward=10.67, Steps=22487, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=704, Total reward=7.91, Steps=22515, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=705, Total reward=13.36, Steps=22540, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=706, Total reward=80.58, Steps=22600, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=707, Total reward=40.39, Steps=22636, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=708, Total reward=46.74, Steps=22662, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=709, Total reward=21.85, Steps=22682, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=710, Total reward=122.53, Steps=22730, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=711, Total reward=16.21, Steps=22761, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=712, Total reward=23.37, Steps=22792, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=713, Total reward=14.68, Steps=22811, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=714, Total reward=42.14, Steps=22833, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=715, Total reward=20.91, Steps=22861, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=716, Total reward=18.87, Steps=22881, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=717, Total reward=25.56, Steps=22917, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=718, Total reward=44.02, Steps=22943, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=719, Total reward=86.57, Steps=22988, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=720, Total reward=70.56, Steps=23028, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=721, Total reward=21.07, Steps=23051, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=722, Total reward=25.42, Steps=23068, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=723, Total reward=7.25, Steps=23086, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=724, Total reward=5.4, Steps=23117, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=725, Total reward=79.08, Steps=23179, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=726, Total reward=44.52, Steps=23225, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=727, Total reward=73.59, Steps=23289, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=728, Total reward=65.19, Steps=23344, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=729, Total reward=36.28, Steps=23369, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=730, Total reward=50.67, Steps=23420, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=731, Total reward=59.66, Steps=23462, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=732, Total reward=43.64, Steps=23494, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=733, Total reward=21.11, Steps=23505, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=734, Total reward=43.87, Steps=23527, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=735, Total reward=20.85, Steps=23556, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=736, Total reward=20.78, Steps=23586, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=737, Total reward=32.46, Steps=23613, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=738, Total reward=43.99, Steps=23636, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=739, Total reward=74.75, Steps=23683, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=740, Total reward=51.94, Steps=23718, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=741, Total reward=32.92, Steps=23744, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=742, Total reward=25.25, Steps=23763, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=743, Total reward=2.93, Steps=23801, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=744, Total reward=20.19, Steps=23850, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=745, Total reward=64.52, Steps=23910, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=746, Total reward=18.32, Steps=23935, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=747, Total reward=29.99, Steps=23971, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=748, Total reward=61.23, Steps=24009, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=749, Total reward=27.27, Steps=24034, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=750, Total reward=67.47, Steps=24070, Training iteration=14
Policy training> Surrogate loss=0.0025363813620060682, KL divergence=0.00011769111733883619, Entropy=0.3134498596191406, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0170572642236948, KL divergence=0.007401387672871351, Entropy=0.31359168887138367, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.038010112941265106, KL divergence=0.02072758600115776, Entropy=0.3099667727947235, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05280584096908569, KL divergence=0.0389237143099308, Entropy=0.3028022348880768, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.057486314326524734, KL divergence=0.05519263073801994, Entropy=0.29796355962753296, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06220643222332001, KL divergence=0.06901470571756363, Entropy=0.2968834340572357, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.07111439853906631, KL divergence=0.08213051408529282, Entropy=0.2941482961177826, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05971881374716759, KL divergence=0.09688464552164078, Entropy=0.29910293221473694, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08279655873775482, KL divergence=0.10160183906555176, Entropy=0.296344518661499, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07235362380743027, KL divergence=0.11226075142621994, Entropy=0.28996869921684265, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/307_Step-24070.ckpt']
Uploaded 3 files for checkpoint 307 in 0.55 seconds
saved intermediate frozen graph: current/model/model_307.pb
Best checkpoint number: 292, Last checkpoint number: 305
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'305'}
Training> Name=main_level/agent, Worker=0, Episode=751, Total reward=25.49, Steps=24098, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=752, Total reward=9.32, Steps=24118, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=753, Total reward=21.24, Steps=24128, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=754, Total reward=25.58, Steps=24139, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=755, Total reward=7.68, Steps=24183, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=756, Total reward=16.05, Steps=24216, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=757, Total reward=57.55, Steps=24279, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=758, Total reward=31.32, Steps=24299, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=759, Total reward=58.88, Steps=24344, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=760, Total reward=60.06, Steps=24378, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=761, Total reward=70.04, Steps=24407, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=762, Total reward=35.86, Steps=24437, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=763, Total reward=7.91, Steps=24477, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=764, Total reward=25.22, Steps=24511, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=765, Total reward=9.97, Steps=24533, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=766, Total reward=48.42, Steps=24580, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=767, Total reward=3.31, Steps=24593, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=768, Total reward=71.48, Steps=24637, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=769, Total reward=31.6, Steps=24664, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=770, Total reward=41.79, Steps=24711, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=771, Total reward=44.02, Steps=24742, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=772, Total reward=55.82, Steps=24776, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=773, Total reward=53.19, Steps=24806, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=774, Total reward=29.85, Steps=24827, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=775, Total reward=21.95, Steps=24854, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=776, Total reward=16.01, Steps=24872, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=777, Total reward=39.97, Steps=24910, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=778, Total reward=29.58, Steps=24956, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=779, Total reward=47.43, Steps=25000, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=780, Total reward=63.11, Steps=25039, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=781, Total reward=27.6, Steps=25062, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=782, Total reward=49.23, Steps=25136, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=783, Total reward=62.34, Steps=25209, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=784, Total reward=10.07, Steps=25239, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=785, Total reward=6.86, Steps=25272, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=786, Total reward=66.25, Steps=25335, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=787, Total reward=67.4, Steps=25377, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=788, Total reward=67.04, Steps=25431, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=789, Total reward=32.16, Steps=25466, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=790, Total reward=29.31, Steps=25504, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=791, Total reward=46.33, Steps=25531, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=792, Total reward=29.28, Steps=25559, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=793, Total reward=51.52, Steps=25587, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=794, Total reward=40.4, Steps=25619, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=795, Total reward=25.25, Steps=25646, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=796, Total reward=32.34, Steps=25677, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=797, Total reward=83.12, Steps=25776, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=798, Total reward=23.51, Steps=25792, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=799, Total reward=9.53, Steps=25811, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=800, Total reward=94.21, Steps=25859, Training iteration=15
Policy training> Surrogate loss=0.0071409097872674465, KL divergence=0.00015267892740666866, Entropy=0.3095429837703705, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.006823640316724777, KL divergence=0.008017625659704208, Entropy=0.3057035505771637, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.024413714185357094, KL divergence=0.022184131667017937, Entropy=0.2980390787124634, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.031081927940249443, KL divergence=0.03941509500145912, Entropy=0.2934171259403229, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.027926893904805183, KL divergence=0.05808015540242195, Entropy=0.28514066338539124, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06531894952058792, KL divergence=0.07579151540994644, Entropy=0.2832320034503937, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06810677796602249, KL divergence=0.08703551441431046, Entropy=0.2765180170536041, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05936233326792717, KL divergence=0.10160829871892929, Entropy=0.2754248380661011, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06608511507511139, KL divergence=0.11444313079118729, Entropy=0.2747107446193695, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06397654861211777, KL divergence=0.12799648940563202, Entropy=0.27850502729415894, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/308_Step-25859.ckpt']
Uploaded 3 files for checkpoint 308 in 0.60 seconds
saved intermediate frozen graph: current/model/model_308.pb
Best checkpoint number: 292, Last checkpoint number: 306
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'306'}
Training> Name=main_level/agent, Worker=0, Episode=801, Total reward=54.57, Steps=25888, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=802, Total reward=24.89, Steps=25906, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=803, Total reward=12.02, Steps=25939, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=804, Total reward=63.51, Steps=26007, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=805, Total reward=78.69, Steps=26068, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=806, Total reward=35.94, Steps=26104, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=807, Total reward=41.49, Steps=26140, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=808, Total reward=38.08, Steps=26162, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=809, Total reward=22.86, Steps=26181, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=810, Total reward=14.91, Steps=26196, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=811, Total reward=33.92, Steps=26239, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=812, Total reward=19.44, Steps=26266, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=813, Total reward=46.49, Steps=26296, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=814, Total reward=26.09, Steps=26307, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=815, Total reward=28.27, Steps=26336, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=816, Total reward=23.4, Steps=26366, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=817, Total reward=109.96, Steps=26442, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=818, Total reward=35.81, Steps=26465, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=819, Total reward=0.0, Steps=26466, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=820, Total reward=78.42, Steps=26503, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=821, Total reward=28.9, Steps=26529, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=822, Total reward=30.42, Steps=26554, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=823, Total reward=9.73, Steps=26576, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=824, Total reward=7.74, Steps=26603, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=825, Total reward=27.59, Steps=26642, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=826, Total reward=50.69, Steps=26688, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=827, Total reward=46.2, Steps=26724, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=828, Total reward=39.03, Steps=26748, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=829, Total reward=21.49, Steps=26765, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=830, Total reward=23.67, Steps=26799, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=831, Total reward=12.46, Steps=26824, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=832, Total reward=5.72, Steps=26843, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=833, Total reward=32.4, Steps=26867, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=834, Total reward=35.23, Steps=26898, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=835, Total reward=32.33, Steps=26927, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=836, Total reward=19.73, Steps=26955, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=837, Total reward=44.68, Steps=26990, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=838, Total reward=41.61, Steps=27017, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=839, Total reward=48.35, Steps=27057, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=840, Total reward=65.76, Steps=27097, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=841, Total reward=45.92, Steps=27126, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=842, Total reward=25.49, Steps=27147, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=843, Total reward=14.45, Steps=27182, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=844, Total reward=10.82, Steps=27214, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=845, Total reward=3.38, Steps=27230, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=846, Total reward=60.84, Steps=27279, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=847, Total reward=58.51, Steps=27318, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=848, Total reward=58.78, Steps=27346, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=849, Total reward=23.62, Steps=27395, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=850, Total reward=28.98, Steps=27411, Training iteration=16
Policy training> Surrogate loss=0.0018549015512689948, KL divergence=0.0001106326308217831, Entropy=0.32516178488731384, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03385712206363678, KL divergence=0.01002497598528862, Entropy=0.32106658816337585, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.046417903155088425, KL divergence=0.028735384345054626, Entropy=0.31378135085105896, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05613377317786217, KL divergence=0.050500813871622086, Entropy=0.30839648842811584, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06154590845108032, KL divergence=0.07100013643503189, Entropy=0.30292800068855286, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.056976933032274246, KL divergence=0.08959589153528214, Entropy=0.30120691657066345, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06323254853487015, KL divergence=0.10020693391561508, Entropy=0.299093633890152, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.061094019562006, KL divergence=0.11658325046300888, Entropy=0.29914042353630066, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06983228027820587, KL divergence=0.12781645357608795, Entropy=0.30003878474235535, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06882204860448837, KL divergence=0.13545024394989014, Entropy=0.2976500689983368, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/309_Step-27411.ckpt']
Uploaded 3 files for checkpoint 309 in 0.60 seconds
saved intermediate frozen graph: current/model/model_309.pb
Best checkpoint number: 292, Last checkpoint number: 307
Copying the frozen checkpoint from ./frozen_models/agent/model_292.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'307'}
Training> Name=main_level/agent, Worker=0, Episode=851, Total reward=35.72, Steps=27450, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=852, Total reward=47.76, Steps=27481, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=853, Total reward=46.97, Steps=27509, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=854, Total reward=40.72, Steps=27531, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=855, Total reward=22.09, Steps=27555, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=856, Total reward=18.17, Steps=27585, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=857, Total reward=22.38, Steps=27612, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=858, Total reward=45.16, Steps=27658, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=859, Total reward=6.75, Steps=27692, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=860, Total reward=88.05, Steps=27733, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=861, Total reward=60.32, Steps=27763, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=862, Total reward=31.4, Steps=27785, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=863, Total reward=3.69, Steps=27801, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=864, Total reward=3.08, Steps=27829, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=865, Total reward=16.34, Steps=27853, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=866, Total reward=48.05, Steps=27898, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=867, Total reward=31.82, Steps=27935, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=868, Total reward=55.48, Steps=27962, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=869, Total reward=66.96, Steps=28009, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=870, Total reward=7.72, Steps=28022, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=871, Total reward=51.22, Steps=28061, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=872, Total reward=29.61, Steps=28089, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=873, Total reward=45.62, Steps=28114, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=874, Total reward=29.02, Steps=28125, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=875, Total reward=17.31, Steps=28171, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=876, Total reward=21.47, Steps=28189, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=877, Total reward=41.7, Steps=28241, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=878, Total reward=36.81, Steps=28264, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=879, Total reward=74.97, Steps=28312, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=880, Total reward=23.85, Steps=28343, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=881, Total reward=62.89, Steps=28376, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=882, Total reward=30.77, Steps=28404, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=883, Total reward=6.76, Steps=28443, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=884, Total reward=5.39, Steps=28492, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=885, Total reward=77.13, Steps=28577, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=886, Total reward=91.44, Steps=28620, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=887, Total reward=60.0, Steps=28663, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=888, Total reward=62.52, Steps=28690, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=889, Total reward=47.06, Steps=28719, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=890, Total reward=39.85, Steps=28754, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=891, Total reward=35.81, Steps=28784, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=892, Total reward=44.62, Steps=28815, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=893, Total reward=21.28, Steps=28826, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=894, Total reward=45.08, Steps=28860, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=895, Total reward=14.61, Steps=28889, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=896, Total reward=15.37, Steps=28921, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=897, Total reward=26.95, Steps=28951, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=898, Total reward=35.39, Steps=28973, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=899, Total reward=19.53, Steps=29004, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=900, Total reward=95.46, Steps=29040, Training iteration=17
Policy training> Surrogate loss=-0.0037347592879086733, KL divergence=0.00014021167589817196, Entropy=0.3053230345249176, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.019810471683740616, KL divergence=0.0071989246644079685, Entropy=0.3070993423461914, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.027389125898480415, KL divergence=0.02172473818063736, Entropy=0.30825304985046387, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.050353050231933594, KL divergence=0.039994221180677414, Entropy=0.3004325330257416, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04740622639656067, KL divergence=0.05819953605532646, Entropy=0.29489174485206604, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04769458994269371, KL divergence=0.07591810822486877, Entropy=0.29625803232192993, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06117190420627594, KL divergence=0.09000403434038162, Entropy=0.29066726565361023, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.058499183505773544, KL divergence=0.1061258539557457, Entropy=0.2923068404197693, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.051277708262205124, KL divergence=0.1130673959851265, Entropy=0.28584107756614685, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07206595689058304, KL divergence=0.12304029613733292, Entropy=0.2899845540523529, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/310_Step-29040.ckpt']
Uploaded 3 files for checkpoint 310 in 0.60 seconds
saved intermediate frozen graph: current/model/model_310.pb
Best checkpoint number: 308, Last checkpoint number: 308
Copying the frozen checkpoint from ./frozen_models/agent/model_308.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'292'}
Training> Name=main_level/agent, Worker=0, Episode=901, Total reward=40.2, Steps=29064, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=902, Total reward=34.01, Steps=29089, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=903, Total reward=7.49, Steps=29123, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=904, Total reward=8.65, Steps=29151, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=905, Total reward=21.93, Steps=29179, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=906, Total reward=46.1, Steps=29226, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=907, Total reward=54.91, Steps=29265, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=908, Total reward=60.36, Steps=29294, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=909, Total reward=34.28, Steps=29323, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=910, Total reward=56.27, Steps=29375, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=911, Total reward=74.18, Steps=29428, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=912, Total reward=5.57, Steps=29439, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=913, Total reward=22.85, Steps=29458, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=914, Total reward=30.79, Steps=29480, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=915, Total reward=18.39, Steps=29507, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=916, Total reward=24.01, Steps=29524, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=917, Total reward=39.96, Steps=29558, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=918, Total reward=30.09, Steps=29583, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=919, Total reward=23.73, Steps=29618, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=920, Total reward=58.03, Steps=29655, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=921, Total reward=35.37, Steps=29684, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=922, Total reward=42.0, Steps=29728, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=923, Total reward=73.43, Steps=29824, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=924, Total reward=10.85, Steps=29879, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=925, Total reward=12.77, Steps=29916, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=926, Total reward=22.53, Steps=29943, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=927, Total reward=59.16, Steps=29981, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=928, Total reward=58.96, Steps=30008, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=929, Total reward=17.53, Steps=30025, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=930, Total reward=59.65, Steps=30072, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=931, Total reward=28.26, Steps=30104, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=932, Total reward=22.64, Steps=30135, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=933, Total reward=30.26, Steps=30156, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=934, Total reward=49.55, Steps=30179, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=935, Total reward=23.28, Steps=30210, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=936, Total reward=22.07, Steps=30230, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=937, Total reward=21.36, Steps=30268, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=938, Total reward=33.21, Steps=30292, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=939, Total reward=60.42, Steps=30345, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=940, Total reward=72.92, Steps=30385, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=941, Total reward=25.04, Steps=30405, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=942, Total reward=33.58, Steps=30441, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=943, Total reward=25.58, Steps=30482, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=944, Total reward=57.48, Steps=30546, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=945, Total reward=36.38, Steps=30585, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=946, Total reward=91.74, Steps=30632, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=947, Total reward=68.92, Steps=30695, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=948, Total reward=106.33, Steps=30772, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=949, Total reward=31.16, Steps=30815, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=950, Total reward=51.83, Steps=30860, Training iteration=18
Policy training> Surrogate loss=0.00384340132586658, KL divergence=0.0001003770375973545, Entropy=0.3330478370189667, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.009336370043456554, KL divergence=0.006153620779514313, Entropy=0.32148414850234985, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.036680761724710464, KL divergence=0.01944214664399624, Entropy=0.32060882449150085, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05297078564763069, KL divergence=0.03813881799578667, Entropy=0.3216002583503723, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.038745149970054626, KL divergence=0.05743276700377464, Entropy=0.31790003180503845, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04530128836631775, KL divergence=0.07196364551782608, Entropy=0.31311455368995667, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04662901163101196, KL divergence=0.0877726599574089, Entropy=0.30595648288726807, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05920172855257988, KL divergence=0.09529147297143936, Entropy=0.3064579665660858, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06356251239776611, KL divergence=0.1066214069724083, Entropy=0.310108482837677, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07835998386144638, KL divergence=0.11896295100450516, Entropy=0.3155241906642914, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/311_Step-30860.ckpt']
Uploaded 3 files for checkpoint 311 in 0.50 seconds
saved intermediate frozen graph: current/model/model_311.pb
Best checkpoint number: 308, Last checkpoint number: 309
Copying the frozen checkpoint from ./frozen_models/agent/model_308.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'292'}
Training> Name=main_level/agent, Worker=0, Episode=951, Total reward=12.9, Steps=30890, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=952, Total reward=1.74, Steps=30901, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=953, Total reward=52.59, Steps=30931, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=954, Total reward=27.63, Steps=30942, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=955, Total reward=17.9, Steps=30973, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=956, Total reward=17.86, Steps=30996, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=957, Total reward=10.62, Steps=31016, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=958, Total reward=42.59, Steps=31041, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=959, Total reward=60.32, Steps=31092, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=960, Total reward=94.63, Steps=31187, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=961, Total reward=52.13, Steps=31216, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=962, Total reward=39.48, Steps=31254, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=963, Total reward=5.16, Steps=31298, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=964, Total reward=10.52, Steps=31325, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=965, Total reward=21.98, Steps=31354, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=966, Total reward=58.37, Steps=31399, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=967, Total reward=60.15, Steps=31438, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=968, Total reward=52.86, Steps=31465, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=969, Total reward=30.05, Steps=31486, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=970, Total reward=26.16, Steps=31525, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=971, Total reward=69.88, Steps=31567, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=972, Total reward=15.9, Steps=31593, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=973, Total reward=21.26, Steps=31603, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=974, Total reward=40.95, Steps=31625, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=975, Total reward=18.78, Steps=31654, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=976, Total reward=19.27, Steps=31688, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=977, Total reward=7.51, Steps=31712, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=978, Total reward=36.42, Steps=31736, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=979, Total reward=82.26, Steps=31782, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=980, Total reward=49.56, Steps=31823, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=981, Total reward=31.03, Steps=31847, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=982, Total reward=30.24, Steps=31870, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=983, Total reward=3.61, Steps=31887, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=984, Total reward=7.59, Steps=31928, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=985, Total reward=63.78, Steps=32004, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=986, Total reward=58.11, Steps=32055, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=987, Total reward=102.62, Steps=32141, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=988, Total reward=58.36, Steps=32172, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=989, Total reward=68.22, Steps=32220, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=990, Total reward=54.43, Steps=32268, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=991, Total reward=32.76, Steps=32305, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=992, Total reward=7.44, Steps=32317, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=993, Total reward=19.55, Steps=32336, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=994, Total reward=40.96, Steps=32359, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=995, Total reward=37.54, Steps=32388, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=996, Total reward=12.79, Steps=32428, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=997, Total reward=44.04, Steps=32467, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=998, Total reward=28.83, Steps=32483, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=999, Total reward=86.55, Steps=32530, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=1000, Total reward=72.8, Steps=32568, Training iteration=19
Policy training> Surrogate loss=0.0025950514245778322, KL divergence=0.00026659792638383806, Entropy=0.3414257764816284, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03416595235466957, KL divergence=0.01000230386853218, Entropy=0.3377681076526642, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.043983787298202515, KL divergence=0.026492854580283165, Entropy=0.33040109276771545, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04672935977578163, KL divergence=0.04427933320403099, Entropy=0.3275040090084076, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0504678376019001, KL divergence=0.05901307985186577, Entropy=0.32709750533103943, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06429022550582886, KL divergence=0.07532796263694763, Entropy=0.3182244300842285, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05747557058930397, KL divergence=0.09005192667245865, Entropy=0.31841281056404114, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0674024224281311, KL divergence=0.10392242670059204, Entropy=0.32053136825561523, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06413407623767853, KL divergence=0.11275630444288254, Entropy=0.32114377617836, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05196612700819969, KL divergence=0.11908372491598129, Entropy=0.3201456367969513, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/312_Step-32568.ckpt']
Uploaded 3 files for checkpoint 312 in 0.56 seconds
saved intermediate frozen graph: current/model/model_312.pb
Best checkpoint number: 308, Last checkpoint number: 310
Copying the frozen checkpoint from ./frozen_models/agent/model_308.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'309'}
Training> Name=main_level/agent, Worker=0, Episode=1001, Total reward=59.63, Steps=32599, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1002, Total reward=43.48, Steps=32623, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1003, Total reward=3.66, Steps=32639, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1004, Total reward=20.62, Steps=32697, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1005, Total reward=67.26, Steps=32756, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1006, Total reward=7.29, Steps=32779, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1007, Total reward=56.65, Steps=32815, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1008, Total reward=50.08, Steps=32843, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1009, Total reward=21.04, Steps=32862, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1010, Total reward=84.21, Steps=32906, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1011, Total reward=28.68, Steps=32937, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1012, Total reward=51.07, Steps=32970, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1013, Total reward=23.72, Steps=32981, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1014, Total reward=25.53, Steps=32992, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1015, Total reward=13.96, Steps=33025, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1016, Total reward=25.29, Steps=33056, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1017, Total reward=44.68, Steps=33094, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1018, Total reward=41.91, Steps=33120, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1019, Total reward=23.49, Steps=33138, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1020, Total reward=74.23, Steps=33175, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1021, Total reward=61.32, Steps=33206, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1022, Total reward=24.83, Steps=33223, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1023, Total reward=13.57, Steps=33265, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1024, Total reward=87.02, Steps=33344, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1025, Total reward=14.66, Steps=33369, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1026, Total reward=53.02, Steps=33418, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1027, Total reward=66.95, Steps=33458, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1028, Total reward=68.83, Steps=33505, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1029, Total reward=24.78, Steps=33528, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1030, Total reward=59.57, Steps=33578, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1031, Total reward=30.92, Steps=33607, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1032, Total reward=60.1, Steps=33650, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1033, Total reward=28.67, Steps=33671, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1034, Total reward=40.98, Steps=33694, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1035, Total reward=18.34, Steps=33721, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1036, Total reward=21.39, Steps=33750, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1037, Total reward=56.19, Steps=33787, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1038, Total reward=47.09, Steps=33834, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1039, Total reward=65.71, Steps=33885, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1040, Total reward=83.91, Steps=33921, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1041, Total reward=57.26, Steps=33949, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1042, Total reward=33.54, Steps=33973, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1043, Total reward=7.76, Steps=34005, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1044, Total reward=15.5, Steps=34033, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1045, Total reward=77.06, Steps=34088, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1046, Total reward=76.7, Steps=34140, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1047, Total reward=117.43, Steps=34227, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1048, Total reward=60.83, Steps=34255, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1049, Total reward=51.37, Steps=34304, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1050, Total reward=25.55, Steps=34330, Training iteration=20
Policy training> Surrogate loss=-0.005640228744596243, KL divergence=8.997946133604273e-05, Entropy=0.34608015418052673, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.028930485248565674, KL divergence=0.005005475599318743, Entropy=0.33789053559303284, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.044752877205610275, KL divergence=0.020193491131067276, Entropy=0.33354055881500244, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.050476253032684326, KL divergence=0.035993486642837524, Entropy=0.324613094329834, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04351905360817909, KL divergence=0.05131234601140022, Entropy=0.32746174931526184, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05267493799328804, KL divergence=0.07079710811376572, Entropy=0.3140918016433716, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06781773269176483, KL divergence=0.08096250891685486, Entropy=0.3204928934574127, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06064988300204277, KL divergence=0.09388396888971329, Entropy=0.31975558400154114, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.060301050543785095, KL divergence=0.10427550226449966, Entropy=0.3221588432788849, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06099855527281761, KL divergence=0.11107257008552551, Entropy=0.3185690939426422, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/313_Step-34330.ckpt']
Uploaded 3 files for checkpoint 313 in 0.53 seconds
saved intermediate frozen graph: current/model/model_313.pb
Best checkpoint number: 308, Last checkpoint number: 311
Copying the frozen checkpoint from ./frozen_models/agent/model_308.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'310'}
Training> Name=main_level/agent, Worker=0, Episode=1051, Total reward=45.23, Steps=34371, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1052, Total reward=45.92, Steps=34402, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1053, Total reward=40.45, Steps=34427, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1054, Total reward=43.66, Steps=34449, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1055, Total reward=14.28, Steps=34493, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1056, Total reward=22.99, Steps=34523, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1057, Total reward=49.29, Steps=34590, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1058, Total reward=32.81, Steps=34614, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1059, Total reward=19.34, Steps=34642, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1060, Total reward=74.09, Steps=34680, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1061, Total reward=26.11, Steps=34696, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1062, Total reward=38.47, Steps=34733, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1063, Total reward=0.01, Steps=34744, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1064, Total reward=20.61, Steps=34774, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1065, Total reward=19.14, Steps=34815, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1066, Total reward=11.11, Steps=34829, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1067, Total reward=67.26, Steps=34871, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1068, Total reward=64.4, Steps=34899, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1069, Total reward=26.6, Steps=34918, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1070, Total reward=52.97, Steps=34966, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1071, Total reward=63.87, Steps=35005, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1072, Total reward=47.96, Steps=35036, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1073, Total reward=21.37, Steps=35047, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1074, Total reward=55.41, Steps=35069, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1075, Total reward=20.34, Steps=35101, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1076, Total reward=18.38, Steps=35117, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1077, Total reward=30.81, Steps=35137, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1078, Total reward=27.76, Steps=35155, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1079, Total reward=17.45, Steps=35173, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1080, Total reward=71.2, Steps=35216, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1081, Total reward=39.12, Steps=35242, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1082, Total reward=24.47, Steps=35260, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1083, Total reward=0.02, Steps=35277, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1084, Total reward=0.02, Steps=35293, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1085, Total reward=25.24, Steps=35315, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1086, Total reward=69.05, Steps=35362, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1087, Total reward=66.71, Steps=35420, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1088, Total reward=60.97, Steps=35448, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1089, Total reward=20.48, Steps=35464, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1090, Total reward=85.65, Steps=35500, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1091, Total reward=44.45, Steps=35532, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1092, Total reward=50.25, Steps=35573, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1093, Total reward=46.64, Steps=35605, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1094, Total reward=28.64, Steps=35616, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1095, Total reward=25.18, Steps=35660, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1096, Total reward=12.07, Steps=35698, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1097, Total reward=26.9, Steps=35718, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1098, Total reward=30.09, Steps=35744, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1099, Total reward=13.66, Steps=35761, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1100, Total reward=95.46, Steps=35797, Training iteration=21
Policy training> Surrogate loss=-0.012542761862277985, KL divergence=2.7774898626375943e-05, Entropy=0.31217241287231445, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0346357636153698, KL divergence=0.0036057799588888884, Entropy=0.3013540506362915, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.02983688749372959, KL divergence=0.012773256748914719, Entropy=0.2937416732311249, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.042937085032463074, KL divergence=0.020202437415719032, Entropy=0.2940928339958191, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04254930466413498, KL divergence=0.03574197739362717, Entropy=0.3070889115333557, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.03364119678735733, KL divergence=0.05455099791288376, Entropy=0.2851319909095764, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04340360313653946, KL divergence=0.06970880925655365, Entropy=0.2816014587879181, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.04209897294640541, KL divergence=0.08412182331085205, Entropy=0.28595131635665894, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.054218728095293045, KL divergence=0.08996625989675522, Entropy=0.2855539321899414, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.03661463409662247, KL divergence=0.09197687357664108, Entropy=0.2768753170967102, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/314_Step-35797.ckpt']
Uploaded 3 files for checkpoint 314 in 0.51 seconds
saved intermediate frozen graph: current/model/model_314.pb
Best checkpoint number: 308, Last checkpoint number: 312
Copying the frozen checkpoint from ./frozen_models/agent/model_308.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'311'}
Training> Name=main_level/agent, Worker=0, Episode=1101, Total reward=19.42, Steps=35810, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1102, Total reward=29.5, Steps=35828, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1103, Total reward=6.41, Steps=35865, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1104, Total reward=63.46, Steps=35938, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1105, Total reward=11.53, Steps=35959, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1106, Total reward=63.62, Steps=36010, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1107, Total reward=54.32, Steps=36050, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1108, Total reward=44.61, Steps=36076, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1109, Total reward=25.22, Steps=36101, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1110, Total reward=52.6, Steps=36149, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1111, Total reward=55.5, Steps=36190, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1112, Total reward=20.24, Steps=36222, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1113, Total reward=36.93, Steps=36248, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1114, Total reward=24.18, Steps=36258, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1115, Total reward=21.61, Steps=36287, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1116, Total reward=25.66, Steps=36329, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1117, Total reward=48.95, Steps=36368, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1118, Total reward=32.72, Steps=36393, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1119, Total reward=69.23, Steps=36442, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1120, Total reward=75.15, Steps=36482, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1121, Total reward=62.15, Steps=36523, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1122, Total reward=33.86, Steps=36542, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1123, Total reward=22.56, Steps=36586, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1124, Total reward=27.95, Steps=36622, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1125, Total reward=19.71, Steps=36647, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1126, Total reward=59.63, Steps=36694, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1127, Total reward=73.84, Steps=36762, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1128, Total reward=61.18, Steps=36792, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1129, Total reward=43.76, Steps=36839, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1130, Total reward=41.3, Steps=36891, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1131, Total reward=45.33, Steps=36930, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1132, Total reward=32.45, Steps=36959, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1133, Total reward=40.01, Steps=36990, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1134, Total reward=36.96, Steps=37012, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1135, Total reward=18.91, Steps=37037, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1136, Total reward=20.07, Steps=37072, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1137, Total reward=52.62, Steps=37122, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1138, Total reward=39.24, Steps=37150, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1139, Total reward=16.16, Steps=37168, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1140, Total reward=53.77, Steps=37203, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1141, Total reward=45.5, Steps=37229, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1142, Total reward=33.62, Steps=37248, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1143, Total reward=3.4, Steps=37267, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1144, Total reward=10.57, Steps=37295, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1145, Total reward=9.88, Steps=37328, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1146, Total reward=87.89, Steps=37373, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1147, Total reward=16.04, Steps=37394, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1148, Total reward=85.28, Steps=37456, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1149, Total reward=61.6, Steps=37498, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1150, Total reward=21.98, Steps=37523, Training iteration=22
Policy training> Surrogate loss=-0.0018026642501354218, KL divergence=0.00013762192975264043, Entropy=0.33168962597846985, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.021296963095664978, KL divergence=0.007536567747592926, Entropy=0.3283732235431671, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03453735262155533, KL divergence=0.027690403163433075, Entropy=0.3196813762187958, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05876191332936287, KL divergence=0.04644974693655968, Entropy=0.31537675857543945, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0473964624106884, KL divergence=0.06206843629479408, Entropy=0.3068356215953827, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04169842600822449, KL divergence=0.07765455543994904, Entropy=0.3077201843261719, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0649300143122673, KL divergence=0.09616035223007202, Entropy=0.30156469345092773, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06125969812273979, KL divergence=0.1059664860367775, Entropy=0.29916876554489136, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07100743055343628, KL divergence=0.12009388208389282, Entropy=0.30561280250549316, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06492677330970764, KL divergence=0.1240980252623558, Entropy=0.307205468416214, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/315_Step-37523.ckpt']
Uploaded 3 files for checkpoint 315 in 0.57 seconds
saved intermediate frozen graph: current/model/model_315.pb
Best checkpoint number: 308, Last checkpoint number: 313
Copying the frozen checkpoint from ./frozen_models/agent/model_308.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'312'}
Training> Name=main_level/agent, Worker=0, Episode=1151, Total reward=73.79, Steps=37564, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1152, Total reward=68.6, Steps=37608, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1153, Total reward=41.44, Steps=37630, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1154, Total reward=35.65, Steps=37642, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1155, Total reward=2.09, Steps=37655, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1156, Total reward=21.63, Steps=37685, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1157, Total reward=41.95, Steps=37719, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1158, Total reward=24.49, Steps=37739, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1159, Total reward=2.85, Steps=37750, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1160, Total reward=88.61, Steps=37789, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1161, Total reward=57.77, Steps=37818, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1162, Total reward=40.08, Steps=37840, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1163, Total reward=0.02, Steps=37858, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1164, Total reward=1.97, Steps=37875, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1165, Total reward=13.32, Steps=37898, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1166, Total reward=80.28, Steps=37966, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1167, Total reward=3.3, Steps=37981, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1168, Total reward=60.8, Steps=38010, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1169, Total reward=94.26, Steps=38070, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1170, Total reward=34.58, Steps=38107, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1171, Total reward=9.66, Steps=38136, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1172, Total reward=58.9, Steps=38169, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1173, Total reward=30.88, Steps=38195, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1174, Total reward=40.49, Steps=38216, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1175, Total reward=22.57, Steps=38239, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1176, Total reward=22.03, Steps=38257, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1177, Total reward=38.37, Steps=38297, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1178, Total reward=29.21, Steps=38320, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1179, Total reward=68.27, Steps=38367, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1180, Total reward=72.1, Steps=38405, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1181, Total reward=56.66, Steps=38445, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1182, Total reward=30.51, Steps=38470, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1183, Total reward=16.23, Steps=38502, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1184, Total reward=3.48, Steps=38521, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1185, Total reward=22.99, Steps=38552, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1186, Total reward=39.67, Steps=38599, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1187, Total reward=40.84, Steps=38637, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1188, Total reward=53.79, Steps=38663, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1189, Total reward=18.25, Steps=38681, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1190, Total reward=70.96, Steps=38717, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1191, Total reward=55.62, Steps=38757, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1192, Total reward=22.96, Steps=38784, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1193, Total reward=54.46, Steps=38814, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1194, Total reward=30.02, Steps=38825, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1195, Total reward=1.4, Steps=38838, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1196, Total reward=17.36, Steps=38872, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1197, Total reward=50.19, Steps=38925, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1198, Total reward=25.31, Steps=38944, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1199, Total reward=86.23, Steps=39003, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1200, Total reward=61.27, Steps=39040, Training iteration=23
Policy training> Surrogate loss=0.023135455325245857, KL divergence=3.744124842341989e-05, Entropy=0.31511449813842773, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.025958966463804245, KL divergence=0.002169207902625203, Entropy=0.3146326541900635, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.08138814568519592, KL divergence=0.008647057227790356, Entropy=0.3030679225921631, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.054279349744319916, KL divergence=0.017389042302966118, Entropy=0.3095349669456482, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.03823761269450188, KL divergence=0.028916092589497566, Entropy=0.30281510949134827, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.08327488601207733, KL divergence=0.038217298686504364, Entropy=0.3029896914958954, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05829159915447235, KL divergence=0.05124939605593681, Entropy=0.2975020408630371, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.02337716892361641, KL divergence=0.064146026968956, Entropy=0.2944994568824768, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.08303998410701752, KL divergence=0.07865560054779053, Entropy=0.2896740436553955, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07638680189847946, KL divergence=0.08882587403059006, Entropy=0.2940080165863037, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/316_Step-39040.ckpt']
Uploaded 3 files for checkpoint 316 in 0.60 seconds
saved intermediate frozen graph: current/model/model_316.pb
Best checkpoint number: 314, Last checkpoint number: 314
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'308'}
Training> Name=main_level/agent, Worker=0, Episode=1201, Total reward=30.95, Steps=39061, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1202, Total reward=29.72, Steps=39080, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1203, Total reward=2.06, Steps=39109, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1204, Total reward=4.24, Steps=39132, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1205, Total reward=69.44, Steps=39190, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1206, Total reward=90.47, Steps=39266, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1207, Total reward=57.85, Steps=39308, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1208, Total reward=68.56, Steps=39348, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1209, Total reward=18.35, Steps=39368, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1210, Total reward=62.91, Steps=39418, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1211, Total reward=78.74, Steps=39457, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1212, Total reward=42.92, Steps=39489, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1213, Total reward=54.15, Steps=39519, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1214, Total reward=31.57, Steps=39540, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1215, Total reward=18.19, Steps=39569, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1216, Total reward=21.04, Steps=39598, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1217, Total reward=29.92, Steps=39633, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1218, Total reward=33.24, Steps=39680, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1219, Total reward=123.77, Steps=39804, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1220, Total reward=72.07, Steps=39846, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1221, Total reward=57.76, Steps=39874, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1222, Total reward=39.48, Steps=39894, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1223, Total reward=0.03, Steps=39921, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1224, Total reward=29.84, Steps=39982, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1225, Total reward=66.36, Steps=40033, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1226, Total reward=72.04, Steps=40082, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1227, Total reward=67.94, Steps=40133, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1228, Total reward=70.49, Steps=40179, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1229, Total reward=22.18, Steps=40195, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1230, Total reward=80.96, Steps=40229, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1231, Total reward=41.99, Steps=40268, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1232, Total reward=3.76, Steps=40279, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1233, Total reward=45.23, Steps=40309, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1234, Total reward=30.21, Steps=40320, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1235, Total reward=21.09, Steps=40368, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1236, Total reward=18.83, Steps=40386, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1237, Total reward=38.4, Steps=40422, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1238, Total reward=29.52, Steps=40461, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1239, Total reward=16.42, Steps=40482, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1240, Total reward=65.08, Steps=40521, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1241, Total reward=55.88, Steps=40575, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1242, Total reward=35.74, Steps=40604, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1243, Total reward=0.02, Steps=40620, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1244, Total reward=11.66, Steps=40645, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1245, Total reward=13.45, Steps=40668, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1246, Total reward=97.17, Steps=40719, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1247, Total reward=61.24, Steps=40760, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1248, Total reward=86.27, Steps=40824, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1249, Total reward=33.32, Steps=40856, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1250, Total reward=55.36, Steps=40893, Training iteration=24
Policy training> Surrogate loss=-0.020553773269057274, KL divergence=9.779475658433512e-05, Entropy=0.33592700958251953, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=0.00028636804199777544, KL divergence=0.006656771060079336, Entropy=0.3347488343715668, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05653074383735657, KL divergence=0.024735035374760628, Entropy=0.3232855498790741, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06011727452278137, KL divergence=0.044287145137786865, Entropy=0.3219739496707916, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.03471854701638222, KL divergence=0.059623684734106064, Entropy=0.31515762209892273, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.051427993923425674, KL divergence=0.07367692142724991, Entropy=0.30279943346977234, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.057624686509370804, KL divergence=0.08808638900518417, Entropy=0.3090040683746338, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.056981995701789856, KL divergence=0.102308489382267, Entropy=0.3080184757709503, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.062063008546829224, KL divergence=0.11065194010734558, Entropy=0.30348721146583557, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.059840455651283264, KL divergence=0.12086784839630127, Entropy=0.3081810474395752, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/317_Step-40893.ckpt']
Uploaded 3 files for checkpoint 317 in 0.64 seconds
saved intermediate frozen graph: current/model/model_317.pb
Best checkpoint number: 314, Last checkpoint number: 315
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'313'}
Training> Name=main_level/agent, Worker=0, Episode=1251, Total reward=19.79, Steps=40921, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1252, Total reward=1.82, Steps=40932, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1253, Total reward=47.83, Steps=40963, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1254, Total reward=29.64, Steps=40975, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1255, Total reward=30.85, Steps=41005, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1256, Total reward=17.54, Steps=41030, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1257, Total reward=42.3, Steps=41069, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1258, Total reward=32.29, Steps=41093, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1259, Total reward=52.79, Steps=41148, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1260, Total reward=79.29, Steps=41190, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1261, Total reward=53.1, Steps=41222, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1262, Total reward=34.88, Steps=41243, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1263, Total reward=3.68, Steps=41260, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1264, Total reward=84.4, Steps=41331, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1265, Total reward=75.5, Steps=41386, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1266, Total reward=49.52, Steps=41433, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1267, Total reward=37.55, Steps=41468, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1268, Total reward=62.33, Steps=41497, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1269, Total reward=75.79, Steps=41552, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1270, Total reward=35.71, Steps=41587, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1271, Total reward=38.68, Steps=41619, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1272, Total reward=3.78, Steps=41630, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1273, Total reward=37.89, Steps=41652, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1274, Total reward=36.06, Steps=41674, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1275, Total reward=24.57, Steps=41705, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1276, Total reward=15.3, Steps=41739, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1277, Total reward=44.61, Steps=41770, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1278, Total reward=30.08, Steps=41793, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1279, Total reward=19.88, Steps=41812, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1280, Total reward=68.28, Steps=41847, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1281, Total reward=28.35, Steps=41874, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1282, Total reward=36.02, Steps=41922, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1283, Total reward=5.84, Steps=41946, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1284, Total reward=2.18, Steps=41978, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1285, Total reward=3.4, Steps=41990, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1286, Total reward=120.55, Steps=42085, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1287, Total reward=25.23, Steps=42124, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1288, Total reward=37.93, Steps=42148, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1289, Total reward=36.75, Steps=42183, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1290, Total reward=74.32, Steps=42217, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1291, Total reward=55.92, Steps=42243, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1292, Total reward=65.46, Steps=42276, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1293, Total reward=58.73, Steps=42305, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1294, Total reward=40.56, Steps=42326, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1295, Total reward=26.61, Steps=42357, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1296, Total reward=22.59, Steps=42374, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1297, Total reward=46.39, Steps=42427, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1298, Total reward=35.18, Steps=42451, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1299, Total reward=15.01, Steps=42492, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1300, Total reward=65.52, Steps=42538, Training iteration=25
Policy training> Surrogate loss=-0.0003236668708268553, KL divergence=0.0001542873797006905, Entropy=0.35012054443359375, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.033264536410570145, KL divergence=0.007596829440444708, Entropy=0.3497726023197174, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03574022278189659, KL divergence=0.02334532141685486, Entropy=0.3466888964176178, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0389125794172287, KL divergence=0.04154267534613609, Entropy=0.3332241475582123, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.050087425857782364, KL divergence=0.05916335806250572, Entropy=0.32862216234207153, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06486990302801132, KL divergence=0.0746140405535698, Entropy=0.323032408952713, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06370190531015396, KL divergence=0.08601215481758118, Entropy=0.320066899061203, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.048542097210884094, KL divergence=0.09709474444389343, Entropy=0.3214964270591736, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05403925105929375, KL divergence=0.10818755626678467, Entropy=0.3191697299480438, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06492485851049423, KL divergence=0.110613614320755, Entropy=0.32124483585357666, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/318_Step-42538.ckpt']
Uploaded 3 files for checkpoint 318 in 0.57 seconds
saved intermediate frozen graph: current/model/model_318.pb
Best checkpoint number: 314, Last checkpoint number: 316
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'315'}
Training> Name=main_level/agent, Worker=0, Episode=1301, Total reward=56.86, Steps=42569, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1302, Total reward=26.82, Steps=42602, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1303, Total reward=3.02, Steps=42626, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1304, Total reward=72.13, Steps=42697, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1305, Total reward=16.26, Steps=42712, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1306, Total reward=39.58, Steps=42743, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1307, Total reward=35.16, Steps=42778, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1308, Total reward=56.01, Steps=42806, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1309, Total reward=19.7, Steps=42827, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1310, Total reward=72.44, Steps=42862, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1311, Total reward=43.39, Steps=42902, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1312, Total reward=38.33, Steps=42933, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1313, Total reward=43.35, Steps=42954, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1314, Total reward=44.41, Steps=42976, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1315, Total reward=31.77, Steps=43008, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1316, Total reward=12.26, Steps=43041, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1317, Total reward=36.06, Steps=43082, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1318, Total reward=32.67, Steps=43122, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1319, Total reward=20.04, Steps=43143, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1320, Total reward=84.59, Steps=43181, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1321, Total reward=57.78, Steps=43211, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1322, Total reward=33.05, Steps=43232, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1323, Total reward=10.29, Steps=43264, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1324, Total reward=0.03, Steps=43291, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1325, Total reward=79.58, Steps=43355, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1326, Total reward=77.85, Steps=43460, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1327, Total reward=6.6, Steps=43475, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1328, Total reward=24.08, Steps=43490, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1329, Total reward=40.3, Steps=43521, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1330, Total reward=66.47, Steps=43557, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1331, Total reward=19.25, Steps=43587, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1332, Total reward=9.41, Steps=43599, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1333, Total reward=32.4, Steps=43620, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1334, Total reward=32.35, Steps=43642, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1335, Total reward=33.27, Steps=43672, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1336, Total reward=23.66, Steps=43704, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1337, Total reward=47.71, Steps=43740, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1338, Total reward=37.85, Steps=43765, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1339, Total reward=74.96, Steps=43813, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1340, Total reward=83.3, Steps=43851, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1341, Total reward=66.03, Steps=43883, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1342, Total reward=36.16, Steps=43905, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1343, Total reward=96.57, Steps=43996, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1344, Total reward=14.49, Steps=44028, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1345, Total reward=6.73, Steps=44040, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1346, Total reward=53.88, Steps=44121, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1347, Total reward=58.25, Steps=44158, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1348, Total reward=51.54, Steps=44181, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1349, Total reward=70.03, Steps=44236, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1350, Total reward=69.19, Steps=44287, Training iteration=26
Policy training> Surrogate loss=-0.004152265843003988, KL divergence=9.255189797841012e-05, Entropy=0.3396775424480438, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02619463950395584, KL divergence=0.007330369204282761, Entropy=0.3340415060520172, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.025510182604193687, KL divergence=0.021070709452033043, Entropy=0.32422778010368347, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.026766164228320122, KL divergence=0.038480259478092194, Entropy=0.3176715075969696, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04704270884394646, KL divergence=0.0579831600189209, Entropy=0.32179391384124756, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.056098029017448425, KL divergence=0.07203671336174011, Entropy=0.3110167682170868, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0642751008272171, KL divergence=0.08640775829553604, Entropy=0.31400132179260254, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05311824008822441, KL divergence=0.10010556131601334, Entropy=0.3114657700061798, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07527080178260803, KL divergence=0.10844974964857101, Entropy=0.31096383929252625, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0620444230735302, KL divergence=0.11165126413106918, Entropy=0.3042052686214447, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/319_Step-44287.ckpt']
Uploaded 3 files for checkpoint 319 in 0.59 seconds
saved intermediate frozen graph: current/model/model_319.pb
Best checkpoint number: 314, Last checkpoint number: 317
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'316'}
Training> Name=main_level/agent, Worker=0, Episode=1351, Total reward=44.34, Steps=44327, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1352, Total reward=43.04, Steps=44358, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1353, Total reward=65.63, Steps=44389, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1354, Total reward=43.79, Steps=44410, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1355, Total reward=20.95, Steps=44441, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1356, Total reward=11.28, Steps=44459, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1357, Total reward=16.17, Steps=44479, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1358, Total reward=31.72, Steps=44532, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1359, Total reward=68.86, Steps=44582, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1360, Total reward=85.03, Steps=44623, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1361, Total reward=42.11, Steps=44650, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1362, Total reward=33.6, Steps=44694, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1363, Total reward=3.46, Steps=44714, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1364, Total reward=14.92, Steps=44748, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1365, Total reward=11.48, Steps=44768, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1366, Total reward=59.08, Steps=44813, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1367, Total reward=62.57, Steps=44852, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1368, Total reward=49.71, Steps=44878, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1369, Total reward=23.64, Steps=44903, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1370, Total reward=60.16, Steps=44948, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1371, Total reward=37.81, Steps=44989, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1372, Total reward=3.75, Steps=45001, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1373, Total reward=48.78, Steps=45031, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1374, Total reward=27.35, Steps=45042, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1375, Total reward=15.5, Steps=45069, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1376, Total reward=26.4, Steps=45095, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1377, Total reward=25.49, Steps=45126, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1378, Total reward=31.43, Steps=45148, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1379, Total reward=56.91, Steps=45196, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1380, Total reward=71.25, Steps=45256, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1381, Total reward=48.36, Steps=45285, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1382, Total reward=28.06, Steps=45307, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1383, Total reward=2.97, Steps=45334, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1384, Total reward=18.8, Steps=45391, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1385, Total reward=23.79, Steps=45425, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1386, Total reward=45.69, Steps=45471, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1387, Total reward=50.3, Steps=45510, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1388, Total reward=49.61, Steps=45537, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1389, Total reward=87.16, Steps=45580, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1390, Total reward=57.82, Steps=45628, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1391, Total reward=28.83, Steps=45660, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1392, Total reward=53.52, Steps=45692, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1393, Total reward=28.4, Steps=45718, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1394, Total reward=39.87, Steps=45752, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1395, Total reward=19.08, Steps=45778, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1396, Total reward=50.3, Steps=45852, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1397, Total reward=36.34, Steps=45883, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1398, Total reward=22.32, Steps=45899, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1399, Total reward=9.54, Steps=45921, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1400, Total reward=96.11, Steps=45960, Training iteration=27
Policy training> Surrogate loss=-0.005455354694277048, KL divergence=0.00010215258225798607, Entropy=0.329823762178421, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.013612146489322186, KL divergence=0.005502635147422552, Entropy=0.3292781412601471, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.053212255239486694, KL divergence=0.021470220759510994, Entropy=0.31747424602508545, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06035204231739044, KL divergence=0.03967295214533806, Entropy=0.3112020790576935, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04126591235399246, KL divergence=0.057121675461530685, Entropy=0.3066360056400299, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05756191536784172, KL divergence=0.07604057341814041, Entropy=0.3092988431453705, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06959662586450577, KL divergence=0.09264111518859863, Entropy=0.29832419753074646, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06419407576322556, KL divergence=0.10225850343704224, Entropy=0.2985149621963501, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06701802462339401, KL divergence=0.11418633908033371, Entropy=0.3005628287792206, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06115427613258362, KL divergence=0.12015002965927124, Entropy=0.30097270011901855, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/320_Step-45960.ckpt']
Uploaded 3 files for checkpoint 320 in 0.57 seconds
saved intermediate frozen graph: current/model/model_320.pb
Best checkpoint number: 314, Last checkpoint number: 318
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'317'}
Training> Name=main_level/agent, Worker=0, Episode=1401, Total reward=56.71, Steps=45999, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1402, Total reward=29.56, Steps=46040, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1403, Total reward=9.37, Steps=46055, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1404, Total reward=63.73, Steps=46127, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1405, Total reward=56.8, Steps=46193, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1406, Total reward=96.77, Steps=46258, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1407, Total reward=44.38, Steps=46297, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1408, Total reward=54.88, Steps=46339, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1409, Total reward=27.76, Steps=46380, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1410, Total reward=54.34, Steps=46417, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1411, Total reward=69.61, Steps=46458, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1412, Total reward=23.92, Steps=46489, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1413, Total reward=19.13, Steps=46508, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1414, Total reward=31.37, Steps=46519, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1415, Total reward=24.12, Steps=46561, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1416, Total reward=16.12, Steps=46580, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1417, Total reward=40.55, Steps=46644, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1418, Total reward=41.16, Steps=46670, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1419, Total reward=57.12, Steps=46715, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1420, Total reward=73.15, Steps=46755, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1421, Total reward=58.93, Steps=46786, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1422, Total reward=42.68, Steps=46827, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1423, Total reward=17.92, Steps=46870, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1424, Total reward=73.73, Steps=46951, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1425, Total reward=21.18, Steps=46986, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1426, Total reward=64.87, Steps=47034, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1427, Total reward=68.5, Steps=47087, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1428, Total reward=72.77, Steps=47145, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1429, Total reward=21.95, Steps=47164, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1430, Total reward=64.48, Steps=47200, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1431, Total reward=44.53, Steps=47241, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1432, Total reward=52.08, Steps=47273, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1433, Total reward=41.43, Steps=47303, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1434, Total reward=38.95, Steps=47325, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1435, Total reward=21.09, Steps=47354, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1436, Total reward=16.79, Steps=47381, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1437, Total reward=51.93, Steps=47435, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1438, Total reward=36.59, Steps=47460, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1439, Total reward=42.12, Steps=47495, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1440, Total reward=87.87, Steps=47540, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1441, Total reward=68.09, Steps=47594, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1442, Total reward=43.72, Steps=47652, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1443, Total reward=20.76, Steps=47679, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1444, Total reward=10.25, Steps=47708, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1445, Total reward=6.27, Steps=47748, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1446, Total reward=102.73, Steps=47804, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1447, Total reward=9.83, Steps=47834, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1448, Total reward=64.04, Steps=47865, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1449, Total reward=34.4, Steps=47899, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1450, Total reward=56.86, Steps=47935, Training iteration=28
Policy training> Surrogate loss=0.018854180350899696, KL divergence=0.00011770093260565773, Entropy=0.3333568274974823, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.01890791766345501, KL divergence=0.005520109552890062, Entropy=0.33786508440971375, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.026753896847367287, KL divergence=0.01787901669740677, Entropy=0.3368915617465973, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03656202182173729, KL divergence=0.033457159996032715, Entropy=0.3331812918186188, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04195873439311981, KL divergence=0.0460224449634552, Entropy=0.31628668308258057, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04097118228673935, KL divergence=0.05794322490692139, Entropy=0.31255272030830383, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.054994720965623856, KL divergence=0.07668274641036987, Entropy=0.31958475708961487, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07131415605545044, KL divergence=0.08487638831138611, Entropy=0.31695011258125305, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07628272473812103, KL divergence=0.09495499730110168, Entropy=0.3121550381183624, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.034783411771059036, KL divergence=0.1065954864025116, Entropy=0.31815263628959656, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/321_Step-47935.ckpt']
Uploaded 3 files for checkpoint 321 in 0.56 seconds
saved intermediate frozen graph: current/model/model_321.pb
Best checkpoint number: 314, Last checkpoint number: 319
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'318'}
Training> Name=main_level/agent, Worker=0, Episode=1451, Total reward=47.94, Steps=47976, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1452, Total reward=67.21, Steps=48017, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1453, Total reward=8.44, Steps=48036, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1454, Total reward=44.83, Steps=48058, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1455, Total reward=20.74, Steps=48087, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1456, Total reward=17.89, Steps=48115, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1457, Total reward=48.25, Steps=48176, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1458, Total reward=31.14, Steps=48199, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1459, Total reward=9.84, Steps=48213, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1460, Total reward=80.85, Steps=48252, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1461, Total reward=33.24, Steps=48280, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1462, Total reward=29.99, Steps=48302, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1463, Total reward=32.19, Steps=48344, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1464, Total reward=5.78, Steps=48371, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1465, Total reward=16.18, Steps=48408, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1466, Total reward=111.49, Steps=48486, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1467, Total reward=60.31, Steps=48526, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1468, Total reward=65.79, Steps=48570, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1469, Total reward=36.59, Steps=48595, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1470, Total reward=59.14, Steps=48634, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1471, Total reward=46.25, Steps=48673, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1472, Total reward=40.77, Steps=48700, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1473, Total reward=47.68, Steps=48729, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1474, Total reward=31.76, Steps=48750, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1475, Total reward=17.82, Steps=48776, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1476, Total reward=12.27, Steps=48794, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1477, Total reward=51.13, Steps=48833, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1478, Total reward=51.82, Steps=48880, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1479, Total reward=9.61, Steps=48896, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1480, Total reward=65.32, Steps=48928, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1481, Total reward=35.54, Steps=48956, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1482, Total reward=44.77, Steps=48980, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1483, Total reward=5.94, Steps=48995, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1484, Total reward=13.61, Steps=49022, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1485, Total reward=27.51, Steps=49059, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1486, Total reward=72.1, Steps=49104, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1487, Total reward=21.72, Steps=49138, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1488, Total reward=59.18, Steps=49166, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1489, Total reward=20.48, Steps=49184, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1490, Total reward=84.03, Steps=49244, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1491, Total reward=45.6, Steps=49281, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1492, Total reward=34.86, Steps=49311, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1493, Total reward=21.31, Steps=49322, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1494, Total reward=35.6, Steps=49344, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1495, Total reward=31.63, Steps=49368, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1496, Total reward=95.94, Steps=49474, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1497, Total reward=32.44, Steps=49505, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1498, Total reward=36.63, Steps=49529, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1499, Total reward=75.61, Steps=49580, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1500, Total reward=87.9, Steps=49617, Training iteration=29
Policy training> Surrogate loss=-0.0030304836109280586, KL divergence=9.975679131457582e-05, Entropy=0.3249981105327606, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.024612732231616974, KL divergence=0.007767044007778168, Entropy=0.3302987515926361, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.026598243042826653, KL divergence=0.02467905730009079, Entropy=0.32337135076522827, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.06470244377851486, KL divergence=0.04318447411060333, Entropy=0.3169962167739868, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.057538360357284546, KL divergence=0.05977479740977287, Entropy=0.3086286783218384, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.049869317561388016, KL divergence=0.07817857712507248, Entropy=0.312377005815506, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06518471986055374, KL divergence=0.09129801392555237, Entropy=0.3029541075229645, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.053705524653196335, KL divergence=0.1010715588927269, Entropy=0.30949196219444275, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05343138054013252, KL divergence=0.1107180118560791, Entropy=0.3056342899799347, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.065089650452137, KL divergence=0.1210797131061554, Entropy=0.3078024089336395, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/322_Step-49617.ckpt']
Uploaded 3 files for checkpoint 322 in 0.60 seconds
saved intermediate frozen graph: current/model/model_322.pb
Best checkpoint number: 314, Last checkpoint number: 320
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'319'}
Training> Name=main_level/agent, Worker=0, Episode=1501, Total reward=47.38, Steps=49644, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1502, Total reward=32.68, Steps=49670, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1503, Total reward=18.4, Steps=49705, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1504, Total reward=16.52, Steps=49731, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1505, Total reward=12.95, Steps=49754, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1506, Total reward=64.68, Steps=49804, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1507, Total reward=69.35, Steps=49857, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1508, Total reward=72.48, Steps=49941, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1509, Total reward=18.68, Steps=49957, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1510, Total reward=30.15, Steps=49981, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1511, Total reward=53.76, Steps=50020, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1512, Total reward=64.16, Steps=50063, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1513, Total reward=42.24, Steps=50094, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1514, Total reward=30.76, Steps=50105, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1515, Total reward=7.4, Steps=50129, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1516, Total reward=21.21, Steps=50160, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1517, Total reward=55.21, Steps=50195, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1518, Total reward=34.73, Steps=50222, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1519, Total reward=18.85, Steps=50246, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1520, Total reward=67.5, Steps=50286, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1521, Total reward=54.89, Steps=50320, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1522, Total reward=30.48, Steps=50353, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1523, Total reward=25.0, Steps=50414, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1524, Total reward=0.03, Steps=50442, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1525, Total reward=6.29, Steps=50458, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1526, Total reward=42.04, Steps=50486, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1527, Total reward=30.44, Steps=50525, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1528, Total reward=62.13, Steps=50555, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1529, Total reward=76.6, Steps=50602, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1530, Total reward=63.64, Steps=50655, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1531, Total reward=47.59, Steps=50693, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1532, Total reward=48.41, Steps=50724, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1533, Total reward=42.45, Steps=50752, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1534, Total reward=44.17, Steps=50774, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1535, Total reward=12.47, Steps=50801, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1536, Total reward=20.69, Steps=50830, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1537, Total reward=112.45, Steps=50915, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1538, Total reward=24.79, Steps=50934, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1539, Total reward=14.57, Steps=50954, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1540, Total reward=55.86, Steps=50993, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1541, Total reward=54.01, Steps=51022, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1542, Total reward=31.9, Steps=51041, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1543, Total reward=2.93, Steps=51080, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1544, Total reward=14.4, Steps=51106, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1545, Total reward=3.39, Steps=51122, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1546, Total reward=43.49, Steps=51174, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1547, Total reward=12.28, Steps=51210, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1548, Total reward=67.45, Steps=51269, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1549, Total reward=71.16, Steps=51329, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1550, Total reward=21.99, Steps=51354, Training iteration=30
Policy training> Surrogate loss=0.007624639663845301, KL divergence=0.00012324820272624493, Entropy=0.32619649171829224, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.020457183942198753, KL divergence=0.008518106304109097, Entropy=0.31710729002952576, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.031167373061180115, KL divergence=0.023521557450294495, Entropy=0.3085515797138214, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04943697154521942, KL divergence=0.04420390725135803, Entropy=0.30968359112739563, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.03932527080178261, KL divergence=0.057297978550195694, Entropy=0.3024977445602417, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.033697281032800674, KL divergence=0.07750384509563446, Entropy=0.300848126411438, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05940994992852211, KL divergence=0.09055149555206299, Entropy=0.295734167098999, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05704927071928978, KL divergence=0.09399744868278503, Entropy=0.29552796483039856, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06082598865032196, KL divergence=0.10750362277030945, Entropy=0.2945501208305359, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05653107166290283, KL divergence=0.11451572179794312, Entropy=0.2978433668613434, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/323_Step-51354.ckpt']
Uploaded 3 files for checkpoint 323 in 0.58 seconds
saved intermediate frozen graph: current/model/model_323.pb
Best checkpoint number: 314, Last checkpoint number: 321
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'320'}
Training> Name=main_level/agent, Worker=0, Episode=1551, Total reward=48.48, Steps=51393, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1552, Total reward=48.13, Steps=51422, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1553, Total reward=47.23, Steps=51452, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1554, Total reward=39.15, Steps=51473, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1555, Total reward=24.77, Steps=51502, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1556, Total reward=30.78, Steps=51532, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1557, Total reward=40.54, Steps=51571, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1558, Total reward=50.17, Steps=51635, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1559, Total reward=66.34, Steps=51684, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1560, Total reward=52.37, Steps=51722, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1561, Total reward=51.61, Steps=51750, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1562, Total reward=30.83, Steps=51775, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1563, Total reward=3.43, Steps=51795, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1564, Total reward=79.53, Steps=51858, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1565, Total reward=80.97, Steps=51921, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1566, Total reward=80.57, Steps=51965, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1567, Total reward=3.31, Steps=51978, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1568, Total reward=61.03, Steps=52005, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1569, Total reward=36.83, Steps=52047, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1570, Total reward=19.02, Steps=52080, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1571, Total reward=50.07, Steps=52119, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1572, Total reward=33.24, Steps=52149, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1573, Total reward=43.88, Steps=52179, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1574, Total reward=37.85, Steps=52200, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1575, Total reward=18.88, Steps=52230, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1576, Total reward=10.79, Steps=52246, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1577, Total reward=52.37, Steps=52287, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1578, Total reward=27.74, Steps=52301, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1579, Total reward=7.12, Steps=52321, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1580, Total reward=85.25, Steps=52358, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1581, Total reward=57.19, Steps=52388, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1582, Total reward=33.01, Steps=52443, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1583, Total reward=19.2, Steps=52475, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1584, Total reward=19.01, Steps=52501, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1585, Total reward=97.21, Steps=52554, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1586, Total reward=85.94, Steps=52604, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1587, Total reward=16.68, Steps=52619, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1588, Total reward=75.34, Steps=52660, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1589, Total reward=28.75, Steps=52684, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1590, Total reward=120.83, Steps=52743, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1591, Total reward=18.52, Steps=52769, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1592, Total reward=40.24, Steps=52797, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1593, Total reward=36.71, Steps=52818, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1594, Total reward=38.41, Steps=52841, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1595, Total reward=29.43, Steps=52873, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1596, Total reward=24.05, Steps=52903, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1597, Total reward=49.61, Steps=52937, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1598, Total reward=35.81, Steps=52960, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1599, Total reward=22.71, Steps=52982, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1600, Total reward=96.94, Steps=53020, Training iteration=31
Policy training> Surrogate loss=0.0007592241163365543, KL divergence=0.00014358777843881398, Entropy=0.3228831887245178, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04054230824112892, KL divergence=0.007093256805092096, Entropy=0.3200221359729767, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04023347049951553, KL divergence=0.0244793388992548, Entropy=0.3127196729183197, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04124520719051361, KL divergence=0.04141295328736305, Entropy=0.3098275363445282, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06646700948476791, KL divergence=0.06061851978302002, Entropy=0.30210426449775696, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.054611895233392715, KL divergence=0.07593893259763718, Entropy=0.29601725935935974, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0677015408873558, KL divergence=0.08801516890525818, Entropy=0.29427891969680786, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05170450732111931, KL divergence=0.09677837044000626, Entropy=0.28675660490989685, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04852946102619171, KL divergence=0.11576145142316818, Entropy=0.29336485266685486, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.04746077582240105, KL divergence=0.1252998262643814, Entropy=0.2947392761707306, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/324_Step-53020.ckpt']
Uploaded 3 files for checkpoint 324 in 0.59 seconds
saved intermediate frozen graph: current/model/model_324.pb
Best checkpoint number: 314, Last checkpoint number: 322
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'321'}
Training> Name=main_level/agent, Worker=0, Episode=1601, Total reward=27.84, Steps=53046, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1602, Total reward=35.38, Steps=53071, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1603, Total reward=0.03, Steps=53096, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1604, Total reward=7.29, Steps=53113, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1605, Total reward=11.31, Steps=53139, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1606, Total reward=10.56, Steps=53159, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1607, Total reward=34.77, Steps=53196, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1608, Total reward=50.41, Steps=53222, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1609, Total reward=93.89, Steps=53278, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1610, Total reward=36.94, Steps=53319, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1611, Total reward=44.32, Steps=53347, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1612, Total reward=46.32, Steps=53378, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1613, Total reward=43.6, Steps=53405, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1614, Total reward=41.32, Steps=53426, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1615, Total reward=31.12, Steps=53449, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1616, Total reward=19.21, Steps=53476, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1617, Total reward=47.74, Steps=53513, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1618, Total reward=42.56, Steps=53536, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1619, Total reward=10.27, Steps=53556, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1620, Total reward=75.59, Steps=53590, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1621, Total reward=49.27, Steps=53619, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1622, Total reward=38.48, Steps=53645, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1623, Total reward=7.15, Steps=53662, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1624, Total reward=1.67, Steps=53689, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1625, Total reward=19.68, Steps=53728, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1626, Total reward=69.93, Steps=53775, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1627, Total reward=59.55, Steps=53829, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1628, Total reward=63.84, Steps=53870, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1629, Total reward=25.51, Steps=53896, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1630, Total reward=48.28, Steps=53932, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1631, Total reward=15.37, Steps=53956, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1632, Total reward=49.58, Steps=53987, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1633, Total reward=50.85, Steps=54015, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1634, Total reward=40.72, Steps=54036, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1635, Total reward=16.32, Steps=54083, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1636, Total reward=21.81, Steps=54112, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1637, Total reward=43.1, Steps=54145, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1638, Total reward=34.92, Steps=54168, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1639, Total reward=11.06, Steps=54189, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1640, Total reward=83.93, Steps=54226, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1641, Total reward=37.29, Steps=54254, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1642, Total reward=42.9, Steps=54275, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1643, Total reward=20.3, Steps=54304, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1644, Total reward=4.03, Steps=54336, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1645, Total reward=10.9, Steps=54354, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1646, Total reward=69.88, Steps=54402, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1647, Total reward=42.62, Steps=54444, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1648, Total reward=60.96, Steps=54472, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1649, Total reward=33.71, Steps=54512, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1650, Total reward=58.06, Steps=54558, Training iteration=32
Policy training> Surrogate loss=8.859360968926921e-05, KL divergence=8.49846619530581e-05, Entropy=0.31876447796821594, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.027527114376425743, KL divergence=0.006295122671872377, Entropy=0.3135826289653778, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03768332302570343, KL divergence=0.02070741541683674, Entropy=0.3061228394508362, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.045010458678007126, KL divergence=0.03816048800945282, Entropy=0.3018484115600586, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.047328706830739975, KL divergence=0.055519938468933105, Entropy=0.2974572479724884, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04964987561106682, KL divergence=0.07116452604532242, Entropy=0.29454901814460754, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05216948315501213, KL divergence=0.08447308093309402, Entropy=0.29235783219337463, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05402378365397453, KL divergence=0.0953567624092102, Entropy=0.291592001914978, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05447227135300636, KL divergence=0.10466540604829788, Entropy=0.29049935936927795, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.054488811641931534, KL divergence=0.11312549561262131, Entropy=0.2921622097492218, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/325_Step-54558.ckpt']
Uploaded 3 files for checkpoint 325 in 0.61 seconds
saved intermediate frozen graph: current/model/model_325.pb
Best checkpoint number: 314, Last checkpoint number: 323
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'322'}
Training> Name=main_level/agent, Worker=0, Episode=1651, Total reward=52.8, Steps=54601, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1652, Total reward=49.92, Steps=54631, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1653, Total reward=51.02, Steps=54661, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1654, Total reward=28.05, Steps=54682, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1655, Total reward=23.91, Steps=54703, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1656, Total reward=20.91, Steps=54739, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1657, Total reward=49.58, Steps=54785, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1658, Total reward=36.34, Steps=54810, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1659, Total reward=18.6, Steps=54827, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1660, Total reward=74.48, Steps=54867, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1661, Total reward=43.04, Steps=54894, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1662, Total reward=50.11, Steps=54935, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1663, Total reward=3.07, Steps=54976, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1664, Total reward=62.52, Steps=55043, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1665, Total reward=76.04, Steps=55133, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1666, Total reward=14.92, Steps=55159, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1667, Total reward=6.73, Steps=55172, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1668, Total reward=77.9, Steps=55212, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1669, Total reward=25.66, Steps=55240, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1670, Total reward=64.32, Steps=55275, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1671, Total reward=20.63, Steps=55299, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1672, Total reward=53.74, Steps=55341, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1673, Total reward=0.01, Steps=55355, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1674, Total reward=35.13, Steps=55377, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1675, Total reward=24.85, Steps=55421, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1676, Total reward=17.68, Steps=55436, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1677, Total reward=42.67, Steps=55472, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1678, Total reward=29.43, Steps=55485, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1679, Total reward=17.44, Steps=55506, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1680, Total reward=57.25, Steps=55538, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1681, Total reward=39.73, Steps=55567, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1682, Total reward=37.94, Steps=55593, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1683, Total reward=15.57, Steps=55639, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1684, Total reward=77.6, Steps=55706, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1685, Total reward=21.88, Steps=55737, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1686, Total reward=36.06, Steps=55768, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1687, Total reward=17.93, Steps=55804, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1688, Total reward=40.69, Steps=55828, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1689, Total reward=57.46, Steps=55875, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1690, Total reward=14.0, Steps=55888, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1691, Total reward=41.99, Steps=55917, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1692, Total reward=43.8, Steps=55949, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1693, Total reward=41.35, Steps=55971, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1694, Total reward=42.22, Steps=56001, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1695, Total reward=14.32, Steps=56031, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1696, Total reward=49.36, Steps=56083, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1697, Total reward=20.35, Steps=56103, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1698, Total reward=78.56, Steps=56193, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1699, Total reward=11.65, Steps=56208, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1700, Total reward=94.85, Steps=56248, Training iteration=33
Policy training> Surrogate loss=0.006113964598625898, KL divergence=0.00011553405784070492, Entropy=0.34303703904151917, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02338920347392559, KL divergence=0.007823307998478413, Entropy=0.340588241815567, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04788022115826607, KL divergence=0.024110453203320503, Entropy=0.3348095715045929, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.050500769168138504, KL divergence=0.04305986687541008, Entropy=0.32809245586395264, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0484476201236248, KL divergence=0.06260498613119125, Entropy=0.32400164008140564, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0538802444934845, KL divergence=0.0781295970082283, Entropy=0.3219357430934906, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06758516281843185, KL divergence=0.09237968921661377, Entropy=0.316743940114975, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.060572970658540726, KL divergence=0.10436618328094482, Entropy=0.3112853467464447, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07693418115377426, KL divergence=0.11262781172990799, Entropy=0.3113236129283905, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.059624236077070236, KL divergence=0.1204524114727974, Entropy=0.3073541820049286, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/326_Step-56248.ckpt']
Uploaded 3 files for checkpoint 326 in 0.52 seconds
saved intermediate frozen graph: current/model/model_326.pb
Best checkpoint number: 314, Last checkpoint number: 324
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'323'}
Training> Name=main_level/agent, Worker=0, Episode=1701, Total reward=47.6, Steps=56276, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1702, Total reward=27.85, Steps=56295, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1703, Total reward=0.02, Steps=56319, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1704, Total reward=86.64, Steps=56399, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1705, Total reward=18.02, Steps=56422, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1706, Total reward=26.23, Steps=56446, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1707, Total reward=9.94, Steps=56467, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1708, Total reward=79.72, Steps=56523, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1709, Total reward=54.89, Steps=56569, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1710, Total reward=68.97, Steps=56607, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1711, Total reward=48.38, Steps=56635, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1712, Total reward=69.9, Steps=56675, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1713, Total reward=36.74, Steps=56707, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1714, Total reward=30.72, Steps=56718, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1715, Total reward=30.35, Steps=56749, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1716, Total reward=18.98, Steps=56780, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1717, Total reward=43.52, Steps=56815, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1718, Total reward=32.87, Steps=56840, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1719, Total reward=52.55, Steps=56898, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1720, Total reward=37.44, Steps=56940, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1721, Total reward=22.28, Steps=56963, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1722, Total reward=28.5, Steps=56985, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1723, Total reward=22.23, Steps=57019, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1724, Total reward=95.21, Steps=57089, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1725, Total reward=7.83, Steps=57105, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1726, Total reward=93.99, Steps=57154, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1727, Total reward=77.36, Steps=57220, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1728, Total reward=66.09, Steps=57261, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1729, Total reward=13.06, Steps=57275, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1730, Total reward=73.89, Steps=57318, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1731, Total reward=52.54, Steps=57357, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1732, Total reward=44.12, Steps=57387, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1733, Total reward=19.63, Steps=57397, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1734, Total reward=6.34, Steps=57418, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1735, Total reward=27.54, Steps=57449, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1736, Total reward=25.88, Steps=57479, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1737, Total reward=13.31, Steps=57491, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1738, Total reward=45.69, Steps=57516, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1739, Total reward=5.78, Steps=57531, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1740, Total reward=89.22, Steps=57574, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1741, Total reward=46.98, Steps=57600, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1742, Total reward=35.46, Steps=57629, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1743, Total reward=0.03, Steps=57657, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1744, Total reward=12.16, Steps=57708, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1745, Total reward=40.9, Steps=57744, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1746, Total reward=50.59, Steps=57780, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1747, Total reward=47.44, Steps=57812, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1748, Total reward=25.7, Steps=57835, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1749, Total reward=36.87, Steps=57882, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1750, Total reward=52.18, Steps=57931, Training iteration=34
Policy training> Surrogate loss=0.006147290114313364, KL divergence=5.4601779993390664e-05, Entropy=0.34144720435142517, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.035906657576560974, KL divergence=0.006309951189905405, Entropy=0.3365665376186371, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04255939647555351, KL divergence=0.0249644722789526, Entropy=0.3298650085926056, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.043573636561632156, KL divergence=0.04314550384879112, Entropy=0.3199232518672943, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05025598034262657, KL divergence=0.06439021974802017, Entropy=0.31434813141822815, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05142045393586159, KL divergence=0.07392438501119614, Entropy=0.3119782507419586, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.060259055346250534, KL divergence=0.09347018599510193, Entropy=0.30915531516075134, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06098256632685661, KL divergence=0.10692143440246582, Entropy=0.3080977201461792, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.07323264330625534, KL divergence=0.11673877388238907, Entropy=0.3087596893310547, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05662321671843529, KL divergence=0.12783904373645782, Entropy=0.3123854100704193, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/327_Step-57931.ckpt']
Uploaded 3 files for checkpoint 327 in 0.58 seconds
saved intermediate frozen graph: current/model/model_327.pb
Best checkpoint number: 314, Last checkpoint number: 325
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'324'}
Training> Name=main_level/agent, Worker=0, Episode=1751, Total reward=55.53, Steps=57968, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1752, Total reward=41.89, Steps=57998, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1753, Total reward=0.02, Steps=58014, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1754, Total reward=45.42, Steps=58036, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1755, Total reward=28.75, Steps=58067, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1756, Total reward=19.88, Steps=58097, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1757, Total reward=44.75, Steps=58128, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1758, Total reward=24.92, Steps=58150, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1759, Total reward=32.17, Steps=58181, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1760, Total reward=63.63, Steps=58218, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1761, Total reward=55.18, Steps=58246, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1762, Total reward=31.28, Steps=58275, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1763, Total reward=6.92, Steps=58315, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1764, Total reward=11.44, Steps=58342, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1765, Total reward=2.79, Steps=58354, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1766, Total reward=72.4, Steps=58412, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1767, Total reward=60.31, Steps=58451, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1768, Total reward=60.06, Steps=58492, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1769, Total reward=24.88, Steps=58516, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1770, Total reward=48.07, Steps=58569, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1771, Total reward=29.68, Steps=58599, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1772, Total reward=17.47, Steps=58624, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1773, Total reward=50.61, Steps=58653, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1774, Total reward=49.02, Steps=58675, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1775, Total reward=36.08, Steps=58705, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1776, Total reward=25.83, Steps=58734, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1777, Total reward=96.83, Steps=58804, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1778, Total reward=32.39, Steps=58828, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1779, Total reward=78.03, Steps=58890, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1780, Total reward=25.17, Steps=58917, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1781, Total reward=146.56, Steps=59021, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1782, Total reward=29.54, Steps=59043, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1783, Total reward=30.6, Steps=59097, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1784, Total reward=13.78, Steps=59122, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1785, Total reward=15.08, Steps=59139, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1786, Total reward=67.15, Steps=59186, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1787, Total reward=25.68, Steps=59223, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1788, Total reward=65.67, Steps=59264, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1789, Total reward=11.04, Steps=59278, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1790, Total reward=50.82, Steps=59317, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1791, Total reward=30.83, Steps=59344, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1792, Total reward=60.55, Steps=59379, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1793, Total reward=51.66, Steps=59407, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1794, Total reward=39.32, Steps=59429, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1795, Total reward=29.54, Steps=59457, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1796, Total reward=30.45, Steps=59485, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1797, Total reward=46.41, Steps=59533, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1798, Total reward=37.43, Steps=59575, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1799, Total reward=8.1, Steps=59611, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1800, Total reward=80.47, Steps=59652, Training iteration=35
Policy training> Surrogate loss=0.012906539253890514, KL divergence=0.00013403645425569266, Entropy=0.3577454090118408, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03848552331328392, KL divergence=0.0070036775432527065, Entropy=0.3461294174194336, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03996717929840088, KL divergence=0.021520724520087242, Entropy=0.3381107747554779, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.0537395216524601, KL divergence=0.039161551743745804, Entropy=0.3381960391998291, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06493262946605682, KL divergence=0.05816129967570305, Entropy=0.3278760612010956, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0697106122970581, KL divergence=0.07004164159297943, Entropy=0.32914838194847107, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.047498177736997604, KL divergence=0.08343357592821121, Entropy=0.32417914271354675, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.053650423884391785, KL divergence=0.0983501672744751, Entropy=0.31913328170776367, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05885469540953636, KL divergence=0.11061066389083862, Entropy=0.32402047514915466, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07728756964206696, KL divergence=0.12172345072031021, Entropy=0.32340121269226074, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/328_Step-59652.ckpt']
Uploaded 3 files for checkpoint 328 in 0.58 seconds
saved intermediate frozen graph: current/model/model_328.pb
Best checkpoint number: 314, Last checkpoint number: 326
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'325'}
Training> Name=main_level/agent, Worker=0, Episode=1801, Total reward=62.8, Steps=59683, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1802, Total reward=32.21, Steps=59707, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1803, Total reward=19.61, Steps=59735, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1804, Total reward=72.58, Steps=59806, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1805, Total reward=20.16, Steps=59845, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1806, Total reward=61.22, Steps=59881, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1807, Total reward=43.63, Steps=59918, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1808, Total reward=55.15, Steps=59946, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1809, Total reward=46.13, Steps=59992, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1810, Total reward=78.52, Steps=60028, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1811, Total reward=71.69, Steps=60067, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1812, Total reward=67.9, Steps=60103, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1813, Total reward=0.02, Steps=60118, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1814, Total reward=40.78, Steps=60140, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1815, Total reward=18.76, Steps=60171, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1816, Total reward=15.05, Steps=60194, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1817, Total reward=63.88, Steps=60234, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1818, Total reward=36.98, Steps=60255, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1819, Total reward=22.92, Steps=60275, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1820, Total reward=86.39, Steps=60311, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1821, Total reward=58.18, Steps=60342, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1822, Total reward=27.34, Steps=60360, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1823, Total reward=6.37, Steps=60384, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1824, Total reward=12.78, Steps=60415, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1825, Total reward=84.4, Steps=60490, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1826, Total reward=139.3, Steps=60585, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1827, Total reward=52.74, Steps=60624, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1828, Total reward=61.82, Steps=60667, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1829, Total reward=31.5, Steps=60694, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1830, Total reward=25.81, Steps=60709, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1831, Total reward=34.86, Steps=60736, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1832, Total reward=38.91, Steps=60768, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1833, Total reward=30.96, Steps=60788, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1834, Total reward=20.75, Steps=60810, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1835, Total reward=33.93, Steps=60835, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1836, Total reward=16.27, Steps=60852, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1837, Total reward=47.9, Steps=60888, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1838, Total reward=19.8, Steps=60915, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1839, Total reward=50.52, Steps=60956, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1840, Total reward=71.56, Steps=60992, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1841, Total reward=40.54, Steps=61020, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1842, Total reward=32.45, Steps=61044, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1843, Total reward=0.02, Steps=61066, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1844, Total reward=14.77, Steps=61090, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1845, Total reward=75.51, Steps=61147, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1846, Total reward=101.42, Steps=61221, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1847, Total reward=19.36, Steps=61237, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1848, Total reward=61.62, Steps=61266, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1849, Total reward=26.61, Steps=61287, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1850, Total reward=61.3, Steps=61323, Training iteration=36
Policy training> Surrogate loss=-0.010467584244906902, KL divergence=0.00010726339678512886, Entropy=0.3278309404850006, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04187802970409393, KL divergence=0.007357984781265259, Entropy=0.3180665969848633, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.027757322415709496, KL divergence=0.023291893303394318, Entropy=0.3132873475551605, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05980181694030762, KL divergence=0.04107287526130676, Entropy=0.3028815984725952, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04939301684498787, KL divergence=0.057203471660614014, Entropy=0.3007737100124359, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05798441171646118, KL divergence=0.07278627902269363, Entropy=0.29807254672050476, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06189187988638878, KL divergence=0.08315317332744598, Entropy=0.29554224014282227, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06163549795746803, KL divergence=0.09083414077758789, Entropy=0.29682520031929016, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.055806536227464676, KL divergence=0.10089651495218277, Entropy=0.2992284297943115, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.054980456829071045, KL divergence=0.10727027803659439, Entropy=0.2994644045829773, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/329_Step-61323.ckpt']
Uploaded 3 files for checkpoint 329 in 0.54 seconds
saved intermediate frozen graph: current/model/model_329.pb
Best checkpoint number: 314, Last checkpoint number: 327
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'326'}
Training> Name=main_level/agent, Worker=0, Episode=1851, Total reward=29.43, Steps=61363, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1852, Total reward=21.41, Steps=61388, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1853, Total reward=30.21, Steps=61408, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1854, Total reward=48.05, Steps=61430, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1855, Total reward=30.98, Steps=61456, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1856, Total reward=16.03, Steps=61487, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1857, Total reward=49.24, Steps=61521, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1858, Total reward=46.18, Steps=61555, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1859, Total reward=74.97, Steps=61604, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1860, Total reward=74.76, Steps=61645, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1861, Total reward=54.06, Steps=61675, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1862, Total reward=30.96, Steps=61718, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1863, Total reward=0.01, Steps=61730, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1864, Total reward=14.83, Steps=61764, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1865, Total reward=69.03, Steps=61832, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1866, Total reward=28.66, Steps=61855, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1867, Total reward=39.59, Steps=61898, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1868, Total reward=65.08, Steps=61939, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1869, Total reward=22.59, Steps=61972, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1870, Total reward=74.51, Steps=62020, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1871, Total reward=47.31, Steps=62058, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1872, Total reward=52.38, Steps=62089, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1873, Total reward=21.55, Steps=62100, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1874, Total reward=32.13, Steps=62121, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1875, Total reward=39.95, Steps=62149, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1876, Total reward=17.02, Steps=62182, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1877, Total reward=17.31, Steps=62198, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1878, Total reward=22.4, Steps=62217, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1879, Total reward=183.97, Steps=62355, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1880, Total reward=49.67, Steps=62401, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1881, Total reward=74.06, Steps=62435, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1882, Total reward=36.9, Steps=62461, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1883, Total reward=8.97, Steps=62488, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1884, Total reward=19.65, Steps=62520, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1885, Total reward=20.31, Steps=62552, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1886, Total reward=105.16, Steps=62600, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1887, Total reward=53.74, Steps=62639, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1888, Total reward=70.59, Steps=62689, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1889, Total reward=70.33, Steps=62750, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1890, Total reward=61.81, Steps=62799, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1891, Total reward=50.36, Steps=62838, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1892, Total reward=62.49, Steps=62881, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1893, Total reward=0.02, Steps=62897, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1894, Total reward=42.59, Steps=62919, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1895, Total reward=24.76, Steps=62954, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1896, Total reward=18.29, Steps=62984, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1897, Total reward=50.59, Steps=63031, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1898, Total reward=34.21, Steps=63054, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1899, Total reward=5.41, Steps=63071, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1900, Total reward=63.93, Steps=63105, Training iteration=37
Policy training> Surrogate loss=-0.0025939394254237413, KL divergence=7.407314114971086e-05, Entropy=0.33021244406700134, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.031225495040416718, KL divergence=0.00472417613491416, Entropy=0.3295491933822632, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0292049590498209, KL divergence=0.01522789429873228, Entropy=0.31618785858154297, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04676678404211998, KL divergence=0.028778478503227234, Entropy=0.30959591269493103, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.0372781902551651, KL divergence=0.04710192605853081, Entropy=0.31492355465888977, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.0655033215880394, KL divergence=0.05825774371623993, Entropy=0.3061991035938263, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04913316294550896, KL divergence=0.06760318577289581, Entropy=0.3084013760089874, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.053533826023340225, KL divergence=0.08422727137804031, Entropy=0.3073394298553467, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0560142882168293, KL divergence=0.0882200077176094, Entropy=0.3005259335041046, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.04632743075489998, KL divergence=0.10157409310340881, Entropy=0.3112436830997467, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/330_Step-63105.ckpt']
Uploaded 3 files for checkpoint 330 in 0.44 seconds
saved intermediate frozen graph: current/model/model_330.pb
Best checkpoint number: 314, Last checkpoint number: 328
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'327'}
Training> Name=main_level/agent, Worker=0, Episode=1901, Total reward=34.85, Steps=63132, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1902, Total reward=107.59, Steps=63235, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1903, Total reward=3.73, Steps=63260, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1904, Total reward=11.31, Steps=63291, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1905, Total reward=9.63, Steps=63308, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1906, Total reward=65.07, Steps=63354, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1907, Total reward=63.65, Steps=63397, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1908, Total reward=57.81, Steps=63440, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1909, Total reward=31.76, Steps=63482, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1910, Total reward=74.78, Steps=63529, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1911, Total reward=55.03, Steps=63566, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1912, Total reward=14.57, Steps=63593, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1913, Total reward=27.09, Steps=63613, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1914, Total reward=38.7, Steps=63635, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1915, Total reward=19.22, Steps=63660, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1916, Total reward=27.37, Steps=63688, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1917, Total reward=29.46, Steps=63739, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1918, Total reward=34.53, Steps=63777, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1919, Total reward=13.05, Steps=63791, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1920, Total reward=64.1, Steps=63823, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1921, Total reward=57.1, Steps=63855, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1922, Total reward=28.23, Steps=63889, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1923, Total reward=35.98, Steps=63933, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1924, Total reward=48.97, Steps=63976, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1925, Total reward=51.06, Steps=64035, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1926, Total reward=98.41, Steps=64101, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1927, Total reward=69.26, Steps=64144, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1928, Total reward=70.34, Steps=64183, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1929, Total reward=20.1, Steps=64211, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1930, Total reward=70.32, Steps=64258, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1931, Total reward=33.71, Steps=64287, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1932, Total reward=35.91, Steps=64308, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1933, Total reward=58.91, Steps=64339, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1934, Total reward=45.95, Steps=64362, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1935, Total reward=12.14, Steps=64390, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1936, Total reward=21.56, Steps=64424, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1937, Total reward=54.64, Steps=64463, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1938, Total reward=24.82, Steps=64477, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1939, Total reward=93.81, Steps=64552, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1940, Total reward=73.28, Steps=64582, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1941, Total reward=38.02, Steps=64606, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1942, Total reward=42.59, Steps=64631, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1943, Total reward=7.63, Steps=64647, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1944, Total reward=17.15, Steps=64695, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1945, Total reward=79.2, Steps=64768, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1946, Total reward=58.1, Steps=64812, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1947, Total reward=40.15, Steps=64847, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1948, Total reward=40.48, Steps=64872, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1949, Total reward=22.58, Steps=64902, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1950, Total reward=57.11, Steps=64940, Training iteration=38
Policy training> Surrogate loss=-0.015933506190776825, KL divergence=0.00012555629655253142, Entropy=0.3443725109100342, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.02722053788602352, KL divergence=0.007297934498637915, Entropy=0.3362698554992676, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03132197633385658, KL divergence=0.022972973063588142, Entropy=0.32487931847572327, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03909459337592125, KL divergence=0.04067857936024666, Entropy=0.33110693097114563, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05970025062561035, KL divergence=0.06155790016055107, Entropy=0.3314887583255768, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.050370678305625916, KL divergence=0.07452365756034851, Entropy=0.3168364465236664, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.0657135471701622, KL divergence=0.09187803417444229, Entropy=0.32111260294914246, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.047031205147504807, KL divergence=0.10001768916845322, Entropy=0.3140425682067871, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.056529659777879715, KL divergence=0.10673429816961288, Entropy=0.3210427761077881, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.05774654075503349, KL divergence=0.11373110860586166, Entropy=0.3267308175563812, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/331_Step-64940.ckpt']
Uploaded 3 files for checkpoint 331 in 0.60 seconds
saved intermediate frozen graph: current/model/model_331.pb
Best checkpoint number: 314, Last checkpoint number: 329
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'328'}
Training> Name=main_level/agent, Worker=0, Episode=1951, Total reward=27.96, Steps=64967, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1952, Total reward=7.59, Steps=64978, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1953, Total reward=43.81, Steps=64999, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1954, Total reward=49.75, Steps=65020, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1955, Total reward=12.36, Steps=65052, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1956, Total reward=19.61, Steps=65071, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1957, Total reward=6.12, Steps=65089, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1958, Total reward=32.6, Steps=65114, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1959, Total reward=68.33, Steps=65190, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1960, Total reward=65.76, Steps=65235, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1961, Total reward=59.82, Steps=65283, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1962, Total reward=33.46, Steps=65304, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1963, Total reward=6.84, Steps=65342, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1964, Total reward=102.57, Steps=65419, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1965, Total reward=43.42, Steps=65477, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1966, Total reward=110.72, Steps=65551, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1967, Total reward=43.37, Steps=65589, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1968, Total reward=69.74, Steps=65629, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1969, Total reward=82.95, Steps=65687, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1970, Total reward=22.02, Steps=65708, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1971, Total reward=49.83, Steps=65737, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1972, Total reward=0.01, Steps=65748, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1973, Total reward=48.81, Steps=65774, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1974, Total reward=26.44, Steps=65785, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1975, Total reward=29.36, Steps=65816, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1976, Total reward=17.16, Steps=65850, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1977, Total reward=31.65, Steps=65883, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1978, Total reward=31.62, Steps=65909, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1979, Total reward=25.64, Steps=65931, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1980, Total reward=18.89, Steps=65944, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1981, Total reward=43.39, Steps=65970, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1982, Total reward=39.38, Steps=65999, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1983, Total reward=3.09, Steps=66024, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1984, Total reward=12.83, Steps=66051, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1985, Total reward=16.45, Steps=66077, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1986, Total reward=53.75, Steps=66134, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1987, Total reward=98.18, Steps=66232, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1988, Total reward=65.86, Steps=66261, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1989, Total reward=42.71, Steps=66299, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1990, Total reward=94.99, Steps=66333, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1991, Total reward=51.55, Steps=66374, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1992, Total reward=65.57, Steps=66416, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1993, Total reward=37.84, Steps=66437, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1994, Total reward=54.55, Steps=66460, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1995, Total reward=17.27, Steps=66492, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1996, Total reward=16.94, Steps=66511, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1997, Total reward=81.97, Steps=66595, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1998, Total reward=30.65, Steps=66621, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1999, Total reward=52.79, Steps=66672, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=2000, Total reward=80.17, Steps=66715, Training iteration=39
Policy training> Surrogate loss=-0.0008218772709369659, KL divergence=8.713288116268814e-05, Entropy=0.3337130844593048, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.019522814080119133, KL divergence=0.005364326760172844, Entropy=0.3341463804244995, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.052350059151649475, KL divergence=0.020348163321614265, Entropy=0.32253536581993103, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05007913336157799, KL divergence=0.03419046476483345, Entropy=0.31764504313468933, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.044984739273786545, KL divergence=0.05433553457260132, Entropy=0.31316623091697693, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04418747499585152, KL divergence=0.0670706108212471, Entropy=0.3007771074771881, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05178748071193695, KL divergence=0.07435782253742218, Entropy=0.3060876429080963, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.04830464720726013, KL divergence=0.0890171229839325, Entropy=0.30520308017730713, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06419022381305695, KL divergence=0.09762882441282272, Entropy=0.3032960891723633, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.059474509209394455, KL divergence=0.10407481342554092, Entropy=0.3023897111415863, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/332_Step-66715.ckpt']
Uploaded 3 files for checkpoint 332 in 0.52 seconds
saved intermediate frozen graph: current/model/model_332.pb
Best checkpoint number: 314, Last checkpoint number: 330
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'329'}
Training> Name=main_level/agent, Worker=0, Episode=2001, Total reward=64.46, Steps=66771, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2002, Total reward=47.9, Steps=66797, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2003, Total reward=3.4, Steps=66816, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2004, Total reward=15.31, Steps=66848, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2005, Total reward=10.04, Steps=66871, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2006, Total reward=61.37, Steps=66915, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2007, Total reward=77.16, Steps=66957, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2008, Total reward=54.66, Steps=67013, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2009, Total reward=23.26, Steps=67042, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2010, Total reward=77.96, Steps=67080, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2011, Total reward=67.65, Steps=67123, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2012, Total reward=39.29, Steps=67155, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2013, Total reward=21.32, Steps=67166, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2014, Total reward=39.33, Steps=67188, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2015, Total reward=17.08, Steps=67203, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2016, Total reward=23.18, Steps=67229, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2017, Total reward=53.31, Steps=67260, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2018, Total reward=33.0, Steps=67283, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2019, Total reward=16.41, Steps=67305, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2020, Total reward=98.01, Steps=67367, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2021, Total reward=66.52, Steps=67397, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2022, Total reward=33.57, Steps=67440, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2023, Total reward=15.92, Steps=67497, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2024, Total reward=19.32, Steps=67541, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2025, Total reward=76.98, Steps=67632, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2026, Total reward=195.18, Steps=67731, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2027, Total reward=63.18, Steps=67772, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2028, Total reward=60.83, Steps=67814, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2029, Total reward=32.86, Steps=67842, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2030, Total reward=40.68, Steps=67876, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2031, Total reward=34.78, Steps=67904, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2032, Total reward=57.24, Steps=67946, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2033, Total reward=34.75, Steps=67966, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2034, Total reward=34.62, Steps=67988, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2035, Total reward=13.05, Steps=68020, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2036, Total reward=13.21, Steps=68037, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2037, Total reward=28.71, Steps=68076, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2038, Total reward=65.91, Steps=68144, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2039, Total reward=62.24, Steps=68194, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2040, Total reward=89.38, Steps=68229, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2041, Total reward=50.44, Steps=68257, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2042, Total reward=27.07, Steps=68276, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2043, Total reward=0.02, Steps=68299, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2044, Total reward=85.78, Steps=68372, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2045, Total reward=59.09, Steps=68434, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2046, Total reward=47.28, Steps=68458, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2047, Total reward=3.35, Steps=68470, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2048, Total reward=23.83, Steps=68485, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2049, Total reward=38.43, Steps=68535, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2050, Total reward=42.27, Steps=68582, Training iteration=40
Policy training> Surrogate loss=0.003974630031734705, KL divergence=8.295848238049075e-05, Entropy=0.3343857228755951, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.00014250166714191437, KL divergence=0.006867628078907728, Entropy=0.3269250690937042, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.014586411416530609, KL divergence=0.02185366116464138, Entropy=0.32488879561424255, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.014757801778614521, KL divergence=0.03856373950839043, Entropy=0.3326181471347809, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.045962464064359665, KL divergence=0.0530809722840786, Entropy=0.3146098554134369, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04587632790207863, KL divergence=0.06404935568571091, Entropy=0.32473501563072205, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04848337545990944, KL divergence=0.08331233263015747, Entropy=0.31620240211486816, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0388108529150486, KL divergence=0.09083805233240128, Entropy=0.32061102986335754, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05175270140171051, KL divergence=0.102610282599926, Entropy=0.3200511038303375, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.047230690717697144, KL divergence=0.1184159591794014, Entropy=0.3180006742477417, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/333_Step-68582.ckpt']
Uploaded 3 files for checkpoint 333 in 0.79 seconds
saved intermediate frozen graph: current/model/model_333.pb
Best checkpoint number: 314, Last checkpoint number: 331
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'330'}
Training> Name=main_level/agent, Worker=0, Episode=2051, Total reward=58.01, Steps=68621, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2052, Total reward=53.79, Steps=68652, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2053, Total reward=42.26, Steps=68682, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2054, Total reward=42.9, Steps=68703, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2055, Total reward=22.92, Steps=68738, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2056, Total reward=15.78, Steps=68757, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2057, Total reward=23.17, Steps=68785, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2058, Total reward=38.13, Steps=68820, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2059, Total reward=31.05, Steps=68847, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2060, Total reward=81.33, Steps=68886, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2061, Total reward=68.65, Steps=68917, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2062, Total reward=44.69, Steps=68941, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2063, Total reward=18.83, Steps=68976, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2064, Total reward=63.28, Steps=69052, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2065, Total reward=31.92, Steps=69110, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2066, Total reward=79.5, Steps=69158, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2067, Total reward=43.35, Steps=69194, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2068, Total reward=58.78, Steps=69222, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2069, Total reward=23.49, Steps=69243, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2070, Total reward=71.17, Steps=69295, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2071, Total reward=51.5, Steps=69321, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2072, Total reward=64.59, Steps=69354, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2073, Total reward=47.44, Steps=69382, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2074, Total reward=40.04, Steps=69403, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2075, Total reward=12.85, Steps=69428, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2076, Total reward=20.97, Steps=69454, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2077, Total reward=29.04, Steps=69472, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2078, Total reward=29.84, Steps=69496, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2079, Total reward=69.94, Steps=69541, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2080, Total reward=88.0, Steps=69579, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2081, Total reward=64.38, Steps=69610, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2082, Total reward=35.86, Steps=69630, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2083, Total reward=67.48, Steps=69728, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2084, Total reward=3.33, Steps=69757, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2085, Total reward=92.81, Steps=69825, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2086, Total reward=83.81, Steps=69888, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2087, Total reward=72.01, Steps=69944, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2088, Total reward=66.0, Steps=69999, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2089, Total reward=26.4, Steps=70040, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2090, Total reward=65.73, Steps=70075, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2091, Total reward=77.97, Steps=70129, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2092, Total reward=62.94, Steps=70160, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2093, Total reward=52.08, Steps=70189, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2094, Total reward=38.9, Steps=70211, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2095, Total reward=12.6, Steps=70238, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2096, Total reward=19.06, Steps=70273, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2097, Total reward=50.05, Steps=70335, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2098, Total reward=34.22, Steps=70348, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2099, Total reward=63.21, Steps=70397, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2100, Total reward=50.08, Steps=70432, Training iteration=41
Policy training> Surrogate loss=-0.006898572668433189, KL divergence=0.00018146047659683973, Entropy=0.3413297235965729, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.004482624586671591, KL divergence=0.006913271266967058, Entropy=0.3234562575817108, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.013348464854061604, KL divergence=0.020367467775940895, Entropy=0.324039489030838, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04147063568234444, KL divergence=0.03804265335202217, Entropy=0.3189985156059265, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06410057842731476, KL divergence=0.057037126272916794, Entropy=0.3155207335948944, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06608986854553223, KL divergence=0.07118623703718185, Entropy=0.3123209774494171, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05575157329440117, KL divergence=0.07746898382902145, Entropy=0.31282222270965576, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05209017172455788, KL divergence=0.09386258572340012, Entropy=0.31106916069984436, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05914689227938652, KL divergence=0.09640327841043472, Entropy=0.3118791878223419, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.049957096576690674, KL divergence=0.10298440605401993, Entropy=0.30485737323760986, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/334_Step-70432.ckpt']
Uploaded 3 files for checkpoint 334 in 0.59 seconds
saved intermediate frozen graph: current/model/model_334.pb
Best checkpoint number: 314, Last checkpoint number: 332
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'331'}
Training> Name=main_level/agent, Worker=0, Episode=2101, Total reward=57.86, Steps=70478, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2102, Total reward=39.57, Steps=70502, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2103, Total reward=16.47, Steps=70544, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2104, Total reward=24.91, Steps=70590, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2105, Total reward=10.02, Steps=70614, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2106, Total reward=88.44, Steps=70662, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2107, Total reward=74.48, Steps=70710, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2108, Total reward=40.35, Steps=70734, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2109, Total reward=23.62, Steps=70764, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2110, Total reward=67.69, Steps=70809, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2111, Total reward=57.48, Steps=70837, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2112, Total reward=62.7, Steps=70877, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2113, Total reward=44.0, Steps=70908, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2114, Total reward=31.57, Steps=70920, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2115, Total reward=4.4, Steps=70942, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2116, Total reward=11.98, Steps=70962, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2117, Total reward=33.31, Steps=70988, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2118, Total reward=42.32, Steps=71037, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2119, Total reward=67.27, Steps=71101, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2120, Total reward=85.13, Steps=71138, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2121, Total reward=59.11, Steps=71167, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2122, Total reward=32.5, Steps=71187, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2123, Total reward=2.9, Steps=71202, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2124, Total reward=71.72, Steps=71270, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2125, Total reward=71.59, Steps=71338, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2126, Total reward=68.86, Steps=71383, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2127, Total reward=13.39, Steps=71397, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2128, Total reward=68.67, Steps=71437, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2129, Total reward=16.63, Steps=71452, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2130, Total reward=66.93, Steps=71487, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2131, Total reward=60.64, Steps=71524, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2132, Total reward=39.48, Steps=71554, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2133, Total reward=45.12, Steps=71574, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2134, Total reward=50.67, Steps=71595, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2135, Total reward=40.12, Steps=71643, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2136, Total reward=16.85, Steps=71661, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2137, Total reward=48.24, Steps=71696, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2138, Total reward=94.17, Steps=71763, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2139, Total reward=19.4, Steps=71787, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2140, Total reward=82.35, Steps=71827, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2141, Total reward=53.96, Steps=71856, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2142, Total reward=42.83, Steps=71880, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2143, Total reward=12.1, Steps=71924, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2144, Total reward=11.31, Steps=71949, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2145, Total reward=32.94, Steps=72003, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2146, Total reward=80.45, Steps=72053, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2147, Total reward=37.78, Steps=72088, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2148, Total reward=119.67, Steps=72163, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2149, Total reward=25.93, Steps=72191, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2150, Total reward=105.36, Steps=72235, Training iteration=42
Policy training> Surrogate loss=0.007281934842467308, KL divergence=0.00015226457617245615, Entropy=0.36160239577293396, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.026489662006497383, KL divergence=0.007496268022805452, Entropy=0.3622204065322876, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05215750262141228, KL divergence=0.02309313230216503, Entropy=0.3592729866504669, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04791310429573059, KL divergence=0.04107586666941643, Entropy=0.34276604652404785, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05047966167330742, KL divergence=0.05831817165017128, Entropy=0.3454197347164154, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04592758044600487, KL divergence=0.07553521543741226, Entropy=0.3531999886035919, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06990070641040802, KL divergence=0.08941650390625, Entropy=0.34318193793296814, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07267988473176956, KL divergence=0.10012882947921753, Entropy=0.3475430905818939, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04236514866352081, KL divergence=0.10634950548410416, Entropy=0.34606456756591797, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07029455155134201, KL divergence=0.11677403002977371, Entropy=0.3501282036304474, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/335_Step-72235.ckpt']
Uploaded 3 files for checkpoint 335 in 0.56 seconds
saved intermediate frozen graph: current/model/model_335.pb
Best checkpoint number: 314, Last checkpoint number: 333
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'332'}
Training> Name=main_level/agent, Worker=0, Episode=2151, Total reward=22.51, Steps=72268, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2152, Total reward=61.84, Steps=72301, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2153, Total reward=38.54, Steps=72321, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2154, Total reward=20.36, Steps=72341, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2155, Total reward=20.58, Steps=72365, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2156, Total reward=21.23, Steps=72382, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2157, Total reward=54.89, Steps=72436, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2158, Total reward=39.99, Steps=72459, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2159, Total reward=75.85, Steps=72520, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2160, Total reward=78.18, Steps=72584, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2161, Total reward=53.47, Steps=72625, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2162, Total reward=31.03, Steps=72653, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2163, Total reward=13.59, Steps=72693, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2164, Total reward=56.13, Steps=72756, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2165, Total reward=14.68, Steps=72792, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2166, Total reward=13.76, Steps=72813, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2167, Total reward=60.05, Steps=72857, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2168, Total reward=61.17, Steps=72886, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2169, Total reward=29.16, Steps=72906, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2170, Total reward=57.31, Steps=72956, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2171, Total reward=64.13, Steps=72993, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2172, Total reward=51.9, Steps=73026, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2173, Total reward=44.73, Steps=73057, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2174, Total reward=22.65, Steps=73068, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2175, Total reward=24.9, Steps=73099, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2176, Total reward=14.0, Steps=73133, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2177, Total reward=52.68, Steps=73170, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2178, Total reward=92.45, Steps=73251, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2179, Total reward=13.01, Steps=73265, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2180, Total reward=79.89, Steps=73300, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2181, Total reward=54.01, Steps=73330, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2182, Total reward=36.27, Steps=73373, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2183, Total reward=6.35, Steps=73395, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2184, Total reward=22.58, Steps=73417, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2185, Total reward=16.4, Steps=73438, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2186, Total reward=137.28, Steps=73529, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2187, Total reward=60.53, Steps=73571, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2188, Total reward=54.59, Steps=73600, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2189, Total reward=31.39, Steps=73635, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2190, Total reward=88.87, Steps=73671, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2191, Total reward=27.05, Steps=73698, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2192, Total reward=37.8, Steps=73727, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2193, Total reward=40.27, Steps=73755, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2194, Total reward=38.13, Steps=73776, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2195, Total reward=20.04, Steps=73805, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2196, Total reward=15.33, Steps=73825, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2197, Total reward=43.11, Steps=73864, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2198, Total reward=28.23, Steps=73887, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2199, Total reward=88.22, Steps=73937, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2200, Total reward=93.83, Steps=73977, Training iteration=43
Policy training> Surrogate loss=-0.000710057734977454, KL divergence=9.651778964325786e-05, Entropy=0.3567221164703369, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03336768224835396, KL divergence=0.006232126150280237, Entropy=0.3552679717540741, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.044925522059202194, KL divergence=0.02198927104473114, Entropy=0.35027948021888733, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03424215689301491, KL divergence=0.03621266782283783, Entropy=0.33379051089286804, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04741942882537842, KL divergence=0.0517263300716877, Entropy=0.3368666172027588, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.057878654450178146, KL divergence=0.06786421686410904, Entropy=0.32816144824028015, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06969454884529114, KL divergence=0.08225520700216293, Entropy=0.33242592215538025, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.058704718947410583, KL divergence=0.0909670814871788, Entropy=0.3249673545360565, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06740478426218033, KL divergence=0.10037145763635635, Entropy=0.3289203643798828, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07041782885789871, KL divergence=0.10739509016275406, Entropy=0.32390880584716797, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/336_Step-73977.ckpt']
Uploaded 3 files for checkpoint 336 in 0.56 seconds
saved intermediate frozen graph: current/model/model_336.pb
Best checkpoint number: 314, Last checkpoint number: 334
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'333'}
Training> Name=main_level/agent, Worker=0, Episode=2201, Total reward=48.64, Steps=74006, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2202, Total reward=41.16, Steps=74041, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2203, Total reward=5.94, Steps=74067, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2204, Total reward=14.53, Steps=74098, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2205, Total reward=77.12, Steps=74155, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2206, Total reward=112.56, Steps=74253, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2207, Total reward=20.59, Steps=74277, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2208, Total reward=55.34, Steps=74305, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2209, Total reward=24.07, Steps=74338, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2210, Total reward=36.73, Steps=74366, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2211, Total reward=27.99, Steps=74389, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2212, Total reward=9.48, Steps=74399, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2213, Total reward=37.17, Steps=74426, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2214, Total reward=31.66, Steps=74438, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2215, Total reward=34.07, Steps=74466, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2216, Total reward=24.81, Steps=74485, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2217, Total reward=71.69, Steps=74558, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2218, Total reward=89.32, Steps=74619, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2219, Total reward=74.26, Steps=74665, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2220, Total reward=56.28, Steps=74695, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2221, Total reward=60.73, Steps=74729, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2222, Total reward=49.25, Steps=74783, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2223, Total reward=0.02, Steps=74803, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2224, Total reward=8.47, Steps=74832, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2225, Total reward=6.98, Steps=74853, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2226, Total reward=105.64, Steps=74912, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2227, Total reward=15.69, Steps=74945, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2228, Total reward=58.42, Steps=74987, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2229, Total reward=54.55, Steps=75051, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2230, Total reward=58.45, Steps=75090, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2231, Total reward=19.14, Steps=75103, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2232, Total reward=40.79, Steps=75131, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2233, Total reward=44.94, Steps=75160, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2234, Total reward=28.93, Steps=75181, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2235, Total reward=15.39, Steps=75213, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2236, Total reward=27.47, Steps=75244, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2237, Total reward=43.61, Steps=75278, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2238, Total reward=25.88, Steps=75298, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2239, Total reward=72.0, Steps=75349, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2240, Total reward=42.89, Steps=75383, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2241, Total reward=61.39, Steps=75413, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2242, Total reward=27.65, Steps=75431, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2243, Total reward=6.94, Steps=75451, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2244, Total reward=59.98, Steps=75542, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2245, Total reward=16.78, Steps=75566, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2246, Total reward=29.43, Steps=75604, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2247, Total reward=57.72, Steps=75675, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2248, Total reward=55.86, Steps=75704, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2249, Total reward=20.23, Steps=75723, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2250, Total reward=80.73, Steps=75767, Training iteration=44
Policy training> Surrogate loss=-0.011571615934371948, KL divergence=0.00018805247964337468, Entropy=0.38840675354003906, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.04403572902083397, KL divergence=0.0059584216214716434, Entropy=0.3850516974925995, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03247185796499252, KL divergence=0.019858187064528465, Entropy=0.37246274948120117, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.045186977833509445, KL divergence=0.03761354088783264, Entropy=0.36444321274757385, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06995094567537308, KL divergence=0.054950978606939316, Entropy=0.35876157879829407, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05905485153198242, KL divergence=0.06688833236694336, Entropy=0.35107216238975525, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06212274730205536, KL divergence=0.07964814454317093, Entropy=0.36353597044944763, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05301604047417641, KL divergence=0.09508480876684189, Entropy=0.35343456268310547, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05694268271327019, KL divergence=0.10247131437063217, Entropy=0.3567543029785156, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0753975510597229, KL divergence=0.10922909528017044, Entropy=0.3538466989994049, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/337_Step-75767.ckpt']
Uploaded 3 files for checkpoint 337 in 0.60 seconds
saved intermediate frozen graph: current/model/model_337.pb
Best checkpoint number: 314, Last checkpoint number: 335
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'334'}
Training> Name=main_level/agent, Worker=0, Episode=2251, Total reward=12.28, Steps=75794, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2252, Total reward=61.41, Steps=75837, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2253, Total reward=48.7, Steps=75867, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2254, Total reward=38.85, Steps=75889, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2255, Total reward=21.69, Steps=75917, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2256, Total reward=23.6, Steps=75941, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2257, Total reward=113.4, Steps=76015, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2258, Total reward=34.26, Steps=76057, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2259, Total reward=15.4, Steps=76094, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2260, Total reward=73.01, Steps=76137, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2261, Total reward=33.15, Steps=76160, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2262, Total reward=31.93, Steps=76184, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2263, Total reward=15.77, Steps=76236, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2264, Total reward=83.23, Steps=76312, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2265, Total reward=75.49, Steps=76387, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2266, Total reward=81.09, Steps=76458, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2267, Total reward=33.34, Steps=76497, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2268, Total reward=61.38, Steps=76536, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2269, Total reward=63.44, Steps=76582, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2270, Total reward=63.9, Steps=76616, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2271, Total reward=48.84, Steps=76655, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2272, Total reward=41.61, Steps=76684, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2273, Total reward=42.77, Steps=76715, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2274, Total reward=31.74, Steps=76735, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2275, Total reward=19.05, Steps=76782, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2276, Total reward=18.44, Steps=76800, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2277, Total reward=5.72, Steps=76811, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2278, Total reward=32.45, Steps=76832, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2279, Total reward=24.47, Steps=76866, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2280, Total reward=34.54, Steps=76902, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2281, Total reward=68.41, Steps=76934, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2282, Total reward=54.19, Steps=76960, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2283, Total reward=2.92, Steps=76987, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2284, Total reward=0.02, Steps=77003, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2285, Total reward=80.34, Steps=77064, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2286, Total reward=97.11, Steps=77123, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2287, Total reward=64.06, Steps=77165, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2288, Total reward=48.8, Steps=77190, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2289, Total reward=23.23, Steps=77217, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2290, Total reward=36.73, Steps=77253, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2291, Total reward=40.57, Steps=77279, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2292, Total reward=53.86, Steps=77312, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2293, Total reward=38.56, Steps=77342, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2294, Total reward=33.67, Steps=77364, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2295, Total reward=21.18, Steps=77392, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2296, Total reward=18.8, Steps=77413, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2297, Total reward=50.29, Steps=77444, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2298, Total reward=29.81, Steps=77469, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2299, Total reward=65.08, Steps=77514, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2300, Total reward=91.54, Steps=77551, Training iteration=45
Policy training> Surrogate loss=0.0005931705236434937, KL divergence=0.00018310298037249595, Entropy=0.36685237288475037, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.011560071259737015, KL divergence=0.006996972020715475, Entropy=0.36084306240081787, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05925638601183891, KL divergence=0.018807604908943176, Entropy=0.35705113410949707, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04854211583733559, KL divergence=0.03325639292597771, Entropy=0.35504385828971863, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.03618155047297478, KL divergence=0.04827529191970825, Entropy=0.3508504331111908, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.053663235157728195, KL divergence=0.06336117535829544, Entropy=0.34686920046806335, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04734319821000099, KL divergence=0.07740192860364914, Entropy=0.34161481261253357, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05052779242396355, KL divergence=0.09055586904287338, Entropy=0.3383721113204956, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.044639796018600464, KL divergence=0.10122406482696533, Entropy=0.3430233895778656, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06483122706413269, KL divergence=0.1048811748623848, Entropy=0.3476440906524658, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/338_Step-77551.ckpt']
Uploaded 3 files for checkpoint 338 in 0.54 seconds
saved intermediate frozen graph: current/model/model_338.pb
Best checkpoint number: 314, Last checkpoint number: 336
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'335'}
Training> Name=main_level/agent, Worker=0, Episode=2301, Total reward=59.54, Steps=77581, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2302, Total reward=32.58, Steps=77600, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2303, Total reward=10.26, Steps=77636, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2304, Total reward=66.44, Steps=77734, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2305, Total reward=13.45, Steps=77759, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2306, Total reward=67.9, Steps=77804, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2307, Total reward=72.31, Steps=77844, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2308, Total reward=64.11, Steps=77873, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2309, Total reward=10.35, Steps=77886, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2310, Total reward=104.45, Steps=77941, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2311, Total reward=18.7, Steps=77957, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2312, Total reward=58.35, Steps=77988, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2313, Total reward=55.06, Steps=78019, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2314, Total reward=30.23, Steps=78039, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2315, Total reward=30.69, Steps=78068, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2316, Total reward=20.9, Steps=78102, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2317, Total reward=51.75, Steps=78138, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2318, Total reward=36.15, Steps=78162, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2319, Total reward=80.87, Steps=78224, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2320, Total reward=94.86, Steps=78272, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2321, Total reward=54.71, Steps=78302, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2322, Total reward=31.22, Steps=78320, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2323, Total reward=0.02, Steps=78344, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2324, Total reward=65.7, Steps=78418, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2325, Total reward=13.06, Steps=78443, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2326, Total reward=18.45, Steps=78459, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2327, Total reward=30.57, Steps=78497, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2328, Total reward=60.42, Steps=78526, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2329, Total reward=57.71, Steps=78572, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2330, Total reward=30.82, Steps=78611, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2331, Total reward=20.31, Steps=78638, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2332, Total reward=60.55, Steps=78670, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2333, Total reward=52.48, Steps=78701, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2334, Total reward=39.26, Steps=78722, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2335, Total reward=18.87, Steps=78750, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2336, Total reward=26.19, Steps=78769, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2337, Total reward=61.89, Steps=78817, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2338, Total reward=36.88, Steps=78842, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2339, Total reward=84.32, Steps=78894, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2340, Total reward=51.48, Steps=78929, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2341, Total reward=34.65, Steps=78954, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2342, Total reward=37.4, Steps=78988, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2343, Total reward=21.25, Steps=79024, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2344, Total reward=14.92, Steps=79057, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2345, Total reward=78.03, Steps=79123, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2346, Total reward=74.66, Steps=79191, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2347, Total reward=94.85, Steps=79272, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2348, Total reward=39.78, Steps=79298, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2349, Total reward=28.75, Steps=79318, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2350, Total reward=29.31, Steps=79343, Training iteration=46
Policy training> Surrogate loss=-0.011149667203426361, KL divergence=9.665440302342176e-05, Entropy=0.3713551461696625, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.028559153899550438, KL divergence=0.006687674671411514, Entropy=0.37186750769615173, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.05189957097172737, KL divergence=0.021425241604447365, Entropy=0.371356725692749, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.042105454951524734, KL divergence=0.03800521790981293, Entropy=0.3675342798233032, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.055261075496673584, KL divergence=0.05552681162953377, Entropy=0.3617289364337921, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06197280064225197, KL divergence=0.07160308957099915, Entropy=0.35505595803260803, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06007745862007141, KL divergence=0.08154240995645523, Entropy=0.3506488800048828, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06478884071111679, KL divergence=0.09170347452163696, Entropy=0.34945330023765564, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05474547669291496, KL divergence=0.10438430309295654, Entropy=0.3560148775577545, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07458450645208359, KL divergence=0.1046290472149849, Entropy=0.35298582911491394, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/339_Step-79343.ckpt']
Uploaded 3 files for checkpoint 339 in 0.53 seconds
saved intermediate frozen graph: current/model/model_339.pb
Best checkpoint number: 314, Last checkpoint number: 337
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'336'}
Training> Name=main_level/agent, Worker=0, Episode=2351, Total reward=46.91, Steps=79379, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2352, Total reward=33.28, Steps=79408, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2353, Total reward=44.7, Steps=79436, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2354, Total reward=34.8, Steps=79457, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2355, Total reward=19.12, Steps=79486, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2356, Total reward=28.47, Steps=79515, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2357, Total reward=117.38, Steps=79599, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2358, Total reward=91.48, Steps=79677, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2359, Total reward=15.18, Steps=79699, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2360, Total reward=94.26, Steps=79735, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2361, Total reward=47.17, Steps=79760, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2362, Total reward=29.77, Steps=79779, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2363, Total reward=30.51, Steps=79820, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2364, Total reward=9.7, Steps=79849, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2365, Total reward=32.24, Steps=79877, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2366, Total reward=84.26, Steps=79945, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2367, Total reward=44.19, Steps=79984, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2368, Total reward=58.6, Steps=80025, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2369, Total reward=25.06, Steps=80049, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2370, Total reward=70.16, Steps=80094, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2371, Total reward=67.28, Steps=80123, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2372, Total reward=51.31, Steps=80154, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2373, Total reward=46.78, Steps=80183, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2374, Total reward=41.35, Steps=80203, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2375, Total reward=19.97, Steps=80233, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2376, Total reward=20.97, Steps=80262, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2377, Total reward=40.85, Steps=80299, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2378, Total reward=35.58, Steps=80323, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2379, Total reward=20.35, Steps=80343, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2380, Total reward=61.58, Steps=80378, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2381, Total reward=54.63, Steps=80408, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2382, Total reward=32.09, Steps=80426, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2383, Total reward=14.17, Steps=80460, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2384, Total reward=16.76, Steps=80488, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2385, Total reward=80.08, Steps=80542, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2386, Total reward=74.99, Steps=80596, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2387, Total reward=61.08, Steps=80634, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2388, Total reward=42.18, Steps=80660, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2389, Total reward=29.56, Steps=80693, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2390, Total reward=25.01, Steps=80710, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2391, Total reward=51.62, Steps=80748, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2392, Total reward=45.69, Steps=80777, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2393, Total reward=34.5, Steps=80803, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2394, Total reward=37.93, Steps=80825, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2395, Total reward=20.34, Steps=80853, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2396, Total reward=16.48, Steps=80868, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2397, Total reward=54.1, Steps=80906, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2398, Total reward=31.17, Steps=80927, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2399, Total reward=93.95, Steps=81026, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2400, Total reward=58.0, Steps=81069, Training iteration=47
Policy training> Surrogate loss=0.004897352773696184, KL divergence=7.91788988863118e-05, Entropy=0.358349472284317, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.032145339995622635, KL divergence=0.006544454488903284, Entropy=0.35570958256721497, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.0353265218436718, KL divergence=0.018432417884469032, Entropy=0.3417871296405792, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.05019759014248848, KL divergence=0.036405112594366074, Entropy=0.3393537998199463, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04355473816394806, KL divergence=0.05424878001213074, Entropy=0.33807241916656494, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.04825209453701973, KL divergence=0.06788109242916107, Entropy=0.327637642621994, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04535425081849098, KL divergence=0.07818663865327835, Entropy=0.3253314197063446, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06796091794967651, KL divergence=0.09078377485275269, Entropy=0.32641714811325073, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06120524927973747, KL divergence=0.0982949510216713, Entropy=0.32404860854148865, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06693338602781296, KL divergence=0.10302597284317017, Entropy=0.3246161639690399, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/340_Step-81069.ckpt']
Uploaded 3 files for checkpoint 340 in 0.61 seconds
saved intermediate frozen graph: current/model/model_340.pb
Best checkpoint number: 314, Last checkpoint number: 338
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'337'}
Training> Name=main_level/agent, Worker=0, Episode=2401, Total reward=47.05, Steps=81097, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2402, Total reward=31.69, Steps=81117, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2403, Total reward=6.51, Steps=81145, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2404, Total reward=5.86, Steps=81161, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2405, Total reward=16.23, Steps=81203, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2406, Total reward=67.14, Steps=81253, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2407, Total reward=43.6, Steps=81290, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2408, Total reward=52.62, Steps=81331, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2409, Total reward=77.46, Steps=81390, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2410, Total reward=37.21, Steps=81425, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2411, Total reward=75.94, Steps=81465, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2412, Total reward=40.31, Steps=81505, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2413, Total reward=40.89, Steps=81528, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2414, Total reward=41.88, Steps=81550, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2415, Total reward=13.81, Steps=81578, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2416, Total reward=25.59, Steps=81604, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2417, Total reward=45.65, Steps=81665, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2418, Total reward=42.18, Steps=81692, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2419, Total reward=95.91, Steps=81751, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2420, Total reward=82.54, Steps=81791, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2421, Total reward=42.5, Steps=81818, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2422, Total reward=35.53, Steps=81845, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2423, Total reward=3.21, Steps=81869, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2424, Total reward=24.72, Steps=81933, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2425, Total reward=16.85, Steps=81956, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2426, Total reward=14.68, Steps=81986, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2427, Total reward=73.78, Steps=82040, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2428, Total reward=73.71, Steps=82095, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2429, Total reward=65.76, Steps=82141, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2430, Total reward=60.91, Steps=82176, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2431, Total reward=18.29, Steps=82208, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2432, Total reward=61.21, Steps=82248, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2433, Total reward=32.22, Steps=82270, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2434, Total reward=28.43, Steps=82291, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2435, Total reward=24.65, Steps=82306, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2436, Total reward=30.8, Steps=82338, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2437, Total reward=44.34, Steps=82372, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2438, Total reward=23.32, Steps=82385, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2439, Total reward=12.83, Steps=82404, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2440, Total reward=85.42, Steps=82440, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2441, Total reward=56.31, Steps=82473, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2442, Total reward=36.68, Steps=82516, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2443, Total reward=13.0, Steps=82556, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2444, Total reward=9.24, Steps=82587, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2445, Total reward=19.89, Steps=82613, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2446, Total reward=99.04, Steps=82681, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2447, Total reward=56.71, Steps=82721, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2448, Total reward=56.87, Steps=82752, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2449, Total reward=40.4, Steps=82799, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2450, Total reward=42.47, Steps=82841, Training iteration=48
Policy training> Surrogate loss=-0.0007743763853795826, KL divergence=9.883029269985855e-05, Entropy=0.36611318588256836, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.014601551927626133, KL divergence=0.004807729739695787, Entropy=0.37236979603767395, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04624699428677559, KL divergence=0.01734047569334507, Entropy=0.3607460558414459, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.03201671317219734, KL divergence=0.034233372658491135, Entropy=0.34838756918907166, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.05478239431977272, KL divergence=0.04749341681599617, Entropy=0.35352596640586853, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05270470306277275, KL divergence=0.061274897307157516, Entropy=0.3363257944583893, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05535600706934929, KL divergence=0.0714927539229393, Entropy=0.3475090563297272, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.04482569918036461, KL divergence=0.08242562413215637, Entropy=0.3426654040813446, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.054079752415418625, KL divergence=0.09257658571004868, Entropy=0.33470869064331055, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07722706347703934, KL divergence=0.09768322855234146, Entropy=0.34065183997154236, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/341_Step-82841.ckpt']
Uploaded 3 files for checkpoint 341 in 0.57 seconds
saved intermediate frozen graph: current/model/model_341.pb
Best checkpoint number: 314, Last checkpoint number: 339
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'338'}
Training> Name=main_level/agent, Worker=0, Episode=2451, Total reward=69.29, Steps=82881, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2452, Total reward=55.89, Steps=82913, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2453, Total reward=48.81, Steps=82942, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2454, Total reward=39.53, Steps=82963, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2455, Total reward=19.56, Steps=82989, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2456, Total reward=26.81, Steps=83012, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2457, Total reward=42.38, Steps=83051, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2458, Total reward=28.74, Steps=83072, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2459, Total reward=23.44, Steps=83091, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2460, Total reward=81.93, Steps=83132, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2461, Total reward=62.75, Steps=83166, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2462, Total reward=26.15, Steps=83186, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2463, Total reward=89.02, Steps=83252, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2464, Total reward=12.87, Steps=83287, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2465, Total reward=9.99, Steps=83312, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2466, Total reward=18.34, Steps=83342, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2467, Total reward=50.71, Steps=83385, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2468, Total reward=58.96, Steps=83414, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2469, Total reward=30.63, Steps=83437, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2470, Total reward=107.25, Steps=83485, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2471, Total reward=51.39, Steps=83526, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2472, Total reward=25.41, Steps=83547, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2473, Total reward=21.42, Steps=83558, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2474, Total reward=42.58, Steps=83579, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2475, Total reward=18.91, Steps=83605, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2476, Total reward=24.47, Steps=83636, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2477, Total reward=59.17, Steps=83674, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2478, Total reward=42.29, Steps=83699, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2479, Total reward=79.71, Steps=83755, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2480, Total reward=47.55, Steps=83781, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2481, Total reward=39.03, Steps=83809, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2482, Total reward=37.03, Steps=83832, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2483, Total reward=11.2, Steps=83874, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2484, Total reward=17.65, Steps=83899, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2485, Total reward=66.24, Steps=83961, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2486, Total reward=101.89, Steps=84047, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2487, Total reward=38.24, Steps=84086, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2488, Total reward=59.03, Steps=84144, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2489, Total reward=19.89, Steps=84174, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2490, Total reward=61.58, Steps=84221, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2491, Total reward=43.44, Steps=84250, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2492, Total reward=3.76, Steps=84261, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2493, Total reward=44.51, Steps=84292, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2494, Total reward=38.85, Steps=84313, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2495, Total reward=16.34, Steps=84339, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2496, Total reward=29.18, Steps=84369, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2497, Total reward=94.78, Steps=84449, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2498, Total reward=29.93, Steps=84484, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2499, Total reward=25.73, Steps=84517, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2500, Total reward=90.02, Steps=84565, Training iteration=49
Policy training> Surrogate loss=-0.002307245507836342, KL divergence=7.372159598162398e-05, Entropy=0.35786470770835876, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.018509872257709503, KL divergence=0.004741269629448652, Entropy=0.3414627015590668, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03708511218428612, KL divergence=0.017009802162647247, Entropy=0.3380809724330902, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04068196192383766, KL divergence=0.03150712326169014, Entropy=0.34445762634277344, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.055584866553545, KL divergence=0.04502706229686737, Entropy=0.3404105603694916, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.03381047397851944, KL divergence=0.0568731315433979, Entropy=0.33341026306152344, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04799164831638336, KL divergence=0.06828496605157852, Entropy=0.33262714743614197, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.04967616870999336, KL divergence=0.07871917635202408, Entropy=0.32169094681739807, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0586540512740612, KL divergence=0.08950444310903549, Entropy=0.33217954635620117, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06061699986457825, KL divergence=0.09352550655603409, Entropy=0.3243021070957184, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/342_Step-84565.ckpt']
Uploaded 3 files for checkpoint 342 in 0.56 seconds
saved intermediate frozen graph: current/model/model_342.pb
Best checkpoint number: 314, Last checkpoint number: 340
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'339'}
Training> Name=main_level/agent, Worker=0, Episode=2501, Total reward=65.95, Steps=84594, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2502, Total reward=23.36, Steps=84610, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2503, Total reward=0.02, Steps=84633, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2504, Total reward=15.92, Steps=84676, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2505, Total reward=2.94, Steps=84685, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2506, Total reward=78.47, Steps=84730, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2507, Total reward=67.32, Steps=84783, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2508, Total reward=91.78, Steps=84862, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2509, Total reward=81.41, Steps=84905, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2510, Total reward=90.49, Steps=84950, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2511, Total reward=47.08, Steps=84979, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2512, Total reward=13.0, Steps=84991, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2513, Total reward=44.72, Steps=85021, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2514, Total reward=27.58, Steps=85032, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2515, Total reward=15.64, Steps=85065, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2516, Total reward=17.88, Steps=85082, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2517, Total reward=80.49, Steps=85161, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2518, Total reward=34.43, Steps=85199, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2519, Total reward=68.77, Steps=85256, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2520, Total reward=71.23, Steps=85304, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2521, Total reward=47.3, Steps=85331, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2522, Total reward=41.25, Steps=85356, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2523, Total reward=28.54, Steps=85397, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2524, Total reward=22.67, Steps=85444, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2525, Total reward=92.49, Steps=85503, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2526, Total reward=82.07, Steps=85580, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2527, Total reward=74.23, Steps=85619, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2528, Total reward=73.89, Steps=85660, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2529, Total reward=26.92, Steps=85686, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2530, Total reward=47.58, Steps=85735, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2531, Total reward=39.24, Steps=85763, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2532, Total reward=51.09, Steps=85794, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2533, Total reward=50.19, Steps=85823, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2534, Total reward=26.63, Steps=85833, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2535, Total reward=17.0, Steps=85880, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2536, Total reward=19.92, Steps=85896, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2537, Total reward=47.93, Steps=85932, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2538, Total reward=34.31, Steps=85973, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2539, Total reward=25.28, Steps=85993, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2540, Total reward=76.0, Steps=86042, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2541, Total reward=42.68, Steps=86069, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2542, Total reward=28.93, Steps=86088, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2543, Total reward=3.55, Steps=86106, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2544, Total reward=20.22, Steps=86131, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2545, Total reward=47.3, Steps=86194, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2546, Total reward=44.58, Steps=86241, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2547, Total reward=53.05, Steps=86282, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2548, Total reward=57.38, Steps=86340, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2549, Total reward=26.75, Steps=86360, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2550, Total reward=63.12, Steps=86392, Training iteration=50
Policy training> Surrogate loss=-0.0006671498413197696, KL divergence=0.00011279444879619405, Entropy=0.3544074296951294, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.03244134038686752, KL divergence=0.0067660994827747345, Entropy=0.3541069030761719, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.030987927690148354, KL divergence=0.023230336606502533, Entropy=0.342693954706192, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04285655915737152, KL divergence=0.03527427092194557, Entropy=0.3451094925403595, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.04836566746234894, KL divergence=0.05056920275092125, Entropy=0.33699676394462585, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06217283010482788, KL divergence=0.060474514961242676, Entropy=0.33922407031059265, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05213313177227974, KL divergence=0.07105907052755356, Entropy=0.3299283981323242, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.06576474756002426, KL divergence=0.07338956743478775, Entropy=0.3283112943172455, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.0415722019970417, KL divergence=0.0850757583975792, Entropy=0.3350270688533783, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0603724829852581, KL divergence=0.09954991936683655, Entropy=0.3365165889263153, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/343_Step-86392.ckpt']
Uploaded 3 files for checkpoint 343 in 0.65 seconds
saved intermediate frozen graph: current/model/model_343.pb
Best checkpoint number: 314, Last checkpoint number: 341
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'340'}
Training> Name=main_level/agent, Worker=0, Episode=2551, Total reward=23.62, Steps=86422, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2552, Total reward=3.74, Steps=86433, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2553, Total reward=28.48, Steps=86453, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2554, Total reward=43.07, Steps=86476, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2555, Total reward=39.19, Steps=86505, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2556, Total reward=16.01, Steps=86539, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2557, Total reward=42.15, Steps=86573, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2558, Total reward=40.12, Steps=86598, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2559, Total reward=45.21, Steps=86667, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2560, Total reward=75.48, Steps=86706, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2561, Total reward=59.59, Steps=86737, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2562, Total reward=42.74, Steps=86760, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2563, Total reward=60.09, Steps=86836, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2564, Total reward=21.54, Steps=86878, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2565, Total reward=54.29, Steps=86936, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2566, Total reward=71.55, Steps=86980, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2567, Total reward=64.22, Steps=87020, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2568, Total reward=83.78, Steps=87095, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2569, Total reward=59.74, Steps=87154, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2570, Total reward=65.37, Steps=87190, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2571, Total reward=44.13, Steps=87217, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2572, Total reward=30.15, Steps=87243, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2573, Total reward=19.82, Steps=87262, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2574, Total reward=40.02, Steps=87284, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2575, Total reward=26.34, Steps=87315, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2576, Total reward=10.22, Steps=87347, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2577, Total reward=42.54, Steps=87395, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2578, Total reward=42.57, Steps=87420, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2579, Total reward=69.72, Steps=87472, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2580, Total reward=89.11, Steps=87521, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2581, Total reward=65.18, Steps=87551, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2582, Total reward=34.6, Steps=87569, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2583, Total reward=14.78, Steps=87604, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2584, Total reward=6.39, Steps=87620, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2585, Total reward=9.65, Steps=87640, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2586, Total reward=82.03, Steps=87687, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2587, Total reward=74.37, Steps=87752, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2588, Total reward=68.57, Steps=87780, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2589, Total reward=71.09, Steps=87827, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2590, Total reward=69.38, Steps=87864, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2591, Total reward=54.47, Steps=87902, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2592, Total reward=63.88, Steps=87933, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2593, Total reward=51.96, Steps=87962, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2594, Total reward=45.31, Steps=87984, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2595, Total reward=17.86, Steps=88010, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2596, Total reward=21.9, Steps=88036, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2597, Total reward=60.25, Steps=88087, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2598, Total reward=33.73, Steps=88110, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2599, Total reward=22.9, Steps=88145, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2600, Total reward=84.14, Steps=88195, Training iteration=51
Policy training> Surrogate loss=-0.008671213872730732, KL divergence=7.636256486875936e-05, Entropy=0.37374064326286316, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.0195635799318552, KL divergence=0.0052610174752771854, Entropy=0.3729077875614166, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04107053950428963, KL divergence=0.01866660825908184, Entropy=0.35746216773986816, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.041441600769758224, KL divergence=0.03076009452342987, Entropy=0.3532527983188629, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.047945354133844376, KL divergence=0.048179518431425095, Entropy=0.3544149696826935, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.06636262685060501, KL divergence=0.05847838148474693, Entropy=0.35093531012535095, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05042130872607231, KL divergence=0.07306250929832458, Entropy=0.35422682762145996, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0416128896176815, KL divergence=0.0823904424905777, Entropy=0.3497849404811859, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.04417775943875313, KL divergence=0.09246579557657242, Entropy=0.34562948346138, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07520348578691483, KL divergence=0.09904599189758301, Entropy=0.3495781123638153, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/344_Step-88195.ckpt']
Uploaded 3 files for checkpoint 344 in 0.57 seconds
saved intermediate frozen graph: current/model/model_344.pb
Best checkpoint number: 314, Last checkpoint number: 342
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'341'}
Training> Name=main_level/agent, Worker=0, Episode=2601, Total reward=55.9, Steps=88225, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2602, Total reward=30.22, Steps=88244, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2603, Total reward=2.76, Steps=88283, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2604, Total reward=5.3, Steps=88322, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2605, Total reward=32.71, Steps=88360, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2606, Total reward=14.75, Steps=88390, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2607, Total reward=57.5, Steps=88429, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2608, Total reward=59.53, Steps=88458, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2609, Total reward=80.03, Steps=88519, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2610, Total reward=65.18, Steps=88559, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2611, Total reward=10.89, Steps=88586, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2612, Total reward=49.56, Steps=88618, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2613, Total reward=37.39, Steps=88645, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2614, Total reward=45.29, Steps=88667, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2615, Total reward=18.68, Steps=88711, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2616, Total reward=18.93, Steps=88743, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2617, Total reward=44.7, Steps=88798, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2618, Total reward=32.14, Steps=88823, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2619, Total reward=22.74, Steps=88842, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2620, Total reward=77.03, Steps=88880, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2621, Total reward=35.2, Steps=88905, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2622, Total reward=33.5, Steps=88923, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2623, Total reward=9.62, Steps=88968, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2624, Total reward=102.47, Steps=89054, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2625, Total reward=42.43, Steps=89118, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2626, Total reward=14.76, Steps=89133, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2627, Total reward=57.04, Steps=89171, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2628, Total reward=42.18, Steps=89196, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2629, Total reward=14.24, Steps=89212, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2630, Total reward=66.24, Steps=89248, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2631, Total reward=30.2, Steps=89276, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2632, Total reward=49.03, Steps=89308, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2633, Total reward=38.47, Steps=89339, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2634, Total reward=42.85, Steps=89362, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2635, Total reward=23.92, Steps=89390, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2636, Total reward=20.4, Steps=89413, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2637, Total reward=63.35, Steps=89471, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2638, Total reward=35.09, Steps=89492, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2639, Total reward=91.6, Steps=89540, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2640, Total reward=65.65, Steps=89582, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2641, Total reward=57.2, Steps=89623, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2642, Total reward=29.27, Steps=89643, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2643, Total reward=10.29, Steps=89689, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2644, Total reward=10.59, Steps=89730, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2645, Total reward=54.36, Steps=89785, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2646, Total reward=11.12, Steps=89798, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2647, Total reward=59.99, Steps=89838, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2648, Total reward=47.85, Steps=89867, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2649, Total reward=23.25, Steps=89885, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2650, Total reward=84.65, Steps=89921, Training iteration=52
Policy training> Surrogate loss=0.017665982246398926, KL divergence=0.00010937217302853242, Entropy=0.36098363995552063, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.014683082699775696, KL divergence=0.005489595700055361, Entropy=0.35697293281555176, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03766531124711037, KL divergence=0.01732449419796467, Entropy=0.35055527091026306, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04174937680363655, KL divergence=0.03215966001152992, Entropy=0.3390573561191559, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.03766758367419243, KL divergence=0.05071932077407837, Entropy=0.33673593401908875, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.039392586797475815, KL divergence=0.06365029513835907, Entropy=0.3305377960205078, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.036104146391153336, KL divergence=0.0737769827246666, Entropy=0.3331640064716339, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.05184491351246834, KL divergence=0.08104557543992996, Entropy=0.32572364807128906, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05020647868514061, KL divergence=0.09221961349248886, Entropy=0.32656964659690857, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0633852556347847, KL divergence=0.09744428843259811, Entropy=0.32801252603530884, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/345_Step-89921.ckpt']
Uploaded 3 files for checkpoint 345 in 0.53 seconds
saved intermediate frozen graph: current/model/model_345.pb
Best checkpoint number: 314, Last checkpoint number: 343
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'342'}
Training> Name=main_level/agent, Worker=0, Episode=2651, Total reward=58.75, Steps=89959, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2652, Total reward=40.99, Steps=89989, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2653, Total reward=48.06, Steps=90018, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2654, Total reward=41.85, Steps=90038, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2655, Total reward=21.51, Steps=90080, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2656, Total reward=19.94, Steps=90099, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2657, Total reward=48.44, Steps=90161, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2658, Total reward=48.62, Steps=90208, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2659, Total reward=21.97, Steps=90228, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2660, Total reward=80.41, Steps=90265, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2661, Total reward=64.61, Steps=90297, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2662, Total reward=30.08, Steps=90315, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2663, Total reward=10.04, Steps=90341, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2664, Total reward=11.63, Steps=90366, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2665, Total reward=23.99, Steps=90405, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2666, Total reward=59.1, Steps=90444, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2667, Total reward=62.22, Steps=90483, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2668, Total reward=66.99, Steps=90524, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2669, Total reward=16.51, Steps=90541, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2670, Total reward=82.82, Steps=90587, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2671, Total reward=65.48, Steps=90614, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2672, Total reward=36.1, Steps=90643, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2673, Total reward=31.75, Steps=90662, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2674, Total reward=37.5, Steps=90683, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2675, Total reward=26.42, Steps=90714, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2676, Total reward=15.84, Steps=90745, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2677, Total reward=52.64, Steps=90780, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2678, Total reward=29.7, Steps=90804, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2679, Total reward=14.71, Steps=90828, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2680, Total reward=67.68, Steps=90870, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2681, Total reward=43.89, Steps=90897, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2682, Total reward=35.89, Steps=90918, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2683, Total reward=0.03, Steps=90946, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2684, Total reward=55.92, Steps=91019, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2685, Total reward=13.2, Steps=91043, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2686, Total reward=27.5, Steps=91073, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2687, Total reward=77.85, Steps=91114, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2688, Total reward=68.39, Steps=91152, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2689, Total reward=59.13, Steps=91181, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2690, Total reward=49.34, Steps=91204, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2691, Total reward=38.48, Steps=91233, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2692, Total reward=56.42, Steps=91264, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2693, Total reward=49.4, Steps=91293, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2694, Total reward=34.68, Steps=91314, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2695, Total reward=20.67, Steps=91341, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2696, Total reward=25.47, Steps=91372, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2697, Total reward=72.96, Steps=91439, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2698, Total reward=33.58, Steps=91464, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2699, Total reward=63.1, Steps=91506, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2700, Total reward=76.42, Steps=91546, Training iteration=53
Policy training> Surrogate loss=0.003541877493262291, KL divergence=8.360330684809014e-05, Entropy=0.3510236442089081, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.026426950469613075, KL divergence=0.004174523055553436, Entropy=0.35137948393821716, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04044722393155098, KL divergence=0.015244799666106701, Entropy=0.3399832248687744, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04632227495312691, KL divergence=0.029438719153404236, Entropy=0.336385577917099, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.046201109886169434, KL divergence=0.04246533289551735, Entropy=0.3334529399871826, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05840732529759407, KL divergence=0.055202316492795944, Entropy=0.32668837904930115, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.05152397230267525, KL divergence=0.0665539801120758, Entropy=0.3309744894504547, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.07138480991125107, KL divergence=0.07457580417394638, Entropy=0.32208576798439026, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06110386922955513, KL divergence=0.08349593728780746, Entropy=0.3271644115447998, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.07154073566198349, KL divergence=0.09022035449743271, Entropy=0.3252657949924469, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/346_Step-91546.ckpt']
Uploaded 3 files for checkpoint 346 in 0.55 seconds
saved intermediate frozen graph: current/model/model_346.pb
Best checkpoint number: 314, Last checkpoint number: 344
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'343'}
Training> Name=main_level/agent, Worker=0, Episode=2701, Total reward=61.35, Steps=91578, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2702, Total reward=32.13, Steps=91602, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2703, Total reward=46.99, Steps=91682, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2704, Total reward=8.64, Steps=91702, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2705, Total reward=9.88, Steps=91724, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2706, Total reward=87.38, Steps=91768, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2707, Total reward=45.92, Steps=91807, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2708, Total reward=60.94, Steps=91837, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2709, Total reward=18.48, Steps=91850, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2710, Total reward=84.0, Steps=91895, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2711, Total reward=76.95, Steps=91932, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2712, Total reward=14.35, Steps=91958, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2713, Total reward=55.43, Steps=91987, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2714, Total reward=43.63, Steps=92008, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2715, Total reward=36.34, Steps=92037, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2716, Total reward=22.75, Steps=92063, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2717, Total reward=44.14, Steps=92110, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2718, Total reward=23.74, Steps=92135, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2719, Total reward=8.08, Steps=92159, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2720, Total reward=83.38, Steps=92197, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2721, Total reward=29.54, Steps=92220, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2722, Total reward=41.54, Steps=92246, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2723, Total reward=9.91, Steps=92284, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2724, Total reward=80.8, Steps=92346, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2725, Total reward=4.89, Steps=92360, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2726, Total reward=69.5, Steps=92406, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2727, Total reward=60.38, Steps=92445, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2728, Total reward=83.63, Steps=92498, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2729, Total reward=48.59, Steps=92530, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2730, Total reward=82.93, Steps=92564, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2731, Total reward=43.99, Steps=92600, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2732, Total reward=58.3, Steps=92632, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2733, Total reward=44.99, Steps=92662, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2734, Total reward=28.69, Steps=92682, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2735, Total reward=19.73, Steps=92731, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2736, Total reward=16.59, Steps=92768, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2737, Total reward=20.94, Steps=92787, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2738, Total reward=29.96, Steps=92810, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2739, Total reward=10.28, Steps=92832, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2740, Total reward=91.89, Steps=92883, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2741, Total reward=53.93, Steps=92910, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2742, Total reward=36.03, Steps=92927, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2743, Total reward=8.78, Steps=92950, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2744, Total reward=11.55, Steps=92990, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2745, Total reward=13.34, Steps=93013, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2746, Total reward=87.9, Steps=93059, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2747, Total reward=107.74, Steps=93154, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2748, Total reward=66.67, Steps=93193, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2749, Total reward=38.92, Steps=93218, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2750, Total reward=82.14, Steps=93265, Training iteration=54
Policy training> Surrogate loss=-0.006745430175215006, KL divergence=0.00011902811093023047, Entropy=0.3738481104373932, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.020301511511206627, KL divergence=0.0066985501907765865, Entropy=0.3733193576335907, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.04616844281554222, KL divergence=0.017792455852031708, Entropy=0.3661646842956543, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04685105010867119, KL divergence=0.03571872413158417, Entropy=0.35612964630126953, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.06322416663169861, KL divergence=0.054026443511247635, Entropy=0.3577701151371002, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05967644229531288, KL divergence=0.06683524698019028, Entropy=0.346222847700119, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.04667080566287041, KL divergence=0.08489087969064713, Entropy=0.3504951000213623, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.0678730383515358, KL divergence=0.09152159094810486, Entropy=0.3450232744216919, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.06988196820020676, KL divergence=0.09878041595220566, Entropy=0.34893468022346497, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.0710798129439354, KL divergence=0.10789506882429123, Entropy=0.34249112010002136, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/347_Step-93265.ckpt']
Uploaded 3 files for checkpoint 347 in 0.60 seconds
saved intermediate frozen graph: current/model/model_347.pb
Best checkpoint number: 314, Last checkpoint number: 345
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'344'}
Training> Name=main_level/agent, Worker=0, Episode=2751, Total reward=57.06, Steps=93293, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2752, Total reward=5.74, Steps=93304, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2753, Total reward=50.61, Steps=93331, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2754, Total reward=43.25, Steps=93353, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2755, Total reward=21.67, Steps=93384, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2756, Total reward=10.0, Steps=93403, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2757, Total reward=38.41, Steps=93437, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2758, Total reward=31.19, Steps=93459, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2759, Total reward=14.84, Steps=93484, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2760, Total reward=77.81, Steps=93527, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2761, Total reward=46.65, Steps=93559, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2762, Total reward=32.17, Steps=93577, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2763, Total reward=17.54, Steps=93613, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2764, Total reward=4.99, Steps=93633, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2765, Total reward=1.4, Steps=93645, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2766, Total reward=89.62, Steps=93706, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2767, Total reward=100.83, Steps=93792, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2768, Total reward=96.98, Steps=93852, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2769, Total reward=62.77, Steps=93911, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2770, Total reward=72.11, Steps=93958, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2771, Total reward=60.82, Steps=93997, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2772, Total reward=61.02, Steps=94040, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2773, Total reward=47.08, Steps=94068, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2774, Total reward=32.18, Steps=94088, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2775, Total reward=29.37, Steps=94116, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2776, Total reward=21.67, Steps=94135, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2777, Total reward=46.34, Steps=94170, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2778, Total reward=38.2, Steps=94195, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2779, Total reward=65.63, Steps=94245, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2780, Total reward=78.84, Steps=94279, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2781, Total reward=66.03, Steps=94329, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2782, Total reward=42.25, Steps=94354, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2783, Total reward=7.05, Steps=94373, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2784, Total reward=1.79, Steps=94393, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2785, Total reward=22.37, Steps=94429, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2786, Total reward=87.39, Steps=94478, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2787, Total reward=63.36, Steps=94520, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2788, Total reward=64.02, Steps=94557, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2789, Total reward=34.48, Steps=94582, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2790, Total reward=58.99, Steps=94619, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2791, Total reward=18.96, Steps=94632, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2792, Total reward=14.66, Steps=94653, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2793, Total reward=58.4, Steps=94683, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2794, Total reward=45.42, Steps=94705, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2795, Total reward=18.54, Steps=94733, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2796, Total reward=26.34, Steps=94764, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2797, Total reward=27.23, Steps=94786, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2798, Total reward=39.83, Steps=94814, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2799, Total reward=91.68, Steps=94863, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2800, Total reward=87.58, Steps=94899, Training iteration=55
Policy training> Surrogate loss=0.006595275830477476, KL divergence=0.00025462298071943223, Entropy=0.3606022596359253, training epoch=0, learning_rate=5e-05
Policy training> Surrogate loss=-0.026837656274437904, KL divergence=0.007225415203720331, Entropy=0.36440715193748474, training epoch=1, learning_rate=5e-05
Policy training> Surrogate loss=-0.03715534508228302, KL divergence=0.022637411952018738, Entropy=0.3602379262447357, training epoch=2, learning_rate=5e-05
Policy training> Surrogate loss=-0.04398755356669426, KL divergence=0.03797735273838043, Entropy=0.35749897360801697, training epoch=3, learning_rate=5e-05
Policy training> Surrogate loss=-0.062050629407167435, KL divergence=0.05366216227412224, Entropy=0.3540550768375397, training epoch=4, learning_rate=5e-05
Policy training> Surrogate loss=-0.05123896524310112, KL divergence=0.067502461373806, Entropy=0.3511688709259033, training epoch=5, learning_rate=5e-05
Policy training> Surrogate loss=-0.06077760085463524, KL divergence=0.0777309462428093, Entropy=0.34780725836753845, training epoch=6, learning_rate=5e-05
Policy training> Surrogate loss=-0.061432380229234695, KL divergence=0.09067799896001816, Entropy=0.35256609320640564, training epoch=7, learning_rate=5e-05
Policy training> Surrogate loss=-0.05345989391207695, KL divergence=0.10190706700086594, Entropy=0.34479522705078125, training epoch=8, learning_rate=5e-05
Policy training> Surrogate loss=-0.06899634748697281, KL divergence=0.11150586605072021, Entropy=0.3515234887599945, training epoch=9, learning_rate=5e-05
Checkpoint> Saving in path=['./checkpoint/348_Step-94899.ckpt']
Uploaded 3 files for checkpoint 348 in 0.52 seconds
saved intermediate frozen graph: current/model/model_348.pb
Best checkpoint number: 314, Last checkpoint number: 346
Copying the frozen checkpoint from ./frozen_models/agent/model_314.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'345'}
Training> Name=main_level/agent, Worker=0, Episode=2801, Total reward=59.57, Steps=94929, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2802, Total reward=39.9, Steps=94950, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2803, Total reward=23.33, Steps=94997, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2804, Total reward=15.18, Steps=95026, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2805, Total reward=74.76, Steps=95097, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2806, Total reward=28.49, Steps=95125, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2807, Total reward=44.64, Steps=95165, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2808, Total reward=77.31, Steps=95206, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2809, Total reward=95.61, Steps=95263, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2810, Total reward=37.33, Steps=95300, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2811, Total reward=32.36, Steps=95329, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2812, Total reward=53.68, Steps=95367, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2813, Total reward=33.9, Steps=95390, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2814, Total reward=44.78, Steps=95411, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2815, Total reward=28.99, Steps=95439, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2816, Total reward=21.71, Steps=95459, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2817, Total reward=49.5, Steps=95495, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2818, Total reward=31.28, Steps=95514, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2819, Total reward=49.22, Steps=95557, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2820, Total reward=68.62, Steps=95593, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2821, Total reward=54.33, Steps=95622, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2822, Total reward=31.74, Steps=95643, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2823, Total reward=7.52, Steps=95659, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2824, Total reward=88.35, Steps=95742, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2825, Total reward=22.9, Steps=95764, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2826, Total reward=82.73, Steps=95810, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2827, Total reward=58.62, Steps=95849, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2828, Total reward=58.12, Steps=95878, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2829, Total reward=29.02, Steps=95896, Training iteration=56
