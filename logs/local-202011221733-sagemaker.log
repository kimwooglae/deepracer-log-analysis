20:C 21 Nov 2020 23:07:53.916 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
20:C 21 Nov 2020 23:07:53.916 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=20, just started
20:C 21 Nov 2020 23:07:53.916 # Configuration loaded
                _._                                                  
           _.-``__ ''-._                                             
      _.-``    `.  `_.  ''-._           Redis 5.0.8 (00000000/0) 64 bit
  .-`` .-```.  ```\/    _.,_ ''-._                                   
 (    '      ,       .-`  | `,    )     Running in standalone mode
 |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
 |    `-._   `._    /     _.-'    |     PID: 20
  `-._    `-._  `-./  _.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |           http://redis.io        
  `-._    `-._`-.__.-'_.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |                                  
  `-._    `-._`-.__.-'_.-'    _.-'                                   
      `-._    `-.__.-'    _.-'                                       
          `-._        _.-'                                           
              `-.__.-'                                               

20:M 21 Nov 2020 23:07:53.917 # WARNING: The TCP backlog setting of 512 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
20:M 21 Nov 2020 23:07:53.917 # Server initialized
20:M 21 Nov 2020 23:07:53.917 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
20:M 21 Nov 2020 23:07:53.917 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
20:M 21 Nov 2020 23:07:53.917 * Ready to accept connections
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2020-11-21 23:07:56,119 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training
2020-11-21 23:07:56,392 sagemaker-containers INFO     Invoking user script

Training Env:

{
    "additional_framework_parameters": {
        "sagemaker_estimator": "RLEstimator"
    },
    "channel_input_dirs": {},
    "current_host": "algo-1-0qfj9",
    "framework_module": "sagemaker_tensorflow_container.training:main",
    "hosts": [
        "algo-1-0qfj9"
    ],
    "hyperparameters": {
        "s3_bucket": "bucket",
        "s3_prefix": "current",
        "aws_region": "us-east-1",
        "model_metadata_s3_key": "s3://bucket/custom_files/model_metadata.json",
        "RLCOACH_PRESET": "deepracer",
        "batch_size": 256,
        "beta_entropy": 0.01,
        "discount_factor": 0.999,
        "e_greedy_value": 0.05,
        "epsilon_steps": 10000,
        "exploration_type": "categorical",
        "loss_type": "huber",
        "lr": 1e-05,
        "num_episodes_between_training": 20,
        "num_epochs": 10,
        "stack_size": 1,
        "term_cond_avg_score": 100000.0,
        "term_cond_max_episodes": 1000000,
        "pretrained_s3_bucket": "bucket",
        "pretrained_s3_prefix": "rl-deepracer-pretrained"
    },
    "input_config_dir": "/opt/ml/input/config",
    "input_data_config": {},
    "input_dir": "/opt/ml/input",
    "is_master": true,
    "job_name": "current",
    "log_level": 20,
    "master_hostname": "algo-1-0qfj9",
    "model_dir": "/opt/ml/model",
    "module_dir": "s3://bucket/current/source/sourcedir.tar.gz",
    "module_name": "training_worker",
    "network_interface_name": "eth0",
    "num_cpus": 16,
    "num_gpus": 3,
    "output_data_dir": "/opt/ml/output/data",
    "output_dir": "/opt/ml/output",
    "output_intermediate_dir": "/opt/ml/output/intermediate",
    "resource_config": {
        "current_host": "algo-1-0qfj9",
        "hosts": [
            "algo-1-0qfj9"
        ]
    },
    "user_entry_point": "training_worker.py"
}

Environment variables:

SM_HOSTS=["algo-1-0qfj9"]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":256,"beta_entropy":0.01,"discount_factor":0.999,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"huber","lr":1e-05,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":20,"num_epochs":10,"pretrained_s3_bucket":"bucket","pretrained_s3_prefix":"rl-deepracer-pretrained","s3_bucket":"bucket","s3_prefix":"current","stack_size":1,"term_cond_avg_score":100000.0,"term_cond_max_episodes":1000000}
SM_USER_ENTRY_POINT=training_worker.py
SM_FRAMEWORK_PARAMS={"sagemaker_estimator":"RLEstimator"}
SM_RESOURCE_CONFIG={"current_host":"algo-1-0qfj9","hosts":["algo-1-0qfj9"]}
SM_INPUT_DATA_CONFIG={}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=[]
SM_CURRENT_HOST=algo-1-0qfj9
SM_MODULE_NAME=training_worker
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=16
SM_NUM_GPUS=3
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://bucket/current/source/sourcedir.tar.gz
SM_TRAINING_ENV={"additional_framework_parameters":{"sagemaker_estimator":"RLEstimator"},"channel_input_dirs":{},"current_host":"algo-1-0qfj9","framework_module":"sagemaker_tensorflow_container.training:main","hosts":["algo-1-0qfj9"],"hyperparameters":{"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":256,"beta_entropy":0.01,"discount_factor":0.999,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"huber","lr":1e-05,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":20,"num_epochs":10,"pretrained_s3_bucket":"bucket","pretrained_s3_prefix":"rl-deepracer-pretrained","s3_bucket":"bucket","s3_prefix":"current","stack_size":1,"term_cond_avg_score":100000.0,"term_cond_max_episodes":1000000},"input_config_dir":"/opt/ml/input/config","input_data_config":{},"input_dir":"/opt/ml/input","is_master":true,"job_name":"current","log_level":20,"master_hostname":"algo-1-0qfj9","model_dir":"/opt/ml/model","module_dir":"s3://bucket/current/source/sourcedir.tar.gz","module_name":"training_worker","network_interface_name":"eth0","num_cpus":16,"num_gpus":3,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1-0qfj9","hosts":["algo-1-0qfj9"]},"user_entry_point":"training_worker.py"}
SM_USER_ARGS=["--RLCOACH_PRESET","deepracer","--aws_region","us-east-1","--batch_size","256","--beta_entropy","0.01","--discount_factor","0.999","--e_greedy_value","0.05","--epsilon_steps","10000","--exploration_type","categorical","--loss_type","huber","--lr","1e-05","--model_metadata_s3_key","s3://bucket/custom_files/model_metadata.json","--num_episodes_between_training","20","--num_epochs","10","--pretrained_s3_bucket","bucket","--pretrained_s3_prefix","rl-deepracer-pretrained","--s3_bucket","bucket","--s3_prefix","current","--stack_size","1","--term_cond_avg_score","100000.0","--term_cond_max_episodes","1000000"]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_HP_S3_BUCKET=bucket
SM_HP_S3_PREFIX=current
SM_HP_AWS_REGION=us-east-1
SM_HP_MODEL_METADATA_S3_KEY=s3://bucket/custom_files/model_metadata.json
SM_HP_RLCOACH_PRESET=deepracer
SM_HP_BATCH_SIZE=256
SM_HP_BETA_ENTROPY=0.01
SM_HP_DISCOUNT_FACTOR=0.999
SM_HP_E_GREEDY_VALUE=0.05
SM_HP_EPSILON_STEPS=10000
SM_HP_EXPLORATION_TYPE=categorical
SM_HP_LOSS_TYPE=huber
SM_HP_LR=1e-05
SM_HP_NUM_EPISODES_BETWEEN_TRAINING=20
SM_HP_NUM_EPOCHS=10
SM_HP_STACK_SIZE=1
SM_HP_TERM_COND_AVG_SCORE=100000.0
SM_HP_TERM_COND_MAX_EPISODES=1000000
SM_HP_PRETRAINED_S3_BUCKET=bucket
SM_HP_PRETRAINED_S3_PREFIX=rl-deepracer-pretrained
PYTHONPATH=/usr/local/bin:/opt/amazon:/opt/ml/code:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages

Invoking script with the following command:

/usr/bin/python training_worker.py --RLCOACH_PRESET deepracer --aws_region us-east-1 --batch_size 256 --beta_entropy 0.01 --discount_factor 0.999 --e_greedy_value 0.05 --epsilon_steps 10000 --exploration_type categorical --loss_type huber --lr 1e-05 --model_metadata_s3_key s3://bucket/custom_files/model_metadata.json --num_episodes_between_training 20 --num_epochs 10 --pretrained_s3_bucket bucket --pretrained_s3_prefix rl-deepracer-pretrained --s3_bucket bucket --s3_prefix current --stack_size 1 --term_cond_avg_score 100000.0 --term_cond_max_episodes 1000000


S3 bucket: bucket 
 S3 prefix: current 
 S3 endpoint URL: http://minio:9000
Initializing SageS3Client...
Successfully downloaded model metadata from custom_files/model_metadata.json.
Sensor list ['STEREO_CAMERAS', 'SECTOR_LIDAR'], network DEEP_CONVOLUTIONAL_NETWORK_SHALLOW, simapp_version 2.0
Loaded action space from file: [{'steering_angle': -30.0, 'speed': 1.5, 'index': 0}, {'steering_angle': -13.7481, 'speed': 2.016, 'index': 1}, {'steering_angle': -4.0046, 'speed': 1.736, 'index': 2}, {'steering_angle': -2.6994, 'speed': 3.5673, 'index': 3}, {'steering_angle': -1.0391, 'speed': 4.794, 'index': 4}, {'steering_angle': -0.9404, 'speed': 4.1988, 'index': 5}, {'steering_angle': 2.9901, 'speed': 2.9986, 'index': 6}, {'steering_angle': 3.7003, 'speed': 2.3287, 'index': 7}, {'steering_angle': 11.9654, 'speed': 1.8113, 'index': 8}, {'steering_angle': 30.0, 'speed': 1.5, 'index': 9}]
Using the following hyper-parameters
{
  "batch_size": 256,
  "beta_entropy": 0.01,
  "discount_factor": 0.999,
  "e_greedy_value": 0.05,
  "epsilon_steps": 10000,
  "exploration_type": "categorical",
  "loss_type": "huber",
  "lr": 1e-05,
  "num_episodes_between_training": 20,
  "num_epochs": 10,
  "stack_size": 1,
  "term_cond_avg_score": 100000.0,
  "term_cond_max_episodes": 1000000
}
Uploaded hyperparameters.json to S3
Uploaded IP address information to S3: 172.18.0.5
Sensor list ['STEREO_CAMERAS', 'SECTOR_LIDAR'], network DEEP_CONVOLUTIONAL_NETWORK_SHALLOW, simapp_version 2.0
Unable to find best model data, using last model
## Creating graph - name: MultiAgentGraphManager
## Start physics before creating graph
## Create graph
## Creating agent - name: agent
## Created agent: agent
## Stop physics after creating graph
## Creating session
Checkpoint> Restoring from path=./pretrained_checkpoint/541_Step-771719.ckpt
Checkpoint> Saving in path=['./checkpoint/542_Step-0.ckpt']
Uploaded 3 files for checkpoint 542 in 0.39 seconds
saved intermediate frozen graph: current/model/model_542.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_542.pb to /opt/ml/model/agent/model.pb.
Uploaded 3 files for checkpoint 542 in 0.62 seconds
saved intermediate frozen graph: current/model/model_542.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_542.pb to /opt/ml/model/agent/model.pb.
DoorMan: installing SIGINT, SIGTERM
Training> Name=main_level/agent, Worker=0, Episode=1, Total reward=466.0, Steps=296, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=2, Total reward=504.88, Steps=585, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=3, Total reward=499.63, Steps=874, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=4, Total reward=152.86, Steps=1034, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=5, Total reward=428.21, Steps=1329, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=6, Total reward=467.26, Steps=1604, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=7, Total reward=398.81, Steps=1804, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=8, Total reward=97.52, Steps=1847, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=9, Total reward=486.22, Steps=2131, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=10, Total reward=455.41, Steps=2420, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=11, Total reward=27.8, Steps=2448, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=12, Total reward=349.8, Steps=2654, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=13, Total reward=97.3, Steps=2718, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=14, Total reward=77.86, Steps=2752, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=15, Total reward=155.48, Steps=2847, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=16, Total reward=300.62, Steps=3078, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=17, Total reward=480.69, Steps=3372, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=18, Total reward=22.76, Steps=3386, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=19, Total reward=492.05, Steps=3671, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=20, Total reward=478.82, Steps=3971, Training iteration=0
Policy training> Surrogate loss=0.0006246097618713975, KL divergence=0.0003623570373747498, Entropy=0.18107368052005768, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017914703115820885, KL divergence=0.0031743727158755064, Entropy=0.18081046640872955, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.019785281270742416, KL divergence=0.005076365079730749, Entropy=0.18179048597812653, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.01749088242650032, KL divergence=0.007017036434262991, Entropy=0.1799192577600479, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.030304603278636932, KL divergence=0.008788792416453362, Entropy=0.17731764912605286, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026682035997509956, KL divergence=0.010510791093111038, Entropy=0.17849288880825043, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.034286610782146454, KL divergence=0.01205950602889061, Entropy=0.17983388900756836, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.0348527655005455, KL divergence=0.013093395158648491, Entropy=0.17847466468811035, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.029512424021959305, KL divergence=0.01423831656575203, Entropy=0.17853476107120514, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03930944576859474, KL divergence=0.0152877913787961, Entropy=0.17699506878852844, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/543_Step-3971.ckpt']
Uploaded 3 files for checkpoint 543 in 0.50 seconds
saved intermediate frozen graph: current/model/model_543.pb
Best checkpoint number: 542, Last checkpoint number: 542
Copying the frozen checkpoint from ./frozen_models/agent/model_542.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=21, Total reward=502.21, Steps=4273, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=22, Total reward=468.53, Steps=4557, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=23, Total reward=33.67, Steps=4646, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=24, Total reward=6.65, Steps=4679, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=25, Total reward=177.57, Steps=4801, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=26, Total reward=229.02, Steps=4933, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=27, Total reward=438.65, Steps=5181, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=28, Total reward=142.81, Steps=5266, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=29, Total reward=268.26, Steps=5438, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=30, Total reward=26.58, Steps=5470, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=31, Total reward=339.94, Steps=5663, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=32, Total reward=136.52, Steps=5738, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=33, Total reward=31.85, Steps=5749, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=34, Total reward=180.24, Steps=5854, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=35, Total reward=160.51, Steps=5966, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=36, Total reward=182.27, Steps=6090, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=37, Total reward=452.88, Steps=6377, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=38, Total reward=392.27, Steps=6636, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=39, Total reward=127.01, Steps=6744, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=40, Total reward=343.73, Steps=6983, Training iteration=1
Policy training> Surrogate loss=-0.0015655051684007049, KL divergence=0.0002775824104901403, Entropy=0.16597293317317963, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.012730947695672512, KL divergence=0.00435235258191824, Entropy=0.16579288244247437, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02071944810450077, KL divergence=0.009478244930505753, Entropy=0.15958508849143982, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.035562947392463684, KL divergence=0.012594935484230518, Entropy=0.16144607961177826, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02720164693892002, KL divergence=0.015639714896678925, Entropy=0.16077198088169098, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02888117916882038, KL divergence=0.017689576372504234, Entropy=0.16007936000823975, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.028011614456772804, KL divergence=0.0187660064548254, Entropy=0.16054405272006989, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03072783164680004, KL divergence=0.020196057856082916, Entropy=0.16188861429691315, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.0343315564095974, KL divergence=0.020901741459965706, Entropy=0.1585836112499237, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.038445331156253815, KL divergence=0.021863175556063652, Entropy=0.15847676992416382, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/544_Step-6983.ckpt']
Uploaded 3 files for checkpoint 544 in 0.54 seconds
saved intermediate frozen graph: current/model/model_544.pb
Best checkpoint number: 542, Last checkpoint number: 542
Copying the frozen checkpoint from ./frozen_models/agent/model_542.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=41, Total reward=303.09, Steps=7188, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=42, Total reward=34.59, Steps=7209, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=43, Total reward=453.34, Steps=7501, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=44, Total reward=462.48, Steps=7797, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=45, Total reward=119.83, Steps=7918, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=46, Total reward=494.4, Steps=8207, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=47, Total reward=503.03, Steps=8490, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=48, Total reward=210.52, Steps=8604, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=49, Total reward=490.94, Steps=8918, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=50, Total reward=113.08, Steps=9002, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=51, Total reward=455.43, Steps=9289, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=52, Total reward=343.48, Steps=9467, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=53, Total reward=352.17, Steps=9621, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=54, Total reward=66.48, Steps=9670, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=55, Total reward=20.06, Steps=9682, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=56, Total reward=34.77, Steps=9704, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=57, Total reward=396.17, Steps=9975, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=58, Total reward=544.8, Steps=10254, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=59, Total reward=372.67, Steps=10494, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=60, Total reward=101.37, Steps=10560, Training iteration=2
Policy training> Surrogate loss=-0.004538238979876041, KL divergence=0.00046077536535449326, Entropy=0.1759503185749054, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.027156757190823555, KL divergence=0.007218760903924704, Entropy=0.17506462335586548, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02065228670835495, KL divergence=0.011567559093236923, Entropy=0.17341862618923187, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.03311040252447128, KL divergence=0.014717716723680496, Entropy=0.17426584661006927, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03430122882127762, KL divergence=0.017078187316656113, Entropy=0.16874948143959045, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03930915147066116, KL divergence=0.018529696390032768, Entropy=0.16989706456661224, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03507383167743683, KL divergence=0.019857535138726234, Entropy=0.17012515664100647, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03466372936964035, KL divergence=0.020812828093767166, Entropy=0.16870005428791046, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03489234298467636, KL divergence=0.021406112238764763, Entropy=0.1653784215450287, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.038828931748867035, KL divergence=0.022739283740520477, Entropy=0.16732937097549438, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/545_Step-10560.ckpt']
Uploaded 3 files for checkpoint 545 in 0.52 seconds
saved intermediate frozen graph: current/model/model_545.pb
Best checkpoint number: 542, Last checkpoint number: 543
Copying the frozen checkpoint from ./frozen_models/agent/model_542.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'543'}
Training> Name=main_level/agent, Worker=0, Episode=61, Total reward=51.13, Steps=10582, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=62, Total reward=304.38, Steps=10797, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=63, Total reward=388.5, Steps=11109, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=64, Total reward=506.66, Steps=11387, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=65, Total reward=231.29, Steps=11542, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=66, Total reward=206.22, Steps=11672, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=67, Total reward=162.27, Steps=11762, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=68, Total reward=236.95, Steps=11867, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=69, Total reward=32.96, Steps=11920, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=70, Total reward=373.29, Steps=12125, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=71, Total reward=330.64, Steps=12379, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=72, Total reward=227.9, Steps=12512, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=73, Total reward=508.35, Steps=12800, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=74, Total reward=58.99, Steps=12830, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=75, Total reward=499.12, Steps=13109, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=76, Total reward=315.67, Steps=13327, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=77, Total reward=524.71, Steps=13602, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=78, Total reward=458.03, Steps=13896, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=79, Total reward=484.05, Steps=14172, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=80, Total reward=481.35, Steps=14470, Training iteration=3
Policy training> Surrogate loss=0.0016146120615303516, KL divergence=0.0008565944735892117, Entropy=0.18673762679100037, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.023600731045007706, KL divergence=0.007989306002855301, Entropy=0.18389634788036346, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.025332026183605194, KL divergence=0.0143319396302104, Entropy=0.180252805352211, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02900119684636593, KL divergence=0.01721888594329357, Entropy=0.180572971701622, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.031986016780138016, KL divergence=0.019550509750843048, Entropy=0.17783291637897491, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030928734689950943, KL divergence=0.02126859873533249, Entropy=0.1766962707042694, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03257247060537338, KL divergence=0.021647434681653976, Entropy=0.17417331039905548, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03644583001732826, KL divergence=0.02288338728249073, Entropy=0.17286773025989532, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03438517078757286, KL divergence=0.024134283885359764, Entropy=0.17336206138134003, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03399836644530296, KL divergence=0.025008022785186768, Entropy=0.1727738231420517, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/546_Step-14470.ckpt']
Uploaded 3 files for checkpoint 546 in 0.55 seconds
saved intermediate frozen graph: current/model/model_546.pb
Best checkpoint number: 542, Last checkpoint number: 544
Copying the frozen checkpoint from ./frozen_models/agent/model_542.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'544'}
Training> Name=main_level/agent, Worker=0, Episode=81, Total reward=425.31, Steps=14766, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=82, Total reward=243.8, Steps=14980, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=83, Total reward=233.8, Steps=15155, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=84, Total reward=96.14, Steps=15241, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=85, Total reward=201.37, Steps=15433, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=86, Total reward=456.71, Steps=15745, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=87, Total reward=211.3, Steps=15849, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=88, Total reward=467.05, Steps=16164, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=89, Total reward=296.0, Steps=16311, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=90, Total reward=267.17, Steps=16472, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=91, Total reward=220.49, Steps=16594, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=92, Total reward=152.19, Steps=16680, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=93, Total reward=90.27, Steps=16720, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=94, Total reward=208.44, Steps=16818, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=95, Total reward=394.67, Steps=17064, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=96, Total reward=470.54, Steps=17356, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=97, Total reward=194.26, Steps=17490, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=98, Total reward=20.38, Steps=17504, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=99, Total reward=319.67, Steps=17735, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=100, Total reward=77.67, Steps=17784, Training iteration=4
Policy training> Surrogate loss=-0.002435738919302821, KL divergence=0.000520644651260227, Entropy=0.17779050767421722, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.024619510397315025, KL divergence=0.007820794358849525, Entropy=0.17513711750507355, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.023814402520656586, KL divergence=0.014009914360940456, Entropy=0.17085857689380646, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.027335723862051964, KL divergence=0.01806279830634594, Entropy=0.16999799013137817, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03361649066209793, KL divergence=0.020145107060670853, Entropy=0.1701655238866806, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03616346791386604, KL divergence=0.02200179360806942, Entropy=0.16724775731563568, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.025661826133728027, KL divergence=0.02312319166958332, Entropy=0.16961698234081268, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.035025034099817276, KL divergence=0.024054760113358498, Entropy=0.16531594097614288, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03827740624547005, KL divergence=0.025491757318377495, Entropy=0.166673943400383, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03742239251732826, KL divergence=0.02675285004079342, Entropy=0.1680174022912979, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/547_Step-17784.ckpt']
Uploaded 3 files for checkpoint 547 in 0.54 seconds
saved intermediate frozen graph: current/model/model_547.pb
Best checkpoint number: 545, Last checkpoint number: 545
Copying the frozen checkpoint from ./frozen_models/agent/model_545.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'542'}
Training> Name=main_level/agent, Worker=0, Episode=101, Total reward=302.41, Steps=17987, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=102, Total reward=59.86, Steps=18039, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=103, Total reward=448.12, Steps=18358, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=104, Total reward=429.35, Steps=18669, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=105, Total reward=443.36, Steps=18985, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=106, Total reward=344.95, Steps=19211, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=107, Total reward=485.31, Steps=19515, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=108, Total reward=151.53, Steps=19603, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=109, Total reward=44.96, Steps=19636, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=110, Total reward=369.98, Steps=19839, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=111, Total reward=365.86, Steps=20037, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=112, Total reward=155.79, Steps=20118, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=113, Total reward=33.91, Steps=20130, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=114, Total reward=53.5, Steps=20163, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=115, Total reward=461.44, Steps=20461, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=116, Total reward=489.69, Steps=20754, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=117, Total reward=87.57, Steps=20826, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=118, Total reward=173.72, Steps=20912, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=119, Total reward=111.03, Steps=20976, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=120, Total reward=382.34, Steps=21221, Training iteration=5
Policy training> Surrogate loss=0.004685369320213795, KL divergence=0.00031497873715125024, Entropy=0.16743697226047516, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.020434634760022163, KL divergence=0.006810662802308798, Entropy=0.16774581372737885, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.028309311717748642, KL divergence=0.013984858058393002, Entropy=0.16511660814285278, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.029102031141519547, KL divergence=0.017218323424458504, Entropy=0.1643078625202179, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028419939801096916, KL divergence=0.020300306379795074, Entropy=0.16456958651542664, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03090371936559677, KL divergence=0.02138552814722061, Entropy=0.1637079417705536, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03204406797885895, KL divergence=0.0231900904327631, Entropy=0.16247938573360443, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.0325399674475193, KL divergence=0.024450065568089485, Entropy=0.16238339245319366, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03656955063343048, KL divergence=0.02522239461541176, Entropy=0.1626850962638855, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.04246743768453598, KL divergence=0.025799861177802086, Entropy=0.16193397343158722, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/548_Step-21221.ckpt']
Uploaded 3 files for checkpoint 548 in 0.55 seconds
saved intermediate frozen graph: current/model/model_548.pb
Best checkpoint number: 545, Last checkpoint number: 546
Copying the frozen checkpoint from ./frozen_models/agent/model_545.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'542'}
Training> Name=main_level/agent, Worker=0, Episode=121, Total reward=481.47, Steps=21521, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=122, Total reward=519.3, Steps=21805, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=123, Total reward=197.64, Steps=21945, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=124, Total reward=488.21, Steps=22233, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=125, Total reward=481.07, Steps=22533, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=126, Total reward=268.05, Steps=22685, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=127, Total reward=444.52, Steps=22994, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=128, Total reward=460.96, Steps=23275, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=129, Total reward=154.84, Steps=23371, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=130, Total reward=461.51, Steps=23664, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=131, Total reward=127.0, Steps=23747, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=132, Total reward=445.3, Steps=24049, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=133, Total reward=488.02, Steps=24352, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=134, Total reward=81.14, Steps=24409, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=135, Total reward=20.03, Steps=24422, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=136, Total reward=210.23, Steps=24541, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=137, Total reward=21.58, Steps=24591, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=138, Total reward=22.95, Steps=24606, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=139, Total reward=482.02, Steps=24894, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=140, Total reward=77.86, Steps=24972, Training iteration=6
Policy training> Surrogate loss=-0.006223458331078291, KL divergence=0.0005152906524017453, Entropy=0.17030712962150574, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014232204295694828, KL divergence=0.006035487167537212, Entropy=0.16898401081562042, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.016882168129086494, KL divergence=0.010971701703965664, Entropy=0.16652750968933105, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.01790025644004345, KL divergence=0.01420742180198431, Entropy=0.1661182940006256, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02313305251300335, KL divergence=0.015960711985826492, Entropy=0.16509604454040527, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026588501408696175, KL divergence=0.017254645004868507, Entropy=0.16457943618297577, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.025161083787679672, KL divergence=0.01885548233985901, Entropy=0.1662123203277588, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.028762025758624077, KL divergence=0.01939503848552704, Entropy=0.1642705351114273, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.021445993334054947, KL divergence=0.020495722070336342, Entropy=0.1642085462808609, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.027526313439011574, KL divergence=0.021449686959385872, Entropy=0.16416239738464355, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/549_Step-24972.ckpt']
Uploaded 3 files for checkpoint 549 in 0.66 seconds
saved intermediate frozen graph: current/model/model_549.pb
Best checkpoint number: 545, Last checkpoint number: 547
Copying the frozen checkpoint from ./frozen_models/agent/model_545.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'546'}
Training> Name=main_level/agent, Worker=0, Episode=141, Total reward=307.51, Steps=25189, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=142, Total reward=54.08, Steps=25234, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=143, Total reward=142.98, Steps=25380, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=144, Total reward=213.07, Steps=25504, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=145, Total reward=512.16, Steps=25784, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=146, Total reward=91.43, Steps=25884, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=147, Total reward=219.79, Steps=26012, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=148, Total reward=193.89, Steps=26134, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=149, Total reward=20.01, Steps=26169, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=150, Total reward=405.02, Steps=26369, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=151, Total reward=480.96, Steps=26670, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=152, Total reward=472.42, Steps=26977, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=153, Total reward=92.98, Steps=27037, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=154, Total reward=449.27, Steps=27339, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=155, Total reward=154.33, Steps=27426, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=156, Total reward=127.33, Steps=27495, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=157, Total reward=288.69, Steps=27690, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=158, Total reward=355.99, Steps=27954, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=159, Total reward=124.64, Steps=28049, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=160, Total reward=95.65, Steps=28146, Training iteration=7
Policy training> Surrogate loss=0.0009673874010331929, KL divergence=0.0005301778437569737, Entropy=0.18234771490097046, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.018850956112146378, KL divergence=0.008643255569040775, Entropy=0.18017689883708954, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02636127360165119, KL divergence=0.015525758266448975, Entropy=0.17655056715011597, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.030075982213020325, KL divergence=0.01893438957631588, Entropy=0.1756848841905594, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03245720639824867, KL divergence=0.02208324335515499, Entropy=0.1734507828950882, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.034672874957323074, KL divergence=0.023773513734340668, Entropy=0.1739448755979538, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03471976891160011, KL divergence=0.0247518178075552, Entropy=0.17256470024585724, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03749019280076027, KL divergence=0.026385536417365074, Entropy=0.17133791744709015, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.04175558686256409, KL divergence=0.028864428400993347, Entropy=0.1726902723312378, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.04261781647801399, KL divergence=0.030074022710323334, Entropy=0.1734534502029419, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/550_Step-28146.ckpt']
Uploaded 3 files for checkpoint 550 in 0.53 seconds
saved intermediate frozen graph: current/model/model_550.pb
Best checkpoint number: 548, Last checkpoint number: 548
Copying the frozen checkpoint from ./frozen_models/agent/model_548.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'547'}
Training> Name=main_level/agent, Worker=0, Episode=161, Total reward=75.66, Steps=28200, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=162, Total reward=231.76, Steps=28422, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=163, Total reward=234.13, Steps=28606, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=164, Total reward=124.63, Steps=28701, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=165, Total reward=439.95, Steps=29014, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=166, Total reward=399.49, Steps=29268, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=167, Total reward=200.0, Steps=29374, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=168, Total reward=487.04, Steps=29664, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=169, Total reward=450.1, Steps=29950, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=170, Total reward=119.46, Steps=30045, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=171, Total reward=7.63, Steps=30060, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=172, Total reward=56.45, Steps=30086, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=173, Total reward=36.25, Steps=30098, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=174, Total reward=208.24, Steps=30195, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=175, Total reward=30.73, Steps=30211, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=176, Total reward=415.72, Steps=30440, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=177, Total reward=21.4, Steps=30481, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=178, Total reward=502.72, Steps=30778, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=179, Total reward=448.68, Steps=31068, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=180, Total reward=77.13, Steps=31121, Training iteration=8
Policy training> Surrogate loss=-0.00785023346543312, KL divergence=0.0003967740631196648, Entropy=0.17712147533893585, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.016994765028357506, KL divergence=0.00654504494741559, Entropy=0.17444950342178345, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.028727106750011444, KL divergence=0.01241714134812355, Entropy=0.17019043862819672, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02400335483253002, KL divergence=0.015960073098540306, Entropy=0.1700064092874527, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.017685476690530777, KL divergence=0.017980437725782394, Entropy=0.16886140406131744, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03391323983669281, KL divergence=0.020762573927640915, Entropy=0.16923139989376068, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02641637995839119, KL divergence=0.022015677765011787, Entropy=0.16946615278720856, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03146347030997276, KL divergence=0.023708941414952278, Entropy=0.1680109053850174, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02837955765426159, KL divergence=0.02506423555314541, Entropy=0.16925552487373352, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.027262525632977486, KL divergence=0.025957224890589714, Entropy=0.1693658083677292, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/551_Step-31121.ckpt']
Uploaded 3 files for checkpoint 551 in 0.50 seconds
saved intermediate frozen graph: current/model/model_551.pb
Best checkpoint number: 548, Last checkpoint number: 549
Copying the frozen checkpoint from ./frozen_models/agent/model_548.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'545'}
Training> Name=main_level/agent, Worker=0, Episode=181, Total reward=107.89, Steps=31220, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=182, Total reward=63.26, Steps=31259, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=183, Total reward=451.21, Steps=31546, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=184, Total reward=357.98, Steps=31784, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=185, Total reward=9.82, Steps=31825, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=186, Total reward=460.45, Steps=32080, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=187, Total reward=251.41, Steps=32235, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=188, Total reward=222.05, Steps=32369, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=189, Total reward=18.32, Steps=32387, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=190, Total reward=112.81, Steps=32463, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=191, Total reward=231.47, Steps=32638, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=192, Total reward=13.19, Steps=32651, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=193, Total reward=34.2, Steps=32663, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=194, Total reward=495.11, Steps=32962, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=195, Total reward=238.07, Steps=33125, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=196, Total reward=493.74, Steps=33405, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=197, Total reward=141.78, Steps=33501, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=198, Total reward=481.59, Steps=33800, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=199, Total reward=135.86, Steps=33902, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=200, Total reward=496.45, Steps=34187, Training iteration=9
Policy training> Surrogate loss=0.00629785843193531, KL divergence=0.0002491191844455898, Entropy=0.17904192209243774, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.021068362519145012, KL divergence=0.00690855085849762, Entropy=0.17905014753341675, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.030268341302871704, KL divergence=0.013892928138375282, Entropy=0.17378157377243042, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.017962081357836723, KL divergence=0.018961461260914803, Entropy=0.17521025240421295, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029866693541407585, KL divergence=0.021384883671998978, Entropy=0.17298907041549683, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03927711024880409, KL divergence=0.023347707465291023, Entropy=0.17238609492778778, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.042627155780792236, KL divergence=0.024840474128723145, Entropy=0.17317534983158112, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03839658200740814, KL divergence=0.027056260034441948, Entropy=0.17565949261188507, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.036712877452373505, KL divergence=0.026619993150234222, Entropy=0.1745327115058899, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03906683251261711, KL divergence=0.029372578486800194, Entropy=0.1740805059671402, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/552_Step-34187.ckpt']
Uploaded 3 files for checkpoint 552 in 0.59 seconds
saved intermediate frozen graph: current/model/model_552.pb
Best checkpoint number: 548, Last checkpoint number: 550
Copying the frozen checkpoint from ./frozen_models/agent/model_548.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'549'}
Training> Name=main_level/agent, Worker=0, Episode=201, Total reward=407.28, Steps=34440, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=202, Total reward=502.95, Steps=34724, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=203, Total reward=446.31, Steps=35015, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=204, Total reward=185.06, Steps=35156, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=205, Total reward=163.48, Steps=35322, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=206, Total reward=488.73, Steps=35607, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=207, Total reward=407.56, Steps=35908, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=208, Total reward=511.79, Steps=36202, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=209, Total reward=122.01, Steps=36312, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=210, Total reward=295.54, Steps=36469, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=211, Total reward=508.57, Steps=36774, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=212, Total reward=426.8, Steps=37082, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=213, Total reward=66.73, Steps=37126, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=214, Total reward=405.16, Steps=37427, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=215, Total reward=105.78, Steps=37499, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=216, Total reward=128.59, Steps=37573, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=217, Total reward=274.21, Steps=37777, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=218, Total reward=82.48, Steps=37824, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=219, Total reward=268.34, Steps=38024, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=220, Total reward=309.58, Steps=38264, Training iteration=10
Policy training> Surrogate loss=-0.0018315230263397098, KL divergence=0.0003420671564526856, Entropy=0.17601607739925385, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.018070226535201073, KL divergence=0.007980692200362682, Entropy=0.1719934344291687, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02837431989610195, KL divergence=0.014209901914000511, Entropy=0.16927357017993927, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.024124229326844215, KL divergence=0.018201161175966263, Entropy=0.17152522504329681, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02813158743083477, KL divergence=0.02053508535027504, Entropy=0.16840335726737976, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.024533580988645554, KL divergence=0.021230706945061684, Entropy=0.16756190359592438, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.031795646995306015, KL divergence=0.022391879931092262, Entropy=0.1671798676252365, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.031921084970235825, KL divergence=0.023103022947907448, Entropy=0.16629967093467712, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.035875000059604645, KL divergence=0.024875370785593987, Entropy=0.16879956424236298, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.029694173485040665, KL divergence=0.0247589573264122, Entropy=0.16597841680049896, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/553_Step-38264.ckpt']
Uploaded 3 files for checkpoint 553 in 0.57 seconds
saved intermediate frozen graph: current/model/model_553.pb
Best checkpoint number: 548, Last checkpoint number: 551
Copying the frozen checkpoint from ./frozen_models/agent/model_548.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'550'}
Training> Name=main_level/agent, Worker=0, Episode=221, Total reward=468.94, Steps=38535, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=222, Total reward=415.53, Steps=38796, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=223, Total reward=233.03, Steps=38968, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=224, Total reward=466.78, Steps=39258, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=225, Total reward=477.84, Steps=39557, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=226, Total reward=451.18, Steps=39867, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=227, Total reward=100.97, Steps=39923, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=228, Total reward=189.28, Steps=39997, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=229, Total reward=504.6, Steps=40277, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=230, Total reward=482.86, Steps=40576, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=231, Total reward=483.29, Steps=40854, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=232, Total reward=273.19, Steps=41002, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=233, Total reward=306.62, Steps=41159, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=234, Total reward=64.22, Steps=41205, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=235, Total reward=208.79, Steps=41339, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=236, Total reward=480.58, Steps=41642, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=237, Total reward=91.42, Steps=41699, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=238, Total reward=293.32, Steps=41944, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=239, Total reward=426.0, Steps=42225, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=240, Total reward=453.27, Steps=42538, Training iteration=11
Policy training> Surrogate loss=0.0010738589335232973, KL divergence=0.0006992312846705317, Entropy=0.18243414163589478, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01835143379867077, KL divergence=0.007568519562482834, Entropy=0.17806106805801392, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.019634928554296494, KL divergence=0.012924626469612122, Entropy=0.17647983133792877, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.029488766565918922, KL divergence=0.015632962808012962, Entropy=0.17335042357444763, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026942463591694832, KL divergence=0.0170569010078907, Entropy=0.1743781417608261, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03346498683094978, KL divergence=0.018734384328126907, Entropy=0.17373573780059814, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.028957093134522438, KL divergence=0.019555265083909035, Entropy=0.17338940501213074, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03332198038697243, KL divergence=0.020496942102909088, Entropy=0.17104315757751465, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03023756667971611, KL divergence=0.02180738002061844, Entropy=0.17360518872737885, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03362685814499855, KL divergence=0.022484412416815758, Entropy=0.17245933413505554, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/554_Step-42538.ckpt']
Uploaded 3 files for checkpoint 554 in 0.54 seconds
saved intermediate frozen graph: current/model/model_554.pb
Best checkpoint number: 548, Last checkpoint number: 552
Copying the frozen checkpoint from ./frozen_models/agent/model_548.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'551'}
Training> Name=main_level/agent, Worker=0, Episode=241, Total reward=260.4, Steps=42709, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=242, Total reward=285.84, Steps=42941, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=243, Total reward=343.11, Steps=43210, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=244, Total reward=2.45, Steps=43237, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=245, Total reward=422.82, Steps=43531, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=246, Total reward=87.45, Steps=43584, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=247, Total reward=187.1, Steps=43704, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=248, Total reward=94.99, Steps=43744, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=249, Total reward=512.71, Steps=44029, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=250, Total reward=120.01, Steps=44135, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=251, Total reward=273.4, Steps=44272, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=252, Total reward=97.9, Steps=44315, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=253, Total reward=39.07, Steps=44361, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=254, Total reward=384.38, Steps=44663, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=255, Total reward=440.74, Steps=44953, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=256, Total reward=204.97, Steps=45147, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=257, Total reward=473.35, Steps=45429, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=258, Total reward=528.18, Steps=45714, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=259, Total reward=340.44, Steps=45941, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=260, Total reward=99.64, Steps=46008, Training iteration=12
Policy training> Surrogate loss=-0.00012394756777212024, KL divergence=0.000550361059140414, Entropy=0.16571584343910217, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01510907057672739, KL divergence=0.010893499478697777, Entropy=0.16221077740192413, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.023399077355861664, KL divergence=0.017964936792850494, Entropy=0.16065941751003265, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025027047842741013, KL divergence=0.022734016180038452, Entropy=0.1603476107120514, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.022714965045452118, KL divergence=0.025335513055324554, Entropy=0.15893331170082092, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.032579995691776276, KL divergence=0.026156051084399223, Entropy=0.15757419168949127, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03483283147215843, KL divergence=0.02639523148536682, Entropy=0.1551688313484192, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03462729603052139, KL divergence=0.02832375094294548, Entropy=0.1572323441505432, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02697809226810932, KL divergence=0.029533151537179947, Entropy=0.15684355795383453, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02970098704099655, KL divergence=0.03002104163169861, Entropy=0.1562887728214264, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/555_Step-46008.ckpt']
Uploaded 3 files for checkpoint 555 in 0.53 seconds
saved intermediate frozen graph: current/model/model_555.pb
Best checkpoint number: 548, Last checkpoint number: 553
Copying the frozen checkpoint from ./frozen_models/agent/model_548.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'552'}
Training> Name=main_level/agent, Worker=0, Episode=261, Total reward=437.94, Steps=46294, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=262, Total reward=290.03, Steps=46486, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=263, Total reward=145.01, Steps=46607, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=264, Total reward=4.78, Steps=46635, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=265, Total reward=212.8, Steps=46784, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=266, Total reward=480.76, Steps=47084, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=267, Total reward=485.62, Steps=47369, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=268, Total reward=495.59, Steps=47669, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=269, Total reward=308.2, Steps=47840, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=270, Total reward=444.78, Steps=48141, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=271, Total reward=71.27, Steps=48189, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=272, Total reward=98.28, Steps=48245, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=273, Total reward=34.1, Steps=48257, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=274, Total reward=442.19, Steps=48563, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=275, Total reward=210.98, Steps=48691, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=276, Total reward=502.9, Steps=48987, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=277, Total reward=466.65, Steps=49291, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=278, Total reward=439.3, Steps=49589, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=279, Total reward=474.01, Steps=49894, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=280, Total reward=353.05, Steps=50145, Training iteration=13
Policy training> Surrogate loss=0.0005083139985799789, KL divergence=0.00045002359547652304, Entropy=0.16981381177902222, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01554135512560606, KL divergence=0.009184874594211578, Entropy=0.16746068000793457, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.019269336014986038, KL divergence=0.016023511067032814, Entropy=0.16665998101234436, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025206224992871284, KL divergence=0.01920187845826149, Entropy=0.16539061069488525, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.025267237797379494, KL divergence=0.021213117986917496, Entropy=0.16435307264328003, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026582028716802597, KL divergence=0.022523418068885803, Entropy=0.16347771883010864, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029452774673700333, KL divergence=0.022885825484991074, Entropy=0.16251033544540405, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02785128355026245, KL divergence=0.02456425502896309, Entropy=0.16249513626098633, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02766593173146248, KL divergence=0.025224503129720688, Entropy=0.16121859848499298, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03221661597490311, KL divergence=0.02571200579404831, Entropy=0.16129642724990845, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/556_Step-50145.ckpt']
Uploaded 3 files for checkpoint 556 in 0.52 seconds
saved intermediate frozen graph: current/model/model_556.pb
Best checkpoint number: 548, Last checkpoint number: 554
Copying the frozen checkpoint from ./frozen_models/agent/model_548.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'553'}
Training> Name=main_level/agent, Worker=0, Episode=281, Total reward=83.37, Steps=50229, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=282, Total reward=35.91, Steps=50270, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=283, Total reward=416.17, Steps=50569, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=284, Total reward=165.66, Steps=50706, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=285, Total reward=251.95, Steps=50891, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=286, Total reward=244.75, Steps=51060, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=287, Total reward=139.1, Steps=51182, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=288, Total reward=345.02, Steps=51373, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=289, Total reward=439.66, Steps=51662, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=290, Total reward=31.53, Steps=51700, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=291, Total reward=241.97, Steps=51852, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=292, Total reward=493.54, Steps=52149, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=293, Total reward=403.07, Steps=52447, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=294, Total reward=426.77, Steps=52775, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=295, Total reward=476.47, Steps=53066, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=296, Total reward=475.43, Steps=53364, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=297, Total reward=230.42, Steps=53566, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=298, Total reward=401.84, Steps=53805, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=299, Total reward=203.36, Steps=54002, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=300, Total reward=432.47, Steps=54304, Training iteration=14
Policy training> Surrogate loss=0.0022105996031314135, KL divergence=0.0005930785555392504, Entropy=0.17488530278205872, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.018170075491070747, KL divergence=0.011134124360978603, Entropy=0.17049890756607056, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020666340366005898, KL divergence=0.016866806894540787, Entropy=0.16833841800689697, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.019117457792162895, KL divergence=0.019294165074825287, Entropy=0.16716371476650238, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.023671254515647888, KL divergence=0.021419707685709, Entropy=0.16697706282138824, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02381690964102745, KL divergence=0.02320541813969612, Entropy=0.1664707213640213, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.028365358710289, KL divergence=0.02399466000497341, Entropy=0.16513793170452118, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.029761750251054764, KL divergence=0.024928418919444084, Entropy=0.16437643766403198, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03101191483438015, KL divergence=0.02548546902835369, Entropy=0.1640632450580597, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.035586267709732056, KL divergence=0.026513375341892242, Entropy=0.16471600532531738, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/557_Step-54304.ckpt']
Uploaded 3 files for checkpoint 557 in 0.55 seconds
saved intermediate frozen graph: current/model/model_557.pb
Best checkpoint number: 548, Last checkpoint number: 555
Copying the frozen checkpoint from ./frozen_models/agent/model_548.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'554'}
Training> Name=main_level/agent, Worker=0, Episode=301, Total reward=106.63, Steps=54385, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=302, Total reward=459.81, Steps=54689, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=303, Total reward=20.08, Steps=54737, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=304, Total reward=156.45, Steps=54875, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=305, Total reward=482.54, Steps=55167, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=306, Total reward=153.54, Steps=55293, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=307, Total reward=204.56, Steps=55416, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=308, Total reward=461.53, Steps=55712, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=309, Total reward=447.07, Steps=56017, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=310, Total reward=485.83, Steps=56299, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=311, Total reward=131.53, Steps=56373, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=312, Total reward=31.77, Steps=56415, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=313, Total reward=137.71, Steps=56480, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=314, Total reward=473.95, Steps=56769, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=315, Total reward=501.09, Steps=57086, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=316, Total reward=27.98, Steps=57106, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=317, Total reward=45.7, Steps=57152, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=318, Total reward=477.81, Steps=57430, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=319, Total reward=508.76, Steps=57707, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=320, Total reward=100.86, Steps=57786, Training iteration=15
Policy training> Surrogate loss=0.005393679253757, KL divergence=0.0003565609804354608, Entropy=0.16842171549797058, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01611688919365406, KL divergence=0.007417313288897276, Entropy=0.16459470987319946, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02355816215276718, KL divergence=0.01321600005030632, Entropy=0.1624995768070221, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.022956548258662224, KL divergence=0.017042703926563263, Entropy=0.16313600540161133, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.021905729547142982, KL divergence=0.019163835793733597, Entropy=0.16089479625225067, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030819641426205635, KL divergence=0.022014884278178215, Entropy=0.16106639802455902, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03366200625896454, KL divergence=0.024141855537891388, Entropy=0.1578799933195114, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02790161781013012, KL divergence=0.025537269189953804, Entropy=0.15746751427650452, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.0402364544570446, KL divergence=0.026835283264517784, Entropy=0.1584116518497467, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.031127022579312325, KL divergence=0.027705928310751915, Entropy=0.15559148788452148, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/558_Step-57786.ckpt']
Uploaded 3 files for checkpoint 558 in 0.60 seconds
saved intermediate frozen graph: current/model/model_558.pb
Best checkpoint number: 548, Last checkpoint number: 556
Copying the frozen checkpoint from ./frozen_models/agent/model_548.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'555'}
Training> Name=main_level/agent, Worker=0, Episode=321, Total reward=436.71, Steps=58073, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=322, Total reward=520.92, Steps=58366, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=323, Total reward=165.19, Steps=58518, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=324, Total reward=206.12, Steps=58644, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=325, Total reward=432.28, Steps=58936, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=326, Total reward=450.6, Steps=59236, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=327, Total reward=307.83, Steps=59417, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=328, Total reward=437.55, Steps=59687, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=329, Total reward=473.54, Steps=60001, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=330, Total reward=94.28, Steps=60069, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=331, Total reward=472.32, Steps=60351, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=332, Total reward=50.17, Steps=60378, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=333, Total reward=468.55, Steps=60657, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=334, Total reward=485.16, Steps=60940, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=335, Total reward=348.04, Steps=61244, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=336, Total reward=311.92, Steps=61463, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=337, Total reward=98.23, Steps=61512, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=338, Total reward=472.76, Steps=61817, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=339, Total reward=493.17, Steps=62098, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=340, Total reward=337.71, Steps=62354, Training iteration=16
Policy training> Surrogate loss=0.0008180891163647175, KL divergence=0.0005987468175590038, Entropy=0.16698451340198517, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.02062951773405075, KL divergence=0.009739500470459461, Entropy=0.1623876839876175, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.013083267025649548, KL divergence=0.014888267032802105, Entropy=0.16334503889083862, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02736538089811802, KL divergence=0.017861617729067802, Entropy=0.16114524006843567, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02603885717689991, KL divergence=0.019505169242620468, Entropy=0.16065633296966553, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.021669354289770126, KL divergence=0.02079027146100998, Entropy=0.16149413585662842, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.026626700535416603, KL divergence=0.02224082313477993, Entropy=0.1620006412267685, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03365551307797432, KL divergence=0.023227766156196594, Entropy=0.16126902401447296, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.030131617560982704, KL divergence=0.023408755660057068, Entropy=0.15998534858226776, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.033693257719278336, KL divergence=0.02413829416036606, Entropy=0.160012885928154, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/559_Step-62354.ckpt']
Uploaded 3 files for checkpoint 559 in 0.50 seconds
saved intermediate frozen graph: current/model/model_559.pb
Best checkpoint number: 548, Last checkpoint number: 557
Copying the frozen checkpoint from ./frozen_models/agent/model_548.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'556'}
Training> Name=main_level/agent, Worker=0, Episode=341, Total reward=490.31, Steps=62648, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=342, Total reward=400.89, Steps=62976, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=343, Total reward=169.32, Steps=63158, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=344, Total reward=458.57, Steps=63467, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=345, Total reward=431.25, Steps=63784, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=346, Total reward=495.41, Steps=64060, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=347, Total reward=416.28, Steps=64361, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=348, Total reward=164.43, Steps=64480, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=349, Total reward=521.74, Steps=64780, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=350, Total reward=111.93, Steps=64867, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=351, Total reward=33.17, Steps=64894, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=352, Total reward=487.26, Steps=65172, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=353, Total reward=91.09, Steps=65213, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=354, Total reward=448.58, Steps=65520, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=355, Total reward=267.88, Steps=65669, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=356, Total reward=474.25, Steps=65967, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=357, Total reward=322.9, Steps=66211, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=358, Total reward=467.1, Steps=66522, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=359, Total reward=95.91, Steps=66600, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=360, Total reward=388.75, Steps=66849, Training iteration=17
Policy training> Surrogate loss=-0.003387833246961236, KL divergence=0.00044375882134772837, Entropy=0.16013361513614655, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014729456044733524, KL divergence=0.007743943948298693, Entropy=0.15859319269657135, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.01724853366613388, KL divergence=0.012575964443385601, Entropy=0.156459242105484, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.020757680758833885, KL divergence=0.014804093167185783, Entropy=0.15671122074127197, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02057182975113392, KL divergence=0.016241634264588356, Entropy=0.15391838550567627, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02526254579424858, KL divergence=0.01758703775703907, Entropy=0.15426413714885712, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.022802868857979774, KL divergence=0.018354475498199463, Entropy=0.15427158772945404, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.022945493459701538, KL divergence=0.01930866204202175, Entropy=0.1546562910079956, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.026150384917855263, KL divergence=0.020576372742652893, Entropy=0.15516430139541626, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02733856625854969, KL divergence=0.021671606227755547, Entropy=0.15336965024471283, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/560_Step-66849.ckpt']
Uploaded 3 files for checkpoint 560 in 0.63 seconds
saved intermediate frozen graph: current/model/model_560.pb
Best checkpoint number: 558, Last checkpoint number: 558
Copying the frozen checkpoint from ./frozen_models/agent/model_558.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'548'}
Training> Name=main_level/agent, Worker=0, Episode=361, Total reward=436.32, Steps=67146, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=362, Total reward=159.23, Steps=67281, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=363, Total reward=25.6, Steps=67327, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=364, Total reward=238.08, Steps=67480, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=365, Total reward=207.23, Steps=67644, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=366, Total reward=441.2, Steps=67941, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=367, Total reward=127.9, Steps=68042, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=368, Total reward=0.0, Steps=68043, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=369, Total reward=424.79, Steps=68340, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=370, Total reward=241.04, Steps=68507, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=371, Total reward=220.43, Steps=68642, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=372, Total reward=411.56, Steps=68937, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=373, Total reward=98.64, Steps=68980, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=374, Total reward=277.65, Steps=69153, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=375, Total reward=253.54, Steps=69308, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=376, Total reward=462.83, Steps=69587, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=377, Total reward=394.41, Steps=69871, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=378, Total reward=476.45, Steps=70166, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=379, Total reward=467.25, Steps=70460, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=380, Total reward=234.28, Steps=70695, Training iteration=18
Policy training> Surrogate loss=0.0004515190958045423, KL divergence=0.0007926921243779361, Entropy=0.1685759425163269, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014979627914726734, KL divergence=0.01191040500998497, Entropy=0.1660299301147461, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020188745111227036, KL divergence=0.016824400052428246, Entropy=0.1636238694190979, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02353724092245102, KL divergence=0.01915843039751053, Entropy=0.16217097640037537, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.024013947695493698, KL divergence=0.02088073268532753, Entropy=0.16082006692886353, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02544366754591465, KL divergence=0.022214990109205246, Entropy=0.16011056303977966, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.026309724897146225, KL divergence=0.023510340601205826, Entropy=0.159799724817276, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02735739015042782, KL divergence=0.024761436507105827, Entropy=0.15943478047847748, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.026523852720856667, KL divergence=0.025375455617904663, Entropy=0.15911392867565155, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.029159707948565483, KL divergence=0.026375003159046173, Entropy=0.15879091620445251, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/561_Step-70695.ckpt']
Uploaded 3 files for checkpoint 561 in 0.59 seconds
saved intermediate frozen graph: current/model/model_561.pb
Best checkpoint number: 559, Last checkpoint number: 559
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'557'}
Training> Name=main_level/agent, Worker=0, Episode=381, Total reward=40.46, Steps=70733, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=382, Total reward=71.28, Steps=70781, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=383, Total reward=445.29, Steps=71072, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=384, Total reward=217.85, Steps=71246, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=385, Total reward=473.64, Steps=71548, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=386, Total reward=219.78, Steps=71675, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=387, Total reward=432.93, Steps=71917, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=388, Total reward=181.99, Steps=71990, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=389, Total reward=33.13, Steps=72025, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=390, Total reward=113.39, Steps=72095, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=391, Total reward=9.54, Steps=72123, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=392, Total reward=102.74, Steps=72178, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=393, Total reward=204.18, Steps=72312, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=394, Total reward=355.87, Steps=72608, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=395, Total reward=468.31, Steps=72894, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=396, Total reward=229.3, Steps=73018, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=397, Total reward=284.99, Steps=73268, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=398, Total reward=459.82, Steps=73590, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=399, Total reward=465.94, Steps=73880, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=400, Total reward=7.09, Steps=73904, Training iteration=19
Policy training> Surrogate loss=0.005282287951558828, KL divergence=0.00029713462572544813, Entropy=0.1648903340101242, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.018259460106492043, KL divergence=0.008993086405098438, Entropy=0.16427452862262726, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02102930098772049, KL divergence=0.016103176400065422, Entropy=0.1613386869430542, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.029229087755084038, KL divergence=0.019874855875968933, Entropy=0.16351918876171112, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03512386605143547, KL divergence=0.02215016819536686, Entropy=0.16190804541110992, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.025002295151352882, KL divergence=0.023457279428839684, Entropy=0.16157002747058868, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.035081420093774796, KL divergence=0.02486908994615078, Entropy=0.15937918424606323, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03476284071803093, KL divergence=0.025310257449746132, Entropy=0.1590917408466339, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03199326619505882, KL divergence=0.027084259316325188, Entropy=0.15876851975917816, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.036507364362478256, KL divergence=0.027506614103913307, Entropy=0.1596267968416214, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/562_Step-73904.ckpt']
Uploaded 3 files for checkpoint 562 in 0.55 seconds
saved intermediate frozen graph: current/model/model_562.pb
Best checkpoint number: 559, Last checkpoint number: 560
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'558'}
Training> Name=main_level/agent, Worker=0, Episode=401, Total reward=251.65, Steps=74117, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=402, Total reward=70.16, Steps=74203, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=403, Total reward=523.16, Steps=74489, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=404, Total reward=505.07, Steps=74785, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=405, Total reward=416.24, Steps=75102, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=406, Total reward=464.16, Steps=75379, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=407, Total reward=497.45, Steps=75671, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=408, Total reward=389.96, Steps=75857, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=409, Total reward=80.23, Steps=75905, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=410, Total reward=20.14, Steps=75953, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=411, Total reward=103.71, Steps=76021, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=412, Total reward=24.15, Steps=76055, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=413, Total reward=405.02, Steps=76360, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=414, Total reward=463.45, Steps=76657, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=415, Total reward=161.73, Steps=76742, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=416, Total reward=141.0, Steps=76813, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=417, Total reward=58.93, Steps=76879, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=418, Total reward=414.01, Steps=77135, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=419, Total reward=315.62, Steps=77329, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=420, Total reward=444.54, Steps=77639, Training iteration=20
Policy training> Surrogate loss=0.001599439070560038, KL divergence=0.00035616333479993045, Entropy=0.165940523147583, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014386392198503017, KL divergence=0.008326838724315166, Entropy=0.1645077019929886, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.027751924470067024, KL divergence=0.01560641173273325, Entropy=0.16132591664791107, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.028720621019601822, KL divergence=0.01956278644502163, Entropy=0.16061149537563324, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.031369563192129135, KL divergence=0.02127593383193016, Entropy=0.16018788516521454, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026156818494200706, KL divergence=0.02298269234597683, Entropy=0.15932905673980713, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.033347390592098236, KL divergence=0.02408480830490589, Entropy=0.15931537747383118, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03562002256512642, KL divergence=0.025706898421049118, Entropy=0.15849296748638153, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.036791376769542694, KL divergence=0.02647414803504944, Entropy=0.1583320051431656, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03838431090116501, KL divergence=0.027537068352103233, Entropy=0.15820808708667755, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/563_Step-77639.ckpt']
Uploaded 3 files for checkpoint 563 in 0.61 seconds
saved intermediate frozen graph: current/model/model_563.pb
Best checkpoint number: 559, Last checkpoint number: 561
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'560'}
Training> Name=main_level/agent, Worker=0, Episode=421, Total reward=484.91, Steps=77950, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=422, Total reward=541.1, Steps=78227, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=423, Total reward=478.96, Steps=78526, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=424, Total reward=474.7, Steps=78799, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=425, Total reward=96.61, Steps=78889, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=426, Total reward=459.99, Steps=79175, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=427, Total reward=85.47, Steps=79230, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=428, Total reward=385.51, Steps=79414, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=429, Total reward=284.39, Steps=79584, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=430, Total reward=382.91, Steps=79842, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=431, Total reward=468.74, Steps=80131, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=432, Total reward=339.14, Steps=80322, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=433, Total reward=130.72, Steps=80385, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=434, Total reward=311.77, Steps=80580, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=435, Total reward=380.66, Steps=80878, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=436, Total reward=13.74, Steps=80891, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=437, Total reward=463.3, Steps=81186, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=438, Total reward=175.26, Steps=81324, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=439, Total reward=480.59, Steps=81621, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=440, Total reward=448.05, Steps=81916, Training iteration=21
Policy training> Surrogate loss=-0.0010445707011967897, KL divergence=0.000886455352883786, Entropy=0.16668234765529633, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01568404585123062, KL divergence=0.010891260579228401, Entropy=0.16260676085948944, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.021159453317523003, KL divergence=0.017169231548905373, Entropy=0.16256862878799438, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.024769101291894913, KL divergence=0.018656495958566666, Entropy=0.16043084859848022, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.021754028275609016, KL divergence=0.019528910517692566, Entropy=0.16197657585144043, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026339495554566383, KL divergence=0.02110900729894638, Entropy=0.16158252954483032, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03319341689348221, KL divergence=0.021335769444704056, Entropy=0.16051197052001953, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02490101382136345, KL divergence=0.023186257109045982, Entropy=0.161221444606781, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.024703459814190865, KL divergence=0.02396928332746029, Entropy=0.15849822759628296, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.027019094675779343, KL divergence=0.024337057024240494, Entropy=0.15983259677886963, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/564_Step-81916.ckpt']
Uploaded 3 files for checkpoint 564 in 0.57 seconds
saved intermediate frozen graph: current/model/model_564.pb
Best checkpoint number: 559, Last checkpoint number: 562
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'561'}
Training> Name=main_level/agent, Worker=0, Episode=441, Total reward=384.45, Steps=82185, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=442, Total reward=48.05, Steps=82209, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=443, Total reward=450.97, Steps=82527, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=444, Total reward=501.24, Steps=82816, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=445, Total reward=392.97, Steps=83126, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=446, Total reward=40.54, Steps=83181, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=447, Total reward=3.14, Steps=83195, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=448, Total reward=259.22, Steps=83309, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=449, Total reward=439.97, Steps=83603, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=450, Total reward=48.57, Steps=83672, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=451, Total reward=70.78, Steps=83715, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=452, Total reward=373.27, Steps=83905, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=453, Total reward=69.75, Steps=83947, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=454, Total reward=30.1, Steps=83959, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=455, Total reward=82.78, Steps=84012, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=456, Total reward=518.86, Steps=84292, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=457, Total reward=431.59, Steps=84598, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=458, Total reward=455.33, Steps=84884, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=459, Total reward=69.59, Steps=84970, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=460, Total reward=495.46, Steps=85267, Training iteration=22
Policy training> Surrogate loss=-0.0017783618532121181, KL divergence=0.0003753512864932418, Entropy=0.16654513776302338, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01669335551559925, KL divergence=0.008426289074122906, Entropy=0.16238965094089508, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02280128188431263, KL divergence=0.0145949712023139, Entropy=0.1610965132713318, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025508621707558632, KL divergence=0.018778987228870392, Entropy=0.1593634933233261, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02640630677342415, KL divergence=0.021554963663220406, Entropy=0.1578090339899063, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03077242150902748, KL divergence=0.023257214576005936, Entropy=0.1574503481388092, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029947837814688683, KL divergence=0.025005271658301353, Entropy=0.1574486792087555, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.029835151508450508, KL divergence=0.026368191465735435, Entropy=0.1571151316165924, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03140528127551079, KL divergence=0.027759093791246414, Entropy=0.15663442015647888, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03379813954234123, KL divergence=0.028630852699279785, Entropy=0.15524612367153168, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/565_Step-85267.ckpt']
Uploaded 3 files for checkpoint 565 in 0.51 seconds
saved intermediate frozen graph: current/model/model_565.pb
Best checkpoint number: 559, Last checkpoint number: 563
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'562'}
Training> Name=main_level/agent, Worker=0, Episode=461, Total reward=419.21, Steps=85550, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=462, Total reward=430.36, Steps=85834, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=463, Total reward=141.32, Steps=86042, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=464, Total reward=435.87, Steps=86329, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=465, Total reward=180.4, Steps=86452, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=466, Total reward=79.52, Steps=86533, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=467, Total reward=511.65, Steps=86831, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=468, Total reward=238.45, Steps=86935, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=469, Total reward=457.21, Steps=87253, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=470, Total reward=93.53, Steps=87343, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=471, Total reward=283.26, Steps=87485, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=472, Total reward=244.86, Steps=87657, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=473, Total reward=251.87, Steps=87792, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=474, Total reward=134.91, Steps=87856, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=475, Total reward=496.81, Steps=88140, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=476, Total reward=393.69, Steps=88384, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=477, Total reward=252.39, Steps=88626, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=478, Total reward=473.28, Steps=88925, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=479, Total reward=489.0, Steps=89205, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=480, Total reward=408.07, Steps=89500, Training iteration=23
Policy training> Surrogate loss=0.0014040116220712662, KL divergence=0.0008542288560420275, Entropy=0.17073550820350647, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01687164232134819, KL divergence=0.009689806960523129, Entropy=0.16784146428108215, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020326636731624603, KL divergence=0.016083795577287674, Entropy=0.16492480039596558, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.024254504591226578, KL divergence=0.018914632499217987, Entropy=0.16646379232406616, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0257632527500391, KL divergence=0.021136730909347534, Entropy=0.16416725516319275, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.028571996837854385, KL divergence=0.022419434040784836, Entropy=0.1633986234664917, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03045656532049179, KL divergence=0.024399448186159134, Entropy=0.16294004023075104, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03281571343541145, KL divergence=0.02615910768508911, Entropy=0.16401676833629608, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.0300147607922554, KL divergence=0.02751700021326542, Entropy=0.162953183054924, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.033888839185237885, KL divergence=0.02911273017525673, Entropy=0.16367819905281067, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/566_Step-89500.ckpt']
Uploaded 3 files for checkpoint 566 in 0.47 seconds
saved intermediate frozen graph: current/model/model_566.pb
Best checkpoint number: 559, Last checkpoint number: 564
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'563'}
Training> Name=main_level/agent, Worker=0, Episode=481, Total reward=105.67, Steps=89608, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=482, Total reward=297.94, Steps=89833, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=483, Total reward=202.27, Steps=89991, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=484, Total reward=482.65, Steps=90291, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=485, Total reward=430.27, Steps=90576, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=486, Total reward=263.34, Steps=90723, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=487, Total reward=121.52, Steps=90808, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=488, Total reward=354.53, Steps=91005, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=489, Total reward=103.73, Steps=91111, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=490, Total reward=438.42, Steps=91442, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=491, Total reward=544.28, Steps=91725, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=492, Total reward=517.09, Steps=92009, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=493, Total reward=93.43, Steps=92050, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=494, Total reward=487.1, Steps=92345, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=495, Total reward=254.39, Steps=92471, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=496, Total reward=503.33, Steps=92757, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=497, Total reward=430.27, Steps=93046, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=498, Total reward=403.12, Steps=93332, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=499, Total reward=82.56, Steps=93394, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=500, Total reward=475.52, Steps=93695, Training iteration=24
Policy training> Surrogate loss=0.0006326483562588692, KL divergence=0.0005115261301398277, Entropy=0.16844598948955536, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01970638893544674, KL divergence=0.008627660572528839, Entropy=0.16588319838047028, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.026424337178468704, KL divergence=0.013577252626419067, Entropy=0.16301026940345764, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.0235427375882864, KL divergence=0.01673441007733345, Entropy=0.16208454966545105, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026075880974531174, KL divergence=0.019179020076990128, Entropy=0.160364031791687, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026057112962007523, KL divergence=0.020937979221343994, Entropy=0.15944799780845642, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029933642596006393, KL divergence=0.022313706576824188, Entropy=0.15795443952083588, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03223800286650658, KL divergence=0.023157792165875435, Entropy=0.15801557898521423, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03313019871711731, KL divergence=0.024202894419431686, Entropy=0.15730081498622894, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03534179553389549, KL divergence=0.024845603853464127, Entropy=0.15587842464447021, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/567_Step-93695.ckpt']
Uploaded 3 files for checkpoint 567 in 0.61 seconds
saved intermediate frozen graph: current/model/model_567.pb
Best checkpoint number: 559, Last checkpoint number: 565
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'564'}
Training> Name=main_level/agent, Worker=0, Episode=501, Total reward=466.31, Steps=93980, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=502, Total reward=476.75, Steps=94269, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=503, Total reward=302.53, Steps=94543, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=504, Total reward=118.03, Steps=94641, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=505, Total reward=167.54, Steps=94770, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=506, Total reward=358.7, Steps=94983, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=507, Total reward=78.31, Steps=95034, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=508, Total reward=509.95, Steps=95347, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=509, Total reward=456.63, Steps=95649, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=510, Total reward=56.85, Steps=95703, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=511, Total reward=491.55, Steps=96009, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=512, Total reward=123.53, Steps=96066, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=513, Total reward=33.21, Steps=96077, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=514, Total reward=399.26, Steps=96384, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=515, Total reward=470.93, Steps=96665, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=516, Total reward=499.54, Steps=96944, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=517, Total reward=337.37, Steps=97187, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=518, Total reward=169.18, Steps=97270, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=519, Total reward=509.76, Steps=97555, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=520, Total reward=330.85, Steps=97808, Training iteration=25
Policy training> Surrogate loss=-8.494104258716106e-05, KL divergence=0.0008588003693148494, Entropy=0.16591912508010864, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.016186434775590897, KL divergence=0.01140680443495512, Entropy=0.1629028618335724, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02031567320227623, KL divergence=0.016051210463047028, Entropy=0.1604774296283722, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.021599579602479935, KL divergence=0.019070139154791832, Entropy=0.15793538093566895, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.024668777361512184, KL divergence=0.02090398781001568, Entropy=0.1567087471485138, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.025061219930648804, KL divergence=0.022062838077545166, Entropy=0.1567050814628601, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.027728212997317314, KL divergence=0.02307606115937233, Entropy=0.15649797022342682, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.026927422732114792, KL divergence=0.02414720319211483, Entropy=0.15593788027763367, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.027652274817228317, KL divergence=0.025097118690609932, Entropy=0.1559736579656601, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.027722250670194626, KL divergence=0.026055051013827324, Entropy=0.15566235780715942, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/568_Step-97808.ckpt']
Uploaded 3 files for checkpoint 568 in 0.54 seconds
saved intermediate frozen graph: current/model/model_568.pb
Best checkpoint number: 559, Last checkpoint number: 566
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'565'}
Training> Name=main_level/agent, Worker=0, Episode=521, Total reward=150.95, Steps=97950, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=522, Total reward=415.47, Steps=98228, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=523, Total reward=503.57, Steps=98511, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=524, Total reward=107.94, Steps=98662, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=525, Total reward=432.09, Steps=98956, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=526, Total reward=7.03, Steps=98977, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=527, Total reward=481.97, Steps=99288, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=528, Total reward=500.36, Steps=99576, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=529, Total reward=426.29, Steps=99829, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=530, Total reward=493.35, Steps=100116, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=531, Total reward=448.64, Steps=100419, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=532, Total reward=18.66, Steps=100432, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=533, Total reward=155.33, Steps=100538, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=534, Total reward=443.49, Steps=100847, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=535, Total reward=453.42, Steps=101140, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=536, Total reward=507.65, Steps=101425, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=537, Total reward=477.59, Steps=101715, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=538, Total reward=154.11, Steps=101815, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=539, Total reward=470.38, Steps=102108, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=540, Total reward=221.44, Steps=102321, Training iteration=26
Policy training> Surrogate loss=-0.002937580458819866, KL divergence=0.0007546053384430707, Entropy=0.17037464678287506, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.016720673069357872, KL divergence=0.0119845075532794, Entropy=0.16381916403770447, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02200593240559101, KL divergence=0.01767417974770069, Entropy=0.16373081505298615, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02365816943347454, KL divergence=0.020139602944254875, Entropy=0.16164258122444153, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.025926219299435616, KL divergence=0.023079464212059975, Entropy=0.1603405922651291, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.025482047349214554, KL divergence=0.02454022504389286, Entropy=0.15983185172080994, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.025062130764126778, KL divergence=0.02526726946234703, Entropy=0.15761683881282806, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03089131973683834, KL divergence=0.02656075917184353, Entropy=0.1579331010580063, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.026350047439336777, KL divergence=0.027444783598184586, Entropy=0.15812689065933228, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.034923046827316284, KL divergence=0.029155800119042397, Entropy=0.1570562720298767, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/569_Step-102321.ckpt']
Uploaded 3 files for checkpoint 569 in 0.56 seconds
saved intermediate frozen graph: current/model/model_569.pb
Best checkpoint number: 559, Last checkpoint number: 567
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'566'}
Training> Name=main_level/agent, Worker=0, Episode=541, Total reward=59.26, Steps=102347, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=542, Total reward=493.2, Steps=102643, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=543, Total reward=401.4, Steps=102916, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=544, Total reward=322.11, Steps=103160, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=545, Total reward=110.16, Steps=103247, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=546, Total reward=173.4, Steps=103379, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=547, Total reward=163.19, Steps=103478, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=548, Total reward=459.78, Steps=103771, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=549, Total reward=538.2, Steps=104061, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=550, Total reward=505.19, Steps=104371, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=551, Total reward=124.41, Steps=104462, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=552, Total reward=139.51, Steps=104541, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=553, Total reward=34.13, Steps=104553, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=554, Total reward=453.32, Steps=104879, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=555, Total reward=514.99, Steps=105155, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=556, Total reward=195.67, Steps=105289, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=557, Total reward=379.68, Steps=105584, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=558, Total reward=433.81, Steps=105879, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=559, Total reward=139.95, Steps=105976, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=560, Total reward=520.26, Steps=106274, Training iteration=27
Policy training> Surrogate loss=-0.0007319828146137297, KL divergence=0.0007109048310667276, Entropy=0.15850503742694855, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.013835940510034561, KL divergence=0.012823815457522869, Entropy=0.153738334774971, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02008730359375477, KL divergence=0.018383564427495003, Entropy=0.15243038535118103, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02281542867422104, KL divergence=0.022791186347603798, Entropy=0.15040603280067444, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.020387405529618263, KL divergence=0.025438960641622543, Entropy=0.15108808875083923, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.023109672591090202, KL divergence=0.026494594290852547, Entropy=0.1504860520362854, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.023748965933918953, KL divergence=0.02776164375245571, Entropy=0.149606391787529, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02754608355462551, KL divergence=0.028319017961621284, Entropy=0.14866408705711365, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02910720370709896, KL divergence=0.029193826019763947, Entropy=0.150152787566185, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02983914688229561, KL divergence=0.030882136896252632, Entropy=0.15085919201374054, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/570_Step-106274.ckpt']
Uploaded 3 files for checkpoint 570 in 0.55 seconds
saved intermediate frozen graph: current/model/model_570.pb
Best checkpoint number: 559, Last checkpoint number: 568
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'567'}
Training> Name=main_level/agent, Worker=0, Episode=561, Total reward=209.26, Steps=106470, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=562, Total reward=137.76, Steps=106629, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=563, Total reward=160.63, Steps=106787, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=564, Total reward=449.39, Steps=107088, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=565, Total reward=430.92, Steps=107388, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=566, Total reward=474.15, Steps=107668, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=567, Total reward=473.19, Steps=107950, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=568, Total reward=442.65, Steps=108244, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=569, Total reward=84.66, Steps=108309, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=570, Total reward=405.97, Steps=108617, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=571, Total reward=160.86, Steps=108702, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=572, Total reward=502.43, Steps=108991, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=573, Total reward=63.77, Steps=109018, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=574, Total reward=438.99, Steps=109312, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=575, Total reward=430.42, Steps=109609, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=576, Total reward=21.95, Steps=109628, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=577, Total reward=403.62, Steps=109912, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=578, Total reward=21.41, Steps=109925, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=579, Total reward=283.7, Steps=110141, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=580, Total reward=489.71, Steps=110438, Training iteration=28
Policy training> Surrogate loss=0.0009527147631160915, KL divergence=0.0004963257815688848, Entropy=0.15743215382099152, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014675604179501534, KL divergence=0.009976444765925407, Entropy=0.1577402800321579, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.017820438370108604, KL divergence=0.015848014503717422, Entropy=0.15627627074718475, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026744240894913673, KL divergence=0.01843229867517948, Entropy=0.15616774559020996, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02489815652370453, KL divergence=0.019553277641534805, Entropy=0.1549583375453949, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026664676144719124, KL divergence=0.022435717284679413, Entropy=0.15597587823867798, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029150288552045822, KL divergence=0.023490674793720245, Entropy=0.15566056966781616, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.028199244290590286, KL divergence=0.023936262354254723, Entropy=0.1554097831249237, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.026630431413650513, KL divergence=0.024828894063830376, Entropy=0.1549031138420105, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02882915548980236, KL divergence=0.02556271106004715, Entropy=0.15584120154380798, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/571_Step-110438.ckpt']
Uploaded 3 files for checkpoint 571 in 0.60 seconds
saved intermediate frozen graph: current/model/model_571.pb
Best checkpoint number: 559, Last checkpoint number: 569
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'568'}
Training> Name=main_level/agent, Worker=0, Episode=581, Total reward=443.87, Steps=110750, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=582, Total reward=448.08, Steps=111044, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=583, Total reward=209.13, Steps=111212, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=584, Total reward=208.09, Steps=111388, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=585, Total reward=186.01, Steps=111552, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=586, Total reward=181.46, Steps=111668, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=587, Total reward=128.3, Steps=111765, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=588, Total reward=102.95, Steps=111834, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=589, Total reward=519.46, Steps=112124, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=590, Total reward=299.05, Steps=112289, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=591, Total reward=391.83, Steps=112615, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=592, Total reward=475.9, Steps=112904, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=593, Total reward=441.28, Steps=113187, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=594, Total reward=247.48, Steps=113316, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=595, Total reward=431.0, Steps=113626, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=596, Total reward=496.41, Steps=113923, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=597, Total reward=82.49, Steps=113979, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=598, Total reward=303.69, Steps=114233, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=599, Total reward=253.94, Steps=114448, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=600, Total reward=465.66, Steps=114750, Training iteration=29
Policy training> Surrogate loss=-0.00119373993948102, KL divergence=0.0007599313976243138, Entropy=0.16330936551094055, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014393926598131657, KL divergence=0.009455699473619461, Entropy=0.15647193789482117, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.023519126698374748, KL divergence=0.014459210447967052, Entropy=0.15757682919502258, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025883853435516357, KL divergence=0.018297571688890457, Entropy=0.15564943850040436, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02358396165072918, KL divergence=0.020813627168536186, Entropy=0.1554369330406189, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.032267339527606964, KL divergence=0.022567670792341232, Entropy=0.1537473499774933, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0278786588460207, KL divergence=0.023610055446624756, Entropy=0.15330293774604797, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030255593359470367, KL divergence=0.025438332930207253, Entropy=0.15459638833999634, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03474967181682587, KL divergence=0.026251930743455887, Entropy=0.15411624312400818, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.029997581616044044, KL divergence=0.02801717445254326, Entropy=0.1544187068939209, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/572_Step-114750.ckpt']
Uploaded 3 files for checkpoint 572 in 0.57 seconds
saved intermediate frozen graph: current/model/model_572.pb
Best checkpoint number: 559, Last checkpoint number: 570
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'569'}
Training> Name=main_level/agent, Worker=0, Episode=601, Total reward=467.48, Steps=115036, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=602, Total reward=490.91, Steps=115347, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=603, Total reward=106.01, Steps=115468, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=604, Total reward=370.91, Steps=115706, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=605, Total reward=435.3, Steps=115984, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=606, Total reward=472.89, Steps=116278, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=607, Total reward=448.75, Steps=116525, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=608, Total reward=347.24, Steps=116736, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=609, Total reward=111.8, Steps=116810, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=610, Total reward=85.53, Steps=116894, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=611, Total reward=140.49, Steps=116986, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=612, Total reward=312.23, Steps=117165, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=613, Total reward=330.26, Steps=117360, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=614, Total reward=61.0, Steps=117410, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=615, Total reward=454.15, Steps=117698, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=616, Total reward=214.73, Steps=117811, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=617, Total reward=432.88, Steps=118114, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=618, Total reward=516.2, Steps=118396, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=619, Total reward=242.61, Steps=118553, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=620, Total reward=261.36, Steps=118743, Training iteration=30
Policy training> Surrogate loss=-0.0011972204083576798, KL divergence=0.0007693861844018102, Entropy=0.1627967655658722, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.012750063091516495, KL divergence=0.010008377954363823, Entropy=0.15980251133441925, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.025179505348205566, KL divergence=0.016411438584327698, Entropy=0.1569216400384903, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.018875299021601677, KL divergence=0.019624250009655952, Entropy=0.1567416936159134, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026270780712366104, KL divergence=0.020855341106653214, Entropy=0.155996635556221, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02611398883163929, KL divergence=0.021341761574149132, Entropy=0.15520581603050232, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0292124655097723, KL divergence=0.022592417895793915, Entropy=0.1547558307647705, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02455645240843296, KL divergence=0.023541085422039032, Entropy=0.15499846637248993, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.029760606586933136, KL divergence=0.024928079918026924, Entropy=0.15530508756637573, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.031877391040325165, KL divergence=0.0259112436324358, Entropy=0.15448659658432007, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/573_Step-118743.ckpt']
Uploaded 3 files for checkpoint 573 in 0.61 seconds
saved intermediate frozen graph: current/model/model_573.pb
Best checkpoint number: 559, Last checkpoint number: 571
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'570'}
Training> Name=main_level/agent, Worker=0, Episode=621, Total reward=237.2, Steps=118967, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=622, Total reward=285.21, Steps=119189, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=623, Total reward=524.61, Steps=119488, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=624, Total reward=309.06, Steps=119694, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=625, Total reward=415.52, Steps=119983, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=626, Total reward=158.79, Steps=120101, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=627, Total reward=381.95, Steps=120310, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=628, Total reward=269.12, Steps=120448, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=629, Total reward=15.55, Steps=120462, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=630, Total reward=309.26, Steps=120681, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=631, Total reward=471.76, Steps=120959, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=632, Total reward=79.44, Steps=121016, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=633, Total reward=321.66, Steps=121193, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=634, Total reward=446.95, Steps=121504, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=635, Total reward=246.18, Steps=121643, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=636, Total reward=461.73, Steps=121945, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=637, Total reward=396.45, Steps=122255, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=638, Total reward=352.87, Steps=122463, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=639, Total reward=290.81, Steps=122723, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=640, Total reward=427.13, Steps=123034, Training iteration=31
Policy training> Surrogate loss=-0.005533150397241116, KL divergence=0.0011943808058276772, Entropy=0.16000071167945862, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017438214272260666, KL divergence=0.009796451777219772, Entropy=0.15659774839878082, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020490607246756554, KL divergence=0.015893522650003433, Entropy=0.15515629947185516, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.015847405418753624, KL divergence=0.018342647701501846, Entropy=0.1536369025707245, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.016295600682497025, KL divergence=0.0195999126881361, Entropy=0.15416862070560455, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02516443096101284, KL divergence=0.020097116008400917, Entropy=0.15211018919944763, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0184779055416584, KL divergence=0.021524254232645035, Entropy=0.1534443199634552, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.025720566511154175, KL divergence=0.022364702075719833, Entropy=0.15310263633728027, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02273949608206749, KL divergence=0.022967761382460594, Entropy=0.15091007947921753, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.028238803148269653, KL divergence=0.0239852387458086, Entropy=0.15087920427322388, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/574_Step-123034.ckpt']
Uploaded 3 files for checkpoint 574 in 0.53 seconds
saved intermediate frozen graph: current/model/model_574.pb
Best checkpoint number: 559, Last checkpoint number: 572
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'571'}
Training> Name=main_level/agent, Worker=0, Episode=641, Total reward=72.18, Steps=123073, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=642, Total reward=319.62, Steps=123265, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=643, Total reward=456.55, Steps=123549, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=644, Total reward=424.24, Steps=123841, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=645, Total reward=108.85, Steps=123954, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=646, Total reward=496.65, Steps=124248, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=647, Total reward=402.08, Steps=124493, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=648, Total reward=114.94, Steps=124566, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=649, Total reward=158.87, Steps=124657, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=650, Total reward=485.32, Steps=124956, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=651, Total reward=152.38, Steps=125053, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=652, Total reward=229.89, Steps=125157, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=653, Total reward=503.32, Steps=125448, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=654, Total reward=474.92, Steps=125755, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=655, Total reward=411.31, Steps=126080, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=656, Total reward=505.8, Steps=126383, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=657, Total reward=467.81, Steps=126683, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=658, Total reward=423.36, Steps=126947, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=659, Total reward=434.13, Steps=127263, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=660, Total reward=464.66, Steps=127555, Training iteration=32
Policy training> Surrogate loss=-0.0009060673182830215, KL divergence=0.0008707616943866014, Entropy=0.1545315831899643, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014040007255971432, KL divergence=0.009254289790987968, Entropy=0.15076273679733276, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02164371870458126, KL divergence=0.014578449539840221, Entropy=0.1499924659729004, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026813000440597534, KL divergence=0.018082503229379654, Entropy=0.14861898124217987, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.024052370339632034, KL divergence=0.01905788481235504, Entropy=0.1486586630344391, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.028712240979075432, KL divergence=0.02061668038368225, Entropy=0.14775240421295166, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02568766102194786, KL divergence=0.02244093455374241, Entropy=0.14715690910816193, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.028287526220083237, KL divergence=0.02304474636912346, Entropy=0.14821457862854004, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02737591415643692, KL divergence=0.024042092263698578, Entropy=0.14755506813526154, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03605067357420921, KL divergence=0.024811839684844017, Entropy=0.14635580778121948, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/575_Step-127555.ckpt']
Uploaded 3 files for checkpoint 575 in 0.59 seconds
saved intermediate frozen graph: current/model/model_575.pb
Best checkpoint number: 559, Last checkpoint number: 573
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'572'}
Training> Name=main_level/agent, Worker=0, Episode=661, Total reward=306.02, Steps=127778, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=662, Total reward=0.0, Steps=127779, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=663, Total reward=154.97, Steps=127899, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=664, Total reward=439.4, Steps=128196, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=665, Total reward=152.8, Steps=128337, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=666, Total reward=174.75, Steps=128457, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=667, Total reward=220.14, Steps=128609, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=668, Total reward=486.42, Steps=128872, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=669, Total reward=304.75, Steps=129071, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=670, Total reward=24.57, Steps=129121, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=671, Total reward=143.92, Steps=129206, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=672, Total reward=265.11, Steps=129329, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=673, Total reward=33.49, Steps=129341, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=674, Total reward=431.2, Steps=129614, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=675, Total reward=37.84, Steps=129632, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=676, Total reward=468.17, Steps=129905, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=677, Total reward=463.43, Steps=130192, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=678, Total reward=367.69, Steps=130418, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=679, Total reward=336.15, Steps=130652, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=680, Total reward=225.2, Steps=130836, Training iteration=33
Policy training> Surrogate loss=0.00018609051767271012, KL divergence=0.0001956354099093005, Entropy=0.16680611670017242, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.011987380683422089, KL divergence=0.007051402237266302, Entropy=0.16388137638568878, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.017333267256617546, KL divergence=0.013961710967123508, Entropy=0.1611429899930954, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.022994183003902435, KL divergence=0.019607793539762497, Entropy=0.1617165356874466, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02605615369975567, KL divergence=0.020714374259114265, Entropy=0.16169048845767975, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03704586997628212, KL divergence=0.023315690457820892, Entropy=0.15986467897891998, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03162199258804321, KL divergence=0.02421559952199459, Entropy=0.15692012012004852, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.029957642778754234, KL divergence=0.025916626676917076, Entropy=0.16004134714603424, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03655008599162102, KL divergence=0.025718675926327705, Entropy=0.15922431647777557, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03254012390971184, KL divergence=0.027170918881893158, Entropy=0.15860922634601593, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/576_Step-130836.ckpt']
Uploaded 3 files for checkpoint 576 in 0.55 seconds
saved intermediate frozen graph: current/model/model_576.pb
Best checkpoint number: 559, Last checkpoint number: 574
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'573'}
Training> Name=main_level/agent, Worker=0, Episode=681, Total reward=62.6, Steps=130862, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=682, Total reward=413.45, Steps=131157, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=683, Total reward=477.26, Steps=131448, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=684, Total reward=492.31, Steps=131744, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=685, Total reward=493.76, Steps=132019, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=686, Total reward=128.75, Steps=132122, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=687, Total reward=177.7, Steps=132225, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=688, Total reward=223.37, Steps=132326, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=689, Total reward=426.58, Steps=132647, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=690, Total reward=129.39, Steps=132752, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=691, Total reward=268.37, Steps=132895, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=692, Total reward=467.21, Steps=133201, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=693, Total reward=287.95, Steps=133375, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=694, Total reward=306.38, Steps=133555, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=695, Total reward=468.14, Steps=133840, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=696, Total reward=421.23, Steps=134119, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=697, Total reward=118.09, Steps=134266, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=698, Total reward=165.87, Steps=134358, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=699, Total reward=398.12, Steps=134671, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=700, Total reward=226.75, Steps=134821, Training iteration=34
Policy training> Surrogate loss=-0.0005355037865228951, KL divergence=0.000914658245164901, Entropy=0.16263161599636078, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015081665478646755, KL divergence=0.009562233462929726, Entropy=0.15762867033481598, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.019279543310403824, KL divergence=0.016082225367426872, Entropy=0.15698964893817902, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.027066582813858986, KL divergence=0.019832206889986992, Entropy=0.15746469795703888, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03261450678110123, KL divergence=0.02164233848452568, Entropy=0.15591104328632355, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02572758123278618, KL divergence=0.024059178307652473, Entropy=0.15776944160461426, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.027397794649004936, KL divergence=0.02454763650894165, Entropy=0.1566638946533203, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02755373902618885, KL divergence=0.02610900066792965, Entropy=0.1545994132757187, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.029693933203816414, KL divergence=0.027256863191723824, Entropy=0.15655414760112762, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03313248232007027, KL divergence=0.028452830389142036, Entropy=0.15622578561306, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/577_Step-134821.ckpt']
Uploaded 3 files for checkpoint 577 in 0.57 seconds
saved intermediate frozen graph: current/model/model_577.pb
Best checkpoint number: 559, Last checkpoint number: 575
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'574'}
Training> Name=main_level/agent, Worker=0, Episode=701, Total reward=256.96, Steps=135004, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=702, Total reward=262.03, Steps=135199, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=703, Total reward=382.08, Steps=135453, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=704, Total reward=0.01, Steps=135466, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=705, Total reward=206.68, Steps=135617, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=706, Total reward=523.41, Steps=135917, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=707, Total reward=295.91, Steps=136123, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=708, Total reward=478.7, Steps=136342, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=709, Total reward=164.25, Steps=136447, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=710, Total reward=48.51, Steps=136539, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=711, Total reward=60.57, Steps=136577, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=712, Total reward=457.44, Steps=136872, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=713, Total reward=403.9, Steps=137187, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=714, Total reward=466.89, Steps=137457, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=715, Total reward=448.74, Steps=137749, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=716, Total reward=493.82, Steps=138032, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=717, Total reward=92.08, Steps=138098, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=718, Total reward=483.7, Steps=138383, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=719, Total reward=470.93, Steps=138682, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=720, Total reward=98.41, Steps=138735, Training iteration=35
Policy training> Surrogate loss=0.0003331740736030042, KL divergence=0.0011437053326517344, Entropy=0.1656259000301361, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015062153339385986, KL divergence=0.010824761353433132, Entropy=0.1626526415348053, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.016515010967850685, KL divergence=0.016968529671430588, Entropy=0.15972909331321716, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023896686732769012, KL divergence=0.019708462059497833, Entropy=0.1603541374206543, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02723887376487255, KL divergence=0.020980339497327805, Entropy=0.1580444723367691, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.024283837527036667, KL divergence=0.022672193124890327, Entropy=0.1569351851940155, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02397465705871582, KL divergence=0.02414105087518692, Entropy=0.15810944139957428, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02541295997798443, KL divergence=0.024631673470139503, Entropy=0.15780475735664368, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.027147838845849037, KL divergence=0.026490505784749985, Entropy=0.158585786819458, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.027936715632677078, KL divergence=0.027577869594097137, Entropy=0.15758413076400757, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/578_Step-138735.ckpt']
Uploaded 3 files for checkpoint 578 in 0.54 seconds
saved intermediate frozen graph: current/model/model_578.pb
Best checkpoint number: 559, Last checkpoint number: 576
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'575'}
Training> Name=main_level/agent, Worker=0, Episode=721, Total reward=304.54, Steps=138955, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=722, Total reward=55.43, Steps=138984, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=723, Total reward=383.1, Steps=139263, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=724, Total reward=421.58, Steps=139554, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=725, Total reward=244.27, Steps=139725, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=726, Total reward=517.61, Steps=140022, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=727, Total reward=441.68, Steps=140319, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=728, Total reward=187.19, Steps=140435, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=729, Total reward=151.25, Steps=140558, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=730, Total reward=103.82, Steps=140640, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=731, Total reward=11.35, Steps=140651, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=732, Total reward=12.86, Steps=140663, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=733, Total reward=470.44, Steps=140912, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=734, Total reward=31.12, Steps=140924, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=735, Total reward=433.34, Steps=141219, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=736, Total reward=489.56, Steps=141514, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=737, Total reward=345.96, Steps=141752, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=738, Total reward=67.69, Steps=141792, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=739, Total reward=232.55, Steps=141957, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=740, Total reward=516.87, Steps=142231, Training iteration=36
Policy training> Surrogate loss=0.0035616857931017876, KL divergence=0.00044109622831456363, Entropy=0.1670297235250473, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.012180970050394535, KL divergence=0.009466206654906273, Entropy=0.1619899421930313, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02478998526930809, KL divergence=0.015545086935162544, Entropy=0.15991774201393127, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023139499127864838, KL divergence=0.018139569088816643, Entropy=0.158516064286232, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02542887255549431, KL divergence=0.020513933151960373, Entropy=0.16153797507286072, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030960451811552048, KL divergence=0.02252483367919922, Entropy=0.1590649038553238, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.032985005527734756, KL divergence=0.02408260479569435, Entropy=0.15907667577266693, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02575906738638878, KL divergence=0.02542779967188835, Entropy=0.1585133671760559, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.033304259181022644, KL divergence=0.026281706988811493, Entropy=0.15914210677146912, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03705016151070595, KL divergence=0.026922741904854774, Entropy=0.16044582426548004, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/579_Step-142231.ckpt']
Uploaded 3 files for checkpoint 579 in 0.56 seconds
saved intermediate frozen graph: current/model/model_579.pb
Best checkpoint number: 559, Last checkpoint number: 577
Copying the frozen checkpoint from ./frozen_models/agent/model_559.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'576'}
Training> Name=main_level/agent, Worker=0, Episode=741, Total reward=357.48, Steps=142441, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=742, Total reward=155.37, Steps=142572, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=743, Total reward=7.21, Steps=142584, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=744, Total reward=372.56, Steps=142833, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=745, Total reward=400.86, Steps=143055, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=746, Total reward=235.8, Steps=143198, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=747, Total reward=467.9, Steps=143481, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=748, Total reward=478.28, Steps=143746, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=749, Total reward=497.32, Steps=144033, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=750, Total reward=121.61, Steps=144121, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=751, Total reward=143.59, Steps=144208, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=752, Total reward=522.22, Steps=144499, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=753, Total reward=59.98, Steps=144543, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=754, Total reward=337.57, Steps=144786, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=755, Total reward=229.77, Steps=144930, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=756, Total reward=469.57, Steps=145225, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=757, Total reward=428.62, Steps=145529, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=758, Total reward=140.78, Steps=145609, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=759, Total reward=513.39, Steps=145908, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=760, Total reward=218.51, Steps=146105, Training iteration=37
Policy training> Surrogate loss=0.00010046412353403866, KL divergence=0.0007234891527332366, Entropy=0.16104961931705475, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015003154054284096, KL divergence=0.011216695420444012, Entropy=0.1584629863500595, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.019635196775197983, KL divergence=0.01634184829890728, Entropy=0.15532155334949493, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02293427102267742, KL divergence=0.01881757564842701, Entropy=0.15451018512248993, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02325856313109398, KL divergence=0.021175507456064224, Entropy=0.15371772646903992, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02671368047595024, KL divergence=0.022603120654821396, Entropy=0.15372498333454132, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.026958586648106575, KL divergence=0.023813903331756592, Entropy=0.15359264612197876, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.027468614280223846, KL divergence=0.024844376370310783, Entropy=0.15319010615348816, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02630060724914074, KL divergence=0.025767551735043526, Entropy=0.15286917984485626, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.028827492147684097, KL divergence=0.026707608252763748, Entropy=0.15297642350196838, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/580_Step-146105.ckpt']
Uploaded 3 files for checkpoint 580 in 0.59 seconds
saved intermediate frozen graph: current/model/model_580.pb
Best checkpoint number: 578, Last checkpoint number: 578
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'577'}
Training> Name=main_level/agent, Worker=0, Episode=761, Total reward=101.08, Steps=146193, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=762, Total reward=263.46, Steps=146392, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=763, Total reward=197.26, Steps=146593, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=764, Total reward=243.89, Steps=146747, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=765, Total reward=461.3, Steps=147047, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=766, Total reward=138.94, Steps=147134, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=767, Total reward=241.72, Steps=147261, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=768, Total reward=354.54, Steps=147457, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=769, Total reward=71.6, Steps=147505, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=770, Total reward=425.66, Steps=147795, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=771, Total reward=142.98, Steps=147865, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=772, Total reward=98.88, Steps=147922, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=773, Total reward=122.46, Steps=147985, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=774, Total reward=437.99, Steps=148278, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=775, Total reward=479.71, Steps=148574, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=776, Total reward=498.72, Steps=148879, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=777, Total reward=521.22, Steps=149168, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=778, Total reward=434.82, Steps=149438, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=779, Total reward=284.07, Steps=149711, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=780, Total reward=15.15, Steps=149725, Training iteration=38
Policy training> Surrogate loss=0.0003199710336048156, KL divergence=0.0005677996086888015, Entropy=0.16419720649719238, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017488082870841026, KL divergence=0.011767551302909851, Entropy=0.16095422208309174, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02463640831410885, KL divergence=0.019668664783239365, Entropy=0.1575128585100174, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023539040237665176, KL divergence=0.02271111309528351, Entropy=0.15674738585948944, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02706843987107277, KL divergence=0.025624852627515793, Entropy=0.1559094488620758, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.027973761782050133, KL divergence=0.026856113225221634, Entropy=0.15573939681053162, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.030011296272277832, KL divergence=0.02813468687236309, Entropy=0.1549714058637619, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.029754528775811195, KL divergence=0.028944427147507668, Entropy=0.15314903855323792, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.033328887075185776, KL divergence=0.030155155807733536, Entropy=0.15440361201763153, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03499652072787285, KL divergence=0.030401740223169327, Entropy=0.1539027988910675, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/581_Step-149725.ckpt']
Uploaded 3 files for checkpoint 581 in 0.54 seconds
saved intermediate frozen graph: current/model/model_581.pb
Best checkpoint number: 578, Last checkpoint number: 579
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'579'}
Training> Name=main_level/agent, Worker=0, Episode=781, Total reward=92.39, Steps=149779, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=782, Total reward=463.91, Steps=150055, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=783, Total reward=412.57, Steps=150318, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=784, Total reward=483.23, Steps=150617, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=785, Total reward=501.25, Steps=150907, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=786, Total reward=455.36, Steps=151126, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=787, Total reward=474.04, Steps=151434, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=788, Total reward=132.6, Steps=151530, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=789, Total reward=35.8, Steps=151551, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=790, Total reward=342.05, Steps=151740, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=791, Total reward=151.93, Steps=151831, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=792, Total reward=370.16, Steps=152022, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=793, Total reward=420.6, Steps=152334, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=794, Total reward=402.85, Steps=152636, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=795, Total reward=127.52, Steps=152717, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=796, Total reward=431.14, Steps=153039, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=797, Total reward=419.0, Steps=153351, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=798, Total reward=196.94, Steps=153466, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=799, Total reward=480.59, Steps=153753, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=800, Total reward=252.61, Steps=153955, Training iteration=39
Policy training> Surrogate loss=0.0002504733856767416, KL divergence=0.0006492905085906386, Entropy=0.15779966115951538, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.02001664601266384, KL divergence=0.009468922391533852, Entropy=0.15328606963157654, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.019390534609556198, KL divergence=0.015459575690329075, Entropy=0.15119358897209167, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.020623261108994484, KL divergence=0.018895402550697327, Entropy=0.15026447176933289, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029600348323583603, KL divergence=0.020949235185980797, Entropy=0.15116667747497559, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0284618828445673, KL divergence=0.02263794280588627, Entropy=0.1499766707420349, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02458544634282589, KL divergence=0.024496540427207947, Entropy=0.14969590306282043, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030726706609129906, KL divergence=0.026286296546459198, Entropy=0.1501457542181015, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02601301670074463, KL divergence=0.026799172163009644, Entropy=0.15018469095230103, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.028847936540842056, KL divergence=0.027920003980398178, Entropy=0.1501425802707672, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/582_Step-153955.ckpt']
Uploaded 3 files for checkpoint 582 in 0.55 seconds
saved intermediate frozen graph: current/model/model_582.pb
Best checkpoint number: 578, Last checkpoint number: 580
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'559'}
Training> Name=main_level/agent, Worker=0, Episode=801, Total reward=317.37, Steps=154178, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=802, Total reward=50.31, Steps=154203, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=803, Total reward=461.76, Steps=154502, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=804, Total reward=3.24, Steps=154535, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=805, Total reward=131.99, Steps=154649, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=806, Total reward=93.51, Steps=154755, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=807, Total reward=478.89, Steps=155024, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=808, Total reward=254.0, Steps=155174, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=809, Total reward=62.45, Steps=155232, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=810, Total reward=417.54, Steps=155521, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=811, Total reward=427.36, Steps=155833, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=812, Total reward=420.88, Steps=156150, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=813, Total reward=95.29, Steps=156208, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=814, Total reward=482.98, Steps=156495, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=815, Total reward=250.35, Steps=156621, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=816, Total reward=368.04, Steps=156899, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=817, Total reward=175.43, Steps=157000, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=818, Total reward=167.3, Steps=157140, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=819, Total reward=339.82, Steps=157420, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=820, Total reward=419.79, Steps=157714, Training iteration=40
Policy training> Surrogate loss=-0.0028243574779480696, KL divergence=0.00038550826138816774, Entropy=0.15734651684761047, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.020717736333608627, KL divergence=0.010524935089051723, Entropy=0.1519509106874466, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.023050207644701004, KL divergence=0.018089624121785164, Entropy=0.15079966187477112, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.022299040108919144, KL divergence=0.021033093333244324, Entropy=0.14922133088111877, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02688264660537243, KL divergence=0.022594688460230827, Entropy=0.1489456444978714, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0303366519510746, KL divergence=0.024979159235954285, Entropy=0.14893533289432526, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03038758970797062, KL divergence=0.02720281109213829, Entropy=0.15037883818149567, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03800360485911369, KL divergence=0.028133872896432877, Entropy=0.14920474588871002, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03266478702425957, KL divergence=0.030215416103601456, Entropy=0.1498226374387741, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.035799503326416016, KL divergence=0.030638422816991806, Entropy=0.14943991601467133, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/583_Step-157714.ckpt']
Uploaded 3 files for checkpoint 583 in 0.49 seconds
saved intermediate frozen graph: current/model/model_583.pb
Best checkpoint number: 578, Last checkpoint number: 581
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'580'}
Training> Name=main_level/agent, Worker=0, Episode=821, Total reward=97.02, Steps=157757, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=822, Total reward=74.88, Steps=157823, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=823, Total reward=466.17, Steps=158136, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=824, Total reward=196.52, Steps=158293, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=825, Total reward=94.4, Steps=158380, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=826, Total reward=430.37, Steps=158680, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=827, Total reward=405.14, Steps=158992, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=828, Total reward=435.36, Steps=159294, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=829, Total reward=31.83, Steps=159318, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=830, Total reward=131.82, Steps=159419, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=831, Total reward=114.62, Steps=159494, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=832, Total reward=75.84, Steps=159536, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=833, Total reward=522.11, Steps=159814, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=834, Total reward=231.67, Steps=159958, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=835, Total reward=82.23, Steps=160008, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=836, Total reward=133.62, Steps=160078, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=837, Total reward=286.54, Steps=160290, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=838, Total reward=472.34, Steps=160599, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=839, Total reward=138.03, Steps=160723, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=840, Total reward=119.81, Steps=160780, Training iteration=41
Policy training> Surrogate loss=0.004452897701412439, KL divergence=0.00017890204617287964, Entropy=0.17272807657718658, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01828981563448906, KL divergence=0.007845775224268436, Entropy=0.1734103411436081, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.017344413325190544, KL divergence=0.017315445467829704, Entropy=0.16814914345741272, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02609844133257866, KL divergence=0.02196325734257698, Entropy=0.16728143393993378, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.027286676689982414, KL divergence=0.024099715054035187, Entropy=0.16375420987606049, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03388455882668495, KL divergence=0.02577662467956543, Entropy=0.16346341371536255, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03263384848833084, KL divergence=0.028615552932024002, Entropy=0.16452263295650482, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03414318338036537, KL divergence=0.030611298978328705, Entropy=0.163624569773674, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03993171453475952, KL divergence=0.02980741672217846, Entropy=0.16400708258152008, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03807561472058296, KL divergence=0.030487509444355965, Entropy=0.16242600977420807, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/584_Step-160780.ckpt']
Uploaded 3 files for checkpoint 584 in 0.53 seconds
saved intermediate frozen graph: current/model/model_584.pb
Best checkpoint number: 578, Last checkpoint number: 582
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'581'}
Training> Name=main_level/agent, Worker=0, Episode=841, Total reward=490.71, Steps=161070, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=842, Total reward=197.53, Steps=161247, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=843, Total reward=478.6, Steps=161544, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=844, Total reward=258.48, Steps=161719, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=845, Total reward=420.51, Steps=162011, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=846, Total reward=490.95, Steps=162309, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=847, Total reward=154.87, Steps=162404, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=848, Total reward=537.5, Steps=162683, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=849, Total reward=95.28, Steps=162748, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=850, Total reward=446.64, Steps=163049, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=851, Total reward=152.7, Steps=163134, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=852, Total reward=87.75, Steps=163191, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=853, Total reward=77.92, Steps=163217, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=854, Total reward=32.61, Steps=163229, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=855, Total reward=382.54, Steps=163478, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=856, Total reward=463.01, Steps=163778, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=857, Total reward=447.56, Steps=164084, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=858, Total reward=69.23, Steps=164125, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=859, Total reward=343.73, Steps=164359, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=860, Total reward=479.92, Steps=164647, Training iteration=42
Policy training> Surrogate loss=-0.005143078044056892, KL divergence=0.000673968403134495, Entropy=0.16943326592445374, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017352866008877754, KL divergence=0.009588371962308884, Entropy=0.1677553504705429, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.019651930779218674, KL divergence=0.014822924509644508, Entropy=0.16479447484016418, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026359520852565765, KL divergence=0.017430176958441734, Entropy=0.16301992535591125, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.025411222130060196, KL divergence=0.019021527841687202, Entropy=0.1616658866405487, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02642931416630745, KL divergence=0.02045050822198391, Entropy=0.16149994730949402, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02688731998205185, KL divergence=0.021337799727916718, Entropy=0.159848153591156, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02787010744214058, KL divergence=0.022750206291675568, Entropy=0.15972933173179626, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.029894070699810982, KL divergence=0.023565929383039474, Entropy=0.15917320549488068, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03083353489637375, KL divergence=0.024282390251755714, Entropy=0.15913026034832, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/585_Step-164647.ckpt']
Uploaded 3 files for checkpoint 585 in 0.53 seconds
saved intermediate frozen graph: current/model/model_585.pb
Best checkpoint number: 578, Last checkpoint number: 583
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'582'}
Training> Name=main_level/agent, Worker=0, Episode=861, Total reward=107.47, Steps=164719, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=862, Total reward=499.97, Steps=165004, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=863, Total reward=231.7, Steps=165185, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=864, Total reward=467.25, Steps=165474, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=865, Total reward=205.18, Steps=165631, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=866, Total reward=491.72, Steps=165925, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=867, Total reward=299.25, Steps=166118, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=868, Total reward=91.43, Steps=166161, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=869, Total reward=480.37, Steps=166449, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=870, Total reward=444.2, Steps=166733, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=871, Total reward=520.13, Steps=167023, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=872, Total reward=450.24, Steps=167311, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=873, Total reward=305.51, Steps=167498, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=874, Total reward=30.82, Steps=167510, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=875, Total reward=54.15, Steps=167529, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=876, Total reward=469.47, Steps=167833, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=877, Total reward=486.4, Steps=168111, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=878, Total reward=340.73, Steps=168342, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=879, Total reward=261.06, Steps=168539, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=880, Total reward=458.06, Steps=168839, Training iteration=43
Policy training> Surrogate loss=0.001100056804716587, KL divergence=0.0005729354452341795, Entropy=0.16774585843086243, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01748610846698284, KL divergence=0.010610727593302727, Entropy=0.164718359708786, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.018466835841536522, KL divergence=0.017064139246940613, Entropy=0.16471406817436218, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.020894818007946014, KL divergence=0.019986338913440704, Entropy=0.16403570771217346, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.023040181025862694, KL divergence=0.021839525550603867, Entropy=0.16399909555912018, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.027340175583958626, KL divergence=0.02377808280289173, Entropy=0.16293634474277496, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02926355041563511, KL divergence=0.02528432570397854, Entropy=0.16392603516578674, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.027645310387015343, KL divergence=0.025707237422466278, Entropy=0.16224190592765808, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.027747178450226784, KL divergence=0.026140596717596054, Entropy=0.1613556295633316, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03174574673175812, KL divergence=0.026887746527791023, Entropy=0.16110768914222717, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/586_Step-168839.ckpt']
Uploaded 3 files for checkpoint 586 in 0.58 seconds
saved intermediate frozen graph: current/model/model_586.pb
Best checkpoint number: 578, Last checkpoint number: 584
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'583'}
Training> Name=main_level/agent, Worker=0, Episode=881, Total reward=500.48, Steps=169129, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=882, Total reward=480.37, Steps=169419, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=883, Total reward=118.07, Steps=169564, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=884, Total reward=344.7, Steps=169816, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=885, Total reward=463.99, Steps=170104, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=886, Total reward=435.33, Steps=170379, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=887, Total reward=484.46, Steps=170675, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=888, Total reward=471.4, Steps=170968, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=889, Total reward=490.13, Steps=171274, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=890, Total reward=0.03, Steps=171301, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=891, Total reward=49.65, Steps=171354, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=892, Total reward=147.57, Steps=171431, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=893, Total reward=395.48, Steps=171728, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=894, Total reward=432.17, Steps=172053, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=895, Total reward=517.11, Steps=172330, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=896, Total reward=440.86, Steps=172653, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=897, Total reward=465.29, Steps=172965, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=898, Total reward=459.11, Steps=173255, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=899, Total reward=381.38, Steps=173495, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=900, Total reward=502.44, Steps=173782, Training iteration=44
Policy training> Surrogate loss=0.002485421486198902, KL divergence=0.0008277962915599346, Entropy=0.1583109349012375, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015374594368040562, KL divergence=0.010569563135504723, Entropy=0.1563209742307663, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02085726521909237, KL divergence=0.015506061725318432, Entropy=0.15481354296207428, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.01811068132519722, KL divergence=0.01762249507009983, Entropy=0.1537652313709259, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0245810616761446, KL divergence=0.019630244001746178, Entropy=0.1522917002439499, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.022390026599168777, KL divergence=0.021236272528767586, Entropy=0.1522400826215744, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.027679309248924255, KL divergence=0.02211969532072544, Entropy=0.1523832082748413, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.024638859555125237, KL divergence=0.023311922326683998, Entropy=0.15115951001644135, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.028228873386979103, KL divergence=0.024086931720376015, Entropy=0.1506565511226654, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02913300320506096, KL divergence=0.024895591661334038, Entropy=0.14972548186779022, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/587_Step-173782.ckpt']
Uploaded 3 files for checkpoint 587 in 0.64 seconds
saved intermediate frozen graph: current/model/model_587.pb
Best checkpoint number: 578, Last checkpoint number: 585
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'584'}
Training> Name=main_level/agent, Worker=0, Episode=901, Total reward=534.29, Steps=174081, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=902, Total reward=551.59, Steps=174362, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=903, Total reward=415.5, Steps=174668, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=904, Total reward=17.17, Steps=174730, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=905, Total reward=446.0, Steps=175026, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=906, Total reward=419.35, Steps=175302, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=907, Total reward=238.65, Steps=175449, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=908, Total reward=207.75, Steps=175576, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=909, Total reward=33.43, Steps=175603, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=910, Total reward=105.58, Steps=175686, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=911, Total reward=383.89, Steps=175904, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=912, Total reward=430.21, Steps=176194, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=913, Total reward=92.92, Steps=176235, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=914, Total reward=64.63, Steps=176284, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=915, Total reward=397.96, Steps=176577, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=916, Total reward=453.74, Steps=176851, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=917, Total reward=474.45, Steps=177165, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=918, Total reward=346.36, Steps=177402, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=919, Total reward=171.66, Steps=177557, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=920, Total reward=449.39, Steps=177870, Training iteration=45
Policy training> Surrogate loss=-0.0011165052419528365, KL divergence=0.0007799359154887497, Entropy=0.16277354955673218, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.013618762604892254, KL divergence=0.011253496631979942, Entropy=0.1586114913225174, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.016078105196356773, KL divergence=0.017999036237597466, Entropy=0.1589505672454834, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.027888381853699684, KL divergence=0.021000947803258896, Entropy=0.15779680013656616, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02198285236954689, KL divergence=0.023615671321749687, Entropy=0.15898214280605316, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.021522410213947296, KL divergence=0.0251002237200737, Entropy=0.15763922035694122, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03168835863471031, KL divergence=0.025146907195448875, Entropy=0.1548086553812027, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030988160520792007, KL divergence=0.026562917977571487, Entropy=0.15550579130649567, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03878789767622948, KL divergence=0.027359969913959503, Entropy=0.15747885406017303, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.034720368683338165, KL divergence=0.026939624920487404, Entropy=0.15733443200588226, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/588_Step-177870.ckpt']
Uploaded 3 files for checkpoint 588 in 0.54 seconds
saved intermediate frozen graph: current/model/model_588.pb
Best checkpoint number: 578, Last checkpoint number: 586
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'585'}
Training> Name=main_level/agent, Worker=0, Episode=921, Total reward=428.67, Steps=178162, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=922, Total reward=418.11, Steps=178468, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=923, Total reward=0.02, Steps=178485, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=924, Total reward=383.6, Steps=178704, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=925, Total reward=197.94, Steps=178862, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=926, Total reward=436.77, Steps=179156, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=927, Total reward=186.5, Steps=179266, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=928, Total reward=211.46, Steps=179409, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=929, Total reward=104.21, Steps=179517, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=930, Total reward=444.78, Steps=179839, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=931, Total reward=358.96, Steps=180057, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=932, Total reward=502.46, Steps=180355, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=933, Total reward=76.26, Steps=180398, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=934, Total reward=146.67, Steps=180510, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=935, Total reward=500.25, Steps=180798, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=936, Total reward=511.18, Steps=181096, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=937, Total reward=186.24, Steps=181197, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=938, Total reward=443.7, Steps=181497, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=939, Total reward=267.21, Steps=181693, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=940, Total reward=402.47, Steps=181933, Training iteration=46
Policy training> Surrogate loss=0.0011360520729795098, KL divergence=0.0008099943515844643, Entropy=0.1607000231742859, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015974652022123337, KL divergence=0.012253787368535995, Entropy=0.1529201865196228, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020900551229715347, KL divergence=0.01826442964375019, Entropy=0.15116411447525024, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026174020022153854, KL divergence=0.01972351409494877, Entropy=0.1496802270412445, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026793098077178, KL divergence=0.022804610431194305, Entropy=0.1505739986896515, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.022692015394568443, KL divergence=0.023235540837049484, Entropy=0.14794419705867767, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.025888632982969284, KL divergence=0.024325866252183914, Entropy=0.15009763836860657, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.025092951953411102, KL divergence=0.025346584618091583, Entropy=0.14713147282600403, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02393251471221447, KL divergence=0.027563758194446564, Entropy=0.14802315831184387, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.029220694676041603, KL divergence=0.027775133028626442, Entropy=0.14906851947307587, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/589_Step-181933.ckpt']
Uploaded 3 files for checkpoint 589 in 0.54 seconds
saved intermediate frozen graph: current/model/model_589.pb
Best checkpoint number: 578, Last checkpoint number: 587
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'586'}
Training> Name=main_level/agent, Worker=0, Episode=941, Total reward=55.62, Steps=181961, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=942, Total reward=488.31, Steps=182248, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=943, Total reward=470.8, Steps=182544, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=944, Total reward=261.65, Steps=182735, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=945, Total reward=3.51, Steps=182759, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=946, Total reward=400.98, Steps=183065, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=947, Total reward=478.18, Steps=183350, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=948, Total reward=493.88, Steps=183633, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=949, Total reward=414.53, Steps=183917, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=950, Total reward=109.09, Steps=184009, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=951, Total reward=188.32, Steps=184109, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=952, Total reward=304.52, Steps=184310, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=953, Total reward=256.53, Steps=184421, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=954, Total reward=282.94, Steps=184584, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=955, Total reward=264.09, Steps=184749, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=956, Total reward=18.28, Steps=184767, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=957, Total reward=337.11, Steps=184980, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=958, Total reward=487.55, Steps=185258, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=959, Total reward=370.87, Steps=185510, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=960, Total reward=252.82, Steps=185712, Training iteration=47
Policy training> Surrogate loss=-0.006098812911659479, KL divergence=0.00037965172668918967, Entropy=0.16763368248939514, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017589544877409935, KL divergence=0.009159994311630726, Entropy=0.1664697378873825, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.024566249921917915, KL divergence=0.017563490197062492, Entropy=0.16440796852111816, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026619480922818184, KL divergence=0.021282821893692017, Entropy=0.16232891380786896, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.025242891162633896, KL divergence=0.023785937577486038, Entropy=0.16284847259521484, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.024098340421915054, KL divergence=0.025358188897371292, Entropy=0.16171284019947052, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.032895904034376144, KL divergence=0.02643442526459694, Entropy=0.16037261486053467, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.023665567860007286, KL divergence=0.028249312192201614, Entropy=0.16135497391223907, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02755211852490902, KL divergence=0.02939123474061489, Entropy=0.16274288296699524, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.035145778208971024, KL divergence=0.030659962445497513, Entropy=0.16134870052337646, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/590_Step-185712.ckpt']
Uploaded 3 files for checkpoint 590 in 0.61 seconds
saved intermediate frozen graph: current/model/model_590.pb
Best checkpoint number: 578, Last checkpoint number: 588
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'587'}
Training> Name=main_level/agent, Worker=0, Episode=961, Total reward=268.53, Steps=185884, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=962, Total reward=42.38, Steps=185935, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=963, Total reward=180.8, Steps=186091, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=964, Total reward=480.23, Steps=186390, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=965, Total reward=251.14, Steps=186566, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=966, Total reward=458.46, Steps=186872, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=967, Total reward=537.72, Steps=187173, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=968, Total reward=448.87, Steps=187402, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=969, Total reward=211.15, Steps=187513, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=970, Total reward=80.53, Steps=187575, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=971, Total reward=121.45, Steps=187641, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=972, Total reward=468.21, Steps=187930, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=973, Total reward=457.14, Steps=188222, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=974, Total reward=511.71, Steps=188513, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=975, Total reward=502.96, Steps=188808, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=976, Total reward=311.19, Steps=189023, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=977, Total reward=204.32, Steps=189162, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=978, Total reward=81.27, Steps=189215, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=979, Total reward=288.45, Steps=189440, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=980, Total reward=345.3, Steps=189751, Training iteration=48
Policy training> Surrogate loss=-0.0005751585122197866, KL divergence=0.0007492353906854987, Entropy=0.16276517510414124, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.022315077483654022, KL divergence=0.011184409260749817, Entropy=0.15882399678230286, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02129730023443699, KL divergence=0.016961395740509033, Entropy=0.15543170273303986, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023435253649950027, KL divergence=0.020847205072641373, Entropy=0.1554558128118515, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.030106421560049057, KL divergence=0.02370765618979931, Entropy=0.1556735634803772, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02494596689939499, KL divergence=0.025194961577653885, Entropy=0.15400031208992004, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03272135555744171, KL divergence=0.026598168537020683, Entropy=0.15206554532051086, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.028736623004078865, KL divergence=0.027765419334173203, Entropy=0.1518917828798294, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.029065506532788277, KL divergence=0.02850678190588951, Entropy=0.152801051735878, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03165177255868912, KL divergence=0.029438642784953117, Entropy=0.15168122947216034, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/591_Step-189751.ckpt']
Uploaded 3 files for checkpoint 591 in 0.52 seconds
saved intermediate frozen graph: current/model/model_591.pb
Best checkpoint number: 578, Last checkpoint number: 589
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'588'}
Training> Name=main_level/agent, Worker=0, Episode=981, Total reward=482.32, Steps=190048, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=982, Total reward=503.24, Steps=190342, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=983, Total reward=477.41, Steps=190646, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=984, Total reward=424.51, Steps=190929, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=985, Total reward=425.41, Steps=191205, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=986, Total reward=486.35, Steps=191511, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=987, Total reward=266.95, Steps=191654, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=988, Total reward=434.87, Steps=191961, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=989, Total reward=104.43, Steps=192026, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=990, Total reward=157.04, Steps=192131, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=991, Total reward=62.21, Steps=192168, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=992, Total reward=480.95, Steps=192471, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=993, Total reward=69.66, Steps=192512, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=994, Total reward=383.01, Steps=192767, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=995, Total reward=480.43, Steps=193080, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=996, Total reward=491.05, Steps=193386, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=997, Total reward=70.66, Steps=193438, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=998, Total reward=320.16, Steps=193650, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=999, Total reward=188.07, Steps=193820, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=1000, Total reward=249.96, Steps=194029, Training iteration=49
Policy training> Surrogate loss=0.002592237200587988, KL divergence=0.0006035742699168622, Entropy=0.160426527261734, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01835811138153076, KL divergence=0.01235002838075161, Entropy=0.15747255086898804, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020109113305807114, KL divergence=0.018215907737612724, Entropy=0.15418106317520142, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.024872569367289543, KL divergence=0.021371599286794662, Entropy=0.15340282022953033, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0249454528093338, KL divergence=0.02204160764813423, Entropy=0.15335318446159363, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.031865064054727554, KL divergence=0.024148795753717422, Entropy=0.15210944414138794, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03245844319462776, KL divergence=0.025683298707008362, Entropy=0.1525404155254364, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03112240694463253, KL divergence=0.025316253304481506, Entropy=0.14928799867630005, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03375309333205223, KL divergence=0.025311408564448357, Entropy=0.1497267782688141, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.030457228422164917, KL divergence=0.027026910334825516, Entropy=0.15158864855766296, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/592_Step-194029.ckpt']
Uploaded 3 files for checkpoint 592 in 0.56 seconds
saved intermediate frozen graph: current/model/model_592.pb
Best checkpoint number: 578, Last checkpoint number: 590
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'589'}
Training> Name=main_level/agent, Worker=0, Episode=1001, Total reward=358.5, Steps=194260, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1002, Total reward=220.0, Steps=194428, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1003, Total reward=262.94, Steps=194648, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1004, Total reward=160.67, Steps=194794, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1005, Total reward=212.87, Steps=194942, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1006, Total reward=67.23, Steps=195016, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1007, Total reward=453.89, Steps=195320, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1008, Total reward=517.69, Steps=195606, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1009, Total reward=474.64, Steps=195902, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1010, Total reward=456.51, Steps=196196, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1011, Total reward=436.3, Steps=196482, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1012, Total reward=368.15, Steps=196703, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1013, Total reward=465.28, Steps=197020, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1014, Total reward=154.65, Steps=197128, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1015, Total reward=170.55, Steps=197209, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1016, Total reward=229.32, Steps=197360, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1017, Total reward=459.87, Steps=197631, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1018, Total reward=509.35, Steps=197908, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1019, Total reward=121.41, Steps=198002, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=1020, Total reward=200.53, Steps=198150, Training iteration=50
Policy training> Surrogate loss=-0.001273744972422719, KL divergence=0.0007800597231835127, Entropy=0.16617213189601898, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.016475889831781387, KL divergence=0.010569481179118156, Entropy=0.16397525370121002, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020492132753133774, KL divergence=0.016072219237685204, Entropy=0.161603644490242, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023955902084708214, KL divergence=0.019319918006658554, Entropy=0.15999773144721985, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02446889318525791, KL divergence=0.02115146815776825, Entropy=0.1587560474872589, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.025205789133906364, KL divergence=0.02333134040236473, Entropy=0.1589924693107605, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.028918901458382607, KL divergence=0.025317784398794174, Entropy=0.1589626520872116, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.031766366213560104, KL divergence=0.0267788153141737, Entropy=0.15872307121753693, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.030037876218557358, KL divergence=0.027507388964295387, Entropy=0.1579228639602661, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03213246911764145, KL divergence=0.02854986861348152, Entropy=0.15783828496932983, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/593_Step-198150.ckpt']
Uploaded 3 files for checkpoint 593 in 0.55 seconds
saved intermediate frozen graph: current/model/model_593.pb
Best checkpoint number: 578, Last checkpoint number: 591
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'590'}
Training> Name=main_level/agent, Worker=0, Episode=1021, Total reward=468.59, Steps=198442, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1022, Total reward=284.54, Steps=198640, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1023, Total reward=212.27, Steps=198831, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1024, Total reward=414.87, Steps=199143, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1025, Total reward=324.29, Steps=199337, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1026, Total reward=544.12, Steps=199619, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1027, Total reward=247.68, Steps=199772, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1028, Total reward=411.72, Steps=200015, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1029, Total reward=477.94, Steps=200310, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1030, Total reward=422.68, Steps=200610, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1031, Total reward=401.27, Steps=200858, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1032, Total reward=131.24, Steps=200940, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1033, Total reward=334.13, Steps=201107, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1034, Total reward=460.47, Steps=201407, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1035, Total reward=331.6, Steps=201653, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1036, Total reward=32.85, Steps=201675, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1037, Total reward=471.42, Steps=201962, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1038, Total reward=330.96, Steps=202194, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1039, Total reward=493.41, Steps=202477, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=1040, Total reward=461.49, Steps=202773, Training iteration=51
Policy training> Surrogate loss=-0.00036477006506174803, KL divergence=0.0006897996063344181, Entropy=0.16024379432201385, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.016318660229444504, KL divergence=0.012767544016242027, Entropy=0.15571169555187225, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020696811378002167, KL divergence=0.01827787049114704, Entropy=0.15340761840343475, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02477153390645981, KL divergence=0.020454861223697662, Entropy=0.1529204249382019, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.027892304584383965, KL divergence=0.021573396399617195, Entropy=0.15123814344406128, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02916683815419674, KL divergence=0.02286672592163086, Entropy=0.15083226561546326, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02816373109817505, KL divergence=0.024120761081576347, Entropy=0.15045055747032166, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.029511816799640656, KL divergence=0.025526907294988632, Entropy=0.14991413056850433, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.028079548850655556, KL divergence=0.02650873176753521, Entropy=0.1496513932943344, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0310394074767828, KL divergence=0.02775554545223713, Entropy=0.14934153854846954, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/594_Step-202773.ckpt']
Uploaded 3 files for checkpoint 594 in 0.47 seconds
saved intermediate frozen graph: current/model/model_594.pb
Best checkpoint number: 578, Last checkpoint number: 592
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'591'}
Training> Name=main_level/agent, Worker=0, Episode=1041, Total reward=273.93, Steps=202991, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1042, Total reward=461.66, Steps=203285, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1043, Total reward=106.57, Steps=203401, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1044, Total reward=244.74, Steps=203568, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1045, Total reward=245.45, Steps=203720, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1046, Total reward=469.67, Steps=204021, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1047, Total reward=213.14, Steps=204158, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1048, Total reward=91.79, Steps=204200, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1049, Total reward=311.62, Steps=204362, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1050, Total reward=237.25, Steps=204502, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1051, Total reward=294.6, Steps=204656, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1052, Total reward=14.83, Steps=204668, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1053, Total reward=179.65, Steps=204780, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1054, Total reward=451.54, Steps=205075, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1055, Total reward=512.74, Steps=205360, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1056, Total reward=381.82, Steps=205655, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1057, Total reward=454.09, Steps=205955, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1058, Total reward=478.62, Steps=206264, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1059, Total reward=390.87, Steps=206524, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=1060, Total reward=94.11, Steps=206573, Training iteration=52
Policy training> Surrogate loss=-0.006142124533653259, KL divergence=0.0008519291877746582, Entropy=0.15065625309944153, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.019827334210276604, KL divergence=0.009845936670899391, Entropy=0.1481344848871231, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.016338014975190163, KL divergence=0.015082514844834805, Entropy=0.14508680999279022, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.018254190683364868, KL divergence=0.018457656726241112, Entropy=0.14543500542640686, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026310043409466743, KL divergence=0.01996973529458046, Entropy=0.1431300938129425, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03507844731211662, KL divergence=0.021274441853165627, Entropy=0.14130032062530518, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.027697263285517693, KL divergence=0.022906597703695297, Entropy=0.14183276891708374, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03241978958249092, KL divergence=0.024499867111444473, Entropy=0.14320562779903412, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.027253031730651855, KL divergence=0.024814773350954056, Entropy=0.1427050083875656, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.025553414598107338, KL divergence=0.026493515819311142, Entropy=0.14120297133922577, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/595_Step-206573.ckpt']
Uploaded 3 files for checkpoint 595 in 0.62 seconds
saved intermediate frozen graph: current/model/model_595.pb
Best checkpoint number: 578, Last checkpoint number: 593
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'592'}
Training> Name=main_level/agent, Worker=0, Episode=1061, Total reward=106.31, Steps=206659, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1062, Total reward=201.82, Steps=206836, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1063, Total reward=17.08, Steps=206883, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1064, Total reward=391.27, Steps=207136, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1065, Total reward=383.42, Steps=207437, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1066, Total reward=113.45, Steps=207525, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1067, Total reward=223.25, Steps=207672, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1068, Total reward=476.91, Steps=207984, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1069, Total reward=264.6, Steps=208116, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1070, Total reward=30.15, Steps=208154, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1071, Total reward=61.07, Steps=208195, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1072, Total reward=419.44, Steps=208475, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1073, Total reward=426.51, Steps=208769, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1074, Total reward=237.66, Steps=208929, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1075, Total reward=275.61, Steps=209075, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1076, Total reward=394.52, Steps=209316, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1077, Total reward=87.36, Steps=209387, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1078, Total reward=462.88, Steps=209692, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1079, Total reward=132.19, Steps=209813, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=1080, Total reward=106.69, Steps=209899, Training iteration=53
Policy training> Surrogate loss=0.0018295482732355595, KL divergence=0.00028011316317133605, Entropy=0.16926340758800507, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01113170012831688, KL divergence=0.009858164004981518, Entropy=0.16825585067272186, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.018054913729429245, KL divergence=0.019191525876522064, Entropy=0.16383402049541473, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.022852079942822456, KL divergence=0.024733632802963257, Entropy=0.16269750893115997, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.031982652842998505, KL divergence=0.027548030018806458, Entropy=0.1613481044769287, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.034847889095544815, KL divergence=0.02946385182440281, Entropy=0.16247735917568207, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03132167086005211, KL divergence=0.03180428966879845, Entropy=0.16296915709972382, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03554835543036461, KL divergence=0.03253569081425667, Entropy=0.16225193440914154, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03713775798678398, KL divergence=0.0339241661131382, Entropy=0.16185465455055237, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.04023207351565361, KL divergence=0.03458106890320778, Entropy=0.16004100441932678, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/596_Step-209899.ckpt']
Uploaded 3 files for checkpoint 596 in 0.47 seconds
saved intermediate frozen graph: current/model/model_596.pb
Best checkpoint number: 578, Last checkpoint number: 594
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'593'}
Training> Name=main_level/agent, Worker=0, Episode=1081, Total reward=467.96, Steps=210198, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1082, Total reward=59.72, Steps=210272, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1083, Total reward=16.97, Steps=210363, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1084, Total reward=0.02, Steps=210381, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1085, Total reward=470.5, Steps=210680, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1086, Total reward=104.63, Steps=210749, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1087, Total reward=139.45, Steps=210832, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1088, Total reward=451.37, Steps=211119, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1089, Total reward=270.12, Steps=211298, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1090, Total reward=321.0, Steps=211516, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1091, Total reward=391.42, Steps=211719, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1092, Total reward=55.0, Steps=211770, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1093, Total reward=510.92, Steps=212057, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1094, Total reward=434.63, Steps=212358, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1095, Total reward=464.27, Steps=212675, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1096, Total reward=397.92, Steps=213009, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1097, Total reward=171.22, Steps=213115, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1098, Total reward=465.31, Steps=213404, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1099, Total reward=300.62, Steps=213608, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=1100, Total reward=249.5, Steps=213795, Training iteration=54
Policy training> Surrogate loss=0.0015824068104848266, KL divergence=0.0007134036859497428, Entropy=0.1525426208972931, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014445751905441284, KL divergence=0.01265748031437397, Entropy=0.14968733489513397, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02409263700246811, KL divergence=0.019188283011317253, Entropy=0.1471351981163025, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.022353501990437508, KL divergence=0.02308746613562107, Entropy=0.14696654677391052, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02584264986217022, KL divergence=0.024830691516399384, Entropy=0.1449923813343048, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.020333921536803246, KL divergence=0.025710536167025566, Entropy=0.14421650767326355, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02666887454688549, KL divergence=0.02667859010398388, Entropy=0.14452120661735535, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.029315300285816193, KL divergence=0.027188533917069435, Entropy=0.14320476353168488, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.029479587450623512, KL divergence=0.027538852766156197, Entropy=0.14347361028194427, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.029751062393188477, KL divergence=0.02875269204378128, Entropy=0.14423659443855286, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/597_Step-213795.ckpt']
Uploaded 3 files for checkpoint 597 in 0.52 seconds
saved intermediate frozen graph: current/model/model_597.pb
Best checkpoint number: 578, Last checkpoint number: 595
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'594'}
Training> Name=main_level/agent, Worker=0, Episode=1101, Total reward=503.43, Steps=214081, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1102, Total reward=280.48, Steps=214288, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1103, Total reward=474.97, Steps=214582, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1104, Total reward=245.66, Steps=214777, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1105, Total reward=0.01, Steps=214791, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1106, Total reward=501.44, Steps=215077, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1107, Total reward=439.32, Steps=215374, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1108, Total reward=421.98, Steps=215621, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1109, Total reward=121.83, Steps=215733, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1110, Total reward=499.77, Steps=216018, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1111, Total reward=464.16, Steps=216316, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1112, Total reward=480.96, Steps=216624, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1113, Total reward=75.01, Steps=216667, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1114, Total reward=422.37, Steps=216931, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1115, Total reward=447.51, Steps=217255, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1116, Total reward=16.96, Steps=217270, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1117, Total reward=192.19, Steps=217364, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1118, Total reward=460.51, Steps=217680, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1119, Total reward=31.07, Steps=217706, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=1120, Total reward=440.88, Steps=218008, Training iteration=55
Policy training> Surrogate loss=-0.0013846675865352154, KL divergence=0.0007492282893508673, Entropy=0.1596859097480774, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01670238934457302, KL divergence=0.009835534729063511, Entropy=0.15853005647659302, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.018127651885151863, KL divergence=0.015166675671935081, Entropy=0.15833118557929993, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02442169189453125, KL divergence=0.01724822446703911, Entropy=0.15607881546020508, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0276082381606102, KL divergence=0.018721986562013626, Entropy=0.15541797876358032, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.027300886809825897, KL divergence=0.01954982429742813, Entropy=0.15356484055519104, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.024981269612908363, KL divergence=0.020769469439983368, Entropy=0.15370342135429382, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030741028487682343, KL divergence=0.02167094126343727, Entropy=0.15246935188770294, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.027270784601569176, KL divergence=0.023208992555737495, Entropy=0.15268102288246155, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.031092416495084763, KL divergence=0.024359405040740967, Entropy=0.1520158052444458, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/598_Step-218008.ckpt']
Uploaded 3 files for checkpoint 598 in 0.55 seconds
saved intermediate frozen graph: current/model/model_598.pb
Best checkpoint number: 578, Last checkpoint number: 596
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'595'}
Training> Name=main_level/agent, Worker=0, Episode=1121, Total reward=446.33, Steps=218299, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1122, Total reward=492.18, Steps=218578, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1123, Total reward=109.72, Steps=218698, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1124, Total reward=7.44, Steps=218741, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1125, Total reward=185.52, Steps=218896, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1126, Total reward=532.23, Steps=219204, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1127, Total reward=229.1, Steps=219348, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1128, Total reward=399.18, Steps=219624, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1129, Total reward=374.91, Steps=219836, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1130, Total reward=435.63, Steps=220150, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1131, Total reward=528.34, Steps=220419, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1132, Total reward=53.03, Steps=220445, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1133, Total reward=467.22, Steps=220740, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1134, Total reward=470.26, Steps=221031, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1135, Total reward=40.34, Steps=221048, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1136, Total reward=416.23, Steps=221346, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1137, Total reward=476.57, Steps=221642, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1138, Total reward=173.09, Steps=221741, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1139, Total reward=449.44, Steps=222067, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=1140, Total reward=310.05, Steps=222307, Training iteration=56
Policy training> Surrogate loss=2.4844484869390726e-05, KL divergence=0.0006121228216215968, Entropy=0.1547105610370636, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.012982936576008797, KL divergence=0.0120382783934474, Entropy=0.1524292379617691, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.017640160396695137, KL divergence=0.017030896618962288, Entropy=0.15104228258132935, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.016877057030797005, KL divergence=0.0196211114525795, Entropy=0.15073968470096588, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.030425040051341057, KL divergence=0.02207915484905243, Entropy=0.1489599198102951, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02040911838412285, KL divergence=0.02310699224472046, Entropy=0.14883875846862793, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.024047141894698143, KL divergence=0.024318354204297066, Entropy=0.1482580006122589, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.028570353984832764, KL divergence=0.02517932467162609, Entropy=0.14922016859054565, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.026673484593629837, KL divergence=0.026420705020427704, Entropy=0.14880287647247314, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.030450165271759033, KL divergence=0.02727241814136505, Entropy=0.14887872338294983, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/599_Step-222307.ckpt']
Uploaded 3 files for checkpoint 599 in 0.53 seconds
saved intermediate frozen graph: current/model/model_599.pb
Best checkpoint number: 578, Last checkpoint number: 597
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'596'}
Training> Name=main_level/agent, Worker=0, Episode=1141, Total reward=454.37, Steps=222596, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1142, Total reward=482.4, Steps=222903, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1143, Total reward=108.56, Steps=223039, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1144, Total reward=170.32, Steps=223226, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1145, Total reward=198.47, Steps=223366, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1146, Total reward=415.32, Steps=223621, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1147, Total reward=170.22, Steps=223709, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1148, Total reward=380.23, Steps=224007, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1149, Total reward=58.89, Steps=224065, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1150, Total reward=473.71, Steps=224373, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1151, Total reward=339.34, Steps=224554, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1152, Total reward=96.49, Steps=224632, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1153, Total reward=33.87, Steps=224644, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1154, Total reward=165.2, Steps=224753, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1155, Total reward=229.82, Steps=224921, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1156, Total reward=482.74, Steps=225219, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1157, Total reward=437.66, Steps=225530, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1158, Total reward=343.33, Steps=225757, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1159, Total reward=416.08, Steps=226041, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=1160, Total reward=465.1, Steps=226341, Training iteration=57
Policy training> Surrogate loss=0.0015370139153674245, KL divergence=0.0006765255820937455, Entropy=0.15594272315502167, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01114039495587349, KL divergence=0.011825342662632465, Entropy=0.15211692452430725, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.0180483628064394, KL divergence=0.01795322634279728, Entropy=0.14998149871826172, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02030210942029953, KL divergence=0.021510077640414238, Entropy=0.14978595077991486, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.030368980020284653, KL divergence=0.023002423346042633, Entropy=0.1486687809228897, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02733607217669487, KL divergence=0.024742450565099716, Entropy=0.1487174779176712, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.026652507483959198, KL divergence=0.02629384770989418, Entropy=0.1473608762025833, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.029642431065440178, KL divergence=0.02688693068921566, Entropy=0.145851269364357, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03178185969591141, KL divergence=0.028718478977680206, Entropy=0.14912296831607819, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0357200987637043, KL divergence=0.029290923848748207, Entropy=0.14780162274837494, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/600_Step-226341.ckpt']
Uploaded 3 files for checkpoint 600 in 0.61 seconds
saved intermediate frozen graph: current/model/model_600.pb
Best checkpoint number: 578, Last checkpoint number: 598
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'597'}
Training> Name=main_level/agent, Worker=0, Episode=1161, Total reward=82.86, Steps=226390, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1162, Total reward=47.69, Steps=226435, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1163, Total reward=17.2, Steps=226456, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1164, Total reward=212.12, Steps=226631, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1165, Total reward=466.06, Steps=226928, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1166, Total reward=446.95, Steps=227212, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1167, Total reward=404.21, Steps=227467, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1168, Total reward=189.33, Steps=227589, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1169, Total reward=419.86, Steps=227871, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1170, Total reward=117.3, Steps=227957, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1171, Total reward=100.57, Steps=228017, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1172, Total reward=9.63, Steps=228029, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1173, Total reward=443.94, Steps=228339, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1174, Total reward=493.72, Steps=228645, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1175, Total reward=344.94, Steps=228905, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1176, Total reward=490.14, Steps=229201, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1177, Total reward=182.75, Steps=229301, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1178, Total reward=395.65, Steps=229562, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1179, Total reward=490.03, Steps=229871, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=1180, Total reward=513.47, Steps=230171, Training iteration=58
Policy training> Surrogate loss=0.0027336529456079006, KL divergence=0.0005417990614660084, Entropy=0.150879368185997, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.010705181397497654, KL divergence=0.012902214191854, Entropy=0.14625774323940277, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.024174952879548073, KL divergence=0.017365802079439163, Entropy=0.14311765134334564, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.014920368790626526, KL divergence=0.020981356501579285, Entropy=0.14420895278453827, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.025074362754821777, KL divergence=0.024097731336951256, Entropy=0.1431247740983963, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.031164133921265602, KL divergence=0.022994132712483406, Entropy=0.14258110523223877, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.031666018068790436, KL divergence=0.02601434849202633, Entropy=0.1466282308101654, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.034983448684215546, KL divergence=0.026524273678660393, Entropy=0.14315111935138702, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.027217337861657143, KL divergence=0.027910063043236732, Entropy=0.14139792323112488, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.028913157060742378, KL divergence=0.027826758101582527, Entropy=0.1419699490070343, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/601_Step-230171.ckpt']
Uploaded 3 files for checkpoint 601 in 0.46 seconds
saved intermediate frozen graph: current/model/model_601.pb
Best checkpoint number: 578, Last checkpoint number: 599
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'598'}
Training> Name=main_level/agent, Worker=0, Episode=1181, Total reward=491.72, Steps=230473, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1182, Total reward=507.4, Steps=230742, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1183, Total reward=449.76, Steps=231061, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1184, Total reward=392.91, Steps=231348, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1185, Total reward=506.6, Steps=231641, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1186, Total reward=126.04, Steps=231718, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1187, Total reward=436.69, Steps=231980, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1188, Total reward=448.34, Steps=232287, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1189, Total reward=455.35, Steps=232602, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1190, Total reward=448.3, Steps=232911, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1191, Total reward=258.81, Steps=233097, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1192, Total reward=486.75, Steps=233405, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1193, Total reward=394.4, Steps=233665, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1194, Total reward=146.21, Steps=233749, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1195, Total reward=76.74, Steps=233797, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1196, Total reward=487.57, Steps=234099, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1197, Total reward=448.35, Steps=234402, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1198, Total reward=365.09, Steps=234674, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1199, Total reward=428.84, Steps=234966, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=1200, Total reward=481.8, Steps=235269, Training iteration=59
Policy training> Surrogate loss=0.002065961714833975, KL divergence=0.0014827016275376081, Entropy=0.15994206070899963, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.018081076443195343, KL divergence=0.010506441816687584, Entropy=0.15518417954444885, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.01831011287868023, KL divergence=0.015021741390228271, Entropy=0.15470793843269348, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.019230986014008522, KL divergence=0.017862996086478233, Entropy=0.1535361111164093, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02016494795680046, KL divergence=0.01946740411221981, Entropy=0.15307481586933136, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.01987369917333126, KL divergence=0.020373672246932983, Entropy=0.15285693109035492, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.024379804730415344, KL divergence=0.02124105393886566, Entropy=0.15254268050193787, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.021002573892474174, KL divergence=0.021766100078821182, Entropy=0.1508146971464157, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.025880854576826096, KL divergence=0.023063817992806435, Entropy=0.1520811766386032, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.026310227811336517, KL divergence=0.02397855371236801, Entropy=0.15178291499614716, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/602_Step-235269.ckpt']
Uploaded 3 files for checkpoint 602 in 0.52 seconds
saved intermediate frozen graph: current/model/model_602.pb
Best checkpoint number: 578, Last checkpoint number: 600
Copying the frozen checkpoint from ./frozen_models/agent/model_578.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'599'}
