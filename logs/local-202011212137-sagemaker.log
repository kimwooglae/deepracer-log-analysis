21:C 20 Nov 2020 10:41:09.529 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
21:C 20 Nov 2020 10:41:09.529 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=21, just started
21:C 20 Nov 2020 10:41:09.529 # Configuration loaded
                _._                                                  
           _.-``__ ''-._                                             
      _.-``    `.  `_.  ''-._           Redis 5.0.8 (00000000/0) 64 bit
  .-`` .-```.  ```\/    _.,_ ''-._                                   
 (    '      ,       .-`  | `,    )     Running in standalone mode
 |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
 |    `-._   `._    /     _.-'    |     PID: 21
  `-._    `-._  `-./  _.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |           http://redis.io        
  `-._    `-._`-.__.-'_.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |                                  
  `-._    `-._`-.__.-'_.-'    _.-'                                   
      `-._    `-.__.-'    _.-'                                       
          `-._        _.-'                                           
              `-.__.-'                                               

21:M 20 Nov 2020 10:41:09.530 # WARNING: The TCP backlog setting of 512 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
21:M 20 Nov 2020 10:41:09.530 # Server initialized
21:M 20 Nov 2020 10:41:09.530 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
21:M 20 Nov 2020 10:41:09.530 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
21:M 20 Nov 2020 10:41:09.530 * Ready to accept connections
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2020-11-20 10:41:11,632 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training
2020-11-20 10:41:11,887 sagemaker-containers INFO     Invoking user script

Training Env:

{
    "additional_framework_parameters": {
        "sagemaker_estimator": "RLEstimator"
    },
    "channel_input_dirs": {},
    "current_host": "algo-1-2j6kx",
    "framework_module": "sagemaker_tensorflow_container.training:main",
    "hosts": [
        "algo-1-2j6kx"
    ],
    "hyperparameters": {
        "s3_bucket": "bucket",
        "s3_prefix": "current",
        "aws_region": "us-east-1",
        "model_metadata_s3_key": "s3://bucket/custom_files/model_metadata.json",
        "RLCOACH_PRESET": "deepracer",
        "batch_size": 512,
        "beta_entropy": 0.01,
        "discount_factor": 0.999,
        "e_greedy_value": 0.05,
        "epsilon_steps": 10000,
        "exploration_type": "categorical",
        "loss_type": "huber",
        "lr": 1e-05,
        "num_episodes_between_training": 50,
        "num_epochs": 10,
        "stack_size": 1,
        "term_cond_avg_score": 100000.0,
        "term_cond_max_episodes": 1000000,
        "pretrained_s3_bucket": "bucket",
        "pretrained_s3_prefix": "rl-deepracer-pretrained"
    },
    "input_config_dir": "/opt/ml/input/config",
    "input_data_config": {},
    "input_dir": "/opt/ml/input",
    "is_master": true,
    "job_name": "current",
    "log_level": 20,
    "master_hostname": "algo-1-2j6kx",
    "model_dir": "/opt/ml/model",
    "module_dir": "s3://bucket/current/source/sourcedir.tar.gz",
    "module_name": "training_worker",
    "network_interface_name": "eth0",
    "num_cpus": 16,
    "num_gpus": 3,
    "output_data_dir": "/opt/ml/output/data",
    "output_dir": "/opt/ml/output",
    "output_intermediate_dir": "/opt/ml/output/intermediate",
    "resource_config": {
        "current_host": "algo-1-2j6kx",
        "hosts": [
            "algo-1-2j6kx"
        ]
    },
    "user_entry_point": "training_worker.py"
}

Environment variables:

SM_HOSTS=["algo-1-2j6kx"]
SM_NETWORK_INTERFACE_NAME=eth0
SM_HPS={"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":512,"beta_entropy":0.01,"discount_factor":0.999,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"huber","lr":1e-05,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":50,"num_epochs":10,"pretrained_s3_bucket":"bucket","pretrained_s3_prefix":"rl-deepracer-pretrained","s3_bucket":"bucket","s3_prefix":"current","stack_size":1,"term_cond_avg_score":100000.0,"term_cond_max_episodes":1000000}
SM_USER_ENTRY_POINT=training_worker.py
SM_FRAMEWORK_PARAMS={"sagemaker_estimator":"RLEstimator"}
SM_RESOURCE_CONFIG={"current_host":"algo-1-2j6kx","hosts":["algo-1-2j6kx"]}
SM_INPUT_DATA_CONFIG={}
SM_OUTPUT_DATA_DIR=/opt/ml/output/data
SM_CHANNELS=[]
SM_CURRENT_HOST=algo-1-2j6kx
SM_MODULE_NAME=training_worker
SM_LOG_LEVEL=20
SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main
SM_INPUT_DIR=/opt/ml/input
SM_INPUT_CONFIG_DIR=/opt/ml/input/config
SM_OUTPUT_DIR=/opt/ml/output
SM_NUM_CPUS=16
SM_NUM_GPUS=3
SM_MODEL_DIR=/opt/ml/model
SM_MODULE_DIR=s3://bucket/current/source/sourcedir.tar.gz
SM_TRAINING_ENV={"additional_framework_parameters":{"sagemaker_estimator":"RLEstimator"},"channel_input_dirs":{},"current_host":"algo-1-2j6kx","framework_module":"sagemaker_tensorflow_container.training:main","hosts":["algo-1-2j6kx"],"hyperparameters":{"RLCOACH_PRESET":"deepracer","aws_region":"us-east-1","batch_size":512,"beta_entropy":0.01,"discount_factor":0.999,"e_greedy_value":0.05,"epsilon_steps":10000,"exploration_type":"categorical","loss_type":"huber","lr":1e-05,"model_metadata_s3_key":"s3://bucket/custom_files/model_metadata.json","num_episodes_between_training":50,"num_epochs":10,"pretrained_s3_bucket":"bucket","pretrained_s3_prefix":"rl-deepracer-pretrained","s3_bucket":"bucket","s3_prefix":"current","stack_size":1,"term_cond_avg_score":100000.0,"term_cond_max_episodes":1000000},"input_config_dir":"/opt/ml/input/config","input_data_config":{},"input_dir":"/opt/ml/input","is_master":true,"job_name":"current","log_level":20,"master_hostname":"algo-1-2j6kx","model_dir":"/opt/ml/model","module_dir":"s3://bucket/current/source/sourcedir.tar.gz","module_name":"training_worker","network_interface_name":"eth0","num_cpus":16,"num_gpus":3,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1-2j6kx","hosts":["algo-1-2j6kx"]},"user_entry_point":"training_worker.py"}
SM_USER_ARGS=["--RLCOACH_PRESET","deepracer","--aws_region","us-east-1","--batch_size","512","--beta_entropy","0.01","--discount_factor","0.999","--e_greedy_value","0.05","--epsilon_steps","10000","--exploration_type","categorical","--loss_type","huber","--lr","1e-05","--model_metadata_s3_key","s3://bucket/custom_files/model_metadata.json","--num_episodes_between_training","50","--num_epochs","10","--pretrained_s3_bucket","bucket","--pretrained_s3_prefix","rl-deepracer-pretrained","--s3_bucket","bucket","--s3_prefix","current","--stack_size","1","--term_cond_avg_score","100000.0","--term_cond_max_episodes","1000000"]
SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate
SM_HP_S3_BUCKET=bucket
SM_HP_S3_PREFIX=current
SM_HP_AWS_REGION=us-east-1
SM_HP_MODEL_METADATA_S3_KEY=s3://bucket/custom_files/model_metadata.json
SM_HP_RLCOACH_PRESET=deepracer
SM_HP_BATCH_SIZE=512
SM_HP_BETA_ENTROPY=0.01
SM_HP_DISCOUNT_FACTOR=0.999
SM_HP_E_GREEDY_VALUE=0.05
SM_HP_EPSILON_STEPS=10000
SM_HP_EXPLORATION_TYPE=categorical
SM_HP_LOSS_TYPE=huber
SM_HP_LR=1e-05
SM_HP_NUM_EPISODES_BETWEEN_TRAINING=50
SM_HP_NUM_EPOCHS=10
SM_HP_STACK_SIZE=1
SM_HP_TERM_COND_AVG_SCORE=100000.0
SM_HP_TERM_COND_MAX_EPISODES=1000000
SM_HP_PRETRAINED_S3_BUCKET=bucket
SM_HP_PRETRAINED_S3_PREFIX=rl-deepracer-pretrained
PYTHONPATH=/usr/local/bin:/opt/amazon:/opt/ml/code:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages

Invoking script with the following command:

/usr/bin/python training_worker.py --RLCOACH_PRESET deepracer --aws_region us-east-1 --batch_size 512 --beta_entropy 0.01 --discount_factor 0.999 --e_greedy_value 0.05 --epsilon_steps 10000 --exploration_type categorical --loss_type huber --lr 1e-05 --model_metadata_s3_key s3://bucket/custom_files/model_metadata.json --num_episodes_between_training 50 --num_epochs 10 --pretrained_s3_bucket bucket --pretrained_s3_prefix rl-deepracer-pretrained --s3_bucket bucket --s3_prefix current --stack_size 1 --term_cond_avg_score 100000.0 --term_cond_max_episodes 1000000


S3 bucket: bucket 
 S3 prefix: current 
 S3 endpoint URL: http://minio:9000
Initializing SageS3Client...
Successfully downloaded model metadata from custom_files/model_metadata.json.
Sensor list ['STEREO_CAMERAS', 'SECTOR_LIDAR'], network DEEP_CONVOLUTIONAL_NETWORK_SHALLOW, simapp_version 2.0
Loaded action space from file: [{'steering_angle': -30.0, 'speed': 1.5, 'index': 0}, {'steering_angle': -13.7481, 'speed': 2.016, 'index': 1}, {'steering_angle': -4.0046, 'speed': 1.736, 'index': 2}, {'steering_angle': -2.6994, 'speed': 3.5673, 'index': 3}, {'steering_angle': -1.0391, 'speed': 4.794, 'index': 4}, {'steering_angle': -0.9404, 'speed': 4.1988, 'index': 5}, {'steering_angle': 2.9901, 'speed': 2.9986, 'index': 6}, {'steering_angle': 3.7003, 'speed': 2.3287, 'index': 7}, {'steering_angle': 11.9654, 'speed': 1.8113, 'index': 8}, {'steering_angle': 30.0, 'speed': 1.5, 'index': 9}]
Using the following hyper-parameters
{
  "batch_size": 512,
  "beta_entropy": 0.01,
  "discount_factor": 0.999,
  "e_greedy_value": 0.05,
  "epsilon_steps": 10000,
  "exploration_type": "categorical",
  "loss_type": "huber",
  "lr": 1e-05,
  "num_episodes_between_training": 50,
  "num_epochs": 10,
  "stack_size": 1,
  "term_cond_avg_score": 100000.0,
  "term_cond_max_episodes": 1000000
}
Uploaded hyperparameters.json to S3
Uploaded IP address information to S3: 172.18.0.5
Sensor list ['STEREO_CAMERAS', 'SECTOR_LIDAR'], network DEEP_CONVOLUTIONAL_NETWORK_SHALLOW, simapp_version 2.0
Unable to find best model data, using last model
## Creating graph - name: MultiAgentGraphManager
## Start physics before creating graph
## Create graph
## Creating agent - name: agent
## Created agent: agent
## Stop physics after creating graph
## Creating session
Checkpoint> Restoring from path=./pretrained_checkpoint/431_Step-726843.ckpt
Checkpoint> Saving in path=['./checkpoint/432_Step-0.ckpt']
Uploaded 3 files for checkpoint 432 in 0.40 seconds
saved intermediate frozen graph: current/model/model_432.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_432.pb to /opt/ml/model/agent/model.pb.
Uploaded 3 files for checkpoint 432 in 0.59 seconds
saved intermediate frozen graph: current/model/model_432.pb
Unable to find best model data, using last model
Unable to find the best checkpoint number. Getting the last checkpoint number
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Unable to find best model data, using last model
Unable to find the last checkpoint number.
Best checkpoint number: -1, Last checkpoint number: -1
Copying the frozen checkpoint from ./frozen_models/agent/model_432.pb to /opt/ml/model/agent/model.pb.
DoorMan: installing SIGINT, SIGTERM
Training> Name=main_level/agent, Worker=0, Episode=1, Total reward=126.66, Steps=73, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=2, Total reward=52.26, Steps=104, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=3, Total reward=23.7, Steps=148, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=4, Total reward=131.5, Steps=240, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=5, Total reward=9.86, Steps=266, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=6, Total reward=132.91, Steps=393, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=7, Total reward=125.11, Steps=486, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=8, Total reward=91.56, Steps=531, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=9, Total reward=29.48, Steps=550, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=10, Total reward=11.92, Steps=607, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=11, Total reward=316.5, Steps=810, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=12, Total reward=114.35, Steps=888, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=13, Total reward=102.01, Steps=940, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=14, Total reward=59.85, Steps=988, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=15, Total reward=47.03, Steps=1034, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=16, Total reward=67.61, Steps=1092, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=17, Total reward=98.44, Steps=1151, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=18, Total reward=130.43, Steps=1202, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=19, Total reward=165.77, Steps=1368, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=20, Total reward=110.07, Steps=1436, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=21, Total reward=78.74, Steps=1473, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=22, Total reward=41.56, Steps=1516, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=23, Total reward=25.83, Steps=1549, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=24, Total reward=107.85, Steps=1634, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=25, Total reward=42.53, Steps=1690, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=26, Total reward=146.33, Steps=1796, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=27, Total reward=93.28, Steps=1862, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=28, Total reward=164.03, Steps=1936, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=29, Total reward=52.4, Steps=1988, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=30, Total reward=81.0, Steps=2064, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=31, Total reward=147.16, Steps=2173, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=32, Total reward=129.75, Steps=2234, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=33, Total reward=298.78, Steps=2416, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=34, Total reward=69.31, Steps=2454, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=35, Total reward=189.64, Steps=2577, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=36, Total reward=205.08, Steps=2675, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=37, Total reward=30.59, Steps=2700, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=38, Total reward=33.12, Steps=2725, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=39, Total reward=45.81, Steps=2769, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=40, Total reward=187.88, Steps=2986, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=41, Total reward=73.58, Steps=3035, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=42, Total reward=48.03, Steps=3066, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=43, Total reward=89.45, Steps=3183, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=44, Total reward=31.59, Steps=3247, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=45, Total reward=3.63, Steps=3297, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=46, Total reward=144.74, Steps=3411, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=47, Total reward=109.15, Steps=3529, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=48, Total reward=127.51, Steps=3638, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=49, Total reward=35.78, Steps=3667, Training iteration=0
Training> Name=main_level/agent, Worker=0, Episode=50, Total reward=101.54, Steps=3737, Training iteration=0
Policy training> Surrogate loss=0.00014884237316437066, KL divergence=0.00014480990648735315, Entropy=0.20714955031871796, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.00820095743983984, KL divergence=0.0013605087297037244, Entropy=0.2071031630039215, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020924223586916924, KL divergence=0.003033423563465476, Entropy=0.20750018954277039, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.020650433376431465, KL divergence=0.004784524440765381, Entropy=0.20810413360595703, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.030910301953554153, KL divergence=0.0065053231082856655, Entropy=0.2066866010427475, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03428522124886513, KL divergence=0.008304057642817497, Entropy=0.2051059454679489, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03582281991839409, KL divergence=0.010002115741372108, Entropy=0.2060547024011612, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.0353381521999836, KL divergence=0.011407081969082355, Entropy=0.2061227709054947, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.035882461816072464, KL divergence=0.012820416130125523, Entropy=0.20704880356788635, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.041545379906892776, KL divergence=0.014046167954802513, Entropy=0.20496979355812073, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/433_Step-3737.ckpt']
Uploaded 3 files for checkpoint 433 in 0.55 seconds
saved intermediate frozen graph: current/model/model_433.pb
Best checkpoint number: 432, Last checkpoint number: 432
Copying the frozen checkpoint from ./frozen_models/agent/model_432.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=51, Total reward=316.02, Steps=3934, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=52, Total reward=268.19, Steps=4082, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=53, Total reward=48.41, Steps=4111, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=54, Total reward=136.33, Steps=4187, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=55, Total reward=128.29, Steps=4263, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=56, Total reward=92.95, Steps=4326, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=57, Total reward=368.29, Steps=4581, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=58, Total reward=123.81, Steps=4660, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=59, Total reward=26.37, Steps=4686, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=60, Total reward=275.11, Steps=4900, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=61, Total reward=88.56, Steps=4947, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=62, Total reward=61.06, Steps=5009, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=63, Total reward=8.04, Steps=5037, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=64, Total reward=25.81, Steps=5095, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=65, Total reward=26.03, Steps=5134, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=66, Total reward=132.49, Steps=5258, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=67, Total reward=96.56, Steps=5325, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=68, Total reward=176.6, Steps=5417, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=69, Total reward=93.83, Steps=5484, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=70, Total reward=3.79, Steps=5498, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=71, Total reward=49.5, Steps=5551, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=72, Total reward=236.83, Steps=5704, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=73, Total reward=106.43, Steps=5769, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=74, Total reward=102.1, Steps=5836, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=75, Total reward=137.33, Steps=5926, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=76, Total reward=183.71, Steps=6048, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=77, Total reward=146.5, Steps=6137, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=78, Total reward=54.72, Steps=6167, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=79, Total reward=101.98, Steps=6237, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=80, Total reward=88.16, Steps=6287, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=81, Total reward=99.86, Steps=6386, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=82, Total reward=286.64, Steps=6600, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=83, Total reward=175.13, Steps=6741, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=84, Total reward=20.36, Steps=6772, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=85, Total reward=160.19, Steps=6889, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=86, Total reward=158.45, Steps=7042, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=87, Total reward=127.41, Steps=7131, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=88, Total reward=133.38, Steps=7237, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=89, Total reward=33.99, Steps=7277, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=90, Total reward=235.63, Steps=7437, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=91, Total reward=104.59, Steps=7525, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=92, Total reward=104.35, Steps=7594, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=93, Total reward=106.04, Steps=7649, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=94, Total reward=192.07, Steps=7775, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=95, Total reward=84.12, Steps=7817, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=96, Total reward=191.72, Steps=7987, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=97, Total reward=159.72, Steps=8125, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=98, Total reward=253.7, Steps=8359, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=99, Total reward=98.49, Steps=8442, Training iteration=1
Training> Name=main_level/agent, Worker=0, Episode=100, Total reward=39.56, Steps=8474, Training iteration=1
Policy training> Surrogate loss=0.0014332056744024158, KL divergence=8.058384992182255e-05, Entropy=0.20866115391254425, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01653866656124592, KL divergence=0.0014750887639820576, Entropy=0.20731164515018463, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.022020546719431877, KL divergence=0.003971089143306017, Entropy=0.20548224449157715, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.027947522699832916, KL divergence=0.006434613838791847, Entropy=0.20441372692584991, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02508816123008728, KL divergence=0.008413595147430897, Entropy=0.20365644991397858, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03137407451868057, KL divergence=0.010213758796453476, Entropy=0.2047014683485031, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03397227078676224, KL divergence=0.011659611016511917, Entropy=0.20418313145637512, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02631010301411152, KL divergence=0.013105296529829502, Entropy=0.2037162184715271, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.034517109394073486, KL divergence=0.014276428148150444, Entropy=0.2042887806892395, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.030749648809432983, KL divergence=0.015114005655050278, Entropy=0.20439141988754272, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/434_Step-8474.ckpt']
Uploaded 3 files for checkpoint 434 in 0.64 seconds
saved intermediate frozen graph: current/model/model_434.pb
Best checkpoint number: 432, Last checkpoint number: 432
Copying the frozen checkpoint from ./frozen_models/agent/model_432.pb to /opt/ml/model/agent/model.pb.
Training> Name=main_level/agent, Worker=0, Episode=101, Total reward=87.7, Steps=8529, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=102, Total reward=55.2, Steps=8571, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=103, Total reward=39.42, Steps=8625, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=104, Total reward=9.39, Steps=8642, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=105, Total reward=77.33, Steps=8775, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=106, Total reward=31.76, Steps=8812, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=107, Total reward=172.76, Steps=8937, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=108, Total reward=174.52, Steps=9043, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=109, Total reward=92.31, Steps=9110, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=110, Total reward=24.97, Steps=9142, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=111, Total reward=175.65, Steps=9249, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=112, Total reward=11.52, Steps=9260, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=113, Total reward=205.32, Steps=9385, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=114, Total reward=166.46, Steps=9487, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=115, Total reward=69.15, Steps=9527, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=116, Total reward=365.46, Steps=9807, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=117, Total reward=269.31, Steps=10030, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=118, Total reward=143.83, Steps=10186, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=119, Total reward=113.58, Steps=10267, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=120, Total reward=22.48, Steps=10284, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=121, Total reward=108.19, Steps=10368, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=122, Total reward=47.99, Steps=10399, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=123, Total reward=16.2, Steps=10456, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=124, Total reward=136.19, Steps=10569, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=125, Total reward=183.41, Steps=10734, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=126, Total reward=92.11, Steps=10807, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=127, Total reward=162.7, Steps=10889, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=128, Total reward=21.17, Steps=10901, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=129, Total reward=273.07, Steps=11149, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=130, Total reward=94.02, Steps=11228, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=131, Total reward=79.9, Steps=11282, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=132, Total reward=322.67, Steps=11488, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=133, Total reward=431.47, Steps=11796, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=134, Total reward=152.28, Steps=11910, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=135, Total reward=30.73, Steps=11926, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=136, Total reward=59.18, Steps=11988, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=137, Total reward=85.86, Steps=12039, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=138, Total reward=242.45, Steps=12238, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=139, Total reward=19.24, Steps=12260, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=140, Total reward=269.75, Steps=12428, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=141, Total reward=75.15, Steps=12473, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=142, Total reward=41.87, Steps=12501, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=143, Total reward=13.82, Steps=12518, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=144, Total reward=39.37, Steps=12582, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=145, Total reward=81.25, Steps=12672, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=146, Total reward=136.48, Steps=12814, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=147, Total reward=136.97, Steps=12906, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=148, Total reward=120.74, Steps=12992, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=149, Total reward=25.41, Steps=13032, Training iteration=2
Training> Name=main_level/agent, Worker=0, Episode=150, Total reward=79.29, Steps=13117, Training iteration=2
Policy training> Surrogate loss=0.0006874791579321027, KL divergence=0.00010487694817129523, Entropy=0.19533585011959076, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014554683119058609, KL divergence=0.0022722934372723103, Entropy=0.19492103159427643, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.022197740152478218, KL divergence=0.004838143941015005, Entropy=0.19312334060668945, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025248495861887932, KL divergence=0.007404305972158909, Entropy=0.19212210178375244, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026349730789661407, KL divergence=0.009592873044312, Entropy=0.19199341535568237, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030923660844564438, KL divergence=0.011325638741254807, Entropy=0.19114091992378235, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029216796159744263, KL divergence=0.01263637375086546, Entropy=0.19078345596790314, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.032339487224817276, KL divergence=0.013808769173920155, Entropy=0.1907622516155243, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.032727960497140884, KL divergence=0.014850063249468803, Entropy=0.19077631831169128, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03390202298760414, KL divergence=0.01565556600689888, Entropy=0.1910470724105835, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/435_Step-13117.ckpt']
Uploaded 3 files for checkpoint 435 in 0.53 seconds
saved intermediate frozen graph: current/model/model_435.pb
Best checkpoint number: 433, Last checkpoint number: 433
Copying the frozen checkpoint from ./frozen_models/agent/model_433.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'432'}
Training> Name=main_level/agent, Worker=0, Episode=151, Total reward=187.18, Steps=13225, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=152, Total reward=134.26, Steps=13308, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=153, Total reward=31.87, Steps=13320, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=154, Total reward=300.27, Steps=13581, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=155, Total reward=164.24, Steps=13662, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=156, Total reward=174.9, Steps=13829, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=157, Total reward=21.68, Steps=13850, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=158, Total reward=161.7, Steps=13933, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=159, Total reward=28.9, Steps=13950, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=160, Total reward=109.77, Steps=14050, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=161, Total reward=74.73, Steps=14093, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=162, Total reward=48.51, Steps=14129, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=163, Total reward=98.92, Steps=14251, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=164, Total reward=12.04, Steps=14282, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=165, Total reward=207.03, Steps=14449, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=166, Total reward=112.52, Steps=14569, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=167, Total reward=94.32, Steps=14622, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=168, Total reward=135.38, Steps=14712, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=169, Total reward=33.39, Steps=14749, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=170, Total reward=133.87, Steps=14845, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=171, Total reward=7.56, Steps=14859, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=172, Total reward=16.09, Steps=14872, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=173, Total reward=111.11, Steps=14940, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=174, Total reward=44.8, Steps=14969, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=175, Total reward=49.27, Steps=15021, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=176, Total reward=112.86, Steps=15091, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=177, Total reward=46.04, Steps=15131, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=178, Total reward=65.61, Steps=15172, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=179, Total reward=71.88, Steps=15278, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=180, Total reward=89.22, Steps=15423, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=181, Total reward=62.56, Steps=15484, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=182, Total reward=68.88, Steps=15568, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=183, Total reward=13.17, Steps=15592, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=184, Total reward=91.7, Steps=15698, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=185, Total reward=215.07, Steps=15887, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=186, Total reward=119.64, Steps=16001, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=187, Total reward=226.42, Steps=16121, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=188, Total reward=125.44, Steps=16244, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=189, Total reward=127.7, Steps=16327, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=190, Total reward=138.08, Steps=16412, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=191, Total reward=195.89, Steps=16540, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=192, Total reward=341.72, Steps=16728, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=193, Total reward=106.63, Steps=16780, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=194, Total reward=110.9, Steps=16876, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=195, Total reward=197.83, Steps=17054, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=196, Total reward=86.84, Steps=17146, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=197, Total reward=259.72, Steps=17379, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=198, Total reward=53.64, Steps=17424, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=199, Total reward=122.57, Steps=17499, Training iteration=3
Training> Name=main_level/agent, Worker=0, Episode=200, Total reward=28.96, Steps=17519, Training iteration=3
Policy training> Surrogate loss=0.00023246300406754017, KL divergence=5.3806594223715365e-05, Entropy=0.1872207224369049, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015446033328771591, KL divergence=0.0018880030838772655, Entropy=0.18743355572223663, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.014567248523235321, KL divergence=0.005640150047838688, Entropy=0.1897163987159729, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.024421919137239456, KL divergence=0.009070896543562412, Entropy=0.1856124997138977, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02677493542432785, KL divergence=0.011661913245916367, Entropy=0.18790042400360107, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03288795426487923, KL divergence=0.013086935505270958, Entropy=0.18804074823856354, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03360077738761902, KL divergence=0.014821779914200306, Entropy=0.19007249176502228, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03429242968559265, KL divergence=0.015620177611708641, Entropy=0.18794599175453186, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03317996487021446, KL divergence=0.01697237603366375, Entropy=0.1884903460741043, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.04187409207224846, KL divergence=0.017501911148428917, Entropy=0.1884326934814453, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/436_Step-17519.ckpt']
Uploaded 3 files for checkpoint 436 in 0.50 seconds
saved intermediate frozen graph: current/model/model_436.pb
Best checkpoint number: 433, Last checkpoint number: 434
Copying the frozen checkpoint from ./frozen_models/agent/model_433.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'432'}
Training> Name=main_level/agent, Worker=0, Episode=201, Total reward=67.83, Steps=17552, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=202, Total reward=386.48, Steps=17836, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=203, Total reward=14.29, Steps=17875, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=204, Total reward=145.63, Steps=18007, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=205, Total reward=76.33, Steps=18094, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=206, Total reward=25.71, Steps=18129, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=207, Total reward=5.15, Steps=18143, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=208, Total reward=98.46, Steps=18212, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=209, Total reward=44.31, Steps=18239, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=210, Total reward=242.93, Steps=18423, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=211, Total reward=58.62, Steps=18471, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=212, Total reward=62.4, Steps=18528, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=213, Total reward=292.39, Steps=18739, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=214, Total reward=91.54, Steps=18788, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=215, Total reward=136.35, Steps=18876, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=216, Total reward=83.03, Steps=18978, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=217, Total reward=31.37, Steps=19033, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=218, Total reward=133.43, Steps=19129, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=219, Total reward=43.31, Steps=19160, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=220, Total reward=272.21, Steps=19352, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=221, Total reward=60.84, Steps=19414, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=222, Total reward=49.31, Steps=19447, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=223, Total reward=15.52, Steps=19468, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=224, Total reward=17.51, Steps=19490, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=225, Total reward=428.85, Steps=19769, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=226, Total reward=393.31, Steps=20027, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=227, Total reward=179.3, Steps=20136, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=228, Total reward=220.53, Steps=20247, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=229, Total reward=20.9, Steps=20263, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=230, Total reward=210.95, Steps=20395, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=231, Total reward=37.58, Steps=20452, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=232, Total reward=211.09, Steps=20577, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=233, Total reward=5.86, Steps=20591, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=234, Total reward=61.23, Steps=20641, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=235, Total reward=23.58, Steps=20654, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=236, Total reward=197.52, Steps=20803, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=237, Total reward=179.99, Steps=20927, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=238, Total reward=147.59, Steps=21024, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=239, Total reward=230.18, Steps=21231, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=240, Total reward=141.06, Steps=21402, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=241, Total reward=68.9, Steps=21452, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=242, Total reward=21.03, Steps=21492, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=243, Total reward=112.33, Steps=21616, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=244, Total reward=3.71, Steps=21628, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=245, Total reward=2.06, Steps=21658, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=246, Total reward=399.87, Steps=21967, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=247, Total reward=221.51, Steps=22099, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=248, Total reward=173.74, Steps=22206, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=249, Total reward=59.52, Steps=22243, Training iteration=4
Training> Name=main_level/agent, Worker=0, Episode=250, Total reward=222.52, Steps=22411, Training iteration=4
Policy training> Surrogate loss=-0.0019196253269910812, KL divergence=0.00016310758655890822, Entropy=0.19970789551734924, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015225958079099655, KL divergence=0.0030342834070324898, Entropy=0.20041584968566895, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.01691492274403572, KL divergence=0.006982402876019478, Entropy=0.19750139117240906, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02671649307012558, KL divergence=0.01015583984553814, Entropy=0.19780026376247406, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028095241636037827, KL divergence=0.01216941699385643, Entropy=0.19639301300048828, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.028854509815573692, KL divergence=0.012925229035317898, Entropy=0.1948535293340683, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03699086233973503, KL divergence=0.014208244159817696, Entropy=0.1945241093635559, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.0329589769244194, KL divergence=0.015439172275364399, Entropy=0.19593745470046997, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.034013696014881134, KL divergence=0.01584877260029316, Entropy=0.19438757002353668, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.031181564554572105, KL divergence=0.016944004222750664, Entropy=0.19532252848148346, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/437_Step-22411.ckpt']
Uploaded 3 files for checkpoint 437 in 0.55 seconds
saved intermediate frozen graph: current/model/model_437.pb
Best checkpoint number: 433, Last checkpoint number: 435
Copying the frozen checkpoint from ./frozen_models/agent/model_433.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'434'}
Training> Name=main_level/agent, Worker=0, Episode=251, Total reward=83.59, Steps=22480, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=252, Total reward=190.46, Steps=22562, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=253, Total reward=33.34, Steps=22572, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=254, Total reward=384.48, Steps=22860, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=255, Total reward=117.56, Steps=22919, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=256, Total reward=219.53, Steps=23032, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=257, Total reward=124.77, Steps=23130, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=258, Total reward=74.56, Steps=23181, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=259, Total reward=291.96, Steps=23475, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=260, Total reward=64.29, Steps=23526, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=261, Total reward=76.14, Steps=23569, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=262, Total reward=59.8, Steps=23652, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=263, Total reward=13.44, Steps=23715, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=264, Total reward=13.96, Steps=23751, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=265, Total reward=2.92, Steps=23776, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=266, Total reward=171.82, Steps=23895, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=267, Total reward=260.2, Steps=24133, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=268, Total reward=151.79, Steps=24227, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=269, Total reward=142.97, Steps=24340, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=270, Total reward=120.5, Steps=24423, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=271, Total reward=81.04, Steps=24486, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=272, Total reward=383.1, Steps=24685, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=273, Total reward=76.97, Steps=24741, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=274, Total reward=74.94, Steps=24791, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=275, Total reward=24.5, Steps=24804, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=276, Total reward=99.03, Steps=24875, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=277, Total reward=171.41, Steps=24975, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=278, Total reward=72.03, Steps=25022, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=279, Total reward=127.46, Steps=25117, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=280, Total reward=127.25, Steps=25226, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=281, Total reward=90.31, Steps=25304, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=282, Total reward=238.32, Steps=25491, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=283, Total reward=3.05, Steps=25516, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=284, Total reward=157.59, Steps=25669, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=285, Total reward=327.46, Steps=25890, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=286, Total reward=328.03, Steps=26114, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=287, Total reward=154.54, Steps=26203, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=288, Total reward=115.98, Steps=26293, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=289, Total reward=40.87, Steps=26323, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=290, Total reward=133.89, Steps=26435, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=291, Total reward=40.78, Steps=26493, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=292, Total reward=97.16, Steps=26551, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=293, Total reward=107.98, Steps=26614, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=294, Total reward=17.2, Steps=26636, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=295, Total reward=132.39, Steps=26704, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=296, Total reward=184.56, Steps=26810, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=297, Total reward=34.73, Steps=26829, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=298, Total reward=154.44, Steps=26920, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=299, Total reward=51.25, Steps=26972, Training iteration=5
Training> Name=main_level/agent, Worker=0, Episode=300, Total reward=215.29, Steps=27130, Training iteration=5
Policy training> Surrogate loss=-0.003814689815044403, KL divergence=0.0001318245631409809, Entropy=0.20028533041477203, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.02328008972108364, KL divergence=0.003470444120466709, Entropy=0.19647905230522156, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.021737268194556236, KL divergence=0.007323017343878746, Entropy=0.19522994756698608, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.028799006715416908, KL divergence=0.00974113866686821, Entropy=0.194588765501976, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02869497798383236, KL divergence=0.01174879353493452, Entropy=0.19633449614048004, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02997329831123352, KL divergence=0.013422961346805096, Entropy=0.19395074248313904, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.033238496631383896, KL divergence=0.014842758886516094, Entropy=0.19426840543746948, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.035674817860126495, KL divergence=0.016488254070281982, Entropy=0.19520345330238342, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.038542959839105606, KL divergence=0.017750972881913185, Entropy=0.19478648900985718, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.039972297847270966, KL divergence=0.018718400970101357, Entropy=0.19283491373062134, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/438_Step-27130.ckpt']
Uploaded 3 files for checkpoint 438 in 0.50 seconds
saved intermediate frozen graph: current/model/model_438.pb
Best checkpoint number: 433, Last checkpoint number: 436
Copying the frozen checkpoint from ./frozen_models/agent/model_433.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'435'}
Training> Name=main_level/agent, Worker=0, Episode=301, Total reward=302.17, Steps=27352, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=302, Total reward=41.19, Steps=27401, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=303, Total reward=100.13, Steps=27522, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=304, Total reward=8.87, Steps=27547, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=305, Total reward=202.38, Steps=27700, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=306, Total reward=88.57, Steps=27782, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=307, Total reward=177.24, Steps=27897, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=308, Total reward=206.28, Steps=28010, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=309, Total reward=99.68, Steps=28118, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=310, Total reward=113.55, Steps=28225, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=311, Total reward=62.14, Steps=28261, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=312, Total reward=96.75, Steps=28315, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=313, Total reward=121.69, Steps=28374, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=314, Total reward=206.64, Steps=28493, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=315, Total reward=96.31, Steps=28534, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=316, Total reward=43.04, Steps=28563, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=317, Total reward=185.27, Steps=28677, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=318, Total reward=151.35, Steps=28760, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=319, Total reward=91.52, Steps=28856, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=320, Total reward=120.59, Steps=28950, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=321, Total reward=32.07, Steps=28979, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=322, Total reward=64.83, Steps=29022, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=323, Total reward=29.81, Steps=29068, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=324, Total reward=74.01, Steps=29156, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=325, Total reward=105.71, Steps=29289, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=326, Total reward=115.05, Steps=29357, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=327, Total reward=97.33, Steps=29430, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=328, Total reward=124.94, Steps=29507, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=329, Total reward=137.04, Steps=29611, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=330, Total reward=13.87, Steps=29642, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=331, Total reward=355.42, Steps=29887, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=332, Total reward=152.2, Steps=29968, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=333, Total reward=111.27, Steps=30032, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=334, Total reward=63.69, Steps=30078, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=335, Total reward=22.39, Steps=30092, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=336, Total reward=55.44, Steps=30139, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=337, Total reward=304.9, Steps=30395, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=338, Total reward=47.82, Steps=30438, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=339, Total reward=265.38, Steps=30647, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=340, Total reward=104.58, Steps=30712, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=341, Total reward=46.12, Steps=30745, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=342, Total reward=59.84, Steps=30809, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=343, Total reward=157.0, Steps=30959, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=344, Total reward=9.17, Steps=30979, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=345, Total reward=133.0, Steps=31058, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=346, Total reward=49.78, Steps=31100, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=347, Total reward=160.52, Steps=31190, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=348, Total reward=162.51, Steps=31296, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=349, Total reward=53.29, Steps=31383, Training iteration=6
Training> Name=main_level/agent, Worker=0, Episode=350, Total reward=268.63, Steps=31594, Training iteration=6
Policy training> Surrogate loss=0.0012729214504361153, KL divergence=0.00010770301742013544, Entropy=0.18891990184783936, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014121359214186668, KL divergence=0.0022746457252651453, Entropy=0.1888560801744461, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.033379681408405304, KL divergence=0.005899852141737938, Entropy=0.18508118391036987, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026750482618808746, KL divergence=0.009245477616786957, Entropy=0.18514947593212128, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028558414429426193, KL divergence=0.01184951514005661, Entropy=0.1860320121049881, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.027698034420609474, KL divergence=0.013792857527732849, Entropy=0.18398639559745789, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03531862050294876, KL divergence=0.015189062803983688, Entropy=0.18215183913707733, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03726527467370033, KL divergence=0.016218991950154305, Entropy=0.1830783635377884, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03212558478116989, KL divergence=0.017428122460842133, Entropy=0.18470342457294464, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02881443314254284, KL divergence=0.017807938158512115, Entropy=0.18554899096488953, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/439_Step-31594.ckpt']
Uploaded 3 files for checkpoint 439 in 0.59 seconds
saved intermediate frozen graph: current/model/model_439.pb
Best checkpoint number: 437, Last checkpoint number: 437
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'436'}
Training> Name=main_level/agent, Worker=0, Episode=351, Total reward=114.96, Steps=31664, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=352, Total reward=103.6, Steps=31743, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=353, Total reward=220.04, Steps=31856, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=354, Total reward=69.53, Steps=31904, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=355, Total reward=18.37, Steps=31917, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=356, Total reward=379.22, Steps=32198, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=357, Total reward=97.06, Steps=32262, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=358, Total reward=69.98, Steps=32314, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=359, Total reward=28.71, Steps=32348, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=360, Total reward=90.48, Steps=32398, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=361, Total reward=24.26, Steps=32414, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=362, Total reward=67.71, Steps=32477, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=363, Total reward=28.39, Steps=32530, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=364, Total reward=22.59, Steps=32563, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=365, Total reward=225.67, Steps=32741, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=366, Total reward=117.19, Steps=32836, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=367, Total reward=80.14, Steps=32908, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=368, Total reward=87.8, Steps=32954, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=369, Total reward=120.57, Steps=33050, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=370, Total reward=15.07, Steps=33084, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=371, Total reward=191.12, Steps=33179, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=372, Total reward=14.63, Steps=33222, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=373, Total reward=191.51, Steps=33323, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=374, Total reward=155.97, Steps=33435, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=375, Total reward=89.12, Steps=33476, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=376, Total reward=114.28, Steps=33552, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=377, Total reward=350.32, Steps=33835, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=378, Total reward=59.91, Steps=33887, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=379, Total reward=22.58, Steps=33912, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=380, Total reward=121.04, Steps=33996, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=381, Total reward=86.49, Steps=34053, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=382, Total reward=39.86, Steps=34095, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=383, Total reward=95.65, Steps=34221, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=384, Total reward=18.39, Steps=34276, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=385, Total reward=273.26, Steps=34413, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=386, Total reward=159.03, Steps=34549, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=387, Total reward=169.83, Steps=34680, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=388, Total reward=89.56, Steps=34735, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=389, Total reward=130.81, Steps=34825, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=390, Total reward=65.74, Steps=34899, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=391, Total reward=61.09, Steps=34952, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=392, Total reward=17.39, Steps=34964, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=393, Total reward=78.9, Steps=35005, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=394, Total reward=67.7, Steps=35045, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=395, Total reward=42.65, Steps=35063, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=396, Total reward=184.72, Steps=35178, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=397, Total reward=252.03, Steps=35390, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=398, Total reward=43.79, Steps=35417, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=399, Total reward=121.31, Steps=35499, Training iteration=7
Training> Name=main_level/agent, Worker=0, Episode=400, Total reward=182.63, Steps=35653, Training iteration=7
Policy training> Surrogate loss=0.0018578035524114966, KL divergence=4.025813905172981e-05, Entropy=0.20325763523578644, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.00958618801087141, KL divergence=0.0019749405328184366, Entropy=0.20242071151733398, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.014777449890971184, KL divergence=0.005215723533183336, Entropy=0.20134620368480682, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026194514706730843, KL divergence=0.008584633469581604, Entropy=0.19918224215507507, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028277648612856865, KL divergence=0.011796141043305397, Entropy=0.19999971985816956, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.036406010389328, KL divergence=0.013740741647779942, Entropy=0.19920627772808075, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.023116542026400566, KL divergence=0.015684053301811218, Entropy=0.1983075588941574, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03300565108656883, KL divergence=0.01751662604510784, Entropy=0.20071575045585632, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.039719972759485245, KL divergence=0.01850561983883381, Entropy=0.19696113467216492, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03374682739377022, KL divergence=0.019391072914004326, Entropy=0.19963470101356506, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/440_Step-35653.ckpt']
Uploaded 3 files for checkpoint 440 in 0.54 seconds
saved intermediate frozen graph: current/model/model_440.pb
Best checkpoint number: 437, Last checkpoint number: 438
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'433'}
Training> Name=main_level/agent, Worker=0, Episode=401, Total reward=222.05, Steps=35837, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=402, Total reward=87.74, Steps=35956, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=403, Total reward=108.34, Steps=36081, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=404, Total reward=0.02, Steps=36098, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=405, Total reward=126.35, Steps=36180, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=406, Total reward=10.49, Steps=36193, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=407, Total reward=97.73, Steps=36259, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=408, Total reward=112.15, Steps=36328, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=409, Total reward=180.53, Steps=36451, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=410, Total reward=181.74, Steps=36592, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=411, Total reward=59.58, Steps=36650, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=412, Total reward=14.29, Steps=36662, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=413, Total reward=91.46, Steps=36720, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=414, Total reward=64.27, Steps=36773, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=415, Total reward=454.51, Steps=37060, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=416, Total reward=29.56, Steps=37081, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=417, Total reward=90.11, Steps=37129, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=418, Total reward=77.94, Steps=37165, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=419, Total reward=232.04, Steps=37363, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=420, Total reward=96.07, Steps=37437, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=421, Total reward=70.09, Steps=37493, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=422, Total reward=46.35, Steps=37528, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=423, Total reward=12.18, Steps=37591, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=424, Total reward=22.64, Steps=37624, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=425, Total reward=36.07, Steps=37682, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=426, Total reward=165.44, Steps=37835, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=427, Total reward=146.52, Steps=37932, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=428, Total reward=107.79, Steps=37989, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=429, Total reward=43.52, Steps=38021, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=430, Total reward=18.42, Steps=38074, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=431, Total reward=416.25, Steps=38351, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=432, Total reward=40.25, Steps=38380, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=433, Total reward=277.18, Steps=38514, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=434, Total reward=87.42, Steps=38588, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=435, Total reward=31.59, Steps=38604, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=436, Total reward=112.11, Steps=38691, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=437, Total reward=32.99, Steps=38726, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=438, Total reward=39.77, Steps=38781, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=439, Total reward=87.04, Steps=38858, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=440, Total reward=112.92, Steps=38942, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=441, Total reward=270.41, Steps=39144, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=442, Total reward=61.85, Steps=39192, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=443, Total reward=393.14, Steps=39468, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=444, Total reward=3.73, Steps=39481, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=445, Total reward=233.9, Steps=39646, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=446, Total reward=7.54, Steps=39681, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=447, Total reward=87.06, Steps=39728, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=448, Total reward=132.55, Steps=39838, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=449, Total reward=153.63, Steps=39953, Training iteration=8
Training> Name=main_level/agent, Worker=0, Episode=450, Total reward=117.89, Steps=40054, Training iteration=8
Policy training> Surrogate loss=0.00317141180858016, KL divergence=5.9567391872406006e-05, Entropy=0.19852137565612793, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01947568729519844, KL divergence=0.002284733112901449, Entropy=0.1981014609336853, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020548120141029358, KL divergence=0.006563300266861916, Entropy=0.19517219066619873, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.03102802112698555, KL divergence=0.009713586419820786, Entropy=0.19539335370063782, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.022374030202627182, KL divergence=0.011802569963037968, Entropy=0.19348689913749695, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.023000027984380722, KL divergence=0.014462989754974842, Entropy=0.19524800777435303, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.023849142715334892, KL divergence=0.016413643956184387, Entropy=0.1957577019929886, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03449155390262604, KL divergence=0.017086900770664215, Entropy=0.1950513869524002, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.04423607513308525, KL divergence=0.018103234469890594, Entropy=0.19552487134933472, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.040158770978450775, KL divergence=0.019142363220453262, Entropy=0.19547198712825775, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/441_Step-40054.ckpt']
Uploaded 3 files for checkpoint 441 in 0.49 seconds
saved intermediate frozen graph: current/model/model_441.pb
Best checkpoint number: 437, Last checkpoint number: 439
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'438'}
Training> Name=main_level/agent, Worker=0, Episode=451, Total reward=348.16, Steps=40271, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=452, Total reward=98.97, Steps=40345, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=453, Total reward=33.07, Steps=40356, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=454, Total reward=412.06, Steps=40649, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=455, Total reward=86.14, Steps=40689, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=456, Total reward=118.91, Steps=40778, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=457, Total reward=81.93, Steps=40850, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=458, Total reward=133.71, Steps=40946, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=459, Total reward=1.35, Steps=40976, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=460, Total reward=287.16, Steps=41189, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=461, Total reward=85.54, Steps=41253, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=462, Total reward=48.15, Steps=41295, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=463, Total reward=26.49, Steps=41336, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=464, Total reward=176.27, Steps=41481, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=465, Total reward=185.4, Steps=41618, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=466, Total reward=41.89, Steps=41656, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=467, Total reward=129.14, Steps=41750, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=468, Total reward=125.63, Steps=41836, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=469, Total reward=29.34, Steps=41852, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=470, Total reward=148.45, Steps=41925, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=471, Total reward=112.04, Steps=42005, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=472, Total reward=237.07, Steps=42136, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=473, Total reward=83.06, Steps=42179, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=474, Total reward=248.3, Steps=42342, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=475, Total reward=116.01, Steps=42413, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=476, Total reward=182.16, Steps=42538, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=477, Total reward=109.71, Steps=42636, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=478, Total reward=89.61, Steps=42711, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=479, Total reward=107.35, Steps=42777, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=480, Total reward=90.62, Steps=42826, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=481, Total reward=73.33, Steps=42885, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=482, Total reward=60.31, Steps=42914, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=483, Total reward=204.06, Steps=43077, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=484, Total reward=186.59, Steps=43221, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=485, Total reward=158.33, Steps=43373, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=486, Total reward=79.26, Steps=43450, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=487, Total reward=76.25, Steps=43520, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=488, Total reward=94.3, Steps=43578, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=489, Total reward=37.5, Steps=43614, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=490, Total reward=23.51, Steps=43648, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=491, Total reward=259.46, Steps=43787, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=492, Total reward=14.08, Steps=43811, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=493, Total reward=311.57, Steps=43985, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=494, Total reward=62.32, Steps=44031, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=495, Total reward=365.5, Steps=44311, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=496, Total reward=83.5, Steps=44382, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=497, Total reward=88.18, Steps=44448, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=498, Total reward=196.55, Steps=44654, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=499, Total reward=14.02, Steps=44683, Training iteration=9
Training> Name=main_level/agent, Worker=0, Episode=500, Total reward=64.33, Steps=44733, Training iteration=9
Policy training> Surrogate loss=-0.0030577650759369135, KL divergence=0.00017104082508012652, Entropy=0.20291778445243835, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015401691198348999, KL divergence=0.00399911729618907, Entropy=0.20104122161865234, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02363644726574421, KL divergence=0.009000574238598347, Entropy=0.19857613742351532, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.028334425762295723, KL divergence=0.011874561198055744, Entropy=0.19755621254444122, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03086143359541893, KL divergence=0.014292031526565552, Entropy=0.19729402661323547, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.033272430300712585, KL divergence=0.015897173434495926, Entropy=0.1965327262878418, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.035676270723342896, KL divergence=0.017294296994805336, Entropy=0.19620800018310547, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03374812752008438, KL divergence=0.018535953015089035, Entropy=0.1973152458667755, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03291280195116997, KL divergence=0.01936611346900463, Entropy=0.19708234071731567, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03785764425992966, KL divergence=0.020448217168450356, Entropy=0.19763420522212982, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/442_Step-44733.ckpt']
Uploaded 3 files for checkpoint 442 in 0.46 seconds
saved intermediate frozen graph: current/model/model_442.pb
Best checkpoint number: 437, Last checkpoint number: 440
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'439'}
Training> Name=main_level/agent, Worker=0, Episode=501, Total reward=81.69, Steps=44819, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=502, Total reward=119.26, Steps=44936, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=503, Total reward=23.28, Steps=45014, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=504, Total reward=170.72, Steps=45141, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=505, Total reward=3.3, Steps=45178, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=506, Total reward=101.99, Steps=45237, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=507, Total reward=291.63, Steps=45447, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=508, Total reward=220.57, Steps=45558, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=509, Total reward=116.61, Steps=45663, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=510, Total reward=195.09, Steps=45793, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=511, Total reward=114.76, Steps=45861, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=512, Total reward=399.8, Steps=46137, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=513, Total reward=299.75, Steps=46297, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=514, Total reward=62.86, Steps=46344, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=515, Total reward=5.76, Steps=46352, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=516, Total reward=38.99, Steps=46373, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=517, Total reward=110.12, Steps=46444, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=518, Total reward=107.08, Steps=46509, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=519, Total reward=128.32, Steps=46633, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=520, Total reward=82.17, Steps=46701, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=521, Total reward=96.67, Steps=46790, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=522, Total reward=129.72, Steps=46942, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=523, Total reward=123.54, Steps=47073, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=524, Total reward=13.33, Steps=47098, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=525, Total reward=211.3, Steps=47241, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=526, Total reward=112.25, Steps=47372, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=527, Total reward=442.78, Steps=47621, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=528, Total reward=114.59, Steps=47694, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=529, Total reward=35.07, Steps=47728, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=530, Total reward=109.37, Steps=47808, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=531, Total reward=458.79, Steps=48093, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=532, Total reward=54.7, Steps=48118, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=533, Total reward=106.19, Steps=48176, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=534, Total reward=194.58, Steps=48292, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=535, Total reward=182.45, Steps=48443, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=536, Total reward=57.2, Steps=48477, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=537, Total reward=312.01, Steps=48715, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=538, Total reward=295.89, Steps=48938, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=539, Total reward=105.48, Steps=49008, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=540, Total reward=206.07, Steps=49141, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=541, Total reward=166.81, Steps=49278, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=542, Total reward=52.38, Steps=49320, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=543, Total reward=16.38, Steps=49338, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=544, Total reward=197.83, Steps=49499, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=545, Total reward=196.59, Steps=49642, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=546, Total reward=3.53, Steps=49654, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=547, Total reward=46.64, Steps=49710, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=548, Total reward=158.95, Steps=49817, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=549, Total reward=33.74, Steps=49853, Training iteration=10
Training> Name=main_level/agent, Worker=0, Episode=550, Total reward=29.08, Steps=49914, Training iteration=10
Policy training> Surrogate loss=0.0009409097256138921, KL divergence=0.00011381772492313758, Entropy=0.2040373831987381, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.0161480363458395, KL divergence=0.003177027450874448, Entropy=0.20304258167743683, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02239350788295269, KL divergence=0.006610694341361523, Entropy=0.2005111277103424, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02356705628335476, KL divergence=0.009145752526819706, Entropy=0.19914613664150238, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028136536478996277, KL divergence=0.010926979593932629, Entropy=0.19952711462974548, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.029507827013731003, KL divergence=0.012677805498242378, Entropy=0.19975078105926514, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03346246853470802, KL divergence=0.013802921399474144, Entropy=0.19956481456756592, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03100455366075039, KL divergence=0.015042180195450783, Entropy=0.19849085807800293, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03352965787053108, KL divergence=0.016345981508493423, Entropy=0.19892755150794983, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03420521691441536, KL divergence=0.017133494839072227, Entropy=0.19939851760864258, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/443_Step-49914.ckpt']
Uploaded 3 files for checkpoint 443 in 0.61 seconds
saved intermediate frozen graph: current/model/model_443.pb
Best checkpoint number: 437, Last checkpoint number: 441
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'440'}
Training> Name=main_level/agent, Worker=0, Episode=551, Total reward=128.71, Steps=50010, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=552, Total reward=108.05, Steps=50088, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=553, Total reward=225.44, Steps=50269, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=554, Total reward=63.83, Steps=50313, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=555, Total reward=24.27, Steps=50328, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=556, Total reward=32.83, Steps=50361, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=557, Total reward=66.46, Steps=50425, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=558, Total reward=50.91, Steps=50461, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=559, Total reward=107.7, Steps=50528, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=560, Total reward=228.42, Steps=50674, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=561, Total reward=95.47, Steps=50749, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=562, Total reward=23.43, Steps=50793, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=563, Total reward=81.49, Steps=50922, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=564, Total reward=363.64, Steps=51222, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=565, Total reward=235.14, Steps=51387, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=566, Total reward=53.11, Steps=51446, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=567, Total reward=143.52, Steps=51560, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=568, Total reward=258.76, Steps=51723, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=569, Total reward=111.45, Steps=51775, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=570, Total reward=7.44, Steps=51788, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=571, Total reward=54.23, Steps=51824, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=572, Total reward=16.98, Steps=51836, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=573, Total reward=164.09, Steps=51958, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=574, Total reward=198.86, Steps=52057, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=575, Total reward=388.5, Steps=52311, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=576, Total reward=209.54, Steps=52430, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=577, Total reward=151.68, Steps=52532, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=578, Total reward=261.9, Steps=52709, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=579, Total reward=33.43, Steps=52743, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=580, Total reward=168.17, Steps=52893, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=581, Total reward=53.46, Steps=52954, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=582, Total reward=129.23, Steps=53108, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=583, Total reward=100.27, Steps=53201, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=584, Total reward=207.02, Steps=53364, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=585, Total reward=117.0, Steps=53454, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=586, Total reward=187.6, Steps=53587, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=587, Total reward=268.85, Steps=53743, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=588, Total reward=461.07, Steps=53981, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=589, Total reward=112.02, Steps=54070, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=590, Total reward=60.17, Steps=54142, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=591, Total reward=273.91, Steps=54336, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=592, Total reward=121.06, Steps=54401, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=593, Total reward=320.41, Steps=54640, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=594, Total reward=59.13, Steps=54690, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=595, Total reward=147.65, Steps=54821, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=596, Total reward=44.9, Steps=54860, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=597, Total reward=187.46, Steps=54948, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=598, Total reward=122.2, Steps=55032, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=599, Total reward=293.08, Steps=55260, Training iteration=11
Training> Name=main_level/agent, Worker=0, Episode=600, Total reward=80.01, Steps=55308, Training iteration=11
Policy training> Surrogate loss=0.0024436439853161573, KL divergence=0.0002436391805531457, Entropy=0.19282859563827515, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014758894219994545, KL divergence=0.0039053955115377903, Entropy=0.19360391795635223, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.023818830028176308, KL divergence=0.00761930737644434, Entropy=0.19046542048454285, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.030791306868195534, KL divergence=0.010258257389068604, Entropy=0.18988588452339172, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028790030628442764, KL divergence=0.012156925164163113, Entropy=0.1893712878227234, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03163813427090645, KL divergence=0.013639700599014759, Entropy=0.18780583143234253, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03918439894914627, KL divergence=0.014425389468669891, Entropy=0.189806267619133, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03437129408121109, KL divergence=0.015529291704297066, Entropy=0.18558327853679657, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03160510212182999, KL divergence=0.016479475423693657, Entropy=0.1886412501335144, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03777400404214859, KL divergence=0.017361897975206375, Entropy=0.189711332321167, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/444_Step-55308.ckpt']
Uploaded 3 files for checkpoint 444 in 0.55 seconds
saved intermediate frozen graph: current/model/model_444.pb
Best checkpoint number: 437, Last checkpoint number: 442
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'441'}
Training> Name=main_level/agent, Worker=0, Episode=601, Total reward=74.83, Steps=55352, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=602, Total reward=148.38, Steps=55475, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=603, Total reward=10.4, Steps=55491, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=604, Total reward=120.61, Steps=55575, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=605, Total reward=78.91, Steps=55665, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=606, Total reward=206.48, Steps=55802, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=607, Total reward=86.1, Steps=55870, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=608, Total reward=151.49, Steps=55958, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=609, Total reward=48.35, Steps=56018, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=610, Total reward=5.71, Steps=56056, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=611, Total reward=77.96, Steps=56126, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=612, Total reward=247.87, Steps=56278, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=613, Total reward=69.16, Steps=56306, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=614, Total reward=142.13, Steps=56410, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=615, Total reward=168.91, Steps=56508, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=616, Total reward=210.64, Steps=56625, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=617, Total reward=217.36, Steps=56792, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=618, Total reward=157.61, Steps=56884, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=619, Total reward=329.4, Steps=57124, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=620, Total reward=59.38, Steps=57186, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=621, Total reward=285.93, Steps=57382, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=622, Total reward=52.52, Steps=57420, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=623, Total reward=197.2, Steps=57579, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=624, Total reward=144.91, Steps=57715, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=625, Total reward=93.75, Steps=57842, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=626, Total reward=146.22, Steps=57975, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=627, Total reward=239.6, Steps=58144, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=628, Total reward=135.43, Steps=58225, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=629, Total reward=29.35, Steps=58248, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=630, Total reward=184.4, Steps=58370, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=631, Total reward=100.44, Steps=58444, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=632, Total reward=82.23, Steps=58503, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=633, Total reward=83.11, Steps=58546, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=634, Total reward=67.78, Steps=58599, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=635, Total reward=114.5, Steps=58693, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=636, Total reward=201.34, Steps=58804, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=637, Total reward=46.77, Steps=58850, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=638, Total reward=112.54, Steps=58922, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=639, Total reward=6.19, Steps=58934, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=640, Total reward=289.69, Steps=59245, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=641, Total reward=73.19, Steps=59297, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=642, Total reward=68.5, Steps=59379, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=643, Total reward=18.97, Steps=59440, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=644, Total reward=284.92, Steps=59638, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=645, Total reward=114.35, Steps=59732, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=646, Total reward=468.15, Steps=60015, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=647, Total reward=150.88, Steps=60105, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=648, Total reward=401.09, Steps=60385, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=649, Total reward=150.37, Steps=60502, Training iteration=12
Training> Name=main_level/agent, Worker=0, Episode=650, Total reward=459.79, Steps=60794, Training iteration=12
Policy training> Surrogate loss=0.0020360113121569157, KL divergence=0.00012907737982459366, Entropy=0.1981470286846161, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01493268646299839, KL divergence=0.0033157989382743835, Entropy=0.19790615141391754, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.016266437247395515, KL divergence=0.007501339074224234, Entropy=0.19906964898109436, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026287749409675598, KL divergence=0.01010068692266941, Entropy=0.19765035808086395, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.027334028854966164, KL divergence=0.011825909838080406, Entropy=0.1959751546382904, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02559523656964302, KL divergence=0.013334311544895172, Entropy=0.19487564265727997, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03432481735944748, KL divergence=0.014742188155651093, Entropy=0.19637104868888855, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03446295112371445, KL divergence=0.015197460539638996, Entropy=0.19498631358146667, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.0341266468167305, KL divergence=0.016607830300927162, Entropy=0.1966974139213562, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0342644527554512, KL divergence=0.01736275479197502, Entropy=0.19573582708835602, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/445_Step-60794.ckpt']
Uploaded 3 files for checkpoint 445 in 0.52 seconds
saved intermediate frozen graph: current/model/model_445.pb
Best checkpoint number: 437, Last checkpoint number: 443
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'442'}
Training> Name=main_level/agent, Worker=0, Episode=651, Total reward=37.08, Steps=60853, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=652, Total reward=59.72, Steps=60878, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=653, Total reward=448.59, Steps=61176, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=654, Total reward=101.82, Steps=61258, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=655, Total reward=268.06, Steps=61429, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=656, Total reward=215.66, Steps=61561, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=657, Total reward=72.26, Steps=61625, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=658, Total reward=151.49, Steps=61739, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=659, Total reward=104.87, Steps=61861, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=660, Total reward=93.03, Steps=61911, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=661, Total reward=188.61, Steps=62080, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=662, Total reward=235.07, Steps=62230, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=663, Total reward=14.63, Steps=62244, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=664, Total reward=68.48, Steps=62354, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=665, Total reward=125.9, Steps=62465, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=666, Total reward=211.86, Steps=62572, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=667, Total reward=117.75, Steps=62672, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=668, Total reward=194.62, Steps=62769, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=669, Total reward=248.9, Steps=62895, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=670, Total reward=115.2, Steps=62963, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=671, Total reward=138.3, Steps=63058, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=672, Total reward=25.51, Steps=63092, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=673, Total reward=266.66, Steps=63256, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=674, Total reward=25.23, Steps=63267, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=675, Total reward=161.27, Steps=63390, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=676, Total reward=382.19, Steps=63669, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=677, Total reward=325.57, Steps=63897, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=678, Total reward=168.18, Steps=64032, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=679, Total reward=291.36, Steps=64235, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=680, Total reward=97.28, Steps=64284, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=681, Total reward=79.5, Steps=64331, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=682, Total reward=44.14, Steps=64363, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=683, Total reward=16.14, Steps=64388, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=684, Total reward=106.46, Steps=64478, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=685, Total reward=190.81, Steps=64628, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=686, Total reward=234.8, Steps=64787, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=687, Total reward=168.44, Steps=64902, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=688, Total reward=159.01, Steps=65013, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=689, Total reward=308.5, Steps=65176, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=690, Total reward=45.07, Steps=65229, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=691, Total reward=243.85, Steps=65389, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=692, Total reward=135.31, Steps=65465, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=693, Total reward=285.4, Steps=65635, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=694, Total reward=30.68, Steps=65647, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=695, Total reward=165.31, Steps=65738, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=696, Total reward=93.58, Steps=65812, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=697, Total reward=358.76, Steps=66027, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=698, Total reward=283.56, Steps=66246, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=699, Total reward=33.2, Steps=66275, Training iteration=13
Training> Name=main_level/agent, Worker=0, Episode=700, Total reward=76.94, Steps=66348, Training iteration=13
Policy training> Surrogate loss=0.0026799500919878483, KL divergence=0.00017361209029331803, Entropy=0.2086683213710785, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.011229224503040314, KL divergence=0.003139261854812503, Entropy=0.20831036567687988, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.025254428386688232, KL divergence=0.007713906466960907, Entropy=0.2048843652009964, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02469911053776741, KL divergence=0.011233795434236526, Entropy=0.20197615027427673, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.030855724588036537, KL divergence=0.013132860884070396, Entropy=0.20220382511615753, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03116924688220024, KL divergence=0.014442019164562225, Entropy=0.2018221914768219, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.027084577828645706, KL divergence=0.016117233783006668, Entropy=0.203166201710701, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03256348893046379, KL divergence=0.016844043508172035, Entropy=0.20283189415931702, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.035927142947912216, KL divergence=0.017397785559296608, Entropy=0.2018236666917801, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03182396665215492, KL divergence=0.01846178248524666, Entropy=0.20142629742622375, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/446_Step-66348.ckpt']
Uploaded 3 files for checkpoint 446 in 0.58 seconds
saved intermediate frozen graph: current/model/model_446.pb
Best checkpoint number: 437, Last checkpoint number: 444
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'443'}
Training> Name=main_level/agent, Worker=0, Episode=701, Total reward=63.97, Steps=66391, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=702, Total reward=242.04, Steps=66555, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=703, Total reward=21.0, Steps=66598, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=704, Total reward=9.23, Steps=66623, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=705, Total reward=12.88, Steps=66663, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=706, Total reward=151.76, Steps=66767, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=707, Total reward=15.18, Steps=66790, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=708, Total reward=319.57, Steps=66970, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=709, Total reward=37.95, Steps=67004, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=710, Total reward=66.77, Steps=67063, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=711, Total reward=88.89, Steps=67121, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=712, Total reward=8.04, Steps=67133, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=713, Total reward=50.59, Steps=67160, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=714, Total reward=251.71, Steps=67341, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=715, Total reward=44.56, Steps=67360, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=716, Total reward=234.26, Steps=67524, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=717, Total reward=29.31, Steps=67552, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=718, Total reward=75.56, Steps=67591, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=719, Total reward=165.06, Steps=67769, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=720, Total reward=95.4, Steps=67842, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=721, Total reward=88.54, Steps=67879, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=722, Total reward=49.19, Steps=67918, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=723, Total reward=8.98, Steps=67975, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=724, Total reward=10.33, Steps=68019, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=725, Total reward=0.03, Steps=68050, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=726, Total reward=105.8, Steps=68174, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=727, Total reward=111.84, Steps=68282, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=728, Total reward=377.32, Steps=68524, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=729, Total reward=153.4, Steps=68635, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=730, Total reward=64.23, Steps=68703, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=731, Total reward=41.03, Steps=68741, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=732, Total reward=84.87, Steps=68792, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=733, Total reward=66.73, Steps=68816, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=734, Total reward=203.93, Steps=68908, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=735, Total reward=119.75, Steps=68972, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=736, Total reward=344.03, Steps=69169, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=737, Total reward=90.59, Steps=69221, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=738, Total reward=163.12, Steps=69364, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=739, Total reward=96.18, Steps=69422, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=740, Total reward=175.6, Steps=69577, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=741, Total reward=174.64, Steps=69731, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=742, Total reward=52.81, Steps=69790, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=743, Total reward=135.89, Steps=69959, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=744, Total reward=13.02, Steps=69990, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=745, Total reward=69.0, Steps=70059, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=746, Total reward=89.47, Steps=70123, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=747, Total reward=145.06, Steps=70253, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=748, Total reward=170.17, Steps=70360, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=749, Total reward=56.38, Steps=70437, Training iteration=14
Training> Name=main_level/agent, Worker=0, Episode=750, Total reward=75.67, Steps=70504, Training iteration=14
Policy training> Surrogate loss=7.82734714448452e-05, KL divergence=7.959934009704739e-05, Entropy=0.21619632840156555, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015576501376926899, KL divergence=0.0027418453246355057, Entropy=0.21484985947608948, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.025825291872024536, KL divergence=0.007291785907000303, Entropy=0.2129252851009369, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02972153201699257, KL divergence=0.01086895726621151, Entropy=0.2141232043504715, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03490694984793663, KL divergence=0.013680722564458847, Entropy=0.21259954571723938, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03305494785308838, KL divergence=0.015711354091763496, Entropy=0.21242934465408325, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03756636008620262, KL divergence=0.01736978255212307, Entropy=0.2132682502269745, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03703712299466133, KL divergence=0.018489452078938484, Entropy=0.21239259839057922, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.0379827581346035, KL divergence=0.01966385915875435, Entropy=0.21259906888008118, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.04164069518446922, KL divergence=0.02068084478378296, Entropy=0.2134544998407364, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/447_Step-70504.ckpt']
Uploaded 3 files for checkpoint 447 in 0.57 seconds
saved intermediate frozen graph: current/model/model_447.pb
Best checkpoint number: 437, Last checkpoint number: 445
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'444'}
Training> Name=main_level/agent, Worker=0, Episode=751, Total reward=118.13, Steps=70599, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=752, Total reward=111.44, Steps=70673, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=753, Total reward=412.33, Steps=70951, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=754, Total reward=145.2, Steps=71067, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=755, Total reward=84.52, Steps=71116, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=756, Total reward=92.11, Steps=71176, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=757, Total reward=274.1, Steps=71376, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=758, Total reward=165.98, Steps=71464, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=759, Total reward=106.38, Steps=71567, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=760, Total reward=108.29, Steps=71636, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=761, Total reward=83.47, Steps=71679, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=762, Total reward=42.13, Steps=71701, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=763, Total reward=38.09, Steps=71743, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=764, Total reward=109.78, Steps=71845, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=765, Total reward=179.74, Steps=72034, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=766, Total reward=7.05, Steps=72047, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=767, Total reward=372.66, Steps=72313, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=768, Total reward=224.96, Steps=72480, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=769, Total reward=325.45, Steps=72721, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=770, Total reward=414.55, Steps=73025, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=771, Total reward=261.34, Steps=73175, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=772, Total reward=52.11, Steps=73201, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=773, Total reward=266.07, Steps=73394, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=774, Total reward=50.78, Steps=73419, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=775, Total reward=38.12, Steps=73436, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=776, Total reward=131.42, Steps=73519, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=777, Total reward=72.57, Steps=73575, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=778, Total reward=170.58, Steps=73662, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=779, Total reward=42.56, Steps=73700, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=780, Total reward=224.63, Steps=73907, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=781, Total reward=214.8, Steps=74067, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=782, Total reward=40.11, Steps=74090, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=783, Total reward=125.14, Steps=74183, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=784, Total reward=182.04, Steps=74330, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=785, Total reward=78.95, Steps=74407, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=786, Total reward=320.05, Steps=74628, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=787, Total reward=178.62, Steps=74759, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=788, Total reward=147.15, Steps=74844, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=789, Total reward=201.93, Steps=74945, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=790, Total reward=27.17, Steps=74994, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=791, Total reward=134.45, Steps=75062, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=792, Total reward=43.81, Steps=75087, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=793, Total reward=86.54, Steps=75128, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=794, Total reward=220.77, Steps=75267, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=795, Total reward=36.31, Steps=75283, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=796, Total reward=249.61, Steps=75564, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=797, Total reward=100.44, Steps=75615, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=798, Total reward=124.7, Steps=75681, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=799, Total reward=115.99, Steps=75759, Training iteration=15
Training> Name=main_level/agent, Worker=0, Episode=800, Total reward=103.81, Steps=75855, Training iteration=15
Policy training> Surrogate loss=-0.005315334536135197, KL divergence=0.00011627719504758716, Entropy=0.2016409933567047, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01620696671307087, KL divergence=0.0030387956649065018, Entropy=0.19855134189128876, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.019997134804725647, KL divergence=0.007121494971215725, Entropy=0.19912894070148468, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.03217350319027901, KL divergence=0.010022500529885292, Entropy=0.1973070204257965, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03315531089901924, KL divergence=0.012010211125016212, Entropy=0.19504544138908386, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03132598102092743, KL divergence=0.013888167217373848, Entropy=0.19760404527187347, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.026314180344343185, KL divergence=0.015036409720778465, Entropy=0.19648703932762146, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03198278322815895, KL divergence=0.01562928408384323, Entropy=0.19684721529483795, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.037368010729551315, KL divergence=0.01626778393983841, Entropy=0.19464454054832458, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03654564544558525, KL divergence=0.017151685431599617, Entropy=0.1970917284488678, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/448_Step-75855.ckpt']
Uploaded 3 files for checkpoint 448 in 0.53 seconds
saved intermediate frozen graph: current/model/model_448.pb
Best checkpoint number: 437, Last checkpoint number: 446
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'445'}
Training> Name=main_level/agent, Worker=0, Episode=801, Total reward=77.46, Steps=75898, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=802, Total reward=220.7, Steps=76099, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=803, Total reward=15.68, Steps=76153, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=804, Total reward=151.34, Steps=76297, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=805, Total reward=153.11, Steps=76433, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=806, Total reward=212.77, Steps=76560, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=807, Total reward=404.55, Steps=76827, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=808, Total reward=123.91, Steps=76917, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=809, Total reward=43.24, Steps=76961, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=810, Total reward=225.14, Steps=77111, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=811, Total reward=269.47, Steps=77255, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=812, Total reward=35.27, Steps=77280, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=813, Total reward=265.3, Steps=77446, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=814, Total reward=314.96, Steps=77589, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=815, Total reward=344.76, Steps=77897, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=816, Total reward=240.37, Steps=78060, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=817, Total reward=266.54, Steps=78256, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=818, Total reward=30.96, Steps=78277, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=819, Total reward=123.61, Steps=78374, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=820, Total reward=71.1, Steps=78430, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=821, Total reward=65.3, Steps=78470, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=822, Total reward=55.91, Steps=78499, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=823, Total reward=250.06, Steps=78684, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=824, Total reward=22.16, Steps=78738, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=825, Total reward=97.22, Steps=78825, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=826, Total reward=160.82, Steps=78938, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=827, Total reward=230.79, Steps=79066, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=828, Total reward=83.09, Steps=79102, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=829, Total reward=40.42, Steps=79139, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=830, Total reward=91.52, Steps=79219, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=831, Total reward=108.66, Steps=79314, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=832, Total reward=74.05, Steps=79364, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=833, Total reward=131.57, Steps=79431, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=834, Total reward=59.25, Steps=79472, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=835, Total reward=378.33, Steps=79742, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=836, Total reward=269.16, Steps=79971, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=837, Total reward=405.32, Steps=80276, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=838, Total reward=234.33, Steps=80468, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=839, Total reward=271.41, Steps=80676, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=840, Total reward=98.49, Steps=80771, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=841, Total reward=401.19, Steps=81054, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=842, Total reward=36.39, Steps=81074, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=843, Total reward=209.46, Steps=81252, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=844, Total reward=13.92, Steps=81282, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=845, Total reward=212.18, Steps=81424, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=846, Total reward=123.5, Steps=81501, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=847, Total reward=260.77, Steps=81661, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=848, Total reward=227.11, Steps=81785, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=849, Total reward=126.23, Steps=81899, Training iteration=16
Training> Name=main_level/agent, Worker=0, Episode=850, Total reward=173.11, Steps=81995, Training iteration=16
Policy training> Surrogate loss=0.006362326443195343, KL divergence=0.00030465374584309757, Entropy=0.19676178693771362, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.013674183748662472, KL divergence=0.004353731870651245, Entropy=0.19632309675216675, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02299891784787178, KL divergence=0.008270340971648693, Entropy=0.1950046569108963, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.027783947065472603, KL divergence=0.011201843619346619, Entropy=0.19547981023788452, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03376760706305504, KL divergence=0.013051883317530155, Entropy=0.1936987042427063, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.027503956109285355, KL divergence=0.014464432373642921, Entropy=0.19586683809757233, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.028761446475982666, KL divergence=0.01600760407745838, Entropy=0.19450443983078003, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.029877208173274994, KL divergence=0.01709284633398056, Entropy=0.19527672231197357, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03343379124999046, KL divergence=0.017467930912971497, Entropy=0.1947537362575531, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03397449478507042, KL divergence=0.018562989309430122, Entropy=0.19441814720630646, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/449_Step-81995.ckpt']
Uploaded 3 files for checkpoint 449 in 0.52 seconds
saved intermediate frozen graph: current/model/model_449.pb
Best checkpoint number: 437, Last checkpoint number: 447
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'446'}
Training> Name=main_level/agent, Worker=0, Episode=851, Total reward=132.34, Steps=82064, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=852, Total reward=125.63, Steps=82123, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=853, Total reward=245.57, Steps=82266, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=854, Total reward=67.95, Steps=82304, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=855, Total reward=253.28, Steps=82428, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=856, Total reward=124.5, Steps=82509, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=857, Total reward=101.95, Steps=82579, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=858, Total reward=124.1, Steps=82665, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=859, Total reward=121.27, Steps=82737, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=860, Total reward=178.21, Steps=82898, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=861, Total reward=69.71, Steps=82947, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=862, Total reward=56.07, Steps=82990, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=863, Total reward=148.96, Steps=83103, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=864, Total reward=425.61, Steps=83386, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=865, Total reward=3.48, Steps=83411, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=866, Total reward=95.31, Steps=83483, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=867, Total reward=124.34, Steps=83602, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=868, Total reward=386.58, Steps=83889, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=869, Total reward=34.51, Steps=83912, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=870, Total reward=78.9, Steps=83965, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=871, Total reward=102.79, Steps=84040, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=872, Total reward=128.85, Steps=84111, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=873, Total reward=32.27, Steps=84123, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=874, Total reward=212.44, Steps=84219, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=875, Total reward=122.86, Steps=84293, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=876, Total reward=203.61, Steps=84419, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=877, Total reward=204.94, Steps=84540, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=878, Total reward=77.19, Steps=84589, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=879, Total reward=107.02, Steps=84654, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=880, Total reward=106.64, Steps=84705, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=881, Total reward=89.14, Steps=84744, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=882, Total reward=39.11, Steps=84774, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=883, Total reward=5.88, Steps=84792, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=884, Total reward=225.28, Steps=84946, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=885, Total reward=0.03, Steps=84973, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=886, Total reward=31.89, Steps=85008, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=887, Total reward=128.21, Steps=85118, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=888, Total reward=210.68, Steps=85254, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=889, Total reward=24.08, Steps=85269, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=890, Total reward=24.55, Steps=85319, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=891, Total reward=34.59, Steps=85345, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=892, Total reward=119.18, Steps=85445, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=893, Total reward=33.41, Steps=85457, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=894, Total reward=262.26, Steps=85605, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=895, Total reward=205.52, Steps=85795, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=896, Total reward=283.84, Steps=86009, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=897, Total reward=394.03, Steps=86293, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=898, Total reward=179.24, Steps=86421, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=899, Total reward=239.94, Steps=86640, Training iteration=17
Training> Name=main_level/agent, Worker=0, Episode=900, Total reward=232.24, Steps=86836, Training iteration=17
Policy training> Surrogate loss=-0.000760761380661279, KL divergence=0.00022028216335456818, Entropy=0.19833511114120483, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014508428052067757, KL divergence=0.0033361539244651794, Entropy=0.19787414371967316, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02590825967490673, KL divergence=0.006692888680845499, Entropy=0.194468155503273, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02588585764169693, KL divergence=0.009100725874304771, Entropy=0.19212967157363892, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.023048242554068565, KL divergence=0.010903413407504559, Entropy=0.19162560999393463, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.024716569110751152, KL divergence=0.012564952485263348, Entropy=0.19128988683223724, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03015824407339096, KL divergence=0.014008019119501114, Entropy=0.1909387707710266, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.032023780047893524, KL divergence=0.014855348505079746, Entropy=0.1918099969625473, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02920987270772457, KL divergence=0.015637248754501343, Entropy=0.1901538223028183, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.035226646810770035, KL divergence=0.01640320010483265, Entropy=0.19094695150852203, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/450_Step-86836.ckpt']
Uploaded 3 files for checkpoint 450 in 0.50 seconds
saved intermediate frozen graph: current/model/model_450.pb
Best checkpoint number: 437, Last checkpoint number: 448
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'447'}
Training> Name=main_level/agent, Worker=0, Episode=901, Total reward=242.96, Steps=87023, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=902, Total reward=47.84, Steps=87054, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=903, Total reward=280.85, Steps=87247, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=904, Total reward=85.33, Steps=87369, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=905, Total reward=460.21, Steps=87640, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=906, Total reward=121.77, Steps=87705, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=907, Total reward=69.86, Steps=87806, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=908, Total reward=85.99, Steps=87850, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=909, Total reward=30.06, Steps=87877, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=910, Total reward=19.42, Steps=87930, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=911, Total reward=359.08, Steps=88167, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=912, Total reward=94.98, Steps=88243, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=913, Total reward=33.45, Steps=88254, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=914, Total reward=60.09, Steps=88297, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=915, Total reward=131.55, Steps=88372, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=916, Total reward=163.42, Steps=88503, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=917, Total reward=174.45, Steps=88603, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=918, Total reward=60.56, Steps=88661, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=919, Total reward=121.99, Steps=88757, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=920, Total reward=163.14, Steps=88922, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=921, Total reward=63.9, Steps=88961, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=922, Total reward=46.3, Steps=89001, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=923, Total reward=7.73, Steps=89029, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=924, Total reward=23.68, Steps=89067, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=925, Total reward=321.89, Steps=89296, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=926, Total reward=182.33, Steps=89441, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=927, Total reward=159.89, Steps=89558, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=928, Total reward=374.56, Steps=89769, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=929, Total reward=440.07, Steps=90059, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=930, Total reward=53.67, Steps=90106, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=931, Total reward=36.49, Steps=90159, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=932, Total reward=80.16, Steps=90215, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=933, Total reward=77.22, Steps=90256, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=934, Total reward=87.27, Steps=90328, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=935, Total reward=35.99, Steps=90377, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=936, Total reward=77.96, Steps=90430, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=937, Total reward=36.85, Steps=90466, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=938, Total reward=83.16, Steps=90505, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=939, Total reward=121.83, Steps=90572, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=940, Total reward=90.42, Steps=90621, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=941, Total reward=238.07, Steps=90824, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=942, Total reward=74.47, Steps=90890, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=943, Total reward=6.79, Steps=90915, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=944, Total reward=184.97, Steps=91087, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=945, Total reward=224.64, Steps=91223, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=946, Total reward=100.2, Steps=91307, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=947, Total reward=294.11, Steps=91483, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=948, Total reward=91.08, Steps=91540, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=949, Total reward=161.11, Steps=91640, Training iteration=18
Training> Name=main_level/agent, Worker=0, Episode=950, Total reward=276.09, Steps=91891, Training iteration=18
Policy training> Surrogate loss=0.002134541980922222, KL divergence=0.00011603397433646023, Entropy=0.1926698088645935, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.011177192442119122, KL divergence=0.002533435355871916, Entropy=0.1925334632396698, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.021691204980015755, KL divergence=0.00649581104516983, Entropy=0.19188515841960907, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.028713073581457138, KL divergence=0.009676744230091572, Entropy=0.1924561709165573, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026265375316143036, KL divergence=0.011838368140161037, Entropy=0.18964160978794098, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03399825841188431, KL divergence=0.013778287917375565, Entropy=0.18907314538955688, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.023129738867282867, KL divergence=0.015300684608519077, Entropy=0.19114087522029877, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03110700473189354, KL divergence=0.016802363097667694, Entropy=0.19132186472415924, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.038759745657444, KL divergence=0.01717992313206196, Entropy=0.19100795686244965, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.034354668110609055, KL divergence=0.019054777920246124, Entropy=0.1904943287372589, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/451_Step-91891.ckpt']
Uploaded 3 files for checkpoint 451 in 0.48 seconds
saved intermediate frozen graph: current/model/model_451.pb
Best checkpoint number: 437, Last checkpoint number: 449
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'448'}
Training> Name=main_level/agent, Worker=0, Episode=951, Total reward=118.09, Steps=91959, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=952, Total reward=306.46, Steps=92108, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=953, Total reward=89.45, Steps=92152, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=954, Total reward=66.25, Steps=92204, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=955, Total reward=68.09, Steps=92254, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=956, Total reward=34.47, Steps=92274, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=957, Total reward=76.95, Steps=92327, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=958, Total reward=289.0, Steps=92553, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=959, Total reward=96.06, Steps=92657, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=960, Total reward=210.29, Steps=92825, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=961, Total reward=84.72, Steps=92877, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=962, Total reward=48.87, Steps=92912, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=963, Total reward=450.39, Steps=93191, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=964, Total reward=207.11, Steps=93376, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=965, Total reward=131.1, Steps=93505, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=966, Total reward=241.64, Steps=93638, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=967, Total reward=168.25, Steps=93779, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=968, Total reward=211.86, Steps=93885, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=969, Total reward=126.83, Steps=93981, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=970, Total reward=302.99, Steps=94180, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=971, Total reward=122.12, Steps=94247, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=972, Total reward=460.12, Steps=94522, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=973, Total reward=79.75, Steps=94563, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=974, Total reward=267.12, Steps=94769, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=975, Total reward=31.61, Steps=94806, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=976, Total reward=82.47, Steps=94883, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=977, Total reward=101.97, Steps=94935, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=978, Total reward=391.84, Steps=95231, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=979, Total reward=28.9, Steps=95259, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=980, Total reward=252.88, Steps=95431, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=981, Total reward=316.83, Steps=95623, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=982, Total reward=182.21, Steps=95801, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=983, Total reward=42.92, Steps=95872, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=984, Total reward=9.34, Steps=95921, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=985, Total reward=377.4, Steps=96218, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=986, Total reward=27.83, Steps=96260, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=987, Total reward=193.09, Steps=96397, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=988, Total reward=89.72, Steps=96441, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=989, Total reward=28.08, Steps=96484, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=990, Total reward=129.48, Steps=96570, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=991, Total reward=24.12, Steps=96610, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=992, Total reward=116.75, Steps=96691, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=993, Total reward=29.68, Steps=96702, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=994, Total reward=141.57, Steps=96793, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=995, Total reward=88.22, Steps=96870, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=996, Total reward=84.52, Steps=96982, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=997, Total reward=99.85, Steps=97041, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=998, Total reward=164.53, Steps=97181, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=999, Total reward=248.63, Steps=97369, Training iteration=19
Training> Name=main_level/agent, Worker=0, Episode=1000, Total reward=134.81, Steps=97499, Training iteration=19
Policy training> Surrogate loss=-0.0017845972906798124, KL divergence=0.00016691313066985458, Entropy=0.19856047630310059, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01372873317450285, KL divergence=0.0037459644954651594, Entropy=0.1966443955898285, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.016030345112085342, KL divergence=0.007956776767969131, Entropy=0.1952473670244217, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.019228370860219002, KL divergence=0.0100770378485322, Entropy=0.1929003745317459, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.027304574847221375, KL divergence=0.012125852517783642, Entropy=0.19259898364543915, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.029569750651717186, KL divergence=0.013703408651053905, Entropy=0.19071616232395172, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03681396320462227, KL divergence=0.015037228353321552, Entropy=0.1928556263446808, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03179096058011055, KL divergence=0.016084540635347366, Entropy=0.1919182389974594, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.029324978590011597, KL divergence=0.016916822642087936, Entropy=0.19127371907234192, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03601040691137314, KL divergence=0.017604181542992592, Entropy=0.19033853709697723, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/452_Step-97499.ckpt']
Uploaded 3 files for checkpoint 452 in 0.56 seconds
saved intermediate frozen graph: current/model/model_452.pb
Best checkpoint number: 437, Last checkpoint number: 450
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'449'}
Training> Name=main_level/agent, Worker=0, Episode=1001, Total reward=67.96, Steps=97548, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1002, Total reward=97.6, Steps=97647, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1003, Total reward=14.52, Steps=97701, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1004, Total reward=130.17, Steps=97844, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1005, Total reward=6.35, Steps=97875, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1006, Total reward=318.32, Steps=98060, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1007, Total reward=76.68, Steps=98122, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1008, Total reward=116.43, Steps=98209, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1009, Total reward=48.52, Steps=98266, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1010, Total reward=14.88, Steps=98278, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1011, Total reward=65.63, Steps=98321, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1012, Total reward=52.24, Steps=98383, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1013, Total reward=89.69, Steps=98423, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1014, Total reward=348.32, Steps=98657, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1015, Total reward=33.06, Steps=98674, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1016, Total reward=51.25, Steps=98705, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1017, Total reward=340.01, Steps=98964, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1018, Total reward=77.11, Steps=99002, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1019, Total reward=97.63, Steps=99076, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1020, Total reward=111.68, Steps=99172, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1021, Total reward=174.71, Steps=99304, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1022, Total reward=51.93, Steps=99341, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1023, Total reward=80.49, Steps=99439, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1024, Total reward=219.76, Steps=99601, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1025, Total reward=405.18, Steps=99895, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1026, Total reward=168.23, Steps=100006, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1027, Total reward=259.24, Steps=100128, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1028, Total reward=247.09, Steps=100256, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1029, Total reward=117.08, Steps=100364, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1030, Total reward=333.92, Steps=100580, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1031, Total reward=123.64, Steps=100645, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1032, Total reward=11.31, Steps=100657, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1033, Total reward=86.35, Steps=100720, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1034, Total reward=453.78, Steps=101003, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1035, Total reward=377.24, Steps=101280, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1036, Total reward=72.53, Steps=101369, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1037, Total reward=47.79, Steps=101425, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1038, Total reward=81.8, Steps=101483, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1039, Total reward=104.81, Steps=101577, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1040, Total reward=26.39, Steps=101608, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1041, Total reward=76.49, Steps=101657, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1042, Total reward=265.15, Steps=101845, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1043, Total reward=21.89, Steps=101875, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1044, Total reward=16.42, Steps=101896, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1045, Total reward=288.97, Steps=102096, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1046, Total reward=119.16, Steps=102165, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1047, Total reward=196.34, Steps=102298, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1048, Total reward=120.12, Steps=102366, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1049, Total reward=41.34, Steps=102399, Training iteration=20
Training> Name=main_level/agent, Worker=0, Episode=1050, Total reward=318.14, Steps=102630, Training iteration=20
Policy training> Surrogate loss=0.0005339415511116385, KL divergence=0.00010541232768446207, Entropy=0.20200487971305847, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017407352104783058, KL divergence=0.0032803681679069996, Entropy=0.20149271190166473, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02469649724662304, KL divergence=0.007959339767694473, Entropy=0.20064707100391388, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.029673561453819275, KL divergence=0.011504463851451874, Entropy=0.19994434714317322, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.030996909365057945, KL divergence=0.013988208957016468, Entropy=0.1998279094696045, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03290243819355965, KL divergence=0.015844862908124924, Entropy=0.1995161920785904, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03384653851389885, KL divergence=0.017513755708932877, Entropy=0.1994699090719223, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03534216433763504, KL divergence=0.018875407055020332, Entropy=0.19947044551372528, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03680951148271561, KL divergence=0.019943704828619957, Entropy=0.19971686601638794, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.037314340472221375, KL divergence=0.020916378125548363, Entropy=0.19994153082370758, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/453_Step-102630.ckpt']
Uploaded 3 files for checkpoint 453 in 0.53 seconds
saved intermediate frozen graph: current/model/model_453.pb
Best checkpoint number: 437, Last checkpoint number: 451
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'450'}
Training> Name=main_level/agent, Worker=0, Episode=1051, Total reward=507.02, Steps=102917, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1052, Total reward=104.96, Steps=102972, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1053, Total reward=130.67, Steps=103039, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1054, Total reward=255.78, Steps=103196, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1055, Total reward=304.02, Steps=103458, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1056, Total reward=182.45, Steps=103592, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1057, Total reward=162.51, Steps=103709, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1058, Total reward=297.78, Steps=103932, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1059, Total reward=41.24, Steps=103976, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1060, Total reward=95.86, Steps=104037, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1061, Total reward=71.39, Steps=104090, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1062, Total reward=307.05, Steps=104329, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1063, Total reward=287.46, Steps=104513, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1064, Total reward=0.01, Steps=104526, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1065, Total reward=3.47, Steps=104551, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1066, Total reward=343.59, Steps=104780, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1067, Total reward=144.5, Steps=104880, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1068, Total reward=234.6, Steps=105006, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1069, Total reward=160.44, Steps=105124, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1070, Total reward=0.02, Steps=105139, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1071, Total reward=284.79, Steps=105289, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1072, Total reward=25.1, Steps=105302, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1073, Total reward=423.89, Steps=105564, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1074, Total reward=181.28, Steps=105676, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1075, Total reward=110.8, Steps=105759, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1076, Total reward=188.68, Steps=105895, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1077, Total reward=456.95, Steps=106177, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1078, Total reward=126.6, Steps=106283, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1079, Total reward=439.06, Steps=106568, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1080, Total reward=211.7, Steps=106711, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1081, Total reward=163.66, Steps=106863, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1082, Total reward=51.56, Steps=106892, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1083, Total reward=27.52, Steps=106925, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1084, Total reward=3.26, Steps=106951, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1085, Total reward=156.4, Steps=107099, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1086, Total reward=440.99, Steps=107372, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1087, Total reward=153.65, Steps=107498, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1088, Total reward=109.1, Steps=107559, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1089, Total reward=70.85, Steps=107634, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1090, Total reward=13.21, Steps=107677, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1091, Total reward=50.78, Steps=107730, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1092, Total reward=83.19, Steps=107788, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1093, Total reward=74.58, Steps=107813, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1094, Total reward=38.33, Steps=107826, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1095, Total reward=136.63, Steps=107908, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1096, Total reward=373.7, Steps=108184, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1097, Total reward=152.83, Steps=108316, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1098, Total reward=148.08, Steps=108435, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1099, Total reward=143.83, Steps=108522, Training iteration=21
Training> Name=main_level/agent, Worker=0, Episode=1100, Total reward=246.42, Steps=108736, Training iteration=21
Policy training> Surrogate loss=0.0020049801096320152, KL divergence=0.00026275229174643755, Entropy=0.20114903151988983, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.019809754565358162, KL divergence=0.005044009070843458, Entropy=0.196438729763031, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.026062769815325737, KL divergence=0.009106832556426525, Entropy=0.1975060999393463, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023884974420070648, KL divergence=0.010994249023497105, Entropy=0.19465208053588867, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.022352678701281548, KL divergence=0.013097160495817661, Entropy=0.1982327103614807, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026274293661117554, KL divergence=0.014591383747756481, Entropy=0.19702404737472534, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02479049563407898, KL divergence=0.01561784092336893, Entropy=0.1972247213125229, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02597585879266262, KL divergence=0.01663229987025261, Entropy=0.19650600850582123, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02626950666308403, KL divergence=0.017479361966252327, Entropy=0.19759438931941986, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.04017862305045128, KL divergence=0.018157677724957466, Entropy=0.19669851660728455, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/454_Step-108736.ckpt']
Uploaded 3 files for checkpoint 454 in 0.58 seconds
saved intermediate frozen graph: current/model/model_454.pb
Best checkpoint number: 437, Last checkpoint number: 452
Copying the frozen checkpoint from ./frozen_models/agent/model_437.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'451'}
Training> Name=main_level/agent, Worker=0, Episode=1101, Total reward=82.25, Steps=108799, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1102, Total reward=55.37, Steps=108870, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1103, Total reward=334.42, Steps=109152, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1104, Total reward=163.43, Steps=109285, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1105, Total reward=9.54, Steps=109307, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1106, Total reward=284.37, Steps=109478, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1107, Total reward=204.87, Steps=109602, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1108, Total reward=393.35, Steps=109889, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1109, Total reward=54.32, Steps=109917, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1110, Total reward=249.93, Steps=110075, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1111, Total reward=25.45, Steps=110102, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1112, Total reward=389.77, Steps=110290, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1113, Total reward=357.99, Steps=110478, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1114, Total reward=84.56, Steps=110527, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1115, Total reward=225.2, Steps=110646, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1116, Total reward=39.34, Steps=110673, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1117, Total reward=260.19, Steps=110901, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1118, Total reward=66.79, Steps=110951, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1119, Total reward=285.27, Steps=111150, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1120, Total reward=262.35, Steps=111377, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1121, Total reward=82.79, Steps=111419, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1122, Total reward=43.71, Steps=111464, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1123, Total reward=236.67, Steps=111668, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1124, Total reward=326.88, Steps=111867, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1125, Total reward=164.3, Steps=112003, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1126, Total reward=218.3, Steps=112131, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1127, Total reward=170.91, Steps=112246, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1128, Total reward=85.08, Steps=112296, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1129, Total reward=407.46, Steps=112520, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1130, Total reward=96.2, Steps=112609, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1131, Total reward=120.02, Steps=112707, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1132, Total reward=439.04, Steps=112983, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1133, Total reward=28.23, Steps=113030, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1134, Total reward=168.62, Steps=113142, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1135, Total reward=153.84, Steps=113218, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1136, Total reward=232.2, Steps=113350, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1137, Total reward=52.63, Steps=113413, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1138, Total reward=125.22, Steps=113501, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1139, Total reward=116.25, Steps=113574, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1140, Total reward=118.26, Steps=113623, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1141, Total reward=70.01, Steps=113672, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1142, Total reward=32.31, Steps=113719, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1143, Total reward=118.63, Steps=113861, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1144, Total reward=166.07, Steps=114001, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1145, Total reward=8.88, Steps=114047, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1146, Total reward=182.37, Steps=114198, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1147, Total reward=434.78, Steps=114434, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1148, Total reward=183.6, Steps=114546, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1149, Total reward=95.43, Steps=114622, Training iteration=22
Training> Name=main_level/agent, Worker=0, Episode=1150, Total reward=104.2, Steps=114728, Training iteration=22
Policy training> Surrogate loss=-0.0007327874773181975, KL divergence=0.00017074713832698762, Entropy=0.2015247493982315, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017588231712579727, KL divergence=0.004125657491385937, Entropy=0.1983102262020111, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.015866445377469063, KL divergence=0.008485916070640087, Entropy=0.19888073205947876, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.032213836908340454, KL divergence=0.011807144619524479, Entropy=0.19925957918167114, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029949694871902466, KL divergence=0.013715516775846481, Entropy=0.1961159110069275, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03269312530755997, KL divergence=0.015155048109591007, Entropy=0.19714738428592682, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0301042553037405, KL divergence=0.016686175018548965, Entropy=0.1982128620147705, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.040105774998664856, KL divergence=0.01762690767645836, Entropy=0.1956251710653305, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03379570320248604, KL divergence=0.01872209459543228, Entropy=0.19653195142745972, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03510168567299843, KL divergence=0.01933133974671364, Entropy=0.19667507708072662, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/455_Step-114728.ckpt']
Uploaded 3 files for checkpoint 455 in 0.64 seconds
saved intermediate frozen graph: current/model/model_455.pb
Best checkpoint number: 453, Last checkpoint number: 453
Copying the frozen checkpoint from ./frozen_models/agent/model_453.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'452'}
Training> Name=main_level/agent, Worker=0, Episode=1151, Total reward=274.68, Steps=114865, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1152, Total reward=15.39, Steps=114878, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1153, Total reward=185.6, Steps=114986, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1154, Total reward=53.62, Steps=115014, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1155, Total reward=36.49, Steps=115032, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1156, Total reward=363.07, Steps=115257, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1157, Total reward=57.08, Steps=115305, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1158, Total reward=150.49, Steps=115378, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1159, Total reward=84.15, Steps=115444, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1160, Total reward=112.03, Steps=115521, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1161, Total reward=435.19, Steps=115779, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1162, Total reward=290.91, Steps=115983, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1163, Total reward=9.61, Steps=115995, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1164, Total reward=153.91, Steps=116164, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1165, Total reward=10.61, Steps=116198, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1166, Total reward=29.85, Steps=116224, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1167, Total reward=67.79, Steps=116274, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1168, Total reward=90.38, Steps=116390, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1169, Total reward=42.75, Steps=116456, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1170, Total reward=73.67, Steps=116549, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1171, Total reward=124.91, Steps=116629, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1172, Total reward=84.42, Steps=116674, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1173, Total reward=76.73, Steps=116744, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1174, Total reward=445.58, Steps=117029, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1175, Total reward=248.88, Steps=117185, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1176, Total reward=241.05, Steps=117328, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1177, Total reward=322.12, Steps=117516, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1178, Total reward=349.31, Steps=117787, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1179, Total reward=101.8, Steps=117869, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1180, Total reward=326.82, Steps=118107, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1181, Total reward=90.45, Steps=118146, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1182, Total reward=196.78, Steps=118366, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1183, Total reward=211.96, Steps=118536, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1184, Total reward=128.87, Steps=118638, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1185, Total reward=80.91, Steps=118729, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1186, Total reward=157.6, Steps=118832, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1187, Total reward=103.28, Steps=118898, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1188, Total reward=126.81, Steps=119021, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1189, Total reward=412.15, Steps=119346, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1190, Total reward=41.79, Steps=119413, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1191, Total reward=436.44, Steps=119710, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1192, Total reward=48.79, Steps=119736, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1193, Total reward=315.77, Steps=119893, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1194, Total reward=27.75, Steps=119904, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1195, Total reward=117.16, Steps=119990, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1196, Total reward=115.87, Steps=120073, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1197, Total reward=62.63, Steps=120131, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1198, Total reward=85.42, Steps=120176, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1199, Total reward=34.51, Steps=120202, Training iteration=23
Training> Name=main_level/agent, Worker=0, Episode=1200, Total reward=279.57, Steps=120417, Training iteration=23
Policy training> Surrogate loss=0.0015879676211625338, KL divergence=0.00010185260907746851, Entropy=0.20418351888656616, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017991870641708374, KL divergence=0.004011480137705803, Entropy=0.20231324434280396, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.024243690073490143, KL divergence=0.008509722538292408, Entropy=0.20111875236034393, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02818170376121998, KL divergence=0.011686638928949833, Entropy=0.19922949373722076, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029338188469409943, KL divergence=0.013902664184570312, Entropy=0.19879712164402008, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03332158178091049, KL divergence=0.015079655684530735, Entropy=0.19755584001541138, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03430664539337158, KL divergence=0.016353977844119072, Entropy=0.19666336476802826, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.034694068133831024, KL divergence=0.017284011468291283, Entropy=0.1970791220664978, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03341132774949074, KL divergence=0.018205327913165092, Entropy=0.19661127030849457, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.036329224705696106, KL divergence=0.018844814971089363, Entropy=0.1965131312608719, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/456_Step-120417.ckpt']
Uploaded 3 files for checkpoint 456 in 0.55 seconds
saved intermediate frozen graph: current/model/model_456.pb
Best checkpoint number: 453, Last checkpoint number: 454
Copying the frozen checkpoint from ./frozen_models/agent/model_453.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'437'}
Training> Name=main_level/agent, Worker=0, Episode=1201, Total reward=64.51, Steps=120457, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1202, Total reward=67.42, Steps=120524, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1203, Total reward=6.84, Steps=120559, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1204, Total reward=251.04, Steps=120764, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1205, Total reward=326.01, Steps=121008, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1206, Total reward=93.61, Steps=121086, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1207, Total reward=431.59, Steps=121332, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1208, Total reward=417.13, Steps=121628, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1209, Total reward=474.5, Steps=121922, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1210, Total reward=33.08, Steps=121961, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1211, Total reward=89.64, Steps=122018, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1212, Total reward=156.58, Steps=122099, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1213, Total reward=33.48, Steps=122110, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1214, Total reward=73.58, Steps=122155, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1215, Total reward=164.18, Steps=122283, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1216, Total reward=225.02, Steps=122388, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1217, Total reward=151.75, Steps=122469, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1218, Total reward=65.69, Steps=122508, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1219, Total reward=94.82, Steps=122581, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1220, Total reward=107.66, Steps=122665, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1221, Total reward=141.31, Steps=122830, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1222, Total reward=34.0, Steps=122851, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1223, Total reward=41.26, Steps=122915, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1224, Total reward=130.8, Steps=123044, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1225, Total reward=352.72, Steps=123326, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1226, Total reward=246.51, Steps=123452, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1227, Total reward=276.15, Steps=123670, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1228, Total reward=356.55, Steps=123889, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1229, Total reward=104.54, Steps=123976, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1230, Total reward=112.84, Steps=124063, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1231, Total reward=89.71, Steps=124130, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1232, Total reward=45.91, Steps=124155, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1233, Total reward=374.14, Steps=124409, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1234, Total reward=58.68, Steps=124453, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1235, Total reward=48.5, Steps=124494, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1236, Total reward=386.36, Steps=124733, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1237, Total reward=300.18, Steps=124924, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1238, Total reward=366.68, Steps=125179, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1239, Total reward=93.3, Steps=125261, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1240, Total reward=87.62, Steps=125310, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1241, Total reward=68.53, Steps=125357, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1242, Total reward=57.99, Steps=125394, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1243, Total reward=210.47, Steps=125534, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1244, Total reward=141.21, Steps=125699, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1245, Total reward=220.27, Steps=125880, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1246, Total reward=390.36, Steps=126165, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1247, Total reward=465.05, Steps=126456, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1248, Total reward=531.95, Steps=126739, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1249, Total reward=31.04, Steps=126760, Training iteration=24
Training> Name=main_level/agent, Worker=0, Episode=1250, Total reward=96.19, Steps=126850, Training iteration=24
Policy training> Surrogate loss=0.004079252947121859, KL divergence=0.0003190176503267139, Entropy=0.19820694625377655, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.02010265551507473, KL divergence=0.00427572475746274, Entropy=0.19656674563884735, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02426265925168991, KL divergence=0.00831644143909216, Entropy=0.1952241212129593, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025966070592403412, KL divergence=0.010911762714385986, Entropy=0.19504497945308685, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028290508314967155, KL divergence=0.012557562440633774, Entropy=0.1941988319158554, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.031718578189611435, KL divergence=0.013460777699947357, Entropy=0.19268853962421417, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03197045624256134, KL divergence=0.015010348521173, Entropy=0.19524133205413818, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030332380905747414, KL divergence=0.015631671994924545, Entropy=0.19423526525497437, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03436141833662987, KL divergence=0.016697729006409645, Entropy=0.19351494312286377, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03229939565062523, KL divergence=0.017468808218836784, Entropy=0.19387583434581757, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/457_Step-126850.ckpt']
Uploaded 3 files for checkpoint 457 in 0.57 seconds
saved intermediate frozen graph: current/model/model_457.pb
Best checkpoint number: 455, Last checkpoint number: 455
Copying the frozen checkpoint from ./frozen_models/agent/model_455.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'453'}
Training> Name=main_level/agent, Worker=0, Episode=1251, Total reward=127.88, Steps=126916, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1252, Total reward=312.32, Steps=127085, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1253, Total reward=85.93, Steps=127142, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1254, Total reward=171.77, Steps=127255, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1255, Total reward=474.3, Steps=127531, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1256, Total reward=272.02, Steps=127769, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1257, Total reward=17.3, Steps=127784, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1258, Total reward=83.3, Steps=127830, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1259, Total reward=377.63, Steps=128119, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1260, Total reward=215.3, Steps=128261, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1261, Total reward=64.7, Steps=128299, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1262, Total reward=306.98, Steps=128507, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1263, Total reward=27.92, Steps=128546, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1264, Total reward=37.17, Steps=128610, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1265, Total reward=225.86, Steps=128758, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1266, Total reward=220.29, Steps=128887, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1267, Total reward=169.72, Steps=129005, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1268, Total reward=195.64, Steps=129107, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1269, Total reward=377.79, Steps=129369, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1270, Total reward=109.64, Steps=129446, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1271, Total reward=73.06, Steps=129521, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1272, Total reward=135.9, Steps=129600, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1273, Total reward=31.35, Steps=129611, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1274, Total reward=132.12, Steps=129695, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1275, Total reward=87.87, Steps=129768, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1276, Total reward=117.41, Steps=129845, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1277, Total reward=211.03, Steps=130037, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1278, Total reward=159.0, Steps=130131, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1279, Total reward=31.17, Steps=130153, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1280, Total reward=72.09, Steps=130211, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1281, Total reward=73.21, Steps=130264, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1282, Total reward=55.19, Steps=130289, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1283, Total reward=150.83, Steps=130419, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1284, Total reward=95.71, Steps=130542, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1285, Total reward=234.54, Steps=130677, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1286, Total reward=90.91, Steps=130746, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1287, Total reward=334.32, Steps=130971, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1288, Total reward=103.98, Steps=131010, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1289, Total reward=17.1, Steps=131023, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1290, Total reward=70.07, Steps=131075, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1291, Total reward=300.93, Steps=131230, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1292, Total reward=107.31, Steps=131284, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1293, Total reward=95.78, Steps=131349, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1294, Total reward=282.55, Steps=131501, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1295, Total reward=78.18, Steps=131559, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1296, Total reward=79.24, Steps=131606, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1297, Total reward=50.46, Steps=131656, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1298, Total reward=171.89, Steps=131735, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1299, Total reward=120.95, Steps=131840, Training iteration=25
Training> Name=main_level/agent, Worker=0, Episode=1300, Total reward=131.54, Steps=131919, Training iteration=25
Policy training> Surrogate loss=-0.004087860230356455, KL divergence=0.00021557706349994987, Entropy=0.20696616172790527, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.012808142229914665, KL divergence=0.0038577641826123, Entropy=0.20383137464523315, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.019968755543231964, KL divergence=0.007803658489137888, Entropy=0.20652534067630768, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.032690927386283875, KL divergence=0.01017819158732891, Entropy=0.20178735256195068, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03416633978486061, KL divergence=0.012494687922298908, Entropy=0.20081818103790283, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.034721944481134415, KL divergence=0.014380861073732376, Entropy=0.20350971817970276, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03233405575156212, KL divergence=0.015838289633393288, Entropy=0.2014429271221161, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.04146559536457062, KL divergence=0.017211420461535454, Entropy=0.19983533024787903, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03311111032962799, KL divergence=0.018022431060671806, Entropy=0.1999949812889099, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.030299441888928413, KL divergence=0.019380351528525352, Entropy=0.19944724440574646, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/458_Step-131919.ckpt']
Uploaded 3 files for checkpoint 458 in 0.63 seconds
saved intermediate frozen graph: current/model/model_458.pb
Best checkpoint number: 455, Last checkpoint number: 456
Copying the frozen checkpoint from ./frozen_models/agent/model_455.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'454'}
Training> Name=main_level/agent, Worker=0, Episode=1301, Total reward=90.96, Steps=132013, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1302, Total reward=40.03, Steps=132059, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1303, Total reward=36.03, Steps=132105, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1304, Total reward=242.8, Steps=132297, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1305, Total reward=206.62, Steps=132451, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1306, Total reward=198.22, Steps=132584, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1307, Total reward=422.92, Steps=132828, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1308, Total reward=181.49, Steps=132948, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1309, Total reward=31.6, Steps=132980, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1310, Total reward=182.06, Steps=133077, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1311, Total reward=155.75, Steps=133171, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1312, Total reward=315.56, Steps=133400, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1313, Total reward=222.5, Steps=133516, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1314, Total reward=445.22, Steps=133794, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1315, Total reward=42.99, Steps=133812, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1316, Total reward=31.68, Steps=133829, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1317, Total reward=398.84, Steps=134072, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1318, Total reward=47.35, Steps=134125, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1319, Total reward=117.5, Steps=134254, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1320, Total reward=45.61, Steps=134314, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1321, Total reward=87.78, Steps=134356, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1322, Total reward=72.24, Steps=134437, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1323, Total reward=231.61, Steps=134606, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1324, Total reward=471.61, Steps=134895, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1325, Total reward=87.09, Steps=135024, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1326, Total reward=237.79, Steps=135177, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1327, Total reward=273.77, Steps=135365, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1328, Total reward=243.14, Steps=135545, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1329, Total reward=141.77, Steps=135629, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1330, Total reward=486.87, Steps=135917, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1331, Total reward=110.76, Steps=135981, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1332, Total reward=379.59, Steps=136282, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1333, Total reward=11.62, Steps=136302, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1334, Total reward=90.07, Steps=136352, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1335, Total reward=238.77, Steps=136473, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1336, Total reward=83.66, Steps=136538, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1337, Total reward=80.98, Steps=136593, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1338, Total reward=149.53, Steps=136740, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1339, Total reward=456.99, Steps=137041, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1340, Total reward=99.49, Steps=137089, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1341, Total reward=81.19, Steps=137131, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1342, Total reward=122.02, Steps=137265, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1343, Total reward=331.78, Steps=137499, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1344, Total reward=94.05, Steps=137586, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1345, Total reward=134.16, Steps=137701, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1346, Total reward=151.05, Steps=137797, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1347, Total reward=144.9, Steps=137886, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1348, Total reward=93.78, Steps=137943, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1349, Total reward=145.68, Steps=138059, Training iteration=26
Training> Name=main_level/agent, Worker=0, Episode=1350, Total reward=326.78, Steps=138312, Training iteration=26
Policy training> Surrogate loss=0.002658986486494541, KL divergence=0.0003433560486882925, Entropy=0.19513529539108276, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015590609051287174, KL divergence=0.004232488106936216, Entropy=0.19178956747055054, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.023718098178505898, KL divergence=0.00803727749735117, Entropy=0.19161798059940338, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025791466236114502, KL divergence=0.010692975483834743, Entropy=0.19151562452316284, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.030386721715331078, KL divergence=0.01239770743995905, Entropy=0.19073279201984406, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02933243103325367, KL divergence=0.013602301478385925, Entropy=0.18987280130386353, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.027128063142299652, KL divergence=0.015011237002909184, Entropy=0.19082631170749664, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03241837024688721, KL divergence=0.015677252784371376, Entropy=0.1896355003118515, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03583460673689842, KL divergence=0.016567310318350792, Entropy=0.1909608691930771, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03583364188671112, KL divergence=0.017379816621541977, Entropy=0.19054466485977173, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/459_Step-138312.ckpt']
Uploaded 3 files for checkpoint 459 in 0.49 seconds
saved intermediate frozen graph: current/model/model_459.pb
Best checkpoint number: 457, Last checkpoint number: 457
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'455'}
Training> Name=main_level/agent, Worker=0, Episode=1351, Total reward=188.46, Steps=138409, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1352, Total reward=74.3, Steps=138466, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1353, Total reward=78.56, Steps=138492, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1354, Total reward=28.7, Steps=138503, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1355, Total reward=255.99, Steps=138652, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1356, Total reward=391.86, Steps=138941, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1357, Total reward=420.08, Steps=139189, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1358, Total reward=75.79, Steps=139259, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1359, Total reward=23.65, Steps=139288, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1360, Total reward=91.58, Steps=139375, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1361, Total reward=301.36, Steps=139579, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1362, Total reward=275.91, Steps=139773, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1363, Total reward=198.93, Steps=139963, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1364, Total reward=15.35, Steps=140019, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1365, Total reward=437.92, Steps=140315, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1366, Total reward=103.77, Steps=140385, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1367, Total reward=203.76, Steps=140490, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1368, Total reward=104.29, Steps=140558, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1369, Total reward=165.13, Steps=140641, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1370, Total reward=424.9, Steps=140926, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1371, Total reward=108.98, Steps=140993, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1372, Total reward=230.68, Steps=141144, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1373, Total reward=72.15, Steps=141186, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1374, Total reward=57.75, Steps=141230, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1375, Total reward=215.68, Steps=141356, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1376, Total reward=140.14, Steps=141429, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1377, Total reward=80.98, Steps=141501, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1378, Total reward=173.37, Steps=141619, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1379, Total reward=78.58, Steps=141702, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1380, Total reward=281.91, Steps=141910, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1381, Total reward=117.61, Steps=141979, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1382, Total reward=54.14, Steps=142012, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1383, Total reward=243.42, Steps=142211, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1384, Total reward=6.55, Steps=142245, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1385, Total reward=205.1, Steps=142400, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1386, Total reward=225.86, Steps=142536, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1387, Total reward=256.11, Steps=142680, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1388, Total reward=150.01, Steps=142769, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1389, Total reward=217.81, Steps=142908, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1390, Total reward=22.99, Steps=142939, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1391, Total reward=353.4, Steps=143165, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1392, Total reward=277.28, Steps=143342, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1393, Total reward=97.39, Steps=143385, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1394, Total reward=185.73, Steps=143508, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1395, Total reward=232.64, Steps=143681, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1396, Total reward=184.62, Steps=143793, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1397, Total reward=427.55, Steps=144077, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1398, Total reward=19.0, Steps=144090, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1399, Total reward=110.83, Steps=144145, Training iteration=27
Training> Name=main_level/agent, Worker=0, Episode=1400, Total reward=270.82, Steps=144370, Training iteration=27
Policy training> Surrogate loss=0.003914023749530315, KL divergence=0.00023450284788850695, Entropy=0.1920391470193863, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01583179086446762, KL divergence=0.00413214648142457, Entropy=0.18893595039844513, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.021274326369166374, KL divergence=0.007494545076042414, Entropy=0.18426214158535004, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.020858783274888992, KL divergence=0.009945564903318882, Entropy=0.18585406243801117, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029387682676315308, KL divergence=0.011877898126840591, Entropy=0.18521417677402496, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030941851437091827, KL divergence=0.013176619075238705, Entropy=0.1861807256937027, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02829088643193245, KL divergence=0.014378195628523827, Entropy=0.18585222959518433, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030836043879389763, KL divergence=0.015409332700073719, Entropy=0.18564185500144958, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03036663495004177, KL divergence=0.016110315918922424, Entropy=0.184162437915802, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.04093833267688751, KL divergence=0.017266057431697845, Entropy=0.18522286415100098, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/460_Step-144370.ckpt']
Uploaded 3 files for checkpoint 460 in 0.57 seconds
saved intermediate frozen graph: current/model/model_460.pb
Best checkpoint number: 457, Last checkpoint number: 458
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'456'}
Training> Name=main_level/agent, Worker=0, Episode=1401, Total reward=69.05, Steps=144403, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1402, Total reward=218.82, Steps=144603, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1403, Total reward=9.24, Steps=144640, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1404, Total reward=461.5, Steps=144919, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1405, Total reward=111.1, Steps=145009, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1406, Total reward=460.4, Steps=145267, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1407, Total reward=193.11, Steps=145376, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1408, Total reward=106.33, Steps=145457, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1409, Total reward=33.08, Steps=145502, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1410, Total reward=356.96, Steps=145729, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1411, Total reward=284.54, Steps=145881, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1412, Total reward=46.08, Steps=145906, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1413, Total reward=240.08, Steps=146018, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1414, Total reward=188.54, Steps=146141, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1415, Total reward=30.44, Steps=146158, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1416, Total reward=43.89, Steps=146191, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1417, Total reward=80.08, Steps=146240, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1418, Total reward=69.21, Steps=146279, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1419, Total reward=127.77, Steps=146378, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1420, Total reward=458.03, Steps=146673, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1421, Total reward=112.23, Steps=146762, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1422, Total reward=45.54, Steps=146810, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1423, Total reward=23.11, Steps=146829, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1424, Total reward=12.04, Steps=146868, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1425, Total reward=118.39, Steps=146980, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1426, Total reward=99.97, Steps=147050, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1427, Total reward=114.69, Steps=147126, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1428, Total reward=458.15, Steps=147398, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1429, Total reward=120.5, Steps=147454, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1430, Total reward=251.98, Steps=147617, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1431, Total reward=142.16, Steps=147715, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1432, Total reward=6.19, Steps=147729, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1433, Total reward=58.48, Steps=147770, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1434, Total reward=271.43, Steps=147912, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1435, Total reward=366.07, Steps=148188, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1436, Total reward=96.57, Steps=148287, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1437, Total reward=88.84, Steps=148353, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1438, Total reward=76.49, Steps=148390, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1439, Total reward=19.95, Steps=148422, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1440, Total reward=83.04, Steps=148512, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1441, Total reward=182.27, Steps=148673, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1442, Total reward=57.01, Steps=148704, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1443, Total reward=17.95, Steps=148730, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1444, Total reward=297.07, Steps=149029, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1445, Total reward=391.83, Steps=149298, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1446, Total reward=94.69, Steps=149361, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1447, Total reward=175.62, Steps=149470, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1448, Total reward=375.03, Steps=149704, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1449, Total reward=32.38, Steps=149766, Training iteration=28
Training> Name=main_level/agent, Worker=0, Episode=1450, Total reward=206.99, Steps=149931, Training iteration=28
Policy training> Surrogate loss=-0.0007033273577690125, KL divergence=0.00016796099953353405, Entropy=0.19770649075508118, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.009481136687099934, KL divergence=0.003660149872303009, Entropy=0.19485923647880554, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.015764562413096428, KL divergence=0.007756808307021856, Entropy=0.19241845607757568, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.031260598450899124, KL divergence=0.011068368330597878, Entropy=0.19230125844478607, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.032065607607364655, KL divergence=0.013384389691054821, Entropy=0.19092950224876404, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02865367755293846, KL divergence=0.015554207377135754, Entropy=0.19172175228595734, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03223540633916855, KL divergence=0.0167712215334177, Entropy=0.19193381071090698, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02916664443910122, KL divergence=0.017728446051478386, Entropy=0.1937514692544937, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.037695012986660004, KL divergence=0.01813780888915062, Entropy=0.1944471150636673, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.04049747809767723, KL divergence=0.019524531438946724, Entropy=0.1937851905822754, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/461_Step-149931.ckpt']
Uploaded 3 files for checkpoint 461 in 0.56 seconds
saved intermediate frozen graph: current/model/model_461.pb
Best checkpoint number: 457, Last checkpoint number: 459
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'458'}
Training> Name=main_level/agent, Worker=0, Episode=1451, Total reward=88.86, Steps=149995, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1452, Total reward=375.7, Steps=150166, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1453, Total reward=89.15, Steps=150206, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1454, Total reward=212.12, Steps=150348, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1455, Total reward=39.37, Steps=150367, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1456, Total reward=400.22, Steps=150627, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1457, Total reward=310.49, Steps=150833, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1458, Total reward=339.03, Steps=151067, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1459, Total reward=124.1, Steps=151183, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1460, Total reward=37.51, Steps=151212, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1461, Total reward=358.0, Steps=151510, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1462, Total reward=247.29, Steps=151702, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1463, Total reward=3.06, Steps=151718, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1464, Total reward=8.47, Steps=151762, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1465, Total reward=14.98, Steps=151807, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1466, Total reward=111.03, Steps=151870, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1467, Total reward=79.5, Steps=151931, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1468, Total reward=110.35, Steps=152010, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1469, Total reward=28.34, Steps=152027, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1470, Total reward=163.69, Steps=152129, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1471, Total reward=112.72, Steps=152196, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1472, Total reward=137.65, Steps=152271, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1473, Total reward=30.0, Steps=152282, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1474, Total reward=236.15, Steps=152483, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1475, Total reward=182.46, Steps=152601, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1476, Total reward=408.5, Steps=152884, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1477, Total reward=215.77, Steps=153105, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1478, Total reward=448.25, Steps=153376, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1479, Total reward=19.95, Steps=153394, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1480, Total reward=97.91, Steps=153504, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1481, Total reward=385.17, Steps=153791, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1482, Total reward=50.48, Steps=153881, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1483, Total reward=172.24, Steps=154029, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1484, Total reward=22.53, Steps=154078, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1485, Total reward=6.89, Steps=154090, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1486, Total reward=432.29, Steps=154362, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1487, Total reward=225.06, Steps=154479, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1488, Total reward=155.16, Steps=154551, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1489, Total reward=119.77, Steps=154667, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1490, Total reward=265.2, Steps=154819, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1491, Total reward=21.91, Steps=154875, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1492, Total reward=125.44, Steps=154949, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1493, Total reward=81.61, Steps=154989, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1494, Total reward=263.6, Steps=155140, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1495, Total reward=409.71, Steps=155398, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1496, Total reward=415.24, Steps=155681, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1497, Total reward=165.54, Steps=155803, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1498, Total reward=143.3, Steps=155925, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1499, Total reward=314.96, Steps=156150, Training iteration=29
Training> Name=main_level/agent, Worker=0, Episode=1500, Total reward=469.79, Steps=156445, Training iteration=29
Policy training> Surrogate loss=0.0016722687287256122, KL divergence=0.0002292457065777853, Entropy=0.19707532227039337, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01438217330724001, KL divergence=0.004036063328385353, Entropy=0.1965411901473999, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.024356631562113762, KL divergence=0.007863052189350128, Entropy=0.19493788480758667, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02565084956586361, KL divergence=0.010454208590090275, Entropy=0.19369997084140778, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028846561908721924, KL divergence=0.012418478727340698, Entropy=0.192987322807312, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.028723470866680145, KL divergence=0.013771481812000275, Entropy=0.19276566803455353, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.030615679919719696, KL divergence=0.014893516898155212, Entropy=0.1936214417219162, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030278386548161507, KL divergence=0.01574542187154293, Entropy=0.1938406080007553, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03738604485988617, KL divergence=0.01627465896308422, Entropy=0.1912974864244461, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.037649352103471756, KL divergence=0.01702631637454033, Entropy=0.19197803735733032, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/462_Step-156445.ckpt']
Uploaded 3 files for checkpoint 462 in 0.50 seconds
saved intermediate frozen graph: current/model/model_462.pb
Best checkpoint number: 457, Last checkpoint number: 460
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'459'}
Training> Name=main_level/agent, Worker=0, Episode=1501, Total reward=476.74, Steps=156720, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1502, Total reward=50.87, Steps=156750, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1503, Total reward=33.94, Steps=156799, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1504, Total reward=175.3, Steps=156925, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1505, Total reward=343.28, Steps=157147, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1506, Total reward=280.5, Steps=157356, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1507, Total reward=437.15, Steps=157632, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1508, Total reward=92.26, Steps=157675, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1509, Total reward=51.3, Steps=157746, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1510, Total reward=389.03, Steps=157953, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1511, Total reward=164.43, Steps=158062, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1512, Total reward=45.44, Steps=158088, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1513, Total reward=94.1, Steps=158142, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1514, Total reward=458.59, Steps=158431, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1515, Total reward=437.92, Steps=158712, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1516, Total reward=144.29, Steps=158797, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1517, Total reward=368.18, Steps=159025, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1518, Total reward=33.82, Steps=159040, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1519, Total reward=298.45, Steps=159265, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1520, Total reward=42.93, Steps=159288, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1521, Total reward=184.75, Steps=159442, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1522, Total reward=239.65, Steps=159615, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1523, Total reward=101.24, Steps=159738, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1524, Total reward=6.88, Steps=159772, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1525, Total reward=3.26, Steps=159807, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1526, Total reward=258.97, Steps=159959, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1527, Total reward=92.95, Steps=160022, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1528, Total reward=196.65, Steps=160129, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1529, Total reward=466.72, Steps=160420, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1530, Total reward=333.24, Steps=160625, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1531, Total reward=150.85, Steps=160759, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1532, Total reward=97.03, Steps=160801, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1533, Total reward=93.29, Steps=160867, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1534, Total reward=421.91, Steps=161128, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1535, Total reward=121.22, Steps=161192, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1536, Total reward=471.13, Steps=161485, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1537, Total reward=349.44, Steps=161740, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1538, Total reward=34.42, Steps=161784, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1539, Total reward=18.23, Steps=161812, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1540, Total reward=383.39, Steps=162025, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1541, Total reward=88.83, Steps=162123, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1542, Total reward=469.05, Steps=162397, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1543, Total reward=3.06, Steps=162414, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1544, Total reward=185.33, Steps=162546, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1545, Total reward=7.54, Steps=162565, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1546, Total reward=182.28, Steps=162703, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1547, Total reward=398.76, Steps=162908, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1548, Total reward=131.8, Steps=163008, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1549, Total reward=129.48, Steps=163116, Training iteration=30
Training> Name=main_level/agent, Worker=0, Episode=1550, Total reward=139.6, Steps=163219, Training iteration=30
Policy training> Surrogate loss=-0.0015064483741298318, KL divergence=0.000240883236983791, Entropy=0.1921163648366928, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.016551833599805832, KL divergence=0.003899458795785904, Entropy=0.19010499119758606, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.025540567934513092, KL divergence=0.008253202773630619, Entropy=0.1895478069782257, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02612117864191532, KL divergence=0.010763794183731079, Entropy=0.18895047903060913, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028045376762747765, KL divergence=0.012566200457513332, Entropy=0.18917804956436157, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026559416204690933, KL divergence=0.013776766136288643, Entropy=0.18898800015449524, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03213244304060936, KL divergence=0.014825135469436646, Entropy=0.18843677639961243, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03555970638990402, KL divergence=0.015427803620696068, Entropy=0.1873050034046173, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03123338893055916, KL divergence=0.016375215724110603, Entropy=0.18718907237052917, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.031882695853710175, KL divergence=0.017171038314700127, Entropy=0.18779215216636658, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/463_Step-163219.ckpt']
Uploaded 3 files for checkpoint 463 in 0.64 seconds
saved intermediate frozen graph: current/model/model_463.pb
Best checkpoint number: 457, Last checkpoint number: 461
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'460'}
Training> Name=main_level/agent, Worker=0, Episode=1551, Total reward=333.23, Steps=163422, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1552, Total reward=179.87, Steps=163520, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1553, Total reward=120.16, Steps=163578, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1554, Total reward=67.11, Steps=163624, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1555, Total reward=30.5, Steps=163655, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1556, Total reward=391.98, Steps=163962, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1557, Total reward=73.61, Steps=164022, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1558, Total reward=140.84, Steps=164113, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1559, Total reward=81.61, Steps=164161, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1560, Total reward=389.53, Steps=164423, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1561, Total reward=202.63, Steps=164605, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1562, Total reward=84.74, Steps=164670, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1563, Total reward=115.75, Steps=164782, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1564, Total reward=234.51, Steps=164968, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1565, Total reward=409.95, Steps=165255, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1566, Total reward=101.68, Steps=165362, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1567, Total reward=414.46, Steps=165598, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1568, Total reward=177.08, Steps=165695, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1569, Total reward=151.67, Steps=165794, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1570, Total reward=7.43, Steps=165808, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1571, Total reward=103.26, Steps=165878, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1572, Total reward=407.19, Steps=166164, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1573, Total reward=83.19, Steps=166205, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1574, Total reward=149.6, Steps=166281, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1575, Total reward=371.29, Steps=166563, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1576, Total reward=283.64, Steps=166788, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1577, Total reward=178.54, Steps=166903, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1578, Total reward=477.0, Steps=167181, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1579, Total reward=37.33, Steps=167210, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1580, Total reward=113.65, Steps=167268, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1581, Total reward=415.99, Steps=167522, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1582, Total reward=69.29, Steps=167569, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1583, Total reward=240.15, Steps=167737, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1584, Total reward=163.0, Steps=167870, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1585, Total reward=426.99, Steps=168152, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1586, Total reward=25.16, Steps=168180, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1587, Total reward=368.25, Steps=168400, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1588, Total reward=119.57, Steps=168479, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1589, Total reward=251.4, Steps=168668, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1590, Total reward=53.94, Steps=168723, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1591, Total reward=36.52, Steps=168751, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1592, Total reward=101.66, Steps=168837, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1593, Total reward=27.99, Steps=168848, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1594, Total reward=75.14, Steps=168903, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1595, Total reward=29.64, Steps=168918, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1596, Total reward=392.82, Steps=169230, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1597, Total reward=200.49, Steps=169426, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1598, Total reward=156.68, Steps=169561, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1599, Total reward=119.41, Steps=169619, Training iteration=31
Training> Name=main_level/agent, Worker=0, Episode=1600, Total reward=451.53, Steps=169917, Training iteration=31
Policy training> Surrogate loss=-0.00018345535499975085, KL divergence=0.00033837431692518294, Entropy=0.1887502819299698, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01774129644036293, KL divergence=0.005374620668590069, Entropy=0.18501898646354675, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.021124638617038727, KL divergence=0.009242461062967777, Entropy=0.18300233781337738, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02573942393064499, KL divergence=0.011273052543401718, Entropy=0.18314506113529205, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026951689273118973, KL divergence=0.012602965347468853, Entropy=0.18249128758907318, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0295547042042017, KL divergence=0.01381660345941782, Entropy=0.18234467506408691, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.030458757653832436, KL divergence=0.014951419085264206, Entropy=0.18308840692043304, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03059488534927368, KL divergence=0.015827495604753494, Entropy=0.18249152600765228, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03375804424285889, KL divergence=0.016564881429076195, Entropy=0.18232785165309906, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03325875476002693, KL divergence=0.017277449369430542, Entropy=0.18264678120613098, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/464_Step-169917.ckpt']
Uploaded 3 files for checkpoint 464 in 0.59 seconds
saved intermediate frozen graph: current/model/model_464.pb
Best checkpoint number: 457, Last checkpoint number: 462
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'461'}
Training> Name=main_level/agent, Worker=0, Episode=1601, Total reward=172.18, Steps=170065, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1602, Total reward=66.79, Steps=170126, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1603, Total reward=186.39, Steps=170259, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1604, Total reward=113.25, Steps=170372, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1605, Total reward=200.81, Steps=170542, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1606, Total reward=124.16, Steps=170611, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1607, Total reward=105.8, Steps=170708, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1608, Total reward=357.74, Steps=170883, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1609, Total reward=54.86, Steps=170948, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1610, Total reward=76.61, Steps=171032, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1611, Total reward=498.2, Steps=171322, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1612, Total reward=208.05, Steps=171439, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1613, Total reward=88.91, Steps=171482, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1614, Total reward=152.17, Steps=171598, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1615, Total reward=455.62, Steps=171871, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1616, Total reward=236.09, Steps=172038, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1617, Total reward=80.17, Steps=172114, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1618, Total reward=219.51, Steps=172304, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1619, Total reward=87.36, Steps=172378, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1620, Total reward=86.65, Steps=172461, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1621, Total reward=448.76, Steps=172727, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1622, Total reward=183.26, Steps=172900, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1623, Total reward=23.12, Steps=172917, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1624, Total reward=144.86, Steps=173073, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1625, Total reward=440.9, Steps=173341, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1626, Total reward=116.75, Steps=173419, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1627, Total reward=185.6, Steps=173551, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1628, Total reward=153.66, Steps=173670, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1629, Total reward=36.25, Steps=173696, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1630, Total reward=217.09, Steps=173848, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1631, Total reward=133.23, Steps=173935, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1632, Total reward=98.84, Steps=174022, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1633, Total reward=407.42, Steps=174312, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1634, Total reward=264.29, Steps=174471, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1635, Total reward=40.31, Steps=174489, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1636, Total reward=426.7, Steps=174794, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1637, Total reward=158.2, Steps=174944, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1638, Total reward=341.59, Steps=175190, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1639, Total reward=451.63, Steps=175469, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1640, Total reward=126.22, Steps=175561, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1641, Total reward=287.28, Steps=175759, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1642, Total reward=59.64, Steps=175800, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1643, Total reward=222.75, Steps=175981, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1644, Total reward=13.2, Steps=176014, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1645, Total reward=96.3, Steps=176093, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1646, Total reward=114.95, Steps=176190, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1647, Total reward=173.59, Steps=176307, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1648, Total reward=100.02, Steps=176347, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1649, Total reward=323.6, Steps=176509, Training iteration=32
Training> Name=main_level/agent, Worker=0, Episode=1650, Total reward=318.29, Steps=176704, Training iteration=32
Policy training> Surrogate loss=0.00021455573732964694, KL divergence=0.0002977823605760932, Entropy=0.19697611033916473, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017590101808309555, KL divergence=0.0036092130467295647, Entropy=0.1943950355052948, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.022199906408786774, KL divergence=0.007817955687642097, Entropy=0.1940343976020813, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025638652965426445, KL divergence=0.010908837430179119, Entropy=0.19315436482429504, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028434058651328087, KL divergence=0.01272868923842907, Entropy=0.19317051768302917, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.027358021587133408, KL divergence=0.014242937788367271, Entropy=0.1919451355934143, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03502749651670456, KL divergence=0.01578417792916298, Entropy=0.1923350691795349, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.0337962843477726, KL divergence=0.016753043979406357, Entropy=0.19191156327724457, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03580326586961746, KL divergence=0.017637286335229874, Entropy=0.19140830636024475, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03513893112540245, KL divergence=0.018471794202923775, Entropy=0.19051454961299896, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/465_Step-176704.ckpt']
Uploaded 3 files for checkpoint 465 in 0.53 seconds
saved intermediate frozen graph: current/model/model_465.pb
Best checkpoint number: 457, Last checkpoint number: 463
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'462'}
Training> Name=main_level/agent, Worker=0, Episode=1651, Total reward=26.02, Steps=176741, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1652, Total reward=373.35, Steps=177024, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1653, Total reward=140.17, Steps=177104, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1654, Total reward=189.11, Steps=177223, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1655, Total reward=120.9, Steps=177296, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1656, Total reward=464.39, Steps=177591, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1657, Total reward=137.48, Steps=177707, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1658, Total reward=127.88, Steps=177793, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1659, Total reward=217.97, Steps=177954, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1660, Total reward=52.48, Steps=177995, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1661, Total reward=258.32, Steps=178173, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1662, Total reward=29.66, Steps=178221, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1663, Total reward=26.31, Steps=178257, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1664, Total reward=214.69, Steps=178426, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1665, Total reward=83.12, Steps=178550, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1666, Total reward=450.38, Steps=178809, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1667, Total reward=307.65, Steps=178980, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1668, Total reward=424.81, Steps=179279, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1669, Total reward=137.56, Steps=179376, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1670, Total reward=265.21, Steps=179537, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1671, Total reward=332.54, Steps=179759, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1672, Total reward=95.0, Steps=179812, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1673, Total reward=82.17, Steps=179854, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1674, Total reward=276.55, Steps=180051, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1675, Total reward=333.42, Steps=180277, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1676, Total reward=87.65, Steps=180344, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1677, Total reward=174.56, Steps=180426, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1678, Total reward=57.44, Steps=180469, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1679, Total reward=26.09, Steps=180502, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1680, Total reward=373.6, Steps=180801, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1681, Total reward=253.66, Steps=180990, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1682, Total reward=53.34, Steps=181019, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1683, Total reward=100.82, Steps=181187, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1684, Total reward=225.59, Steps=181370, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1685, Total reward=136.13, Steps=181479, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1686, Total reward=129.09, Steps=181596, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1687, Total reward=264.57, Steps=181766, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1688, Total reward=134.86, Steps=181847, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1689, Total reward=31.06, Steps=181865, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1690, Total reward=287.21, Steps=182015, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1691, Total reward=125.06, Steps=182107, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1692, Total reward=388.17, Steps=182386, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1693, Total reward=81.47, Steps=182427, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1694, Total reward=275.27, Steps=182585, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1695, Total reward=98.78, Steps=182680, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1696, Total reward=287.85, Steps=182899, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1697, Total reward=170.23, Steps=183034, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1698, Total reward=353.53, Steps=183278, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1699, Total reward=170.37, Steps=183424, Training iteration=33
Training> Name=main_level/agent, Worker=0, Episode=1700, Total reward=302.88, Steps=183651, Training iteration=33
Policy training> Surrogate loss=-0.000920482852961868, KL divergence=0.0004063005035277456, Entropy=0.19669793546199799, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.019320975989103317, KL divergence=0.004842662252485752, Entropy=0.195591002702713, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.025632180273532867, KL divergence=0.00904988031834364, Entropy=0.19252675771713257, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025309216231107712, KL divergence=0.01113833300769329, Entropy=0.19315870106220245, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.030450858175754547, KL divergence=0.012833853252232075, Entropy=0.19326359033584595, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.025634808465838432, KL divergence=0.013930246233940125, Entropy=0.19230739772319794, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03283268213272095, KL divergence=0.015008960850536823, Entropy=0.19205668568611145, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030418597161769867, KL divergence=0.015742121264338493, Entropy=0.19247685372829437, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.032686516642570496, KL divergence=0.016163146123290062, Entropy=0.19206850230693817, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03615709766745567, KL divergence=0.017111487686634064, Entropy=0.19252432882785797, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/466_Step-183651.ckpt']
Uploaded 3 files for checkpoint 466 in 0.60 seconds
saved intermediate frozen graph: current/model/model_466.pb
Best checkpoint number: 457, Last checkpoint number: 464
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'463'}
Training> Name=main_level/agent, Worker=0, Episode=1701, Total reward=385.58, Steps=183964, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1702, Total reward=53.89, Steps=183991, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1703, Total reward=118.53, Steps=184115, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1704, Total reward=428.3, Steps=184405, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1705, Total reward=446.13, Steps=184680, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1706, Total reward=159.23, Steps=184797, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1707, Total reward=96.28, Steps=184857, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1708, Total reward=92.29, Steps=184899, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1709, Total reward=310.45, Steps=185116, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1710, Total reward=124.34, Steps=185197, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1711, Total reward=239.6, Steps=185357, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1712, Total reward=9.84, Steps=185369, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1713, Total reward=31.71, Steps=185381, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1714, Total reward=60.25, Steps=185410, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1715, Total reward=45.36, Steps=185460, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1716, Total reward=362.39, Steps=185733, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1717, Total reward=277.17, Steps=185971, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1718, Total reward=357.54, Steps=186197, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1719, Total reward=467.6, Steps=186496, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1720, Total reward=266.5, Steps=186671, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1721, Total reward=321.28, Steps=186885, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1722, Total reward=50.65, Steps=186931, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1723, Total reward=34.06, Steps=186973, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1724, Total reward=450.69, Steps=187265, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1725, Total reward=396.41, Steps=187534, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1726, Total reward=215.91, Steps=187651, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1727, Total reward=390.42, Steps=187889, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1728, Total reward=79.95, Steps=187937, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1729, Total reward=410.14, Steps=188242, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1730, Total reward=303.32, Steps=188393, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1731, Total reward=350.5, Steps=188666, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1732, Total reward=274.68, Steps=188807, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1733, Total reward=37.95, Steps=188833, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1734, Total reward=85.86, Steps=188906, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1735, Total reward=481.79, Steps=189187, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1736, Total reward=96.67, Steps=189269, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1737, Total reward=60.78, Steps=189338, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1738, Total reward=374.36, Steps=189607, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1739, Total reward=33.59, Steps=189635, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1740, Total reward=232.15, Steps=189840, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1741, Total reward=330.04, Steps=190078, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1742, Total reward=82.62, Steps=190166, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1743, Total reward=15.65, Steps=190214, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1744, Total reward=107.36, Steps=190343, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1745, Total reward=385.67, Steps=190640, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1746, Total reward=160.45, Steps=190749, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1747, Total reward=59.94, Steps=190806, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1748, Total reward=210.74, Steps=190908, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1749, Total reward=73.61, Steps=190961, Training iteration=34
Training> Name=main_level/agent, Worker=0, Episode=1750, Total reward=15.05, Steps=191005, Training iteration=34
Policy training> Surrogate loss=0.0010216837981715798, KL divergence=0.0004542111710179597, Entropy=0.19300992786884308, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015430418774485588, KL divergence=0.0047205607406795025, Entropy=0.19280095398426056, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.023686544969677925, KL divergence=0.007870807312428951, Entropy=0.19180157780647278, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02423301339149475, KL divergence=0.010169034823775291, Entropy=0.1918053925037384, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02448192611336708, KL divergence=0.011765017174184322, Entropy=0.1912149339914322, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.028779197484254837, KL divergence=0.013006203807890415, Entropy=0.19010992348194122, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02958589792251587, KL divergence=0.01378499437123537, Entropy=0.19015951454639435, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030638298019766808, KL divergence=0.014920556917786598, Entropy=0.19021795690059662, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03159886971116066, KL divergence=0.015609067864716053, Entropy=0.19059614837169647, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03441987186670303, KL divergence=0.016228001564741135, Entropy=0.19078299403190613, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/467_Step-191005.ckpt']
Uploaded 3 files for checkpoint 467 in 0.50 seconds
saved intermediate frozen graph: current/model/model_467.pb
Best checkpoint number: 457, Last checkpoint number: 465
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'464'}
Training> Name=main_level/agent, Worker=0, Episode=1751, Total reward=3.82, Steps=191019, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1752, Total reward=11.22, Steps=191031, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1753, Total reward=89.89, Steps=191088, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1754, Total reward=113.15, Steps=191149, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1755, Total reward=214.22, Steps=191298, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1756, Total reward=119.68, Steps=191374, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1757, Total reward=107.5, Steps=191434, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1758, Total reward=79.4, Steps=191489, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1759, Total reward=237.74, Steps=191698, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1760, Total reward=469.86, Steps=192000, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1761, Total reward=69.99, Steps=192041, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1762, Total reward=54.98, Steps=192111, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1763, Total reward=225.59, Steps=192288, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1764, Total reward=138.48, Steps=192426, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1765, Total reward=208.4, Steps=192601, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1766, Total reward=3.54, Steps=192614, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1767, Total reward=64.09, Steps=192673, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1768, Total reward=66.44, Steps=192732, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1769, Total reward=167.97, Steps=192846, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1770, Total reward=110.56, Steps=192938, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1771, Total reward=135.89, Steps=193020, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1772, Total reward=312.34, Steps=193256, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1773, Total reward=125.15, Steps=193315, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1774, Total reward=485.89, Steps=193600, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1775, Total reward=239.27, Steps=193733, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1776, Total reward=117.24, Steps=193810, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1777, Total reward=80.93, Steps=193874, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1778, Total reward=161.49, Steps=193960, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1779, Total reward=376.68, Steps=194204, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1780, Total reward=231.44, Steps=194385, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1781, Total reward=47.9, Steps=194410, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1782, Total reward=72.76, Steps=194496, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1783, Total reward=239.33, Steps=194683, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1784, Total reward=445.63, Steps=194951, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1785, Total reward=130.51, Steps=195082, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1786, Total reward=85.65, Steps=195157, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1787, Total reward=484.27, Steps=195434, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1788, Total reward=199.19, Steps=195544, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1789, Total reward=41.58, Steps=195580, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1790, Total reward=54.14, Steps=195666, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1791, Total reward=385.71, Steps=195873, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1792, Total reward=114.31, Steps=195926, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1793, Total reward=73.8, Steps=195951, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1794, Total reward=173.56, Steps=196048, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1795, Total reward=117.64, Steps=196146, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1796, Total reward=118.4, Steps=196225, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1797, Total reward=19.98, Steps=196244, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1798, Total reward=392.85, Steps=196490, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1799, Total reward=186.73, Steps=196641, Training iteration=35
Training> Name=main_level/agent, Worker=0, Episode=1800, Total reward=301.52, Steps=196877, Training iteration=35
Policy training> Surrogate loss=-0.0005533500225283206, KL divergence=0.0003517075383570045, Entropy=0.19354456663131714, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01738244667649269, KL divergence=0.005340231582522392, Entropy=0.1905912458896637, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.023315662518143654, KL divergence=0.008869955316185951, Entropy=0.18980790674686432, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.029059795662760735, KL divergence=0.011571263894438744, Entropy=0.1898222416639328, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02739820070564747, KL divergence=0.01378946378827095, Entropy=0.18857331573963165, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02993800677359104, KL divergence=0.01489311084151268, Entropy=0.18615008890628815, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03530843183398247, KL divergence=0.016162758693099022, Entropy=0.18691390752792358, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.0320083387196064, KL divergence=0.01708287186920643, Entropy=0.18624438345432281, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03752097114920616, KL divergence=0.017906837165355682, Entropy=0.18812422454357147, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.033864621073007584, KL divergence=0.018877703696489334, Entropy=0.18716798722743988, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/468_Step-196877.ckpt']
Uploaded 3 files for checkpoint 468 in 0.61 seconds
saved intermediate frozen graph: current/model/model_468.pb
Best checkpoint number: 457, Last checkpoint number: 466
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'465'}
Training> Name=main_level/agent, Worker=0, Episode=1801, Total reward=87.38, Steps=196943, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1802, Total reward=46.57, Steps=196977, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1803, Total reward=227.28, Steps=197149, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1804, Total reward=3.96, Steps=197179, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1805, Total reward=196.32, Steps=197345, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1806, Total reward=73.8, Steps=197419, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1807, Total reward=86.78, Steps=197473, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1808, Total reward=218.0, Steps=197579, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1809, Total reward=90.98, Steps=197643, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1810, Total reward=151.42, Steps=197746, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1811, Total reward=61.0, Steps=197799, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1812, Total reward=9.34, Steps=197812, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1813, Total reward=136.33, Steps=197874, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1814, Total reward=80.16, Steps=197917, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1815, Total reward=29.56, Steps=197934, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1816, Total reward=196.82, Steps=198079, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1817, Total reward=170.06, Steps=198172, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1818, Total reward=104.36, Steps=198224, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1819, Total reward=467.51, Steps=198494, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1820, Total reward=342.19, Steps=198699, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1821, Total reward=71.03, Steps=198780, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1822, Total reward=167.73, Steps=198896, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1823, Total reward=110.67, Steps=199011, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1824, Total reward=234.63, Steps=199202, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1825, Total reward=449.27, Steps=199498, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1826, Total reward=64.65, Steps=199545, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1827, Total reward=470.41, Steps=199827, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1828, Total reward=211.53, Steps=199962, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1829, Total reward=37.39, Steps=199997, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1830, Total reward=90.96, Steps=200084, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1831, Total reward=359.91, Steps=200283, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1832, Total reward=144.55, Steps=200356, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1833, Total reward=33.84, Steps=200367, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1834, Total reward=252.18, Steps=200517, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1835, Total reward=133.01, Steps=200600, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1836, Total reward=88.9, Steps=200661, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1837, Total reward=14.87, Steps=200675, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1838, Total reward=166.84, Steps=200793, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1839, Total reward=150.96, Steps=200896, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1840, Total reward=183.21, Steps=201116, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1841, Total reward=43.02, Steps=201170, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1842, Total reward=60.01, Steps=201211, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1843, Total reward=90.62, Steps=201349, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1844, Total reward=144.14, Steps=201495, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1845, Total reward=107.72, Steps=201576, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1846, Total reward=78.7, Steps=201648, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1847, Total reward=155.98, Steps=201784, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1848, Total reward=363.69, Steps=201969, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1849, Total reward=90.17, Steps=202047, Training iteration=36
Training> Name=main_level/agent, Worker=0, Episode=1850, Total reward=6.66, Steps=202072, Training iteration=36
Policy training> Surrogate loss=-0.0042578959837555885, KL divergence=0.00022154134057927877, Entropy=0.19827266037464142, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.011231396347284317, KL divergence=0.004499683156609535, Entropy=0.19632242619991302, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020445704460144043, KL divergence=0.008296704851090908, Entropy=0.19394655525684357, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026025408878922462, KL divergence=0.011163688264787197, Entropy=0.19388477504253387, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03312617167830467, KL divergence=0.01328316144645214, Entropy=0.19263233244419098, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03209991008043289, KL divergence=0.01518198661506176, Entropy=0.1916235238313675, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03546060994267464, KL divergence=0.016275256872177124, Entropy=0.19113662838935852, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.035143714398145676, KL divergence=0.01751372031867504, Entropy=0.1917628049850464, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03522421047091484, KL divergence=0.018338151276111603, Entropy=0.19123299419879913, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03685130551457405, KL divergence=0.01929458975791931, Entropy=0.1914728581905365, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/469_Step-202072.ckpt']
Uploaded 3 files for checkpoint 469 in 0.53 seconds
saved intermediate frozen graph: current/model/model_469.pb
Best checkpoint number: 457, Last checkpoint number: 467
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'466'}
Training> Name=main_level/agent, Worker=0, Episode=1851, Total reward=370.02, Steps=202261, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1852, Total reward=40.71, Steps=202286, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1853, Total reward=394.26, Steps=202595, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1854, Total reward=352.22, Steps=202895, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1855, Total reward=417.34, Steps=203172, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1856, Total reward=200.4, Steps=203314, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1857, Total reward=14.14, Steps=203328, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1858, Total reward=245.86, Steps=203548, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1859, Total reward=53.77, Steps=203593, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1860, Total reward=321.41, Steps=203809, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1861, Total reward=240.24, Steps=203984, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1862, Total reward=40.81, Steps=204017, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1863, Total reward=107.59, Steps=204139, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1864, Total reward=11.39, Steps=204172, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1865, Total reward=129.83, Steps=204314, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1866, Total reward=138.34, Steps=204420, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1867, Total reward=226.61, Steps=204560, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1868, Total reward=102.23, Steps=204653, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1869, Total reward=150.92, Steps=204766, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1870, Total reward=0.01, Steps=204779, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1871, Total reward=97.01, Steps=204848, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1872, Total reward=333.64, Steps=205066, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1873, Total reward=81.51, Steps=205122, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1874, Total reward=64.94, Steps=205179, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1875, Total reward=24.11, Steps=205195, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1876, Total reward=132.53, Steps=205259, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1877, Total reward=45.72, Steps=205295, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1878, Total reward=397.62, Steps=205578, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1879, Total reward=37.42, Steps=205605, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1880, Total reward=110.1, Steps=205679, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1881, Total reward=196.14, Steps=205845, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1882, Total reward=45.6, Steps=205883, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1883, Total reward=362.99, Steps=206129, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1884, Total reward=98.05, Steps=206223, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1885, Total reward=195.13, Steps=206396, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1886, Total reward=146.43, Steps=206502, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1887, Total reward=111.69, Steps=206583, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1888, Total reward=327.03, Steps=206805, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1889, Total reward=458.47, Steps=207102, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1890, Total reward=46.65, Steps=207155, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1891, Total reward=60.36, Steps=207211, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1892, Total reward=283.58, Steps=207343, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1893, Total reward=120.61, Steps=207397, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1894, Total reward=412.91, Steps=207690, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1895, Total reward=489.58, Steps=207953, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1896, Total reward=223.57, Steps=208099, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1897, Total reward=108.29, Steps=208150, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1898, Total reward=18.01, Steps=208162, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1899, Total reward=419.65, Steps=208450, Training iteration=37
Training> Name=main_level/agent, Worker=0, Episode=1900, Total reward=103.06, Steps=208515, Training iteration=37
Policy training> Surrogate loss=0.004167895298451185, KL divergence=0.0002362518571317196, Entropy=0.20063136518001556, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017979437485337257, KL divergence=0.004391794092953205, Entropy=0.19875633716583252, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.024305788800120354, KL divergence=0.008170642890036106, Entropy=0.19665026664733887, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02353110909461975, KL divergence=0.011215675622224808, Entropy=0.196988046169281, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026499470695853233, KL divergence=0.013227912597358227, Entropy=0.19723530113697052, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026393897831439972, KL divergence=0.014782826416194439, Entropy=0.19686265289783478, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.027190567925572395, KL divergence=0.015112842433154583, Entropy=0.19501499831676483, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03629877045750618, KL divergence=0.016299692913889885, Entropy=0.1953052133321762, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.033212218433618546, KL divergence=0.017086612060666084, Entropy=0.19521373510360718, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0330323688685894, KL divergence=0.01781480759382248, Entropy=0.19540250301361084, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/470_Step-208515.ckpt']
Uploaded 3 files for checkpoint 470 in 0.56 seconds
saved intermediate frozen graph: current/model/model_470.pb
Best checkpoint number: 457, Last checkpoint number: 468
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'467'}
Training> Name=main_level/agent, Worker=0, Episode=1901, Total reward=467.03, Steps=208796, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1902, Total reward=58.79, Steps=208826, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1903, Total reward=21.16, Steps=208864, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1904, Total reward=200.9, Steps=209019, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1905, Total reward=330.01, Steps=209253, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1906, Total reward=468.31, Steps=209528, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1907, Total reward=290.03, Steps=209807, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1908, Total reward=448.54, Steps=210043, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1909, Total reward=35.57, Steps=210082, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1910, Total reward=84.76, Steps=210170, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1911, Total reward=111.2, Steps=210244, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1912, Total reward=464.11, Steps=210518, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1913, Total reward=73.66, Steps=210557, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1914, Total reward=245.85, Steps=210712, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1915, Total reward=158.46, Steps=210796, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1916, Total reward=34.42, Steps=210853, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1917, Total reward=169.04, Steps=211006, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1918, Total reward=167.32, Steps=211104, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1919, Total reward=102.72, Steps=211169, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1920, Total reward=85.51, Steps=211265, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1921, Total reward=339.54, Steps=211492, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1922, Total reward=63.67, Steps=211529, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1923, Total reward=410.65, Steps=211834, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1924, Total reward=11.25, Steps=211856, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1925, Total reward=13.91, Steps=211889, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1926, Total reward=83.94, Steps=211980, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1927, Total reward=84.96, Steps=212025, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1928, Total reward=140.59, Steps=212092, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1929, Total reward=42.57, Steps=212131, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1930, Total reward=22.11, Steps=212174, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1931, Total reward=32.09, Steps=212207, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1932, Total reward=477.35, Steps=212509, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1933, Total reward=328.21, Steps=212733, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1934, Total reward=304.23, Steps=212966, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1935, Total reward=56.23, Steps=213011, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1936, Total reward=452.92, Steps=213279, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1937, Total reward=184.53, Steps=213380, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1938, Total reward=97.79, Steps=213427, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1939, Total reward=253.27, Steps=213638, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1940, Total reward=7.53, Steps=213652, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1941, Total reward=107.1, Steps=213715, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1942, Total reward=65.62, Steps=213777, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1943, Total reward=6.17, Steps=213837, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1944, Total reward=285.68, Steps=214028, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1945, Total reward=84.56, Steps=214115, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1946, Total reward=86.48, Steps=214200, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1947, Total reward=60.18, Steps=214260, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1948, Total reward=380.64, Steps=214537, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1949, Total reward=49.31, Steps=214587, Training iteration=38
Training> Name=main_level/agent, Worker=0, Episode=1950, Total reward=308.24, Steps=214793, Training iteration=38
Policy training> Surrogate loss=0.00033392992918379605, KL divergence=0.00033056692336685956, Entropy=0.2067234367132187, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01699790544807911, KL divergence=0.004897255916148424, Entropy=0.20550303161144257, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.0254301056265831, KL divergence=0.009986293502151966, Entropy=0.20271079242229462, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.028595680370926857, KL divergence=0.01309890765696764, Entropy=0.201684832572937, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.030645521357655525, KL divergence=0.015077009797096252, Entropy=0.20145387947559357, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.034778039902448654, KL divergence=0.01657349430024624, Entropy=0.2007513791322708, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03485879674553871, KL divergence=0.017467165365815163, Entropy=0.20039790868759155, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03438212350010872, KL divergence=0.018800092861056328, Entropy=0.20159859955310822, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.035454824566841125, KL divergence=0.019660986959934235, Entropy=0.20127953588962555, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03693973645567894, KL divergence=0.020823420956730843, Entropy=0.2020103931427002, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/471_Step-214793.ckpt']
Uploaded 3 files for checkpoint 471 in 0.56 seconds
saved intermediate frozen graph: current/model/model_471.pb
Best checkpoint number: 457, Last checkpoint number: 469
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'468'}
Training> Name=main_level/agent, Worker=0, Episode=1951, Total reward=132.21, Steps=214880, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1952, Total reward=499.63, Steps=215151, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1953, Total reward=81.73, Steps=215195, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1954, Total reward=77.14, Steps=215244, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1955, Total reward=186.83, Steps=215397, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1956, Total reward=91.11, Steps=215485, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1957, Total reward=200.4, Steps=215610, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1958, Total reward=422.64, Steps=215901, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1959, Total reward=486.26, Steps=216191, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1960, Total reward=80.39, Steps=216261, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1961, Total reward=477.72, Steps=216551, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1962, Total reward=470.6, Steps=216854, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1963, Total reward=11.72, Steps=216889, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1964, Total reward=97.37, Steps=216990, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1965, Total reward=172.92, Steps=217141, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1966, Total reward=269.38, Steps=217281, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1967, Total reward=455.28, Steps=217557, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1968, Total reward=203.82, Steps=217659, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1969, Total reward=134.52, Steps=217748, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1970, Total reward=98.66, Steps=217818, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1971, Total reward=20.68, Steps=217858, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1972, Total reward=204.29, Steps=217995, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1973, Total reward=241.45, Steps=218135, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1974, Total reward=46.79, Steps=218177, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1975, Total reward=218.2, Steps=218313, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1976, Total reward=382.07, Steps=218593, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1977, Total reward=335.67, Steps=218844, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1978, Total reward=62.48, Steps=218899, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1979, Total reward=393.92, Steps=219205, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1980, Total reward=101.84, Steps=219263, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1981, Total reward=95.63, Steps=219306, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1982, Total reward=57.78, Steps=219378, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1983, Total reward=6.06, Steps=219405, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1984, Total reward=14.46, Steps=219427, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1985, Total reward=152.68, Steps=219551, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1986, Total reward=105.34, Steps=219624, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1987, Total reward=195.16, Steps=219726, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1988, Total reward=126.91, Steps=219795, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1989, Total reward=155.93, Steps=219936, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1990, Total reward=359.12, Steps=220166, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1991, Total reward=347.72, Steps=220366, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1992, Total reward=137.33, Steps=220440, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1993, Total reward=223.59, Steps=220554, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1994, Total reward=78.99, Steps=220605, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1995, Total reward=27.55, Steps=220636, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1996, Total reward=125.29, Steps=220709, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1997, Total reward=102.19, Steps=220767, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1998, Total reward=280.68, Steps=221008, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=1999, Total reward=202.79, Steps=221202, Training iteration=39
Training> Name=main_level/agent, Worker=0, Episode=2000, Total reward=491.35, Steps=221499, Training iteration=39
Policy training> Surrogate loss=3.5837580071529374e-05, KL divergence=0.00041823976789601147, Entropy=0.19113557040691376, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017326952889561653, KL divergence=0.005876587703824043, Entropy=0.18787604570388794, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.024103211238980293, KL divergence=0.0098681366071105, Entropy=0.1863182634115219, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.0271434485912323, KL divergence=0.011988343670964241, Entropy=0.18520644307136536, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029836492612957954, KL divergence=0.013429572805762291, Entropy=0.18509580194950104, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02954322099685669, KL divergence=0.014826679602265358, Entropy=0.18581482768058777, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.030727505683898926, KL divergence=0.0156595129519701, Entropy=0.18510405719280243, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03527916967868805, KL divergence=0.01659322716295719, Entropy=0.18556135892868042, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03394073620438576, KL divergence=0.01721959188580513, Entropy=0.18541432917118073, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03509758040308952, KL divergence=0.017839340493083, Entropy=0.18586094677448273, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/472_Step-221499.ckpt']
Uploaded 3 files for checkpoint 472 in 0.52 seconds
saved intermediate frozen graph: current/model/model_472.pb
Best checkpoint number: 457, Last checkpoint number: 470
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'469'}
Training> Name=main_level/agent, Worker=0, Episode=2001, Total reward=375.89, Steps=221774, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2002, Total reward=72.0, Steps=221831, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2003, Total reward=242.58, Steps=222000, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2004, Total reward=107.37, Steps=222103, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2005, Total reward=3.3, Steps=222157, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2006, Total reward=115.77, Steps=222218, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2007, Total reward=180.56, Steps=222341, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2008, Total reward=66.48, Steps=222399, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2009, Total reward=120.88, Steps=222504, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2010, Total reward=197.39, Steps=222652, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2011, Total reward=31.84, Steps=222678, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2012, Total reward=131.31, Steps=222759, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2013, Total reward=32.15, Steps=222771, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2014, Total reward=51.54, Steps=222814, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2015, Total reward=26.35, Steps=222829, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2016, Total reward=274.31, Steps=223061, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2017, Total reward=78.08, Steps=223132, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2018, Total reward=165.26, Steps=223232, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2019, Total reward=110.5, Steps=223320, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2020, Total reward=95.03, Steps=223425, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2021, Total reward=469.44, Steps=223716, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2022, Total reward=54.38, Steps=223756, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2023, Total reward=130.75, Steps=223877, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2024, Total reward=8.37, Steps=223903, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2025, Total reward=391.89, Steps=224133, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2026, Total reward=116.62, Steps=224213, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2027, Total reward=344.19, Steps=224419, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2028, Total reward=127.03, Steps=224505, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2029, Total reward=226.77, Steps=224676, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2030, Total reward=161.08, Steps=224804, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2031, Total reward=90.3, Steps=224861, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2032, Total reward=67.83, Steps=224919, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2033, Total reward=123.33, Steps=224984, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2034, Total reward=421.29, Steps=225212, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2035, Total reward=296.02, Steps=225507, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2036, Total reward=231.91, Steps=225652, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2037, Total reward=192.45, Steps=225760, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2038, Total reward=418.22, Steps=225999, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2039, Total reward=97.32, Steps=226075, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2040, Total reward=32.31, Steps=226103, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2041, Total reward=223.81, Steps=226314, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2042, Total reward=322.2, Steps=226587, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2043, Total reward=500.17, Steps=226870, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2044, Total reward=220.02, Steps=227071, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2045, Total reward=126.99, Steps=227229, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2046, Total reward=45.99, Steps=227264, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2047, Total reward=264.36, Steps=227477, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2048, Total reward=88.37, Steps=227513, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2049, Total reward=34.71, Steps=227536, Training iteration=40
Training> Name=main_level/agent, Worker=0, Episode=2050, Total reward=481.21, Steps=227835, Training iteration=40
Policy training> Surrogate loss=-0.0024477753322571516, KL divergence=0.00029061760869808495, Entropy=0.19685184955596924, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015991080552339554, KL divergence=0.005415767431259155, Entropy=0.19402189552783966, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020405465736985207, KL divergence=0.010606694966554642, Entropy=0.1941458135843277, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025576703250408173, KL divergence=0.013294890522956848, Entropy=0.1940210610628128, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.034473005682229996, KL divergence=0.015197197906672955, Entropy=0.194078728556633, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02865706942975521, KL divergence=0.016411127522587776, Entropy=0.19365398585796356, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.027490638196468353, KL divergence=0.017469121143221855, Entropy=0.19422800838947296, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03517378494143486, KL divergence=0.01822098158299923, Entropy=0.19367873668670654, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03543509170413017, KL divergence=0.019178563728928566, Entropy=0.19447340071201324, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0351562462747097, KL divergence=0.01964508555829525, Entropy=0.19332046806812286, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/473_Step-227835.ckpt']
Uploaded 3 files for checkpoint 473 in 0.51 seconds
saved intermediate frozen graph: current/model/model_473.pb
Best checkpoint number: 457, Last checkpoint number: 471
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'470'}
Training> Name=main_level/agent, Worker=0, Episode=2051, Total reward=25.62, Steps=227861, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2052, Total reward=316.64, Steps=228019, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2053, Total reward=68.23, Steps=228062, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2054, Total reward=444.61, Steps=228338, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2055, Total reward=98.19, Steps=228413, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2056, Total reward=388.0, Steps=228732, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2057, Total reward=47.07, Steps=228769, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2058, Total reward=469.8, Steps=229062, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2059, Total reward=108.58, Steps=229188, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2060, Total reward=421.59, Steps=229479, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2061, Total reward=67.16, Steps=229513, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2062, Total reward=211.19, Steps=229674, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2063, Total reward=160.07, Steps=229811, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2064, Total reward=3.18, Steps=229843, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2065, Total reward=92.23, Steps=229927, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2066, Total reward=461.37, Steps=230214, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2067, Total reward=90.4, Steps=230264, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2068, Total reward=483.32, Steps=230494, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2069, Total reward=44.42, Steps=230522, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2070, Total reward=28.04, Steps=230576, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2071, Total reward=104.29, Steps=230641, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2072, Total reward=155.69, Steps=230721, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2073, Total reward=130.06, Steps=230787, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2074, Total reward=78.04, Steps=230836, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2075, Total reward=223.01, Steps=230945, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2076, Total reward=52.01, Steps=230979, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2077, Total reward=405.64, Steps=231257, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2078, Total reward=258.14, Steps=231464, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2079, Total reward=304.23, Steps=231683, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2080, Total reward=99.36, Steps=231735, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2081, Total reward=85.32, Steps=231835, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2082, Total reward=311.11, Steps=232024, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2083, Total reward=433.41, Steps=232300, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2084, Total reward=92.56, Steps=232412, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2085, Total reward=169.43, Steps=232519, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2086, Total reward=261.98, Steps=232664, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2087, Total reward=132.29, Steps=232768, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2088, Total reward=180.54, Steps=232870, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2089, Total reward=150.54, Steps=232941, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2090, Total reward=11.11, Steps=232955, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2091, Total reward=117.1, Steps=233037, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2092, Total reward=87.06, Steps=233092, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2093, Total reward=82.67, Steps=233135, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2094, Total reward=55.88, Steps=233175, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2095, Total reward=119.81, Steps=233247, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2096, Total reward=20.31, Steps=233267, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2097, Total reward=155.32, Steps=233370, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2098, Total reward=393.14, Steps=233612, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2099, Total reward=109.86, Steps=233689, Training iteration=41
Training> Name=main_level/agent, Worker=0, Episode=2100, Total reward=270.21, Steps=233893, Training iteration=41
Policy training> Surrogate loss=-0.004414035473018885, KL divergence=0.00022000940225552768, Entropy=0.19575895369052887, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01718711107969284, KL divergence=0.0036605382338166237, Entropy=0.19274622201919556, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.018979554995894432, KL divergence=0.00665266951546073, Entropy=0.19084200263023376, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023832736536860466, KL divergence=0.009115856140851974, Entropy=0.19029365479946136, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.021184084936976433, KL divergence=0.011236537247896194, Entropy=0.1918872743844986, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02520083077251911, KL divergence=0.01277680043131113, Entropy=0.19048085808753967, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.028936946764588356, KL divergence=0.014219098724424839, Entropy=0.1890711635351181, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02728530950844288, KL divergence=0.015127754770219326, Entropy=0.19015587866306305, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03897761180996895, KL divergence=0.015597508288919926, Entropy=0.18801173567771912, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03273586183786392, KL divergence=0.016742480918765068, Entropy=0.19129540026187897, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/474_Step-233893.ckpt']
Uploaded 3 files for checkpoint 474 in 0.55 seconds
saved intermediate frozen graph: current/model/model_474.pb
Best checkpoint number: 457, Last checkpoint number: 472
Copying the frozen checkpoint from ./frozen_models/agent/model_457.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'471'}
Training> Name=main_level/agent, Worker=0, Episode=2101, Total reward=92.37, Steps=233965, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2102, Total reward=56.85, Steps=233998, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2103, Total reward=6.92, Steps=234047, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2104, Total reward=433.17, Steps=234345, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2105, Total reward=204.79, Steps=234530, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2106, Total reward=437.15, Steps=234839, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2107, Total reward=355.1, Steps=235061, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2108, Total reward=123.67, Steps=235180, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2109, Total reward=61.79, Steps=235233, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2110, Total reward=211.11, Steps=235351, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2111, Total reward=111.52, Steps=235418, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2112, Total reward=490.59, Steps=235687, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2113, Total reward=95.34, Steps=235732, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2114, Total reward=138.31, Steps=235840, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2115, Total reward=201.36, Steps=235986, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2116, Total reward=116.97, Steps=236066, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2117, Total reward=93.77, Steps=236119, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2118, Total reward=168.8, Steps=236217, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2119, Total reward=187.66, Steps=236432, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2120, Total reward=109.75, Steps=236525, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2121, Total reward=291.06, Steps=236730, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2122, Total reward=141.69, Steps=236852, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2123, Total reward=433.39, Steps=237152, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2124, Total reward=122.01, Steps=237245, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2125, Total reward=233.78, Steps=237409, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2126, Total reward=504.66, Steps=237649, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2127, Total reward=183.72, Steps=237750, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2128, Total reward=124.3, Steps=237857, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2129, Total reward=159.29, Steps=237980, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2130, Total reward=303.61, Steps=238136, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2131, Total reward=142.37, Steps=238216, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2132, Total reward=83.04, Steps=238270, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2133, Total reward=81.04, Steps=238308, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2134, Total reward=485.36, Steps=238603, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2135, Total reward=86.08, Steps=238668, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2136, Total reward=332.79, Steps=238911, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2137, Total reward=183.05, Steps=239032, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2138, Total reward=69.8, Steps=239067, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2139, Total reward=56.94, Steps=239110, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2140, Total reward=229.99, Steps=239297, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2141, Total reward=64.51, Steps=239337, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2142, Total reward=46.58, Steps=239418, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2143, Total reward=16.45, Steps=239466, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2144, Total reward=128.79, Steps=239615, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2145, Total reward=3.09, Steps=239649, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2146, Total reward=342.68, Steps=239849, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2147, Total reward=92.15, Steps=239909, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2148, Total reward=440.77, Steps=240168, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2149, Total reward=195.95, Steps=240259, Training iteration=42
Training> Name=main_level/agent, Worker=0, Episode=2150, Total reward=0.03, Steps=240285, Training iteration=42
Policy training> Surrogate loss=-0.0008087356691248715, KL divergence=0.00018765318964142352, Entropy=0.18878470361232758, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.013229417614638805, KL divergence=0.004032160621136427, Entropy=0.18569384515285492, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02305903285741806, KL divergence=0.00817771628499031, Entropy=0.18414364755153656, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02374785579741001, KL divergence=0.010665097273886204, Entropy=0.18497955799102783, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026051485911011696, KL divergence=0.012343586422502995, Entropy=0.18363721668720245, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.031523242592811584, KL divergence=0.013403914868831635, Entropy=0.18435615301132202, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.022663528099656105, KL divergence=0.014291598461568356, Entropy=0.18297332525253296, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03840271010994911, KL divergence=0.015113643370568752, Entropy=0.18341606855392456, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.031722430139780045, KL divergence=0.016189174726605415, Entropy=0.18442849814891815, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.030863827094435692, KL divergence=0.01725119911134243, Entropy=0.18537284433841705, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/475_Step-240285.ckpt']
Uploaded 3 files for checkpoint 475 in 0.57 seconds
saved intermediate frozen graph: current/model/model_475.pb
Best checkpoint number: 473, Last checkpoint number: 473
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'472'}
Training> Name=main_level/agent, Worker=0, Episode=2151, Total reward=56.69, Steps=240325, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2152, Total reward=119.42, Steps=240407, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2153, Total reward=111.31, Steps=240468, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2154, Total reward=150.34, Steps=240569, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2155, Total reward=217.47, Steps=240689, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2156, Total reward=349.65, Steps=240941, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2157, Total reward=269.1, Steps=241135, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2158, Total reward=142.08, Steps=241224, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2159, Total reward=90.93, Steps=241324, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2160, Total reward=101.67, Steps=241377, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2161, Total reward=116.11, Steps=241452, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2162, Total reward=117.21, Steps=241594, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2163, Total reward=392.71, Steps=241902, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2164, Total reward=72.98, Steps=241982, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2165, Total reward=129.53, Steps=242100, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2166, Total reward=275.65, Steps=242247, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2167, Total reward=213.01, Steps=242375, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2168, Total reward=376.45, Steps=242660, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2169, Total reward=183.72, Steps=242785, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2170, Total reward=182.97, Steps=242901, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2171, Total reward=120.75, Steps=242969, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2172, Total reward=105.92, Steps=243038, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2173, Total reward=33.85, Steps=243050, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2174, Total reward=207.22, Steps=243160, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2175, Total reward=166.31, Steps=243255, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2176, Total reward=247.74, Steps=243488, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2177, Total reward=437.94, Steps=243774, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2178, Total reward=422.86, Steps=244037, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2179, Total reward=15.96, Steps=244068, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2180, Total reward=140.77, Steps=244181, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2181, Total reward=103.8, Steps=244255, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2182, Total reward=64.81, Steps=244314, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2183, Total reward=12.03, Steps=244392, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2184, Total reward=262.09, Steps=244581, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2185, Total reward=208.54, Steps=244726, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2186, Total reward=222.18, Steps=244893, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2187, Total reward=91.1, Steps=244995, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2188, Total reward=152.01, Steps=245072, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2189, Total reward=148.53, Steps=245192, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2190, Total reward=280.57, Steps=245354, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2191, Total reward=11.2, Steps=245381, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2192, Total reward=12.53, Steps=245394, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2193, Total reward=89.94, Steps=245435, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2194, Total reward=405.73, Steps=245733, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2195, Total reward=155.58, Steps=245818, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2196, Total reward=122.26, Steps=245885, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2197, Total reward=297.44, Steps=246085, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2198, Total reward=51.18, Steps=246124, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2199, Total reward=444.66, Steps=246437, Training iteration=43
Training> Name=main_level/agent, Worker=0, Episode=2200, Total reward=93.33, Steps=246496, Training iteration=43
Policy training> Surrogate loss=-0.0007325152982957661, KL divergence=0.00027411282644607127, Entropy=0.18991003930568695, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.013135750778019428, KL divergence=0.004007050301879644, Entropy=0.18889741599559784, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02419455163180828, KL divergence=0.008760258555412292, Entropy=0.1861392706632614, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025100497528910637, KL divergence=0.011500087566673756, Entropy=0.1856938600540161, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028417030349373817, KL divergence=0.01323084905743599, Entropy=0.183930441737175, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030833380296826363, KL divergence=0.014402312226593494, Entropy=0.18332713842391968, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029629947617650032, KL divergence=0.015540361404418945, Entropy=0.1833776980638504, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03192291781306267, KL divergence=0.016588518396019936, Entropy=0.18388263881206512, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03077695518732071, KL divergence=0.017232799902558327, Entropy=0.1822666972875595, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03221854567527771, KL divergence=0.01820559985935688, Entropy=0.18284620344638824, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/476_Step-246496.ckpt']
Uploaded 3 files for checkpoint 476 in 0.51 seconds
saved intermediate frozen graph: current/model/model_476.pb
Best checkpoint number: 473, Last checkpoint number: 474
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'457'}
Training> Name=main_level/agent, Worker=0, Episode=2201, Total reward=66.71, Steps=246534, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2202, Total reward=241.14, Steps=246731, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2203, Total reward=7.73, Steps=246760, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2204, Total reward=34.04, Steps=246827, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2205, Total reward=224.78, Steps=246979, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2206, Total reward=126.94, Steps=247074, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2207, Total reward=65.84, Steps=247120, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2208, Total reward=467.56, Steps=247393, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2209, Total reward=213.59, Steps=247550, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2210, Total reward=82.96, Steps=247636, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2211, Total reward=67.68, Steps=247676, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2212, Total reward=110.2, Steps=247745, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2213, Total reward=330.26, Steps=247908, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2214, Total reward=153.86, Steps=248020, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2215, Total reward=365.33, Steps=248271, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2216, Total reward=242.15, Steps=248570, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2217, Total reward=96.67, Steps=248633, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2218, Total reward=392.65, Steps=248885, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2219, Total reward=406.75, Steps=249191, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2220, Total reward=100.04, Steps=249248, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2221, Total reward=88.77, Steps=249341, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2222, Total reward=79.05, Steps=249402, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2223, Total reward=1.79, Steps=249440, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2224, Total reward=105.31, Steps=249534, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2225, Total reward=432.33, Steps=249826, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2226, Total reward=64.78, Steps=249876, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2227, Total reward=218.21, Steps=250009, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2228, Total reward=217.99, Steps=250119, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2229, Total reward=388.57, Steps=250326, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2230, Total reward=101.12, Steps=250417, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2231, Total reward=362.44, Steps=250621, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2232, Total reward=337.61, Steps=250814, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2233, Total reward=215.48, Steps=250931, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2234, Total reward=166.99, Steps=251016, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2235, Total reward=21.04, Steps=251030, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2236, Total reward=170.33, Steps=251147, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2237, Total reward=388.91, Steps=251454, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2238, Total reward=156.2, Steps=251563, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2239, Total reward=139.1, Steps=251715, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2240, Total reward=293.59, Steps=251931, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2241, Total reward=84.5, Steps=251987, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2242, Total reward=148.95, Steps=252108, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2243, Total reward=111.48, Steps=252226, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2244, Total reward=3.71, Steps=252238, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2245, Total reward=100.77, Steps=252320, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2246, Total reward=440.81, Steps=252582, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2247, Total reward=232.97, Steps=252725, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2248, Total reward=118.96, Steps=252797, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2249, Total reward=31.88, Steps=252830, Training iteration=44
Training> Name=main_level/agent, Worker=0, Episode=2250, Total reward=123.45, Steps=252931, Training iteration=44
Policy training> Surrogate loss=-0.0017173656960949302, KL divergence=0.00039157093851827085, Entropy=0.20342212915420532, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.019305411726236343, KL divergence=0.005422794725745916, Entropy=0.20132090151309967, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02312535233795643, KL divergence=0.01020082738250494, Entropy=0.1998535841703415, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.03045549988746643, KL divergence=0.012571250088512897, Entropy=0.2019793838262558, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02773059904575348, KL divergence=0.014708251692354679, Entropy=0.19959741830825806, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.034688640385866165, KL divergence=0.016296446323394775, Entropy=0.2007373422384262, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03205746039748192, KL divergence=0.017345109954476357, Entropy=0.20085735619068146, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03781215101480484, KL divergence=0.018690213561058044, Entropy=0.2012331336736679, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03567617014050484, KL divergence=0.019376449286937714, Entropy=0.20243771374225616, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03385946527123451, KL divergence=0.02018009126186371, Entropy=0.20188647508621216, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/477_Step-252931.ckpt']
Uploaded 3 files for checkpoint 477 in 0.54 seconds
saved intermediate frozen graph: current/model/model_477.pb
Best checkpoint number: 473, Last checkpoint number: 475
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'474'}
Training> Name=main_level/agent, Worker=0, Episode=2251, Total reward=318.98, Steps=253147, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2252, Total reward=2.68, Steps=253160, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2253, Total reward=193.72, Steps=253280, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2254, Total reward=27.82, Steps=253290, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2255, Total reward=336.58, Steps=253552, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2256, Total reward=77.87, Steps=253599, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2257, Total reward=478.24, Steps=253890, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2258, Total reward=69.27, Steps=253937, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2259, Total reward=125.3, Steps=254055, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2260, Total reward=459.78, Steps=254354, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2261, Total reward=48.82, Steps=254378, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2262, Total reward=34.26, Steps=254398, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2263, Total reward=292.76, Steps=254596, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2264, Total reward=248.58, Steps=254774, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2265, Total reward=191.12, Steps=254938, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2266, Total reward=495.31, Steps=255202, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2267, Total reward=448.5, Steps=255471, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2268, Total reward=483.57, Steps=255715, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2269, Total reward=123.54, Steps=255799, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2270, Total reward=369.77, Steps=256113, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2271, Total reward=154.66, Steps=256197, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2272, Total reward=243.84, Steps=256330, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2273, Total reward=141.95, Steps=256407, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2274, Total reward=61.69, Steps=256461, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2275, Total reward=34.47, Steps=256478, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2276, Total reward=348.96, Steps=256719, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2277, Total reward=148.28, Steps=256803, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2278, Total reward=181.21, Steps=256923, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2279, Total reward=82.57, Steps=257000, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2280, Total reward=131.38, Steps=257072, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2281, Total reward=223.68, Steps=257253, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2282, Total reward=44.3, Steps=257299, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2283, Total reward=137.25, Steps=257437, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2284, Total reward=468.55, Steps=257726, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2285, Total reward=441.96, Steps=258050, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2286, Total reward=67.56, Steps=258123, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2287, Total reward=496.71, Steps=258409, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2288, Total reward=414.85, Steps=258659, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2289, Total reward=104.32, Steps=258716, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2290, Total reward=338.7, Steps=258930, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2291, Total reward=21.46, Steps=258967, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2292, Total reward=116.4, Steps=259048, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2293, Total reward=328.41, Steps=259194, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2294, Total reward=53.8, Steps=259244, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2295, Total reward=252.15, Steps=259368, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2296, Total reward=138.34, Steps=259451, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2297, Total reward=285.92, Steps=259700, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2298, Total reward=297.92, Steps=259960, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2299, Total reward=316.97, Steps=260226, Training iteration=45
Training> Name=main_level/agent, Worker=0, Episode=2300, Total reward=206.58, Steps=260432, Training iteration=45
Policy training> Surrogate loss=0.00037175737088546157, KL divergence=0.000355654105078429, Entropy=0.19127018749713898, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015837175771594048, KL divergence=0.0069756051525473595, Entropy=0.19001975655555725, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02077547274529934, KL divergence=0.010357209481298923, Entropy=0.18942371010780334, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026309069246053696, KL divergence=0.01200133841484785, Entropy=0.18888868391513824, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02589263580739498, KL divergence=0.013498405925929546, Entropy=0.18702910840511322, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030449410900473595, KL divergence=0.014860009774565697, Entropy=0.18807046115398407, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03280860558152199, KL divergence=0.015864942222833633, Entropy=0.18807251751422882, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.029004881158471107, KL divergence=0.016687767580151558, Entropy=0.18706847727298737, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03228069096803665, KL divergence=0.017118897289037704, Entropy=0.1879497915506363, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03660434111952782, KL divergence=0.01773572526872158, Entropy=0.188710555434227, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/478_Step-260432.ckpt']
Uploaded 3 files for checkpoint 478 in 0.62 seconds
saved intermediate frozen graph: current/model/model_478.pb
Best checkpoint number: 473, Last checkpoint number: 476
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'475'}
Training> Name=main_level/agent, Worker=0, Episode=2301, Total reward=84.36, Steps=260513, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2302, Total reward=201.18, Steps=260664, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2303, Total reward=14.44, Steps=260680, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2304, Total reward=3.29, Steps=260713, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2305, Total reward=171.72, Steps=260880, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2306, Total reward=168.06, Steps=260986, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2307, Total reward=124.64, Steps=261073, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2308, Total reward=198.67, Steps=261186, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2309, Total reward=469.01, Steps=261458, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2310, Total reward=208.09, Steps=261615, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2311, Total reward=217.48, Steps=261782, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2312, Total reward=270.29, Steps=261962, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2313, Total reward=426.63, Steps=262270, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2314, Total reward=174.74, Steps=262375, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2315, Total reward=205.52, Steps=262487, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2316, Total reward=326.62, Steps=262725, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2317, Total reward=13.93, Steps=262738, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2318, Total reward=72.98, Steps=262776, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2319, Total reward=197.5, Steps=262928, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2320, Total reward=272.55, Steps=263125, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2321, Total reward=54.61, Steps=263149, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2322, Total reward=309.83, Steps=263399, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2323, Total reward=485.93, Steps=263691, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2324, Total reward=451.37, Steps=263968, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2325, Total reward=188.4, Steps=264127, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2326, Total reward=453.73, Steps=264389, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2327, Total reward=169.21, Steps=264512, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2328, Total reward=305.08, Steps=264703, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2329, Total reward=50.41, Steps=264736, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2330, Total reward=76.58, Steps=264821, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2331, Total reward=125.43, Steps=264886, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2332, Total reward=290.85, Steps=265064, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2333, Total reward=373.75, Steps=265304, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2334, Total reward=337.34, Steps=265569, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2335, Total reward=103.21, Steps=265645, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2336, Total reward=208.97, Steps=265767, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2337, Total reward=5.72, Steps=265778, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2338, Total reward=154.91, Steps=265901, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2339, Total reward=428.59, Steps=266188, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2340, Total reward=106.68, Steps=266277, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2341, Total reward=417.93, Steps=266536, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2342, Total reward=521.18, Steps=266820, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2343, Total reward=234.09, Steps=267008, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2344, Total reward=155.5, Steps=267151, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2345, Total reward=122.99, Steps=267270, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2346, Total reward=212.28, Steps=267440, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2347, Total reward=469.38, Steps=267717, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2348, Total reward=108.61, Steps=267787, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2349, Total reward=381.55, Steps=268045, Training iteration=46
Training> Name=main_level/agent, Worker=0, Episode=2350, Total reward=26.68, Steps=268096, Training iteration=46
Policy training> Surrogate loss=-0.0035097263753414154, KL divergence=0.00041545365820638835, Entropy=0.18591776490211487, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014848664402961731, KL divergence=0.006301793269813061, Entropy=0.18345747888088226, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.027365360409021378, KL divergence=0.009879717603325844, Entropy=0.18260715901851654, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02367989346385002, KL divergence=0.011481364257633686, Entropy=0.18092966079711914, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.031008480116724968, KL divergence=0.012803927063941956, Entropy=0.18106627464294434, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02519821748137474, KL divergence=0.014199241064488888, Entropy=0.18203134834766388, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.030920710414648056, KL divergence=0.015133721753954887, Entropy=0.18249991536140442, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03259886056184769, KL divergence=0.016051102429628372, Entropy=0.18120141327381134, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03421134874224663, KL divergence=0.016741853207349777, Entropy=0.1804516464471817, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03377758711576462, KL divergence=0.017657194286584854, Entropy=0.18112336099147797, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/479_Step-268096.ckpt']
Uploaded 3 files for checkpoint 479 in 0.67 seconds
saved intermediate frozen graph: current/model/model_479.pb
Best checkpoint number: 473, Last checkpoint number: 477
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'476'}
Training> Name=main_level/agent, Worker=0, Episode=2351, Total reward=307.24, Steps=268297, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2352, Total reward=100.53, Steps=268353, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2353, Total reward=106.16, Steps=268410, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2354, Total reward=290.23, Steps=268633, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2355, Total reward=236.04, Steps=268760, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2356, Total reward=456.08, Steps=269047, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2357, Total reward=72.13, Steps=269169, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2358, Total reward=315.62, Steps=269425, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2359, Total reward=407.53, Steps=269700, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2360, Total reward=88.22, Steps=269764, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2361, Total reward=216.87, Steps=269959, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2362, Total reward=375.64, Steps=270179, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2363, Total reward=102.27, Steps=270292, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2364, Total reward=99.75, Steps=270396, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2365, Total reward=440.63, Steps=270677, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2366, Total reward=256.84, Steps=270835, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2367, Total reward=398.97, Steps=271101, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2368, Total reward=483.45, Steps=271402, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2369, Total reward=45.7, Steps=271466, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2370, Total reward=20.39, Steps=271499, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2371, Total reward=127.98, Steps=271631, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2372, Total reward=20.9, Steps=271643, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2373, Total reward=314.26, Steps=271855, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2374, Total reward=33.39, Steps=271868, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2375, Total reward=337.82, Steps=272085, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2376, Total reward=215.64, Steps=272205, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2377, Total reward=200.39, Steps=272338, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2378, Total reward=293.47, Steps=272508, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2379, Total reward=23.55, Steps=272528, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2380, Total reward=189.24, Steps=272683, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2381, Total reward=248.98, Steps=272850, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2382, Total reward=453.42, Steps=273138, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2383, Total reward=445.05, Steps=273450, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2384, Total reward=18.89, Steps=273479, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2385, Total reward=209.26, Steps=273624, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2386, Total reward=96.59, Steps=273710, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2387, Total reward=182.76, Steps=273841, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2388, Total reward=262.96, Steps=273952, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2389, Total reward=257.6, Steps=274149, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2390, Total reward=127.08, Steps=274249, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2391, Total reward=30.66, Steps=274286, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2392, Total reward=287.25, Steps=274429, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2393, Total reward=79.85, Steps=274458, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2394, Total reward=25.13, Steps=274469, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2395, Total reward=329.35, Steps=274689, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2396, Total reward=508.24, Steps=274978, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2397, Total reward=178.65, Steps=275078, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2398, Total reward=361.25, Steps=275330, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2399, Total reward=108.3, Steps=275395, Training iteration=47
Training> Name=main_level/agent, Worker=0, Episode=2400, Total reward=527.08, Steps=275688, Training iteration=47
Policy training> Surrogate loss=0.0016472289571538568, KL divergence=0.00042142640450038016, Entropy=0.1934821903705597, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014947987161576748, KL divergence=0.007041091565042734, Entropy=0.1918577253818512, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02169099822640419, KL divergence=0.010417893528938293, Entropy=0.18900133669376373, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.030507832765579224, KL divergence=0.012589557096362114, Entropy=0.18810537457466125, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029134372249245644, KL divergence=0.013461901806294918, Entropy=0.1873161643743515, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.027311543002724648, KL divergence=0.01503774058073759, Entropy=0.1884293258190155, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03179316222667694, KL divergence=0.01601887308061123, Entropy=0.18800775706768036, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03179534524679184, KL divergence=0.01697409711778164, Entropy=0.18807028234004974, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.033227864652872086, KL divergence=0.017286991700530052, Entropy=0.18863870203495026, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.032008882611989975, KL divergence=0.017851468175649643, Entropy=0.18684272468090057, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/480_Step-275688.ckpt']
Uploaded 3 files for checkpoint 480 in 0.53 seconds
saved intermediate frozen graph: current/model/model_480.pb
Best checkpoint number: 473, Last checkpoint number: 478
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'477'}
Training> Name=main_level/agent, Worker=0, Episode=2401, Total reward=89.55, Steps=275729, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2402, Total reward=52.86, Steps=275770, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2403, Total reward=296.69, Steps=275964, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2404, Total reward=5.6, Steps=275982, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2405, Total reward=160.84, Steps=276107, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2406, Total reward=127.57, Steps=276188, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2407, Total reward=168.37, Steps=276315, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2408, Total reward=94.25, Steps=276352, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2409, Total reward=327.56, Steps=276537, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2410, Total reward=333.23, Steps=276766, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2411, Total reward=134.06, Steps=276848, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2412, Total reward=352.12, Steps=277034, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2413, Total reward=97.06, Steps=277095, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2414, Total reward=84.7, Steps=277154, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2415, Total reward=10.32, Steps=277165, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2416, Total reward=106.41, Steps=277244, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2417, Total reward=383.58, Steps=277517, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2418, Total reward=487.73, Steps=277787, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2419, Total reward=22.89, Steps=277805, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2420, Total reward=364.24, Steps=278023, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2421, Total reward=19.55, Steps=278038, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2422, Total reward=57.82, Steps=278061, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2423, Total reward=250.55, Steps=278253, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2424, Total reward=157.63, Steps=278403, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2425, Total reward=171.32, Steps=278521, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2426, Total reward=52.6, Steps=278554, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2427, Total reward=494.93, Steps=278840, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2428, Total reward=202.71, Steps=278943, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2429, Total reward=137.89, Steps=279048, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2430, Total reward=482.86, Steps=279343, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2431, Total reward=66.55, Steps=279389, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2432, Total reward=494.11, Steps=279684, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2433, Total reward=94.66, Steps=279726, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2434, Total reward=26.24, Steps=279735, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2435, Total reward=382.64, Steps=280027, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2436, Total reward=40.31, Steps=280058, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2437, Total reward=128.61, Steps=280180, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2438, Total reward=414.01, Steps=280476, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2439, Total reward=24.89, Steps=280495, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2440, Total reward=255.91, Steps=280711, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2441, Total reward=450.28, Steps=281007, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2442, Total reward=82.82, Steps=281073, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2443, Total reward=15.83, Steps=281091, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2444, Total reward=263.23, Steps=281278, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2445, Total reward=251.4, Steps=281473, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2446, Total reward=253.13, Steps=281600, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2447, Total reward=424.5, Steps=281890, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2448, Total reward=442.93, Steps=282166, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2449, Total reward=355.92, Steps=282367, Training iteration=48
Training> Name=main_level/agent, Worker=0, Episode=2450, Total reward=101.85, Steps=282476, Training iteration=48
Policy training> Surrogate loss=-0.00042955836397595704, KL divergence=0.0004001715569756925, Entropy=0.18996265530586243, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01627718284726143, KL divergence=0.005736767780035734, Entropy=0.18768686056137085, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.023622462525963783, KL divergence=0.009324608370661736, Entropy=0.18554078042507172, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025303099304437637, KL divergence=0.011971374042332172, Entropy=0.1849345862865448, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02715667337179184, KL divergence=0.013551904819905758, Entropy=0.18515412509441376, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026681294664740562, KL divergence=0.014691590331494808, Entropy=0.1845284402370453, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03466528654098511, KL divergence=0.016046104952692986, Entropy=0.18411694467067719, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.031014790758490562, KL divergence=0.01701606623828411, Entropy=0.18420924246311188, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03249817341566086, KL divergence=0.018132971599698067, Entropy=0.185328409075737, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03548624739050865, KL divergence=0.018498430028557777, Entropy=0.18330571055412292, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/481_Step-282476.ckpt']
Uploaded 3 files for checkpoint 481 in 0.57 seconds
saved intermediate frozen graph: current/model/model_481.pb
Best checkpoint number: 473, Last checkpoint number: 479
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'478'}
Training> Name=main_level/agent, Worker=0, Episode=2451, Total reward=122.24, Steps=282562, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2452, Total reward=256.92, Steps=282698, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2453, Total reward=118.55, Steps=282757, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2454, Total reward=176.83, Steps=282838, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2455, Total reward=54.26, Steps=282857, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2456, Total reward=227.34, Steps=282967, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2457, Total reward=69.16, Steps=283038, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2458, Total reward=66.31, Steps=283076, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2459, Total reward=467.45, Steps=283383, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2460, Total reward=80.72, Steps=283461, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2461, Total reward=212.63, Steps=283650, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2462, Total reward=310.58, Steps=283869, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2463, Total reward=39.23, Steps=283919, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2464, Total reward=87.69, Steps=284032, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2465, Total reward=222.89, Steps=284184, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2466, Total reward=108.86, Steps=284274, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2467, Total reward=60.25, Steps=284320, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2468, Total reward=413.41, Steps=284502, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2469, Total reward=55.84, Steps=284566, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2470, Total reward=69.23, Steps=284617, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2471, Total reward=101.54, Steps=284687, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2472, Total reward=9.34, Steps=284699, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2473, Total reward=105.19, Steps=284763, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2474, Total reward=86.9, Steps=284811, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2475, Total reward=451.88, Steps=285077, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2476, Total reward=201.43, Steps=285210, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2477, Total reward=86.87, Steps=285286, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2478, Total reward=149.46, Steps=285408, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2479, Total reward=208.19, Steps=285568, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2480, Total reward=74.74, Steps=285635, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2481, Total reward=171.38, Steps=285790, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2482, Total reward=52.94, Steps=285814, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2483, Total reward=14.26, Steps=285830, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2484, Total reward=417.57, Steps=286130, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2485, Total reward=151.6, Steps=286248, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2486, Total reward=18.21, Steps=286272, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2487, Total reward=87.04, Steps=286353, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2488, Total reward=418.02, Steps=286587, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2489, Total reward=40.16, Steps=286616, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2490, Total reward=53.92, Steps=286663, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2491, Total reward=128.67, Steps=286736, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2492, Total reward=115.01, Steps=286814, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2493, Total reward=313.26, Steps=286973, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2494, Total reward=177.51, Steps=287094, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2495, Total reward=396.95, Steps=287368, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2496, Total reward=531.58, Steps=287644, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2497, Total reward=96.37, Steps=287705, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2498, Total reward=502.8, Steps=287983, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2499, Total reward=22.13, Steps=288009, Training iteration=49
Training> Name=main_level/agent, Worker=0, Episode=2500, Total reward=100.12, Steps=288060, Training iteration=49
Policy training> Surrogate loss=0.00501727219671011, KL divergence=0.00016365910414606333, Entropy=0.18634089827537537, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.011481104418635368, KL divergence=0.0040510063990950584, Entropy=0.18545600771903992, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.022765832021832466, KL divergence=0.0077497526071965694, Entropy=0.18548285961151123, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.030135223641991615, KL divergence=0.011020961217582226, Entropy=0.18318746984004974, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02945271134376526, KL divergence=0.012553572654724121, Entropy=0.18099382519721985, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03095564804971218, KL divergence=0.014548282139003277, Entropy=0.1835046261548996, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03742126375436783, KL divergence=0.015552242286503315, Entropy=0.18168865144252777, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.0368833914399147, KL divergence=0.016710836440324783, Entropy=0.18288913369178772, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03488054871559143, KL divergence=0.017708884552121162, Entropy=0.18386845290660858, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03999103605747223, KL divergence=0.018231526017189026, Entropy=0.18178656697273254, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/482_Step-288060.ckpt']
Uploaded 3 files for checkpoint 482 in 0.56 seconds
saved intermediate frozen graph: current/model/model_482.pb
Best checkpoint number: 473, Last checkpoint number: 480
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'479'}
Training> Name=main_level/agent, Worker=0, Episode=2501, Total reward=49.0, Steps=288086, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2502, Total reward=51.89, Steps=288118, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2503, Total reward=158.47, Steps=288283, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2504, Total reward=115.58, Steps=288424, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2505, Total reward=164.45, Steps=288542, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2506, Total reward=251.47, Steps=288710, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2507, Total reward=504.5, Steps=288976, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2508, Total reward=438.18, Steps=289207, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2509, Total reward=39.99, Steps=289247, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2510, Total reward=75.19, Steps=289332, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2511, Total reward=488.06, Steps=289638, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2512, Total reward=453.6, Steps=289912, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2513, Total reward=109.71, Steps=289955, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2514, Total reward=269.61, Steps=290129, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2515, Total reward=203.56, Steps=290320, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2516, Total reward=77.32, Steps=290379, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2517, Total reward=183.87, Steps=290474, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2518, Total reward=157.22, Steps=290560, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2519, Total reward=117.03, Steps=290661, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2520, Total reward=466.47, Steps=290954, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2521, Total reward=319.55, Steps=291178, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2522, Total reward=230.29, Steps=291380, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2523, Total reward=118.26, Steps=291496, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2524, Total reward=5.48, Steps=291519, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2525, Total reward=103.98, Steps=291632, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2526, Total reward=382.01, Steps=291935, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2527, Total reward=84.09, Steps=292007, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2528, Total reward=432.16, Steps=292298, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2529, Total reward=59.28, Steps=292357, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2530, Total reward=79.25, Steps=292440, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2531, Total reward=443.59, Steps=292730, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2532, Total reward=123.02, Steps=292808, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2533, Total reward=31.53, Steps=292819, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2534, Total reward=354.73, Steps=293064, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2535, Total reward=18.52, Steps=293077, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2536, Total reward=427.27, Steps=293359, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2537, Total reward=153.96, Steps=293506, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2538, Total reward=369.88, Steps=293764, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2539, Total reward=319.54, Steps=294009, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2540, Total reward=43.08, Steps=294040, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2541, Total reward=101.16, Steps=294099, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2542, Total reward=57.49, Steps=294130, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2543, Total reward=351.47, Steps=294389, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2544, Total reward=17.13, Steps=294441, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2545, Total reward=196.61, Steps=294598, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2546, Total reward=482.16, Steps=294878, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2547, Total reward=418.52, Steps=295183, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2548, Total reward=123.72, Steps=295256, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2549, Total reward=331.71, Steps=295433, Training iteration=50
Training> Name=main_level/agent, Worker=0, Episode=2550, Total reward=443.65, Steps=295717, Training iteration=50
Policy training> Surrogate loss=0.002325324574485421, KL divergence=0.00031651105382479727, Entropy=0.1925504058599472, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017999734729528427, KL divergence=0.006426787935197353, Entropy=0.19095246493816376, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.023680049926042557, KL divergence=0.010971168987452984, Entropy=0.18881961703300476, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02444215677678585, KL divergence=0.013091480359435081, Entropy=0.18964187800884247, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.024768376722931862, KL divergence=0.013832270167768002, Entropy=0.18945828080177307, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.029177306219935417, KL divergence=0.014175002463161945, Entropy=0.18764393031597137, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02868553064763546, KL divergence=0.015747614204883575, Entropy=0.18874917924404144, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.027237161993980408, KL divergence=0.016704542562365532, Entropy=0.1880098581314087, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.031838320195674896, KL divergence=0.01751674897968769, Entropy=0.18867850303649902, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03454010933637619, KL divergence=0.018133442848920822, Entropy=0.18713773787021637, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/483_Step-295717.ckpt']
Uploaded 3 files for checkpoint 483 in 0.59 seconds
saved intermediate frozen graph: current/model/model_483.pb
Best checkpoint number: 473, Last checkpoint number: 481
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'480'}
Training> Name=main_level/agent, Worker=0, Episode=2551, Total reward=129.89, Steps=295821, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2552, Total reward=200.86, Steps=295934, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2553, Total reward=35.54, Steps=295958, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2554, Total reward=31.46, Steps=295970, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2555, Total reward=441.53, Steps=296268, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2556, Total reward=331.48, Steps=296476, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2557, Total reward=86.96, Steps=296542, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2558, Total reward=64.06, Steps=296581, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2559, Total reward=418.31, Steps=296843, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2560, Total reward=114.93, Steps=296896, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2561, Total reward=466.95, Steps=297181, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2562, Total reward=240.77, Steps=297380, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2563, Total reward=166.75, Steps=297572, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2564, Total reward=16.66, Steps=297602, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2565, Total reward=0.0, Steps=297603, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2566, Total reward=473.33, Steps=297882, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2567, Total reward=383.11, Steps=298189, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2568, Total reward=176.5, Steps=298319, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2569, Total reward=38.64, Steps=298347, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2570, Total reward=19.9, Steps=298396, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2571, Total reward=470.55, Steps=298707, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2572, Total reward=162.87, Steps=298777, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2573, Total reward=86.18, Steps=298841, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2574, Total reward=115.95, Steps=298917, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2575, Total reward=466.4, Steps=299213, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2576, Total reward=47.21, Steps=299246, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2577, Total reward=479.69, Steps=299544, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2578, Total reward=167.13, Steps=299625, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2579, Total reward=118.25, Steps=299732, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2580, Total reward=492.68, Steps=300026, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2581, Total reward=329.85, Steps=300254, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2582, Total reward=253.52, Steps=300455, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2583, Total reward=224.17, Steps=300634, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2584, Total reward=9.8, Steps=300660, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2585, Total reward=220.56, Steps=300775, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2586, Total reward=160.19, Steps=300872, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2587, Total reward=88.34, Steps=300931, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2588, Total reward=151.93, Steps=301058, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2589, Total reward=439.46, Steps=301361, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2590, Total reward=443.28, Steps=301650, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2591, Total reward=251.35, Steps=301785, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2592, Total reward=134.76, Steps=301867, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2593, Total reward=102.73, Steps=301930, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2594, Total reward=215.13, Steps=302036, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2595, Total reward=97.82, Steps=302080, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2596, Total reward=123.22, Steps=302153, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2597, Total reward=204.77, Steps=302284, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2598, Total reward=296.16, Steps=302510, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2599, Total reward=96.06, Steps=302573, Training iteration=51
Training> Name=main_level/agent, Worker=0, Episode=2600, Total reward=291.93, Steps=302811, Training iteration=51
Policy training> Surrogate loss=0.0021175513975322247, KL divergence=0.0004414304275996983, Entropy=0.1964065283536911, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.016694702208042145, KL divergence=0.006166292354464531, Entropy=0.19596657156944275, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.022351842373609543, KL divergence=0.009968282654881477, Entropy=0.19434720277786255, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02838665433228016, KL divergence=0.011978257447481155, Entropy=0.19177128374576569, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028720177710056305, KL divergence=0.014227996580302715, Entropy=0.19333145022392273, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03099687583744526, KL divergence=0.015386154875159264, Entropy=0.19315674901008606, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02816718816757202, KL divergence=0.01591821014881134, Entropy=0.19216419756412506, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03536521643400192, KL divergence=0.017108822241425514, Entropy=0.1923542320728302, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03085843101143837, KL divergence=0.018057167530059814, Entropy=0.19218240678310394, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03575187548995018, KL divergence=0.017975447699427605, Entropy=0.18924552202224731, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/484_Step-302811.ckpt']
Uploaded 3 files for checkpoint 484 in 0.51 seconds
saved intermediate frozen graph: current/model/model_484.pb
Best checkpoint number: 473, Last checkpoint number: 482
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'481'}
Training> Name=main_level/agent, Worker=0, Episode=2601, Total reward=286.35, Steps=302998, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2602, Total reward=97.54, Steps=303100, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2603, Total reward=196.6, Steps=303291, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2604, Total reward=409.24, Steps=303580, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2605, Total reward=212.18, Steps=303758, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2606, Total reward=3.42, Steps=303769, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2607, Total reward=412.22, Steps=304052, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2608, Total reward=219.49, Steps=304192, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2609, Total reward=48.19, Steps=304225, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2610, Total reward=74.76, Steps=304277, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2611, Total reward=50.98, Steps=304323, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2612, Total reward=475.39, Steps=304577, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2613, Total reward=481.47, Steps=304862, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2614, Total reward=442.87, Steps=305155, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2615, Total reward=98.94, Steps=305233, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2616, Total reward=12.74, Steps=305252, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2617, Total reward=177.31, Steps=305357, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2618, Total reward=72.97, Steps=305393, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2619, Total reward=428.72, Steps=305681, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2620, Total reward=201.82, Steps=305871, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2621, Total reward=89.58, Steps=305939, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2622, Total reward=53.45, Steps=305978, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2623, Total reward=110.13, Steps=306094, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2624, Total reward=17.98, Steps=306132, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2625, Total reward=400.62, Steps=306429, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2626, Total reward=380.54, Steps=306647, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2627, Total reward=213.93, Steps=306792, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2628, Total reward=220.32, Steps=306899, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2629, Total reward=28.57, Steps=306923, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2630, Total reward=69.89, Steps=306991, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2631, Total reward=245.06, Steps=307132, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2632, Total reward=442.58, Steps=307409, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2633, Total reward=303.52, Steps=307558, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2634, Total reward=57.81, Steps=307597, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2635, Total reward=463.31, Steps=307855, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2636, Total reward=164.67, Steps=307950, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2637, Total reward=32.48, Steps=307976, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2638, Total reward=370.66, Steps=308256, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2639, Total reward=489.96, Steps=308533, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2640, Total reward=123.1, Steps=308610, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2641, Total reward=418.23, Steps=308878, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2642, Total reward=41.11, Steps=308914, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2643, Total reward=38.65, Steps=308979, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2644, Total reward=17.49, Steps=309027, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2645, Total reward=202.92, Steps=309160, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2646, Total reward=339.41, Steps=309391, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2647, Total reward=245.28, Steps=309521, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2648, Total reward=149.78, Steps=309625, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2649, Total reward=379.34, Steps=309935, Training iteration=52
Training> Name=main_level/agent, Worker=0, Episode=2650, Total reward=107.91, Steps=310022, Training iteration=52
Policy training> Surrogate loss=-0.0020043053664267063, KL divergence=0.0006851766374893486, Entropy=0.1874329149723053, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017662247642874718, KL divergence=0.007574922405183315, Entropy=0.18551479279994965, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02060520276427269, KL divergence=0.010443052276968956, Entropy=0.1852792501449585, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026061873883008957, KL divergence=0.012400436215102673, Entropy=0.18554475903511047, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029220759868621826, KL divergence=0.01402964536100626, Entropy=0.18551850318908691, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02861608751118183, KL divergence=0.015290151350200176, Entropy=0.18519103527069092, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03159058839082718, KL divergence=0.016460085287690163, Entropy=0.1849050670862198, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03200316056609154, KL divergence=0.01751571334898472, Entropy=0.18460650742053986, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.033183518797159195, KL divergence=0.018077388405799866, Entropy=0.18405243754386902, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0337165966629982, KL divergence=0.018767552450299263, Entropy=0.18475142121315002, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/485_Step-310022.ckpt']
Uploaded 3 files for checkpoint 485 in 0.57 seconds
saved intermediate frozen graph: current/model/model_485.pb
Best checkpoint number: 473, Last checkpoint number: 483
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'482'}
Training> Name=main_level/agent, Worker=0, Episode=2651, Total reward=221.44, Steps=310185, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2652, Total reward=150.43, Steps=310265, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2653, Total reward=29.64, Steps=310276, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2654, Total reward=241.72, Steps=310413, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2655, Total reward=472.14, Steps=310676, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2656, Total reward=418.14, Steps=310972, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2657, Total reward=84.77, Steps=311037, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2658, Total reward=123.68, Steps=311102, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2659, Total reward=444.92, Steps=311404, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2660, Total reward=400.77, Steps=311694, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2661, Total reward=112.23, Steps=311789, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2662, Total reward=195.11, Steps=311958, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2663, Total reward=433.55, Steps=312270, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2664, Total reward=21.76, Steps=312330, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2665, Total reward=8.87, Steps=312357, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2666, Total reward=104.68, Steps=312431, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2667, Total reward=303.77, Steps=312667, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2668, Total reward=144.05, Steps=312756, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2669, Total reward=113.95, Steps=312821, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2670, Total reward=266.38, Steps=312977, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2671, Total reward=38.5, Steps=313003, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2672, Total reward=70.58, Steps=313058, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2673, Total reward=434.44, Steps=313350, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2674, Total reward=61.69, Steps=313405, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2675, Total reward=28.64, Steps=313453, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2676, Total reward=121.96, Steps=313532, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2677, Total reward=15.83, Steps=313555, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2678, Total reward=181.58, Steps=313672, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2679, Total reward=36.93, Steps=313707, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2680, Total reward=445.12, Steps=314004, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2681, Total reward=86.04, Steps=314051, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2682, Total reward=183.7, Steps=314229, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2683, Total reward=30.49, Steps=314282, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2684, Total reward=363.92, Steps=314590, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2685, Total reward=197.69, Steps=314776, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2686, Total reward=422.08, Steps=315084, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2687, Total reward=280.66, Steps=315223, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2688, Total reward=98.3, Steps=315277, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2689, Total reward=186.98, Steps=315367, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2690, Total reward=245.63, Steps=315520, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2691, Total reward=49.64, Steps=315594, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2692, Total reward=246.22, Steps=315742, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2693, Total reward=69.18, Steps=315766, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2694, Total reward=35.03, Steps=315778, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2695, Total reward=80.57, Steps=315828, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2696, Total reward=120.93, Steps=315895, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2697, Total reward=468.6, Steps=316192, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2698, Total reward=462.59, Steps=316469, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2699, Total reward=289.36, Steps=316676, Training iteration=53
Training> Name=main_level/agent, Worker=0, Episode=2700, Total reward=439.49, Steps=316966, Training iteration=53
Policy training> Surrogate loss=0.005681379698216915, KL divergence=0.0005427870783023536, Entropy=0.193037748336792, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014640005305409431, KL divergence=0.005848385393619537, Entropy=0.18981172144412994, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.021402273327112198, KL divergence=0.009230555966496468, Entropy=0.1890619695186615, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.01975071430206299, KL divergence=0.011654594913125038, Entropy=0.1896093189716339, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02739206701517105, KL divergence=0.01304627489298582, Entropy=0.1886402666568756, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030574418604373932, KL divergence=0.014827036298811436, Entropy=0.1898202896118164, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.028020985424518585, KL divergence=0.015526681207120419, Entropy=0.1878431737422943, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.032887935638427734, KL divergence=0.01638983190059662, Entropy=0.18843936920166016, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.0322837308049202, KL divergence=0.017737707123160362, Entropy=0.18848372995853424, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03502970188856125, KL divergence=0.018531959503889084, Entropy=0.18742717802524567, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/486_Step-316966.ckpt']
Uploaded 3 files for checkpoint 486 in 0.68 seconds
saved intermediate frozen graph: current/model/model_486.pb
Best checkpoint number: 473, Last checkpoint number: 484
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'483'}
Training> Name=main_level/agent, Worker=0, Episode=2701, Total reward=74.1, Steps=317007, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2702, Total reward=154.52, Steps=317129, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2703, Total reward=276.59, Steps=317319, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2704, Total reward=15.42, Steps=317368, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2705, Total reward=126.66, Steps=317498, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2706, Total reward=14.09, Steps=317590, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2707, Total reward=154.84, Steps=317678, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2708, Total reward=141.48, Steps=317777, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2709, Total reward=118.97, Steps=317879, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2710, Total reward=132.95, Steps=317960, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2711, Total reward=332.88, Steps=318188, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2712, Total reward=95.05, Steps=318270, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2713, Total reward=236.2, Steps=318413, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2714, Total reward=26.21, Steps=318424, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2715, Total reward=237.08, Steps=318550, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2716, Total reward=110.42, Steps=318629, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2717, Total reward=395.85, Steps=318905, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2718, Total reward=389.29, Steps=319142, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2719, Total reward=62.87, Steps=319220, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2720, Total reward=102.27, Steps=319270, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2721, Total reward=89.11, Steps=319332, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2722, Total reward=72.63, Steps=319393, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2723, Total reward=382.34, Steps=319676, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2724, Total reward=7.66, Steps=319703, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2725, Total reward=460.79, Steps=320003, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2726, Total reward=182.37, Steps=320125, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2727, Total reward=212.58, Steps=320271, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2728, Total reward=476.56, Steps=320564, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2729, Total reward=29.17, Steps=320606, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2730, Total reward=13.18, Steps=320654, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2731, Total reward=122.61, Steps=320710, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2732, Total reward=9.35, Steps=320722, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2733, Total reward=33.99, Steps=320734, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2734, Total reward=58.8, Steps=320781, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2735, Total reward=26.55, Steps=320797, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2736, Total reward=280.43, Steps=321053, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2737, Total reward=279.28, Steps=321273, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2738, Total reward=143.62, Steps=321357, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2739, Total reward=488.89, Steps=321621, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2740, Total reward=93.74, Steps=321678, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2741, Total reward=247.16, Steps=321860, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2742, Total reward=479.33, Steps=322154, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2743, Total reward=392.09, Steps=322438, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2744, Total reward=388.39, Steps=322732, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2745, Total reward=251.72, Steps=322910, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2746, Total reward=193.08, Steps=323000, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2747, Total reward=173.87, Steps=323151, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2748, Total reward=395.66, Steps=323413, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2749, Total reward=313.21, Steps=323611, Training iteration=54
Training> Name=main_level/agent, Worker=0, Episode=2750, Total reward=306.31, Steps=323766, Training iteration=54
Policy training> Surrogate loss=0.0021700819488614798, KL divergence=0.0002207444777013734, Entropy=0.18702489137649536, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017380880191922188, KL divergence=0.004324080888181925, Entropy=0.18506528437137604, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02075345255434513, KL divergence=0.008582173846662045, Entropy=0.1837201863527298, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02480900287628174, KL divergence=0.0107887526974082, Entropy=0.18288205564022064, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.027469882741570473, KL divergence=0.01241993810981512, Entropy=0.18216142058372498, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030400216579437256, KL divergence=0.013801854103803635, Entropy=0.18316738307476044, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03317182883620262, KL divergence=0.01476957369595766, Entropy=0.1835445761680603, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.034054338932037354, KL divergence=0.015428693033754826, Entropy=0.18119828402996063, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03352892026305199, KL divergence=0.0164052564650774, Entropy=0.18214981257915497, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03274787962436676, KL divergence=0.017055775970220566, Entropy=0.18187394738197327, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/487_Step-323766.ckpt']
Uploaded 3 files for checkpoint 487 in 0.63 seconds
saved intermediate frozen graph: current/model/model_487.pb
Best checkpoint number: 473, Last checkpoint number: 485
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'484'}
Training> Name=main_level/agent, Worker=0, Episode=2751, Total reward=277.22, Steps=323920, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2752, Total reward=435.01, Steps=324202, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2753, Total reward=380.76, Steps=324478, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2754, Total reward=262.36, Steps=324614, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2755, Total reward=484.47, Steps=324876, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2756, Total reward=63.5, Steps=324923, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2757, Total reward=6.77, Steps=324938, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2758, Total reward=119.89, Steps=325029, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2759, Total reward=246.05, Steps=325231, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2760, Total reward=201.01, Steps=325435, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2761, Total reward=92.99, Steps=325520, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2762, Total reward=219.89, Steps=325685, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2763, Total reward=378.89, Steps=326001, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2764, Total reward=19.05, Steps=326022, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2765, Total reward=371.28, Steps=326306, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2766, Total reward=379.88, Steps=326607, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2767, Total reward=189.82, Steps=326713, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2768, Total reward=373.35, Steps=327016, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2769, Total reward=67.71, Steps=327083, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2770, Total reward=367.25, Steps=327383, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2771, Total reward=192.07, Steps=327514, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2772, Total reward=101.19, Steps=327589, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2773, Total reward=26.55, Steps=327600, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2774, Total reward=54.08, Steps=327645, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2775, Total reward=26.54, Steps=327659, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2776, Total reward=421.62, Steps=327975, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2777, Total reward=188.83, Steps=328113, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2778, Total reward=46.84, Steps=328146, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2779, Total reward=345.09, Steps=328378, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2780, Total reward=83.74, Steps=328484, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2781, Total reward=259.05, Steps=328709, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2782, Total reward=48.76, Steps=328739, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2783, Total reward=478.13, Steps=329023, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2784, Total reward=461.52, Steps=329328, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2785, Total reward=435.16, Steps=329607, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2786, Total reward=159.84, Steps=329708, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2787, Total reward=433.82, Steps=329959, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2788, Total reward=485.33, Steps=330249, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2789, Total reward=124.41, Steps=330372, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2790, Total reward=348.06, Steps=330628, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2791, Total reward=150.88, Steps=330717, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2792, Total reward=244.45, Steps=330860, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2793, Total reward=68.37, Steps=330888, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2794, Total reward=46.87, Steps=330914, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2795, Total reward=447.4, Steps=331205, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2796, Total reward=375.17, Steps=331493, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2797, Total reward=303.12, Steps=331720, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2798, Total reward=147.0, Steps=331813, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2799, Total reward=303.37, Steps=332045, Training iteration=55
Training> Name=main_level/agent, Worker=0, Episode=2800, Total reward=282.44, Steps=332228, Training iteration=55
Policy training> Surrogate loss=0.0005310376291163266, KL divergence=0.0005995117826387286, Entropy=0.1958187073469162, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.016491718590259552, KL divergence=0.007299727760255337, Entropy=0.1946481466293335, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02541097067296505, KL divergence=0.010205072350800037, Entropy=0.19404387474060059, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025185449048876762, KL divergence=0.012180257588624954, Entropy=0.19274328649044037, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02676732838153839, KL divergence=0.01384994201362133, Entropy=0.193304643034935, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02708076313138008, KL divergence=0.014542600139975548, Entropy=0.19364428520202637, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03209056705236435, KL divergence=0.01590104028582573, Entropy=0.19217321276664734, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03529602661728859, KL divergence=0.01671433448791504, Entropy=0.19201324880123138, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03477102518081665, KL divergence=0.017366426065564156, Entropy=0.1913621574640274, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03248872235417366, KL divergence=0.017928089946508408, Entropy=0.1907009482383728, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/488_Step-332228.ckpt']
Uploaded 3 files for checkpoint 488 in 0.49 seconds
saved intermediate frozen graph: current/model/model_488.pb
Best checkpoint number: 473, Last checkpoint number: 486
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'485'}
Training> Name=main_level/agent, Worker=0, Episode=2801, Total reward=405.37, Steps=332554, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2802, Total reward=84.97, Steps=332649, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2803, Total reward=268.13, Steps=332837, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2804, Total reward=217.07, Steps=332978, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2805, Total reward=187.09, Steps=333119, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2806, Total reward=387.7, Steps=333420, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2807, Total reward=146.66, Steps=333517, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2808, Total reward=76.15, Steps=333555, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2809, Total reward=163.37, Steps=333670, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2810, Total reward=263.51, Steps=333859, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2811, Total reward=295.71, Steps=334057, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2812, Total reward=160.83, Steps=334159, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2813, Total reward=125.63, Steps=334212, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2814, Total reward=343.68, Steps=334455, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2815, Total reward=398.53, Steps=334755, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2816, Total reward=409.68, Steps=335048, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2817, Total reward=364.27, Steps=335291, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2818, Total reward=278.47, Steps=335492, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2819, Total reward=98.91, Steps=335563, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2820, Total reward=81.67, Steps=335615, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2821, Total reward=515.27, Steps=335906, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2822, Total reward=151.84, Steps=336030, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2823, Total reward=6.02, Steps=336042, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2824, Total reward=149.72, Steps=336182, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2825, Total reward=227.16, Steps=336342, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2826, Total reward=131.86, Steps=336446, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2827, Total reward=418.56, Steps=336690, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2828, Total reward=145.99, Steps=336778, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2829, Total reward=34.35, Steps=336806, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2830, Total reward=14.78, Steps=336833, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2831, Total reward=301.93, Steps=337047, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2832, Total reward=16.53, Steps=337060, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2833, Total reward=33.08, Steps=337071, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2834, Total reward=151.11, Steps=337156, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2835, Total reward=364.97, Steps=337370, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2836, Total reward=306.1, Steps=337620, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2837, Total reward=7.71, Steps=337632, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2838, Total reward=70.9, Steps=337667, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2839, Total reward=232.67, Steps=337826, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2840, Total reward=91.74, Steps=337889, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2841, Total reward=459.97, Steps=338176, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2842, Total reward=220.06, Steps=338348, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2843, Total reward=25.92, Steps=338392, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2844, Total reward=6.01, Steps=338437, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2845, Total reward=2.93, Steps=338470, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2846, Total reward=489.25, Steps=338769, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2847, Total reward=143.05, Steps=338863, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2848, Total reward=110.89, Steps=338930, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2849, Total reward=206.96, Steps=339044, Training iteration=56
Training> Name=main_level/agent, Worker=0, Episode=2850, Total reward=55.81, Steps=339092, Training iteration=56
Policy training> Surrogate loss=0.0004283446178305894, KL divergence=0.00036807716242037714, Entropy=0.19000479578971863, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.013251097872853279, KL divergence=0.005657173693180084, Entropy=0.18915356695652008, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.017776811495423317, KL divergence=0.008589968085289001, Entropy=0.18767975270748138, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.0221086535602808, KL divergence=0.01046247873455286, Entropy=0.18730269372463226, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.027083514258265495, KL divergence=0.012315765954554081, Entropy=0.18717652559280396, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02876245602965355, KL divergence=0.01365666463971138, Entropy=0.18724828958511353, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03224741667509079, KL divergence=0.014827555976808071, Entropy=0.18667834997177124, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03111148066818714, KL divergence=0.015713458880782127, Entropy=0.1876525729894638, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.032574016600847244, KL divergence=0.0160235445946455, Entropy=0.18586400151252747, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.030181724578142166, KL divergence=0.016905702650547028, Entropy=0.18692129850387573, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/489_Step-339092.ckpt']
Uploaded 3 files for checkpoint 489 in 0.52 seconds
saved intermediate frozen graph: current/model/model_489.pb
Best checkpoint number: 473, Last checkpoint number: 487
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'486'}
Training> Name=main_level/agent, Worker=0, Episode=2851, Total reward=28.95, Steps=339145, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2852, Total reward=233.83, Steps=339291, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2853, Total reward=86.64, Steps=339330, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2854, Total reward=77.57, Steps=339380, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2855, Total reward=388.51, Steps=339657, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2856, Total reward=127.0, Steps=339739, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2857, Total reward=129.97, Steps=339819, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2858, Total reward=39.91, Steps=339835, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2859, Total reward=242.52, Steps=340048, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2860, Total reward=14.08, Steps=340072, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2861, Total reward=322.25, Steps=340303, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2862, Total reward=249.37, Steps=340516, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2863, Total reward=19.34, Steps=340566, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2864, Total reward=148.05, Steps=340692, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2865, Total reward=237.26, Steps=340869, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2866, Total reward=156.29, Steps=341011, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2867, Total reward=238.68, Steps=341147, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2868, Total reward=285.85, Steps=341278, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2869, Total reward=66.09, Steps=341347, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2870, Total reward=352.66, Steps=341559, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2871, Total reward=435.14, Steps=341844, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2872, Total reward=516.85, Steps=342136, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2873, Total reward=89.16, Steps=342177, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2874, Total reward=468.98, Steps=342457, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2875, Total reward=167.8, Steps=342541, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2876, Total reward=229.36, Steps=342684, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2877, Total reward=406.65, Steps=342980, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2878, Total reward=155.99, Steps=343114, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2879, Total reward=261.81, Steps=343363, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2880, Total reward=96.6, Steps=343426, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2881, Total reward=93.21, Steps=343501, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2882, Total reward=257.76, Steps=343712, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2883, Total reward=10.04, Steps=343725, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2884, Total reward=211.23, Steps=343911, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2885, Total reward=0.02, Steps=343928, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2886, Total reward=252.64, Steps=344073, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2887, Total reward=487.07, Steps=344332, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2888, Total reward=198.61, Steps=344425, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2889, Total reward=97.78, Steps=344512, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2890, Total reward=419.99, Steps=344838, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2891, Total reward=105.0, Steps=344906, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2892, Total reward=10.2, Steps=344918, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2893, Total reward=33.85, Steps=344930, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2894, Total reward=148.16, Steps=345022, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2895, Total reward=249.02, Steps=345202, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2896, Total reward=431.41, Steps=345513, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2897, Total reward=98.99, Steps=345581, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2898, Total reward=311.26, Steps=345838, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2899, Total reward=90.45, Steps=345898, Training iteration=57
Training> Name=main_level/agent, Worker=0, Episode=2900, Total reward=359.12, Steps=346123, Training iteration=57
Policy training> Surrogate loss=-0.0008471779874525964, KL divergence=0.0002670473186299205, Entropy=0.18503333628177643, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014595484361052513, KL divergence=0.004401641897857189, Entropy=0.18251429498195648, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.01979379542171955, KL divergence=0.007574143819510937, Entropy=0.18358519673347473, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025140423327684402, KL divergence=0.010270758531987667, Entropy=0.1839245855808258, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026952825486660004, KL divergence=0.012327993288636208, Entropy=0.18313637375831604, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.024632474407553673, KL divergence=0.0131501704454422, Entropy=0.1834164708852768, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.025798264890909195, KL divergence=0.013712338171899319, Entropy=0.18141290545463562, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02882099151611328, KL divergence=0.01513927523046732, Entropy=0.18222038447856903, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.030061619356274605, KL divergence=0.015982115641236305, Entropy=0.18148095905780792, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.030774343758821487, KL divergence=0.01670449785888195, Entropy=0.18150360882282257, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/490_Step-346123.ckpt']
Uploaded 3 files for checkpoint 490 in 0.52 seconds
saved intermediate frozen graph: current/model/model_490.pb
Best checkpoint number: 473, Last checkpoint number: 488
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'487'}
Training> Name=main_level/agent, Worker=0, Episode=2901, Total reward=101.53, Steps=346218, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2902, Total reward=339.7, Steps=346443, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2903, Total reward=427.2, Steps=346718, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2904, Total reward=192.81, Steps=346871, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2905, Total reward=0.02, Steps=346887, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2906, Total reward=260.81, Steps=347034, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2907, Total reward=114.59, Steps=347142, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2908, Total reward=435.21, Steps=347377, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2909, Total reward=122.05, Steps=347482, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2910, Total reward=22.68, Steps=347504, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2911, Total reward=121.37, Steps=347594, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2912, Total reward=91.4, Steps=347650, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2913, Total reward=108.23, Steps=347708, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2914, Total reward=7.71, Steps=347718, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2915, Total reward=74.54, Steps=347772, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2916, Total reward=451.33, Steps=348060, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2917, Total reward=290.98, Steps=348244, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2918, Total reward=82.52, Steps=348288, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2919, Total reward=46.35, Steps=348345, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2920, Total reward=122.5, Steps=348450, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2921, Total reward=449.23, Steps=348734, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2922, Total reward=143.28, Steps=348865, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2923, Total reward=114.02, Steps=348991, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2924, Total reward=153.94, Steps=349151, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2925, Total reward=124.31, Steps=349283, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2926, Total reward=342.27, Steps=349483, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2927, Total reward=200.04, Steps=349604, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2928, Total reward=360.13, Steps=349803, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2929, Total reward=178.07, Steps=349925, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2930, Total reward=136.81, Steps=350010, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2931, Total reward=342.27, Steps=350233, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2932, Total reward=472.07, Steps=350528, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2933, Total reward=76.77, Steps=350569, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2934, Total reward=213.57, Steps=350699, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2935, Total reward=480.9, Steps=350972, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2936, Total reward=38.87, Steps=351033, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2937, Total reward=278.22, Steps=351228, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2938, Total reward=48.0, Steps=351274, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2939, Total reward=441.6, Steps=351573, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2940, Total reward=65.44, Steps=351635, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2941, Total reward=59.46, Steps=351662, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2942, Total reward=459.35, Steps=351913, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2943, Total reward=300.04, Steps=352149, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2944, Total reward=5.66, Steps=352184, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2945, Total reward=290.55, Steps=352399, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2946, Total reward=133.78, Steps=352519, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2947, Total reward=456.47, Steps=352772, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2948, Total reward=406.49, Steps=353010, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2949, Total reward=163.61, Steps=353103, Training iteration=58
Training> Name=main_level/agent, Worker=0, Episode=2950, Total reward=317.95, Steps=353318, Training iteration=58
Policy training> Surrogate loss=-8.975421951618046e-05, KL divergence=0.0006225568358786404, Entropy=0.18564918637275696, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01508238073438406, KL divergence=0.005346166901290417, Entropy=0.18465550243854523, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02137836627662182, KL divergence=0.008840052410960197, Entropy=0.1831900179386139, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026494186371564865, KL divergence=0.010677630081772804, Entropy=0.18287912011146545, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026992490515112877, KL divergence=0.011877993121743202, Entropy=0.18233101069927216, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.027268728241324425, KL divergence=0.012887532822787762, Entropy=0.1825711876153946, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.030721332877874374, KL divergence=0.013797348365187645, Entropy=0.18232128024101257, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.031056750565767288, KL divergence=0.014690249226987362, Entropy=0.18241038918495178, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.032379209995269775, KL divergence=0.015856187790632248, Entropy=0.1827826201915741, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03263256698846817, KL divergence=0.016549205407500267, Entropy=0.18199649453163147, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/491_Step-353318.ckpt']
Uploaded 3 files for checkpoint 491 in 0.49 seconds
saved intermediate frozen graph: current/model/model_491.pb
Best checkpoint number: 473, Last checkpoint number: 489
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'488'}
Training> Name=main_level/agent, Worker=0, Episode=2951, Total reward=62.67, Steps=353353, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2952, Total reward=15.81, Steps=353365, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2953, Total reward=121.86, Steps=353426, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2954, Total reward=56.33, Steps=353458, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2955, Total reward=425.76, Steps=353749, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2956, Total reward=509.44, Steps=354038, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2957, Total reward=42.88, Steps=354073, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2958, Total reward=31.86, Steps=354097, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2959, Total reward=24.26, Steps=354116, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2960, Total reward=312.41, Steps=354335, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2961, Total reward=465.14, Steps=354625, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2962, Total reward=47.77, Steps=354652, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2963, Total reward=214.38, Steps=354844, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2964, Total reward=20.85, Steps=354902, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2965, Total reward=109.51, Steps=355027, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2966, Total reward=68.28, Steps=355092, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2967, Total reward=1.92, Steps=355102, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2968, Total reward=72.77, Steps=355164, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2969, Total reward=132.08, Steps=355268, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2970, Total reward=317.38, Steps=355431, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2971, Total reward=311.4, Steps=355636, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2972, Total reward=457.5, Steps=355919, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2973, Total reward=85.73, Steps=355965, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2974, Total reward=58.5, Steps=355995, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2975, Total reward=122.1, Steps=356076, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2976, Total reward=116.38, Steps=356162, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2977, Total reward=265.99, Steps=356370, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2978, Total reward=463.09, Steps=356666, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2979, Total reward=41.34, Steps=356698, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2980, Total reward=225.81, Steps=356857, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2981, Total reward=55.94, Steps=356880, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2982, Total reward=122.81, Steps=357004, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2983, Total reward=175.76, Steps=357161, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2984, Total reward=168.47, Steps=357320, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2985, Total reward=495.0, Steps=357623, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2986, Total reward=184.7, Steps=357784, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2987, Total reward=475.68, Steps=358079, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2988, Total reward=439.57, Steps=358364, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2989, Total reward=172.62, Steps=358464, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2990, Total reward=3.77, Steps=358477, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2991, Total reward=104.8, Steps=358532, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2992, Total reward=2.7, Steps=358544, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2993, Total reward=83.35, Steps=358584, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2994, Total reward=266.9, Steps=358763, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2995, Total reward=289.47, Steps=358979, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2996, Total reward=105.31, Steps=359048, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2997, Total reward=272.33, Steps=359265, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2998, Total reward=89.29, Steps=359312, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=2999, Total reward=34.73, Steps=359338, Training iteration=59
Training> Name=main_level/agent, Worker=0, Episode=3000, Total reward=250.26, Steps=359533, Training iteration=59
Policy training> Surrogate loss=0.000240073844906874, KL divergence=0.00025196533533744514, Entropy=0.19600635766983032, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.016883892938494682, KL divergence=0.0055042714811861515, Entropy=0.1939658522605896, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.021907424554228783, KL divergence=0.01015587616711855, Entropy=0.19370080530643463, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026347694918513298, KL divergence=0.012687954120337963, Entropy=0.19358201324939728, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.030191102996468544, KL divergence=0.014365074224770069, Entropy=0.1919964700937271, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.029980763792991638, KL divergence=0.015952713787555695, Entropy=0.19185151159763336, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.032539594918489456, KL divergence=0.017248427495360374, Entropy=0.19120417535305023, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.0350295789539814, KL divergence=0.018273720517754555, Entropy=0.19240523874759674, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03367922082543373, KL divergence=0.019241128116846085, Entropy=0.19221405684947968, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.035247113555669785, KL divergence=0.02002294361591339, Entropy=0.19184689223766327, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/492_Step-359533.ckpt']
Uploaded 3 files for checkpoint 492 in 0.56 seconds
saved intermediate frozen graph: current/model/model_492.pb
Best checkpoint number: 473, Last checkpoint number: 490
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'489'}
Training> Name=main_level/agent, Worker=0, Episode=3001, Total reward=168.44, Steps=359678, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3002, Total reward=53.58, Steps=359711, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3003, Total reward=360.86, Steps=359952, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3004, Total reward=84.24, Steps=360070, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3005, Total reward=14.09, Steps=360122, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3006, Total reward=465.07, Steps=360407, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3007, Total reward=162.96, Steps=360547, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3008, Total reward=215.16, Steps=360680, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3009, Total reward=420.84, Steps=360979, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3010, Total reward=56.84, Steps=361027, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3011, Total reward=240.17, Steps=361173, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3012, Total reward=102.88, Steps=361229, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3013, Total reward=460.58, Steps=361509, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3014, Total reward=87.83, Steps=361549, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3015, Total reward=26.79, Steps=361566, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3016, Total reward=25.04, Steps=361581, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3017, Total reward=402.64, Steps=361849, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3018, Total reward=436.98, Steps=362106, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3019, Total reward=26.46, Steps=362137, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3020, Total reward=97.64, Steps=362227, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3021, Total reward=529.16, Steps=362499, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3022, Total reward=56.79, Steps=362561, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3023, Total reward=16.1, Steps=362589, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3024, Total reward=502.33, Steps=362877, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3025, Total reward=300.56, Steps=363104, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3026, Total reward=189.97, Steps=363214, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3027, Total reward=218.03, Steps=363347, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3028, Total reward=435.73, Steps=363621, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3029, Total reward=108.19, Steps=363726, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3030, Total reward=78.75, Steps=363788, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3031, Total reward=453.74, Steps=364077, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3032, Total reward=463.8, Steps=364376, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3033, Total reward=99.23, Steps=364439, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3034, Total reward=277.05, Steps=364603, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3035, Total reward=471.07, Steps=364895, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3036, Total reward=91.02, Steps=364950, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3037, Total reward=459.55, Steps=365238, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3038, Total reward=279.14, Steps=365406, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3039, Total reward=451.91, Steps=365683, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3040, Total reward=400.89, Steps=365954, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3041, Total reward=86.51, Steps=366055, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3042, Total reward=418.72, Steps=366325, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3043, Total reward=85.89, Steps=366448, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3044, Total reward=65.01, Steps=366531, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3045, Total reward=229.94, Steps=366693, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3046, Total reward=452.13, Steps=367004, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3047, Total reward=90.52, Steps=367069, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3048, Total reward=167.51, Steps=367150, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3049, Total reward=474.33, Steps=367436, Training iteration=60
Training> Name=main_level/agent, Worker=0, Episode=3050, Total reward=54.27, Steps=367510, Training iteration=60
Policy training> Surrogate loss=-0.0007503407541662455, KL divergence=0.0005976802203804255, Entropy=0.19421878457069397, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.010563229210674763, KL divergence=0.006636826321482658, Entropy=0.1915414035320282, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.01959465816617012, KL divergence=0.008998798206448555, Entropy=0.19108225405216217, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023671377450227737, KL divergence=0.011618198826909065, Entropy=0.19047923386096954, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03179719299077988, KL divergence=0.012878024950623512, Entropy=0.18873968720436096, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.025178618729114532, KL divergence=0.014289481565356255, Entropy=0.18879295885562897, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0302734337747097, KL divergence=0.015025436878204346, Entropy=0.18922242522239685, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.027677658945322037, KL divergence=0.016284190118312836, Entropy=0.1888325959444046, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03104359284043312, KL divergence=0.016798820346593857, Entropy=0.1883697807788849, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03420041874051094, KL divergence=0.017579298466444016, Entropy=0.1870035082101822, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/493_Step-367510.ckpt']
Uploaded 3 files for checkpoint 493 in 0.54 seconds
saved intermediate frozen graph: current/model/model_493.pb
Best checkpoint number: 473, Last checkpoint number: 491
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'490'}
Training> Name=main_level/agent, Worker=0, Episode=3051, Total reward=34.36, Steps=367547, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3052, Total reward=59.0, Steps=367609, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3053, Total reward=33.24, Steps=367620, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3054, Total reward=152.52, Steps=367740, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3055, Total reward=113.23, Steps=367811, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3056, Total reward=20.73, Steps=367833, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3057, Total reward=446.0, Steps=368105, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3058, Total reward=298.54, Steps=368364, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3059, Total reward=78.97, Steps=368407, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3060, Total reward=96.01, Steps=368499, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3061, Total reward=48.27, Steps=368523, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3062, Total reward=287.23, Steps=368726, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3063, Total reward=116.51, Steps=368844, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3064, Total reward=125.45, Steps=368974, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3065, Total reward=0.01, Steps=368986, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3066, Total reward=48.33, Steps=369026, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3067, Total reward=400.77, Steps=369309, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3068, Total reward=90.79, Steps=369347, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3069, Total reward=318.9, Steps=369527, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3070, Total reward=90.69, Steps=369620, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3071, Total reward=523.7, Steps=369920, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3072, Total reward=335.15, Steps=370095, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3073, Total reward=488.65, Steps=370366, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3074, Total reward=275.49, Steps=370541, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3075, Total reward=489.04, Steps=370821, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3076, Total reward=57.39, Steps=370860, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3077, Total reward=178.41, Steps=371010, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3078, Total reward=363.72, Steps=371240, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3079, Total reward=81.95, Steps=371347, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3080, Total reward=102.0, Steps=371404, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3081, Total reward=70.52, Steps=371457, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3082, Total reward=230.79, Steps=371620, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3083, Total reward=16.74, Steps=371678, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3084, Total reward=82.8, Steps=371774, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3085, Total reward=94.63, Steps=371859, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3086, Total reward=181.08, Steps=371999, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3087, Total reward=419.41, Steps=372304, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3088, Total reward=515.05, Steps=372586, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3089, Total reward=350.61, Steps=372766, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3090, Total reward=475.24, Steps=373055, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3091, Total reward=117.58, Steps=373121, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3092, Total reward=262.96, Steps=373269, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3093, Total reward=119.88, Steps=373331, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3094, Total reward=70.08, Steps=373384, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3095, Total reward=37.08, Steps=373401, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3096, Total reward=362.49, Steps=373666, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3097, Total reward=415.11, Steps=373956, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3098, Total reward=25.23, Steps=373969, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3099, Total reward=90.7, Steps=374032, Training iteration=61
Training> Name=main_level/agent, Worker=0, Episode=3100, Total reward=492.17, Steps=374327, Training iteration=61
Policy training> Surrogate loss=0.003425696399062872, KL divergence=0.00033224638900719583, Entropy=0.1936129927635193, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01432447973638773, KL divergence=0.004859838169068098, Entropy=0.1930065155029297, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02146357297897339, KL divergence=0.009697200730443, Entropy=0.1915406435728073, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026066580787301064, KL divergence=0.012351838871836662, Entropy=0.1902683824300766, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02891618013381958, KL divergence=0.014248213730752468, Entropy=0.19156497716903687, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.029054297134280205, KL divergence=0.015662312507629395, Entropy=0.19117972254753113, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.031620487570762634, KL divergence=0.016510965302586555, Entropy=0.19025179743766785, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.033200543373823166, KL divergence=0.017625154927372932, Entropy=0.1898566633462906, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03333175927400589, KL divergence=0.01848561502993107, Entropy=0.18855783343315125, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.032544512301683426, KL divergence=0.019443728029727936, Entropy=0.18929363787174225, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/494_Step-374327.ckpt']
Uploaded 3 files for checkpoint 494 in 0.53 seconds
saved intermediate frozen graph: current/model/model_494.pb
Best checkpoint number: 473, Last checkpoint number: 492
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'491'}
Training> Name=main_level/agent, Worker=0, Episode=3101, Total reward=261.83, Steps=374506, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3102, Total reward=38.47, Steps=374538, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3103, Total reward=236.4, Steps=374716, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3104, Total reward=9.84, Steps=374738, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3105, Total reward=6.76, Steps=374776, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3106, Total reward=243.85, Steps=374911, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3107, Total reward=128.38, Steps=375033, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3108, Total reward=107.57, Steps=375075, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3109, Total reward=144.74, Steps=375193, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3110, Total reward=365.24, Steps=375442, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3111, Total reward=168.28, Steps=375551, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3112, Total reward=156.59, Steps=375631, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3113, Total reward=34.16, Steps=375643, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3114, Total reward=66.37, Steps=375691, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3115, Total reward=25.23, Steps=375703, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3116, Total reward=170.31, Steps=375799, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3117, Total reward=11.09, Steps=375821, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3118, Total reward=32.58, Steps=375856, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3119, Total reward=266.84, Steps=376054, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3120, Total reward=444.33, Steps=376339, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3121, Total reward=409.91, Steps=376614, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3122, Total reward=68.08, Steps=376703, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3123, Total reward=200.46, Steps=376881, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3124, Total reward=104.0, Steps=376979, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3125, Total reward=119.29, Steps=377096, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3126, Total reward=160.79, Steps=377204, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3127, Total reward=96.53, Steps=377268, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3128, Total reward=231.66, Steps=377401, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3129, Total reward=76.7, Steps=377482, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3130, Total reward=156.66, Steps=377579, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3131, Total reward=373.29, Steps=377764, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3132, Total reward=511.08, Steps=378063, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3133, Total reward=286.18, Steps=378303, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3134, Total reward=64.45, Steps=378332, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3135, Total reward=344.73, Steps=378571, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3136, Total reward=115.34, Steps=378651, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3137, Total reward=287.09, Steps=378855, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3138, Total reward=364.08, Steps=379128, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3139, Total reward=238.55, Steps=379345, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3140, Total reward=366.11, Steps=379652, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3141, Total reward=171.88, Steps=379790, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3142, Total reward=16.07, Steps=379810, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3143, Total reward=385.89, Steps=380122, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3144, Total reward=3.76, Steps=380153, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3145, Total reward=262.72, Steps=380351, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3146, Total reward=160.85, Steps=380437, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3147, Total reward=375.77, Steps=380668, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3148, Total reward=258.68, Steps=380801, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3149, Total reward=110.78, Steps=380899, Training iteration=62
Training> Name=main_level/agent, Worker=0, Episode=3150, Total reward=492.69, Steps=381192, Training iteration=62
Policy training> Surrogate loss=0.00020934318308718503, KL divergence=0.0002943657455034554, Entropy=0.18617211282253265, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.020515533164143562, KL divergence=0.0053464449010789394, Entropy=0.18481263518333435, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.024382654577493668, KL divergence=0.009407581761479378, Entropy=0.18391180038452148, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02160436287522316, KL divergence=0.011818156577646732, Entropy=0.18351861834526062, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029641591012477875, KL divergence=0.01352156512439251, Entropy=0.18103528022766113, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02885221689939499, KL divergence=0.014744862914085388, Entropy=0.18232443928718567, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.031583141535520554, KL divergence=0.01573256216943264, Entropy=0.1827242374420166, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030971825122833252, KL divergence=0.016365034505724907, Entropy=0.18212038278579712, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03246220201253891, KL divergence=0.017311247065663338, Entropy=0.1815560907125473, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.033252034336328506, KL divergence=0.018107570707798004, Entropy=0.18081551790237427, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/495_Step-381192.ckpt']
Uploaded 3 files for checkpoint 495 in 0.56 seconds
saved intermediate frozen graph: current/model/model_495.pb
Best checkpoint number: 473, Last checkpoint number: 493
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'492'}
Training> Name=main_level/agent, Worker=0, Episode=3151, Total reward=498.49, Steps=381477, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3152, Total reward=238.07, Steps=381601, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3153, Total reward=96.48, Steps=381643, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3154, Total reward=435.21, Steps=381941, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3155, Total reward=194.1, Steps=382060, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3156, Total reward=194.75, Steps=382222, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3157, Total reward=182.6, Steps=382331, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3158, Total reward=161.33, Steps=382432, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3159, Total reward=318.64, Steps=382710, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3160, Total reward=3.84, Steps=382722, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3161, Total reward=66.12, Steps=382757, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3162, Total reward=324.91, Steps=382989, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3163, Total reward=259.01, Steps=383197, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3164, Total reward=0.01, Steps=383209, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3165, Total reward=131.7, Steps=383348, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3166, Total reward=156.46, Steps=383468, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3167, Total reward=477.01, Steps=383767, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3168, Total reward=180.36, Steps=383839, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3169, Total reward=40.98, Steps=383873, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3170, Total reward=110.94, Steps=383986, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3171, Total reward=124.31, Steps=384082, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3172, Total reward=46.79, Steps=384108, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3173, Total reward=33.93, Steps=384120, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3174, Total reward=435.91, Steps=384422, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3175, Total reward=475.9, Steps=384702, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3176, Total reward=129.22, Steps=384774, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3177, Total reward=143.42, Steps=384891, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3178, Total reward=28.38, Steps=384945, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3179, Total reward=267.55, Steps=385148, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3180, Total reward=396.92, Steps=385436, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3181, Total reward=79.88, Steps=385503, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3182, Total reward=481.56, Steps=385793, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3183, Total reward=154.43, Steps=385962, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3184, Total reward=118.19, Steps=386062, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3185, Total reward=253.05, Steps=386198, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3186, Total reward=413.08, Steps=386402, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3187, Total reward=208.41, Steps=386502, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3188, Total reward=171.58, Steps=386624, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3189, Total reward=460.48, Steps=386934, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3190, Total reward=483.2, Steps=387225, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3191, Total reward=81.31, Steps=387270, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3192, Total reward=140.11, Steps=387349, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3193, Total reward=99.76, Steps=387415, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3194, Total reward=342.16, Steps=387727, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3195, Total reward=150.85, Steps=387818, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3196, Total reward=35.72, Steps=387838, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3197, Total reward=146.84, Steps=387936, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3198, Total reward=195.41, Steps=388040, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3199, Total reward=371.46, Steps=388288, Training iteration=63
Training> Name=main_level/agent, Worker=0, Episode=3200, Total reward=61.02, Steps=388350, Training iteration=63
Policy training> Surrogate loss=0.0006849868805147707, KL divergence=0.0003749876923393458, Entropy=0.184562548995018, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.012520586140453815, KL divergence=0.004669218324124813, Entropy=0.18208827078342438, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.019377578049898148, KL divergence=0.008440803736448288, Entropy=0.1822815239429474, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.03010598011314869, KL divergence=0.010597944259643555, Entropy=0.18279656767845154, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029137643054127693, KL divergence=0.012484976090490818, Entropy=0.1796545684337616, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03243054449558258, KL divergence=0.013722416013479233, Entropy=0.1782652735710144, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029810665175318718, KL divergence=0.01459293719381094, Entropy=0.1783236414194107, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.027765097096562386, KL divergence=0.01595381088554859, Entropy=0.18016059696674347, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.025589291006326675, KL divergence=0.016861265525221825, Entropy=0.18024080991744995, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.029306890442967415, KL divergence=0.017580019310116768, Entropy=0.17936168611049652, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/496_Step-388350.ckpt']
Uploaded 3 files for checkpoint 496 in 0.63 seconds
saved intermediate frozen graph: current/model/model_496.pb
Best checkpoint number: 473, Last checkpoint number: 494
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'493'}
Training> Name=main_level/agent, Worker=0, Episode=3201, Total reward=231.33, Steps=388515, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3202, Total reward=513.12, Steps=388794, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3203, Total reward=14.87, Steps=388863, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3204, Total reward=500.75, Steps=389129, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3205, Total reward=420.29, Steps=389436, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3206, Total reward=86.82, Steps=389500, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3207, Total reward=235.0, Steps=389654, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3208, Total reward=102.14, Steps=389710, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3209, Total reward=6.08, Steps=389719, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3210, Total reward=267.04, Steps=389964, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3211, Total reward=365.96, Steps=390208, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3212, Total reward=441.1, Steps=390483, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3213, Total reward=84.38, Steps=390526, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3214, Total reward=71.92, Steps=390584, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3215, Total reward=449.19, Steps=390862, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3216, Total reward=241.76, Steps=391018, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3217, Total reward=164.7, Steps=391111, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3218, Total reward=31.04, Steps=391135, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3219, Total reward=42.01, Steps=391177, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3220, Total reward=439.65, Steps=391482, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3221, Total reward=457.34, Steps=391761, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3222, Total reward=503.58, Steps=392039, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3223, Total reward=216.18, Steps=392218, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3224, Total reward=433.9, Steps=392465, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3225, Total reward=79.21, Steps=392592, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3226, Total reward=202.64, Steps=392741, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3227, Total reward=119.18, Steps=392843, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3228, Total reward=231.88, Steps=392983, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3229, Total reward=315.44, Steps=393154, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3230, Total reward=45.14, Steps=393207, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3231, Total reward=22.19, Steps=393235, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3232, Total reward=440.91, Steps=393537, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3233, Total reward=70.5, Steps=393574, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3234, Total reward=38.37, Steps=393587, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3235, Total reward=205.13, Steps=393763, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3236, Total reward=123.64, Steps=393836, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3237, Total reward=155.56, Steps=393941, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3238, Total reward=248.16, Steps=394134, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3239, Total reward=20.35, Steps=394166, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3240, Total reward=238.64, Steps=394337, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3241, Total reward=283.74, Steps=394547, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3242, Total reward=260.86, Steps=394750, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3243, Total reward=158.68, Steps=394909, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3244, Total reward=398.66, Steps=395154, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3245, Total reward=149.44, Steps=395300, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3246, Total reward=7.22, Steps=395319, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3247, Total reward=496.67, Steps=395585, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3248, Total reward=154.46, Steps=395659, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3249, Total reward=19.74, Steps=395675, Training iteration=64
Training> Name=main_level/agent, Worker=0, Episode=3250, Total reward=28.58, Steps=395727, Training iteration=64
Policy training> Surrogate loss=0.0024364516139030457, KL divergence=0.0005408095312304795, Entropy=0.18954496085643768, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.011387609876692295, KL divergence=0.005742677021771669, Entropy=0.1896289736032486, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02179088070988655, KL divergence=0.009476887062191963, Entropy=0.1874852478504181, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026800526306033134, KL divergence=0.01185707189142704, Entropy=0.18646502494812012, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02417658269405365, KL divergence=0.01318552065640688, Entropy=0.1860291063785553, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02375292219221592, KL divergence=0.014184786006808281, Entropy=0.1854935735464096, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03034672513604164, KL divergence=0.015506221912801266, Entropy=0.18602457642555237, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.029018348082900047, KL divergence=0.01627051830291748, Entropy=0.18551971018314362, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.029002439230680466, KL divergence=0.016964714974164963, Entropy=0.18561626970767975, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0332442931830883, KL divergence=0.017888998612761497, Entropy=0.1852698028087616, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/497_Step-395727.ckpt']
Uploaded 3 files for checkpoint 497 in 0.54 seconds
saved intermediate frozen graph: current/model/model_497.pb
Best checkpoint number: 473, Last checkpoint number: 495
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'494'}
Training> Name=main_level/agent, Worker=0, Episode=3251, Total reward=178.59, Steps=395825, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3252, Total reward=276.17, Steps=395965, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3253, Total reward=85.6, Steps=396008, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3254, Total reward=171.85, Steps=396105, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3255, Total reward=262.13, Steps=396257, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3256, Total reward=13.45, Steps=396275, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3257, Total reward=447.23, Steps=396571, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3258, Total reward=178.56, Steps=396717, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3259, Total reward=382.8, Steps=396953, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3260, Total reward=83.11, Steps=397005, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3261, Total reward=423.8, Steps=397283, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3262, Total reward=290.6, Steps=397496, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3263, Total reward=391.51, Steps=397793, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3264, Total reward=32.77, Steps=397863, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3265, Total reward=152.99, Steps=398000, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3266, Total reward=455.65, Steps=398264, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3267, Total reward=115.61, Steps=398371, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3268, Total reward=310.36, Steps=398574, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3269, Total reward=70.83, Steps=398628, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3270, Total reward=485.12, Steps=398916, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3271, Total reward=428.08, Steps=399210, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3272, Total reward=139.5, Steps=399287, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3273, Total reward=29.43, Steps=399297, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3274, Total reward=91.94, Steps=399352, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3275, Total reward=28.92, Steps=399367, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3276, Total reward=135.49, Steps=399443, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3277, Total reward=72.13, Steps=399498, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3278, Total reward=453.21, Steps=399788, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3279, Total reward=461.71, Steps=400082, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3280, Total reward=373.77, Steps=400338, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3281, Total reward=116.99, Steps=400432, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3282, Total reward=167.62, Steps=400585, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3283, Total reward=401.13, Steps=400876, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3284, Total reward=467.11, Steps=401164, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3285, Total reward=168.79, Steps=401288, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3286, Total reward=334.02, Steps=401477, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3287, Total reward=200.72, Steps=401602, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3288, Total reward=167.23, Steps=401682, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3289, Total reward=120.45, Steps=401784, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3290, Total reward=231.08, Steps=401945, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3291, Total reward=25.33, Steps=401982, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3292, Total reward=424.41, Steps=402281, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3293, Total reward=308.23, Steps=402459, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3294, Total reward=33.85, Steps=402472, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3295, Total reward=230.09, Steps=402588, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3296, Total reward=131.84, Steps=402660, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3297, Total reward=477.08, Steps=402955, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3298, Total reward=483.42, Steps=403240, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3299, Total reward=456.31, Steps=403555, Training iteration=65
Training> Name=main_level/agent, Worker=0, Episode=3300, Total reward=346.79, Steps=403805, Training iteration=65
Policy training> Surrogate loss=-0.00016114463505800813, KL divergence=0.0008214878034777939, Entropy=0.18146909773349762, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015638096258044243, KL divergence=0.007497974671423435, Entropy=0.17976242303848267, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020725585520267487, KL divergence=0.009611242450773716, Entropy=0.17909349501132965, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023337317630648613, KL divergence=0.01193213276565075, Entropy=0.17853672802448273, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02399507723748684, KL divergence=0.013449681922793388, Entropy=0.1813465803861618, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.027198901399970055, KL divergence=0.014446547254920006, Entropy=0.1800795942544937, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03242022916674614, KL divergence=0.01503960881382227, Entropy=0.17796997725963593, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.026663346216082573, KL divergence=0.016072789207100868, Entropy=0.17923735082149506, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03098667412996292, KL divergence=0.01672862097620964, Entropy=0.17946098744869232, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.029027776792645454, KL divergence=0.017396539449691772, Entropy=0.17835302650928497, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/498_Step-403805.ckpt']
Uploaded 3 files for checkpoint 498 in 0.47 seconds
saved intermediate frozen graph: current/model/model_498.pb
Best checkpoint number: 473, Last checkpoint number: 496
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'495'}
Training> Name=main_level/agent, Worker=0, Episode=3301, Total reward=250.81, Steps=403994, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3302, Total reward=443.35, Steps=404291, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3303, Total reward=247.17, Steps=404502, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3304, Total reward=297.97, Steps=404737, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3305, Total reward=332.28, Steps=404983, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3306, Total reward=145.47, Steps=405093, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3307, Total reward=105.32, Steps=405151, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3308, Total reward=136.29, Steps=405264, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3309, Total reward=207.44, Steps=405380, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3310, Total reward=60.14, Steps=405450, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3311, Total reward=141.69, Steps=405537, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3312, Total reward=36.61, Steps=405562, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3313, Total reward=135.42, Steps=405630, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3314, Total reward=406.09, Steps=405944, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3315, Total reward=383.49, Steps=406223, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3316, Total reward=191.93, Steps=406343, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3317, Total reward=237.2, Steps=406541, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3318, Total reward=23.32, Steps=406557, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3319, Total reward=113.66, Steps=406684, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3320, Total reward=128.51, Steps=406778, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3321, Total reward=306.45, Steps=406978, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3322, Total reward=58.24, Steps=407018, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3323, Total reward=358.0, Steps=407281, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3324, Total reward=481.6, Steps=407568, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3325, Total reward=429.15, Steps=407868, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3326, Total reward=416.1, Steps=408086, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3327, Total reward=391.43, Steps=408312, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3328, Total reward=85.07, Steps=408363, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3329, Total reward=92.24, Steps=408424, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3330, Total reward=0.01, Steps=408435, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3331, Total reward=306.61, Steps=408571, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3332, Total reward=80.7, Steps=408624, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3333, Total reward=482.58, Steps=408925, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3334, Total reward=74.96, Steps=408980, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3335, Total reward=430.79, Steps=409246, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3336, Total reward=18.18, Steps=409263, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3337, Total reward=183.29, Steps=409383, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3338, Total reward=487.85, Steps=409674, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3339, Total reward=274.13, Steps=409901, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3340, Total reward=98.88, Steps=409955, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3341, Total reward=207.51, Steps=410143, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3342, Total reward=438.51, Steps=410426, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3343, Total reward=171.1, Steps=410576, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3344, Total reward=12.75, Steps=410625, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3345, Total reward=0.02, Steps=410648, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3346, Total reward=457.05, Steps=410928, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3347, Total reward=16.83, Steps=410958, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3348, Total reward=464.0, Steps=411183, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3349, Total reward=35.93, Steps=411251, Training iteration=66
Training> Name=main_level/agent, Worker=0, Episode=3350, Total reward=411.51, Steps=411546, Training iteration=66
Policy training> Surrogate loss=0.0010573136387392879, KL divergence=0.0003147287352476269, Entropy=0.18779349327087402, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014680028893053532, KL divergence=0.006253689527511597, Entropy=0.18432655930519104, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.021181946620345116, KL divergence=0.010284718126058578, Entropy=0.18303361535072327, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02454688400030136, KL divergence=0.012339425273239613, Entropy=0.18273428082466125, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026895148679614067, KL divergence=0.013626963831484318, Entropy=0.1823987513780594, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030813833698630333, KL divergence=0.014573009684681892, Entropy=0.1820763647556305, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029240336269140244, KL divergence=0.015400992706418037, Entropy=0.1819814294576645, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030978629365563393, KL divergence=0.016144026070833206, Entropy=0.18219392001628876, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.031264159828424454, KL divergence=0.01673506572842598, Entropy=0.18100135028362274, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.032107722014188766, KL divergence=0.01755261793732643, Entropy=0.18118511140346527, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/499_Step-411546.ckpt']
Uploaded 3 files for checkpoint 499 in 0.55 seconds
saved intermediate frozen graph: current/model/model_499.pb
Best checkpoint number: 473, Last checkpoint number: 497
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'496'}
Training> Name=main_level/agent, Worker=0, Episode=3351, Total reward=113.46, Steps=411639, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3352, Total reward=290.7, Steps=411843, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3353, Total reward=395.44, Steps=412122, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3354, Total reward=376.51, Steps=412393, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3355, Total reward=26.87, Steps=412409, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3356, Total reward=413.25, Steps=412687, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3357, Total reward=461.43, Steps=412990, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3358, Total reward=184.96, Steps=413107, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3359, Total reward=432.18, Steps=413391, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3360, Total reward=344.24, Steps=413644, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3361, Total reward=87.82, Steps=413686, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3362, Total reward=130.61, Steps=413835, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3363, Total reward=106.72, Steps=413967, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3364, Total reward=542.16, Steps=414244, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3365, Total reward=427.53, Steps=414537, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3366, Total reward=206.34, Steps=414704, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3367, Total reward=199.09, Steps=414839, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3368, Total reward=233.85, Steps=414979, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3369, Total reward=446.27, Steps=415289, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3370, Total reward=484.95, Steps=415599, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3371, Total reward=374.5, Steps=415771, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3372, Total reward=500.94, Steps=416062, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3373, Total reward=237.85, Steps=416180, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3374, Total reward=172.66, Steps=416274, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3375, Total reward=207.59, Steps=416409, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3376, Total reward=469.99, Steps=416695, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3377, Total reward=463.19, Steps=416971, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3378, Total reward=404.72, Steps=417241, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3379, Total reward=99.5, Steps=417338, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3380, Total reward=56.95, Steps=417376, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3381, Total reward=433.7, Steps=417682, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3382, Total reward=235.06, Steps=417863, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3383, Total reward=164.55, Steps=418064, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3384, Total reward=9.95, Steps=418106, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3385, Total reward=372.88, Steps=418339, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3386, Total reward=201.19, Steps=418486, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3387, Total reward=253.43, Steps=418634, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3388, Total reward=296.99, Steps=418823, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3389, Total reward=363.02, Steps=419035, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3390, Total reward=289.37, Steps=419292, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3391, Total reward=492.24, Steps=419582, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3392, Total reward=365.74, Steps=419877, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3393, Total reward=78.03, Steps=419919, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3394, Total reward=53.01, Steps=419966, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3395, Total reward=251.92, Steps=420146, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3396, Total reward=320.9, Steps=420354, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3397, Total reward=428.92, Steps=420610, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3398, Total reward=455.39, Steps=420904, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3399, Total reward=57.4, Steps=420941, Training iteration=67
Training> Name=main_level/agent, Worker=0, Episode=3400, Total reward=75.4, Steps=421010, Training iteration=67
Policy training> Surrogate loss=0.00042276622843928635, KL divergence=0.0008272607228718698, Entropy=0.18102537095546722, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015350684523582458, KL divergence=0.0058440109714865685, Entropy=0.17885470390319824, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.018856456503272057, KL divergence=0.009159708395600319, Entropy=0.17808133363723755, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025659915059804916, KL divergence=0.011083869263529778, Entropy=0.17775589227676392, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.022792574018239975, KL divergence=0.011983305215835571, Entropy=0.17733366787433624, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.024025561287999153, KL divergence=0.012964232824742794, Entropy=0.17680399119853973, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02958926185965538, KL divergence=0.013911232352256775, Entropy=0.17703106999397278, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030267387628555298, KL divergence=0.014747414737939835, Entropy=0.17724327743053436, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02965712547302246, KL divergence=0.015610373578965664, Entropy=0.17653970420360565, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.030582686886191368, KL divergence=0.016350168734788895, Entropy=0.17700156569480896, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/500_Step-421010.ckpt']
Uploaded 3 files for checkpoint 500 in 0.66 seconds
saved intermediate frozen graph: current/model/model_500.pb
Best checkpoint number: 473, Last checkpoint number: 498
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'497'}
Training> Name=main_level/agent, Worker=0, Episode=3401, Total reward=249.74, Steps=421218, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3402, Total reward=413.8, Steps=421506, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3403, Total reward=20.56, Steps=421555, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3404, Total reward=7.21, Steps=421589, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3405, Total reward=105.74, Steps=421663, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3406, Total reward=236.08, Steps=421842, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3407, Total reward=466.03, Steps=422107, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3408, Total reward=116.11, Steps=422192, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3409, Total reward=114.58, Steps=422282, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3410, Total reward=171.31, Steps=422426, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3411, Total reward=111.46, Steps=422516, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3412, Total reward=117.63, Steps=422575, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3413, Total reward=134.6, Steps=422639, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3414, Total reward=289.08, Steps=422841, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3415, Total reward=124.36, Steps=422933, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3416, Total reward=35.44, Steps=422975, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3417, Total reward=319.73, Steps=423254, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3418, Total reward=142.75, Steps=423328, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3419, Total reward=515.52, Steps=423625, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3420, Total reward=267.64, Steps=423845, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3421, Total reward=497.05, Steps=424138, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3422, Total reward=186.03, Steps=424309, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3423, Total reward=458.89, Steps=424594, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3424, Total reward=105.37, Steps=424691, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3425, Total reward=6.78, Steps=424718, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3426, Total reward=353.05, Steps=425050, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3427, Total reward=197.57, Steps=425180, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3428, Total reward=517.56, Steps=425457, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3429, Total reward=29.8, Steps=425499, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3430, Total reward=412.79, Steps=425757, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3431, Total reward=102.74, Steps=425828, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3432, Total reward=365.82, Steps=426054, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3433, Total reward=351.86, Steps=426238, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3434, Total reward=386.42, Steps=426530, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3435, Total reward=101.22, Steps=426601, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3436, Total reward=292.04, Steps=426814, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3437, Total reward=95.34, Steps=426862, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3438, Total reward=382.56, Steps=427123, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3439, Total reward=369.21, Steps=427381, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3440, Total reward=276.89, Steps=427613, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3441, Total reward=517.7, Steps=427894, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3442, Total reward=164.36, Steps=428024, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3443, Total reward=38.86, Steps=428075, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3444, Total reward=434.86, Steps=428358, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3445, Total reward=172.15, Steps=428478, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3446, Total reward=385.5, Steps=428765, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3447, Total reward=159.89, Steps=428854, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3448, Total reward=309.05, Steps=429019, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3449, Total reward=31.3, Steps=429046, Training iteration=68
Training> Name=main_level/agent, Worker=0, Episode=3450, Total reward=11.04, Steps=429076, Training iteration=68
Policy training> Surrogate loss=-0.0007058938499540091, KL divergence=0.0005754304002039135, Entropy=0.18701349198818207, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014164324849843979, KL divergence=0.0067490083165466785, Entropy=0.18373864889144897, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.025486983358860016, KL divergence=0.010375697165727615, Entropy=0.1834317147731781, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025053083896636963, KL divergence=0.012026065960526466, Entropy=0.18231774866580963, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02966964617371559, KL divergence=0.01322579849511385, Entropy=0.18263070285320282, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.025922782719135284, KL divergence=0.014870885759592056, Entropy=0.18409742414951324, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029501207172870636, KL divergence=0.015568842180073261, Entropy=0.18184345960617065, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03099062666296959, KL divergence=0.01669173501431942, Entropy=0.18377012014389038, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02683131955564022, KL divergence=0.01709630899131298, Entropy=0.18117056787014008, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03212053328752518, KL divergence=0.017636394128203392, Entropy=0.18115799129009247, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/501_Step-429076.ckpt']
Uploaded 3 files for checkpoint 501 in 0.57 seconds
saved intermediate frozen graph: current/model/model_501.pb
Best checkpoint number: 473, Last checkpoint number: 499
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'498'}
Training> Name=main_level/agent, Worker=0, Episode=3451, Total reward=127.56, Steps=429156, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3452, Total reward=130.14, Steps=429235, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3453, Total reward=33.86, Steps=429247, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3454, Total reward=248.91, Steps=429382, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3455, Total reward=144.9, Steps=429478, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3456, Total reward=344.87, Steps=429775, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3457, Total reward=275.29, Steps=430003, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3458, Total reward=50.66, Steps=430030, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3459, Total reward=28.95, Steps=430055, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3460, Total reward=462.51, Steps=430365, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3461, Total reward=162.98, Steps=430502, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3462, Total reward=47.46, Steps=430557, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3463, Total reward=26.47, Steps=430601, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3464, Total reward=176.73, Steps=430731, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3465, Total reward=462.05, Steps=431020, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3466, Total reward=457.12, Steps=431252, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3467, Total reward=226.71, Steps=431387, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3468, Total reward=407.3, Steps=431675, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3469, Total reward=83.78, Steps=431728, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3470, Total reward=35.82, Steps=431784, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3471, Total reward=106.56, Steps=431849, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3472, Total reward=383.8, Steps=432181, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3473, Total reward=101.45, Steps=432246, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3474, Total reward=281.86, Steps=432391, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3475, Total reward=261.0, Steps=432556, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3476, Total reward=498.49, Steps=432852, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3477, Total reward=350.07, Steps=433124, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3478, Total reward=138.83, Steps=433249, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3479, Total reward=121.86, Steps=433325, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3480, Total reward=377.62, Steps=433621, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3481, Total reward=473.18, Steps=433909, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3482, Total reward=153.91, Steps=434043, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3483, Total reward=167.44, Steps=434191, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3484, Total reward=165.48, Steps=434347, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3485, Total reward=461.12, Steps=434649, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3486, Total reward=400.27, Steps=434936, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3487, Total reward=441.65, Steps=435206, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3488, Total reward=115.43, Steps=435273, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3489, Total reward=83.32, Steps=435346, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3490, Total reward=438.29, Steps=435646, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3491, Total reward=466.72, Steps=435937, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3492, Total reward=272.42, Steps=436053, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3493, Total reward=73.78, Steps=436093, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3494, Total reward=266.14, Steps=436245, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3495, Total reward=239.86, Steps=436413, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3496, Total reward=458.03, Steps=436685, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3497, Total reward=345.2, Steps=436966, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3498, Total reward=81.06, Steps=437004, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3499, Total reward=37.56, Steps=437034, Training iteration=69
Training> Name=main_level/agent, Worker=0, Episode=3500, Total reward=340.44, Steps=437261, Training iteration=69
Policy training> Surrogate loss=-0.0011867672437801957, KL divergence=0.0006157562020234764, Entropy=0.18750861287117004, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.012780444696545601, KL divergence=0.006014258600771427, Entropy=0.18416188657283783, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.024139540269970894, KL divergence=0.009105986915528774, Entropy=0.18415872752666473, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.019963497295975685, KL divergence=0.011386889033019543, Entropy=0.18166032433509827, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02302800491452217, KL divergence=0.012902241200208664, Entropy=0.18393608927726746, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026676597073674202, KL divergence=0.013750567100942135, Entropy=0.18302297592163086, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.028730465099215508, KL divergence=0.014222352765500546, Entropy=0.18173032999038696, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.035650886595249176, KL divergence=0.015401441603899002, Entropy=0.1809515357017517, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.032691869884729385, KL divergence=0.016603998839855194, Entropy=0.18339158594608307, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.029244037345051765, KL divergence=0.017127519473433495, Entropy=0.18228136003017426, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/502_Step-437261.ckpt']
Uploaded 3 files for checkpoint 502 in 0.47 seconds
saved intermediate frozen graph: current/model/model_502.pb
Best checkpoint number: 473, Last checkpoint number: 500
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'499'}
Training> Name=main_level/agent, Worker=0, Episode=3501, Total reward=69.11, Steps=437347, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3502, Total reward=26.87, Steps=437388, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3503, Total reward=22.11, Steps=437439, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3504, Total reward=6.44, Steps=437470, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3505, Total reward=441.16, Steps=437713, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3506, Total reward=102.0, Steps=437797, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3507, Total reward=203.13, Steps=437918, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3508, Total reward=501.82, Steps=438207, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3509, Total reward=438.42, Steps=438497, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3510, Total reward=0.02, Steps=438513, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3511, Total reward=158.87, Steps=438612, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3512, Total reward=488.55, Steps=438905, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3513, Total reward=81.72, Steps=438948, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3514, Total reward=439.16, Steps=439248, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3515, Total reward=254.35, Steps=439369, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3516, Total reward=488.38, Steps=439652, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3517, Total reward=483.2, Steps=439939, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3518, Total reward=172.12, Steps=440046, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3519, Total reward=139.47, Steps=440161, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3520, Total reward=490.02, Steps=440448, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3521, Total reward=314.6, Steps=440701, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3522, Total reward=10.03, Steps=440721, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3523, Total reward=8.25, Steps=440763, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3524, Total reward=182.08, Steps=440899, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3525, Total reward=207.62, Steps=441072, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3526, Total reward=10.52, Steps=441085, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3527, Total reward=491.5, Steps=441374, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3528, Total reward=260.49, Steps=441510, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3529, Total reward=204.65, Steps=441632, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3530, Total reward=449.46, Steps=441912, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3531, Total reward=134.92, Steps=441986, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3532, Total reward=445.75, Steps=442287, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3533, Total reward=261.22, Steps=442461, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3534, Total reward=114.15, Steps=442543, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3535, Total reward=197.31, Steps=442667, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3536, Total reward=303.48, Steps=442854, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3537, Total reward=56.75, Steps=442892, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3538, Total reward=429.46, Steps=443210, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3539, Total reward=476.41, Steps=443506, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3540, Total reward=162.02, Steps=443648, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3541, Total reward=97.33, Steps=443753, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3542, Total reward=273.34, Steps=443955, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3543, Total reward=67.59, Steps=444079, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3544, Total reward=3.79, Steps=444091, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3545, Total reward=121.32, Steps=444183, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3546, Total reward=247.46, Steps=444331, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3547, Total reward=97.36, Steps=444404, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3548, Total reward=127.56, Steps=444492, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3549, Total reward=73.88, Steps=444573, Training iteration=70
Training> Name=main_level/agent, Worker=0, Episode=3550, Total reward=466.32, Steps=444872, Training iteration=70
Policy training> Surrogate loss=0.0037689802702516317, KL divergence=0.00025187767460010946, Entropy=0.18609018623828888, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015194850042462349, KL divergence=0.004461175296455622, Entropy=0.1850365549325943, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.021661367267370224, KL divergence=0.008947658352553844, Entropy=0.18335261940956116, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.022360539063811302, KL divergence=0.011643759906291962, Entropy=0.1836237907409668, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.027270006015896797, KL divergence=0.013628029264509678, Entropy=0.1837705671787262, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02223176695406437, KL divergence=0.014585928060114384, Entropy=0.1828262358903885, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02802952751517296, KL divergence=0.015716249123215675, Entropy=0.1819399893283844, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.033281419426202774, KL divergence=0.016709918156266212, Entropy=0.18131788074970245, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02567974664270878, KL divergence=0.01723102666437626, Entropy=0.18148890137672424, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03399498015642166, KL divergence=0.01784505322575569, Entropy=0.18055115640163422, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/503_Step-444872.ckpt']
Uploaded 3 files for checkpoint 503 in 0.57 seconds
saved intermediate frozen graph: current/model/model_503.pb
Best checkpoint number: 473, Last checkpoint number: 501
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'500'}
Training> Name=main_level/agent, Worker=0, Episode=3551, Total reward=388.55, Steps=445180, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3552, Total reward=365.93, Steps=445370, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3553, Total reward=459.68, Steps=445665, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3554, Total reward=425.2, Steps=445962, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3555, Total reward=254.74, Steps=446124, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3556, Total reward=142.46, Steps=446207, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3557, Total reward=20.33, Steps=446261, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3558, Total reward=40.85, Steps=446276, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3559, Total reward=499.67, Steps=446568, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3560, Total reward=222.21, Steps=446752, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3561, Total reward=91.68, Steps=446827, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3562, Total reward=477.25, Steps=447105, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3563, Total reward=6.77, Steps=447138, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3564, Total reward=217.32, Steps=447315, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3565, Total reward=331.1, Steps=447536, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3566, Total reward=465.86, Steps=447817, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3567, Total reward=139.53, Steps=447949, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3568, Total reward=477.66, Steps=448238, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3569, Total reward=392.68, Steps=448461, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3570, Total reward=20.58, Steps=448507, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3571, Total reward=91.03, Steps=448558, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3572, Total reward=470.9, Steps=448851, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3573, Total reward=359.38, Steps=449032, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3574, Total reward=438.54, Steps=449324, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3575, Total reward=365.43, Steps=449550, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3576, Total reward=9.56, Steps=449561, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3577, Total reward=26.57, Steps=449601, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3578, Total reward=197.12, Steps=449709, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3579, Total reward=57.57, Steps=449831, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3580, Total reward=483.68, Steps=450120, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3581, Total reward=262.36, Steps=450317, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3582, Total reward=311.17, Steps=450533, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3583, Total reward=194.37, Steps=450698, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3584, Total reward=12.42, Steps=450732, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3585, Total reward=424.32, Steps=450986, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3586, Total reward=355.6, Steps=451193, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3587, Total reward=135.14, Steps=451315, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3588, Total reward=88.26, Steps=451356, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3589, Total reward=38.27, Steps=451397, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3590, Total reward=249.34, Steps=451565, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3591, Total reward=156.12, Steps=451654, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3592, Total reward=115.05, Steps=451729, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3593, Total reward=256.3, Steps=451869, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3594, Total reward=78.17, Steps=451918, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3595, Total reward=413.88, Steps=452187, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3596, Total reward=374.75, Steps=452485, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3597, Total reward=31.31, Steps=452526, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3598, Total reward=197.9, Steps=452629, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3599, Total reward=95.52, Steps=452700, Training iteration=71
Training> Name=main_level/agent, Worker=0, Episode=3600, Total reward=93.09, Steps=452755, Training iteration=71
Policy training> Surrogate loss=2.081592901959084e-05, KL divergence=0.0007287379703484476, Entropy=0.19007374346256256, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.012569250538945198, KL divergence=0.006132244132459164, Entropy=0.1874956637620926, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020245064049959183, KL divergence=0.009628885425627232, Entropy=0.186713308095932, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02516830712556839, KL divergence=0.011743918992578983, Entropy=0.1858903020620346, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.024426864460110664, KL divergence=0.013111102394759655, Entropy=0.1856217235326767, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030964920297265053, KL divergence=0.014091518707573414, Entropy=0.18466104567050934, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029754476621747017, KL divergence=0.014970937743782997, Entropy=0.18593338131904602, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03313882276415825, KL divergence=0.015944061800837517, Entropy=0.186309352517128, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.034475814551115036, KL divergence=0.016753381118178368, Entropy=0.18481388688087463, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.030164847150444984, KL divergence=0.01778985746204853, Entropy=0.18472810089588165, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/504_Step-452755.ckpt']
Uploaded 3 files for checkpoint 504 in 0.51 seconds
saved intermediate frozen graph: current/model/model_504.pb
Best checkpoint number: 473, Last checkpoint number: 502
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'501'}
Training> Name=main_level/agent, Worker=0, Episode=3601, Total reward=86.95, Steps=452796, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3602, Total reward=425.03, Steps=453075, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3603, Total reward=127.68, Steps=453254, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3604, Total reward=105.81, Steps=453364, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3605, Total reward=517.45, Steps=453646, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3606, Total reward=504.69, Steps=453912, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3607, Total reward=425.38, Steps=454172, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3608, Total reward=448.69, Steps=454447, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3609, Total reward=252.56, Steps=454632, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3610, Total reward=121.57, Steps=454742, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3611, Total reward=369.45, Steps=454994, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3612, Total reward=5.36, Steps=455006, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3613, Total reward=210.49, Steps=455133, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3614, Total reward=36.48, Steps=455146, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3615, Total reward=156.86, Steps=455231, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3616, Total reward=462.47, Steps=455521, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3617, Total reward=523.97, Steps=455805, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3618, Total reward=467.76, Steps=456094, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3619, Total reward=466.09, Steps=456377, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3620, Total reward=81.78, Steps=456430, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3621, Total reward=97.36, Steps=456509, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3622, Total reward=61.22, Steps=456567, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3623, Total reward=292.39, Steps=456765, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3624, Total reward=468.06, Steps=457050, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3625, Total reward=83.55, Steps=457131, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3626, Total reward=209.09, Steps=457286, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3627, Total reward=219.83, Steps=457445, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3628, Total reward=178.76, Steps=457514, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3629, Total reward=50.08, Steps=457568, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3630, Total reward=32.56, Steps=457637, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3631, Total reward=103.26, Steps=457702, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3632, Total reward=190.64, Steps=457809, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3633, Total reward=111.16, Steps=457856, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3634, Total reward=62.41, Steps=457904, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3635, Total reward=238.89, Steps=458080, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3636, Total reward=458.69, Steps=458387, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3637, Total reward=96.46, Steps=458454, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3638, Total reward=487.03, Steps=458730, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3639, Total reward=25.3, Steps=458745, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3640, Total reward=454.29, Steps=459032, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3641, Total reward=206.23, Steps=459161, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3642, Total reward=454.69, Steps=459431, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3643, Total reward=30.33, Steps=459493, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3644, Total reward=282.16, Steps=459683, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3645, Total reward=437.03, Steps=459934, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3646, Total reward=441.97, Steps=460227, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3647, Total reward=515.17, Steps=460520, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3648, Total reward=138.64, Steps=460609, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3649, Total reward=78.17, Steps=460666, Training iteration=72
Training> Name=main_level/agent, Worker=0, Episode=3650, Total reward=447.58, Steps=460965, Training iteration=72
Policy training> Surrogate loss=0.00020539900287985802, KL divergence=0.00039480370469391346, Entropy=0.1826249659061432, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014718262478709221, KL divergence=0.005809752270579338, Entropy=0.18269892036914825, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.022284958511590958, KL divergence=0.008780688047409058, Entropy=0.1806492954492569, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02483988367021084, KL divergence=0.01068425178527832, Entropy=0.1802368462085724, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.027005385607481003, KL divergence=0.012127131223678589, Entropy=0.1797904074192047, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.028948605060577393, KL divergence=0.0133864376693964, Entropy=0.17979860305786133, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029977038502693176, KL divergence=0.014440078288316727, Entropy=0.17943549156188965, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.031245123594999313, KL divergence=0.015264442190527916, Entropy=0.17946109175682068, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03338004648685455, KL divergence=0.01608540676534176, Entropy=0.17929747700691223, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03288383036851883, KL divergence=0.016763972118496895, Entropy=0.1790321320295334, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/505_Step-460965.ckpt']
Uploaded 3 files for checkpoint 505 in 0.63 seconds
saved intermediate frozen graph: current/model/model_505.pb
Best checkpoint number: 473, Last checkpoint number: 503
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'502'}
Training> Name=main_level/agent, Worker=0, Episode=3651, Total reward=131.22, Steps=461036, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3652, Total reward=473.87, Steps=461316, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3653, Total reward=86.05, Steps=461355, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3654, Total reward=123.67, Steps=461434, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3655, Total reward=460.78, Steps=461737, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3656, Total reward=28.53, Steps=461759, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3657, Total reward=305.2, Steps=462011, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3658, Total reward=190.21, Steps=462107, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3659, Total reward=261.88, Steps=462311, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3660, Total reward=308.19, Steps=462536, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3661, Total reward=213.29, Steps=462723, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3662, Total reward=61.04, Steps=462774, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3663, Total reward=500.61, Steps=463063, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3664, Total reward=105.63, Steps=463173, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3665, Total reward=241.79, Steps=463339, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3666, Total reward=3.5, Steps=463350, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3667, Total reward=489.37, Steps=463641, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3668, Total reward=80.49, Steps=463679, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3669, Total reward=73.96, Steps=463793, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3670, Total reward=30.73, Steps=463838, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3671, Total reward=89.92, Steps=463908, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3672, Total reward=268.06, Steps=464043, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3673, Total reward=35.31, Steps=464067, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3674, Total reward=210.58, Steps=464183, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3675, Total reward=123.55, Steps=464250, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3676, Total reward=419.32, Steps=464571, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3677, Total reward=405.73, Steps=464882, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3678, Total reward=175.72, Steps=465017, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3679, Total reward=328.55, Steps=465242, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3680, Total reward=503.03, Steps=465522, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3681, Total reward=207.57, Steps=465661, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3682, Total reward=218.56, Steps=465825, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3683, Total reward=439.69, Steps=466100, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3684, Total reward=137.14, Steps=466235, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3685, Total reward=385.63, Steps=466511, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3686, Total reward=140.84, Steps=466612, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3687, Total reward=419.52, Steps=466928, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3688, Total reward=457.08, Steps=467208, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3689, Total reward=413.9, Steps=467488, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3690, Total reward=390.17, Steps=467706, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3691, Total reward=395.91, Steps=468017, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3692, Total reward=469.04, Steps=468311, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3693, Total reward=103.49, Steps=468352, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3694, Total reward=35.35, Steps=468365, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3695, Total reward=265.91, Steps=468488, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3696, Total reward=357.69, Steps=468777, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3697, Total reward=331.03, Steps=469006, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3698, Total reward=508.62, Steps=469293, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3699, Total reward=427.6, Steps=469602, Training iteration=73
Training> Name=main_level/agent, Worker=0, Episode=3700, Total reward=204.61, Steps=469747, Training iteration=73
Policy training> Surrogate loss=0.00039964847383089364, KL divergence=0.0005380260408855975, Entropy=0.18864969909191132, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014605067670345306, KL divergence=0.006039428524672985, Entropy=0.18618926405906677, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02251625806093216, KL divergence=0.0091366246342659, Entropy=0.18495118618011475, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023222625255584717, KL divergence=0.010751251131296158, Entropy=0.18409468233585358, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02486586384475231, KL divergence=0.012118709273636341, Entropy=0.18406276404857635, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026597443968057632, KL divergence=0.013271654024720192, Entropy=0.1832674741744995, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.028974052518606186, KL divergence=0.014293752610683441, Entropy=0.18305425345897675, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03031541407108307, KL divergence=0.015337667427957058, Entropy=0.1822403222322464, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.030161965638399124, KL divergence=0.016150616109371185, Entropy=0.1822979897260666, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03292851895093918, KL divergence=0.016728051006793976, Entropy=0.18198315799236298, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/506_Step-469747.ckpt']
Uploaded 3 files for checkpoint 506 in 0.60 seconds
saved intermediate frozen graph: current/model/model_506.pb
Best checkpoint number: 473, Last checkpoint number: 504
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'503'}
Training> Name=main_level/agent, Worker=0, Episode=3701, Total reward=124.23, Steps=469959, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3702, Total reward=168.46, Steps=470090, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3703, Total reward=218.49, Steps=470307, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3704, Total reward=180.93, Steps=470438, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3705, Total reward=482.99, Steps=470735, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3706, Total reward=132.23, Steps=470831, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3707, Total reward=442.55, Steps=471135, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3708, Total reward=252.17, Steps=471259, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3709, Total reward=34.65, Steps=471300, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3710, Total reward=209.54, Steps=471462, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3711, Total reward=494.25, Steps=471764, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3712, Total reward=460.47, Steps=472060, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3713, Total reward=68.98, Steps=472100, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3714, Total reward=64.32, Steps=472146, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3715, Total reward=28.83, Steps=472161, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3716, Total reward=207.23, Steps=472292, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3717, Total reward=383.67, Steps=472611, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3718, Total reward=252.17, Steps=472805, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3719, Total reward=128.14, Steps=472905, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3720, Total reward=384.29, Steps=473196, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3721, Total reward=198.19, Steps=473393, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3722, Total reward=35.41, Steps=473426, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3723, Total reward=30.28, Steps=473476, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3724, Total reward=6.26, Steps=473498, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3725, Total reward=534.66, Steps=473776, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3726, Total reward=461.13, Steps=474063, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3727, Total reward=341.95, Steps=474254, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3728, Total reward=368.29, Steps=474448, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3729, Total reward=96.54, Steps=474538, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3730, Total reward=92.66, Steps=474583, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3731, Total reward=508.1, Steps=474861, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3732, Total reward=11.77, Steps=474885, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3733, Total reward=313.8, Steps=475061, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3734, Total reward=480.12, Steps=475333, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3735, Total reward=419.32, Steps=475613, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3736, Total reward=18.58, Steps=475631, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3737, Total reward=127.47, Steps=475738, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3738, Total reward=481.4, Steps=476024, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3739, Total reward=246.47, Steps=476246, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3740, Total reward=388.0, Steps=476476, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3741, Total reward=442.51, Steps=476759, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3742, Total reward=441.51, Steps=477026, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3743, Total reward=202.02, Steps=477170, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3744, Total reward=138.38, Steps=477324, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3745, Total reward=424.73, Steps=477616, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3746, Total reward=237.22, Steps=477767, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3747, Total reward=472.63, Steps=478058, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3748, Total reward=359.13, Steps=478255, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3749, Total reward=86.05, Steps=478322, Training iteration=74
Training> Name=main_level/agent, Worker=0, Episode=3750, Total reward=6.95, Steps=478354, Training iteration=74
Policy training> Surrogate loss=0.004518729168921709, KL divergence=0.0007294795359484851, Entropy=0.18318326771259308, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.016070012003183365, KL divergence=0.006402264349162579, Entropy=0.18102487921714783, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020728403702378273, KL divergence=0.009215854108333588, Entropy=0.18032395839691162, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.024516237899661064, KL divergence=0.010890765115618706, Entropy=0.18064430356025696, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029098765924572945, KL divergence=0.012464058585464954, Entropy=0.180860236287117, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.024426855146884918, KL divergence=0.013643832877278328, Entropy=0.1820620894432068, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.027765288949012756, KL divergence=0.014829354360699654, Entropy=0.17946650087833405, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02818329446017742, KL divergence=0.015847481787204742, Entropy=0.17992356419563293, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.032558172941207886, KL divergence=0.016486093401908875, Entropy=0.18010085821151733, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03069077618420124, KL divergence=0.0170269962400198, Entropy=0.17981527745723724, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/507_Step-478354.ckpt']
Uploaded 3 files for checkpoint 507 in 0.50 seconds
saved intermediate frozen graph: current/model/model_507.pb
Best checkpoint number: 473, Last checkpoint number: 505
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'504'}
Training> Name=main_level/agent, Worker=0, Episode=3751, Total reward=481.52, Steps=478649, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3752, Total reward=287.24, Steps=478830, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3753, Total reward=99.83, Steps=478890, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3754, Total reward=175.76, Steps=478990, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3755, Total reward=42.52, Steps=479007, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3756, Total reward=62.65, Steps=479070, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3757, Total reward=362.18, Steps=479333, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3758, Total reward=177.8, Steps=479413, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3759, Total reward=127.95, Steps=479490, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3760, Total reward=22.97, Steps=479513, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3761, Total reward=411.52, Steps=479789, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3762, Total reward=12.84, Steps=479805, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3763, Total reward=159.59, Steps=479955, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3764, Total reward=25.13, Steps=480027, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3765, Total reward=216.55, Steps=480176, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3766, Total reward=434.05, Steps=480474, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3767, Total reward=400.85, Steps=480755, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3768, Total reward=155.19, Steps=480826, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3769, Total reward=48.17, Steps=480867, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3770, Total reward=330.45, Steps=481093, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3771, Total reward=370.47, Steps=481299, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3772, Total reward=124.06, Steps=481376, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3773, Total reward=37.88, Steps=481388, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3774, Total reward=74.81, Steps=481436, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3775, Total reward=39.28, Steps=481472, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3776, Total reward=503.94, Steps=481765, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3777, Total reward=426.59, Steps=482064, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3778, Total reward=379.54, Steps=482337, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3779, Total reward=112.14, Steps=482404, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3780, Total reward=485.57, Steps=482694, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3781, Total reward=83.73, Steps=482731, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3782, Total reward=59.82, Steps=482761, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3783, Total reward=202.71, Steps=482946, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3784, Total reward=482.85, Steps=483242, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3785, Total reward=463.0, Steps=483527, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3786, Total reward=469.57, Steps=483820, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3787, Total reward=214.39, Steps=483952, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3788, Total reward=225.56, Steps=484063, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3789, Total reward=475.14, Steps=484355, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3790, Total reward=82.43, Steps=484402, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3791, Total reward=155.54, Steps=484484, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3792, Total reward=81.71, Steps=484540, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3793, Total reward=67.56, Steps=484583, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3794, Total reward=251.28, Steps=484736, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3795, Total reward=39.58, Steps=484754, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3796, Total reward=444.59, Steps=485042, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3797, Total reward=176.74, Steps=485146, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3798, Total reward=458.22, Steps=485456, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3799, Total reward=136.52, Steps=485566, Training iteration=75
Training> Name=main_level/agent, Worker=0, Episode=3800, Total reward=314.67, Steps=485784, Training iteration=75
Policy training> Surrogate loss=-0.0019705367740243673, KL divergence=0.0003454975376371294, Entropy=0.18854165077209473, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.018945913761854172, KL divergence=0.005298578180372715, Entropy=0.18681158125400543, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.01950673945248127, KL divergence=0.010665380395948887, Entropy=0.1851474493741989, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02617357298731804, KL divergence=0.013150585815310478, Entropy=0.1847621351480484, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029034141451120377, KL divergence=0.014477157965302467, Entropy=0.18462787568569183, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.025673579424619675, KL divergence=0.015518327243626118, Entropy=0.1830827295780182, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.028892094269394875, KL divergence=0.016773102805018425, Entropy=0.18361292779445648, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03213755041360855, KL divergence=0.01743718981742859, Entropy=0.18333439528942108, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.0340312160551548, KL divergence=0.01825096271932125, Entropy=0.18268096446990967, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.033881887793540955, KL divergence=0.018992315977811813, Entropy=0.18223348259925842, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/508_Step-485784.ckpt']
Uploaded 3 files for checkpoint 508 in 0.54 seconds
saved intermediate frozen graph: current/model/model_508.pb
Best checkpoint number: 473, Last checkpoint number: 506
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'505'}
Training> Name=main_level/agent, Worker=0, Episode=3801, Total reward=337.98, Steps=486019, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3802, Total reward=208.97, Steps=486211, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3803, Total reward=177.81, Steps=486358, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3804, Total reward=339.93, Steps=486612, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3805, Total reward=146.93, Steps=486742, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3806, Total reward=136.73, Steps=486855, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3807, Total reward=240.66, Steps=486990, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3808, Total reward=403.47, Steps=487273, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3809, Total reward=71.85, Steps=487339, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3810, Total reward=121.71, Steps=487422, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3811, Total reward=324.27, Steps=487569, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3812, Total reward=11.34, Steps=487581, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3813, Total reward=92.67, Steps=487624, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3814, Total reward=316.32, Steps=487792, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3815, Total reward=17.15, Steps=487805, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3816, Total reward=434.35, Steps=488067, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3817, Total reward=438.35, Steps=488351, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3818, Total reward=159.49, Steps=488447, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3819, Total reward=459.83, Steps=488741, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3820, Total reward=92.88, Steps=488808, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3821, Total reward=89.42, Steps=488861, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3822, Total reward=473.37, Steps=489137, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3823, Total reward=483.7, Steps=489441, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3824, Total reward=106.29, Steps=489576, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3825, Total reward=154.09, Steps=489696, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3826, Total reward=451.55, Steps=489994, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3827, Total reward=260.18, Steps=490138, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3828, Total reward=514.8, Steps=490418, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3829, Total reward=186.78, Steps=490528, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3830, Total reward=111.76, Steps=490602, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3831, Total reward=64.86, Steps=490642, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3832, Total reward=369.67, Steps=490870, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3833, Total reward=493.72, Steps=491144, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3834, Total reward=445.32, Steps=491441, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3835, Total reward=206.14, Steps=491569, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3836, Total reward=406.36, Steps=491847, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3837, Total reward=299.17, Steps=492113, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3838, Total reward=155.61, Steps=492204, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3839, Total reward=30.95, Steps=492262, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3840, Total reward=89.75, Steps=492335, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3841, Total reward=271.37, Steps=492522, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3842, Total reward=454.72, Steps=492803, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3843, Total reward=444.08, Steps=493118, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3844, Total reward=130.95, Steps=493251, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3845, Total reward=141.89, Steps=493420, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3846, Total reward=83.47, Steps=493479, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3847, Total reward=291.27, Steps=493650, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3848, Total reward=307.05, Steps=493814, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3849, Total reward=78.02, Steps=493899, Training iteration=76
Training> Name=main_level/agent, Worker=0, Episode=3850, Total reward=266.78, Steps=494072, Training iteration=76
Policy training> Surrogate loss=0.0012426369357854128, KL divergence=0.0014330859994515777, Entropy=0.1827782690525055, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01489802822470665, KL divergence=0.00683311652392149, Entropy=0.18069444596767426, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02064075693488121, KL divergence=0.009409226477146149, Entropy=0.18163874745368958, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02574342116713524, KL divergence=0.011485569179058075, Entropy=0.1799696385860443, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.0257085133343935, KL divergence=0.012546507641673088, Entropy=0.17951086163520813, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02685931883752346, KL divergence=0.013798495754599571, Entropy=0.180036723613739, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.030238525941967964, KL divergence=0.014843158423900604, Entropy=0.17995308339595795, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02897953800857067, KL divergence=0.015673227608203888, Entropy=0.17934681475162506, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03183307126164436, KL divergence=0.01659366488456726, Entropy=0.17911389470100403, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0315299890935421, KL divergence=0.016966287046670914, Entropy=0.17890316247940063, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/509_Step-494072.ckpt']
Uploaded 3 files for checkpoint 509 in 0.54 seconds
saved intermediate frozen graph: current/model/model_509.pb
Best checkpoint number: 473, Last checkpoint number: 507
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'506'}
Training> Name=main_level/agent, Worker=0, Episode=3851, Total reward=130.87, Steps=494164, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3852, Total reward=132.84, Steps=494242, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3853, Total reward=33.74, Steps=494254, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3854, Total reward=82.18, Steps=494307, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3855, Total reward=39.75, Steps=494359, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3856, Total reward=504.16, Steps=494625, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3857, Total reward=352.54, Steps=494918, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3858, Total reward=13.5, Steps=494932, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3859, Total reward=451.12, Steps=495230, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3860, Total reward=268.15, Steps=495415, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3861, Total reward=497.8, Steps=495717, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3862, Total reward=173.13, Steps=495830, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3863, Total reward=136.75, Steps=495939, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3864, Total reward=228.34, Steps=496120, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3865, Total reward=252.4, Steps=496304, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3866, Total reward=347.91, Steps=496513, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3867, Total reward=138.6, Steps=496609, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3868, Total reward=526.3, Steps=496885, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3869, Total reward=42.38, Steps=496926, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3870, Total reward=104.23, Steps=497008, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3871, Total reward=148.94, Steps=497101, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3872, Total reward=207.49, Steps=497245, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3873, Total reward=312.06, Steps=497447, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3874, Total reward=424.31, Steps=497745, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3875, Total reward=398.81, Steps=498034, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3876, Total reward=457.06, Steps=498323, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3877, Total reward=400.33, Steps=498607, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3878, Total reward=174.98, Steps=498706, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3879, Total reward=495.29, Steps=498997, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3880, Total reward=381.16, Steps=499234, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3881, Total reward=494.99, Steps=499519, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3882, Total reward=415.65, Steps=499820, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3883, Total reward=415.11, Steps=500114, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3884, Total reward=197.54, Steps=500303, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3885, Total reward=368.43, Steps=500533, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3886, Total reward=148.04, Steps=500638, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3887, Total reward=458.66, Steps=500923, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3888, Total reward=107.51, Steps=500969, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3889, Total reward=456.77, Steps=501253, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3890, Total reward=17.29, Steps=501289, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3891, Total reward=339.07, Steps=501475, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3892, Total reward=462.44, Steps=501776, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3893, Total reward=90.93, Steps=501836, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3894, Total reward=436.74, Steps=502146, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3895, Total reward=146.55, Steps=502254, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3896, Total reward=297.67, Steps=502463, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3897, Total reward=9.54, Steps=502476, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3898, Total reward=81.78, Steps=502512, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3899, Total reward=208.24, Steps=502679, Training iteration=77
Training> Name=main_level/agent, Worker=0, Episode=3900, Total reward=38.85, Steps=502711, Training iteration=77
Policy training> Surrogate loss=0.004782835952937603, KL divergence=0.0004756576963700354, Entropy=0.17682205140590668, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.012546582147479057, KL divergence=0.005810703616589308, Entropy=0.1759192943572998, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.018110070377588272, KL divergence=0.009041126817464828, Entropy=0.17425087094306946, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02025481127202511, KL divergence=0.010915047489106655, Entropy=0.173747256398201, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.023271795362234116, KL divergence=0.01199040375649929, Entropy=0.1724071502685547, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.025277312844991684, KL divergence=0.013132588937878609, Entropy=0.17339947819709778, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02693970873951912, KL divergence=0.014367158524692059, Entropy=0.1738174855709076, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.029839012771844864, KL divergence=0.014823045581579208, Entropy=0.17255589365959167, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.028258008882403374, KL divergence=0.015743430703878403, Entropy=0.1738075613975525, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.028193466365337372, KL divergence=0.01650332100689411, Entropy=0.1727302521467209, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/510_Step-502711.ckpt']
Uploaded 3 files for checkpoint 510 in 0.61 seconds
saved intermediate frozen graph: current/model/model_510.pb
Best checkpoint number: 473, Last checkpoint number: 508
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'507'}
Training> Name=main_level/agent, Worker=0, Episode=3901, Total reward=247.1, Steps=502900, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3902, Total reward=53.23, Steps=502932, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3903, Total reward=21.95, Steps=502978, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3904, Total reward=129.41, Steps=503088, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3905, Total reward=0.01, Steps=503097, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3906, Total reward=77.53, Steps=503165, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3907, Total reward=419.36, Steps=503449, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3908, Total reward=399.1, Steps=503619, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3909, Total reward=38.3, Steps=503693, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3910, Total reward=425.23, Steps=503989, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3911, Total reward=432.72, Steps=504263, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3912, Total reward=246.07, Steps=504411, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3913, Total reward=104.74, Steps=504470, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3914, Total reward=115.22, Steps=504558, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3915, Total reward=62.45, Steps=504622, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3916, Total reward=104.27, Steps=504712, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3917, Total reward=179.5, Steps=504844, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3918, Total reward=271.66, Steps=505042, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3919, Total reward=103.9, Steps=505128, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3920, Total reward=348.55, Steps=505434, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3921, Total reward=110.77, Steps=505508, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3922, Total reward=52.72, Steps=505535, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3923, Total reward=264.36, Steps=505709, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3924, Total reward=331.13, Steps=505952, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3925, Total reward=255.63, Steps=506131, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3926, Total reward=516.35, Steps=506421, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3927, Total reward=195.29, Steps=506579, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3928, Total reward=510.69, Steps=506874, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3929, Total reward=44.26, Steps=506910, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3930, Total reward=112.43, Steps=506984, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3931, Total reward=216.86, Steps=507150, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3932, Total reward=445.4, Steps=507436, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3933, Total reward=78.55, Steps=507473, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3934, Total reward=456.76, Steps=507758, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3935, Total reward=122.39, Steps=507836, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3936, Total reward=93.56, Steps=507929, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3937, Total reward=98.0, Steps=507979, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3938, Total reward=463.54, Steps=508271, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3939, Total reward=451.07, Steps=508553, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3940, Total reward=430.75, Steps=508860, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3941, Total reward=291.87, Steps=509073, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3942, Total reward=426.94, Steps=509388, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3943, Total reward=0.03, Steps=509417, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3944, Total reward=12.29, Steps=509441, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3945, Total reward=461.18, Steps=509711, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3946, Total reward=489.49, Steps=510003, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3947, Total reward=230.28, Steps=510153, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3948, Total reward=381.74, Steps=510339, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3949, Total reward=510.45, Steps=510617, Training iteration=78
Training> Name=main_level/agent, Worker=0, Episode=3950, Total reward=99.55, Steps=510709, Training iteration=78
Policy training> Surrogate loss=-0.002490991959348321, KL divergence=0.00039926060708239675, Entropy=0.17614775896072388, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.013075337745249271, KL divergence=0.00500426534563303, Entropy=0.174994558095932, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02367047779262066, KL divergence=0.009395157918334007, Entropy=0.1750878393650055, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026690678671002388, KL divergence=0.011542683467268944, Entropy=0.17457178235054016, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029310088604688644, KL divergence=0.01284177228808403, Entropy=0.17383292317390442, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03230057656764984, KL divergence=0.013756588101387024, Entropy=0.174825519323349, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.030515585094690323, KL divergence=0.014664029702544212, Entropy=0.1746685951948166, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.032386649399995804, KL divergence=0.015639837831258774, Entropy=0.17479173839092255, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03119559958577156, KL divergence=0.016505947336554527, Entropy=0.1750832200050354, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03230728581547737, KL divergence=0.017208654433488846, Entropy=0.17328466475009918, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/511_Step-510709.ckpt']
Uploaded 3 files for checkpoint 511 in 0.58 seconds
saved intermediate frozen graph: current/model/model_511.pb
Best checkpoint number: 473, Last checkpoint number: 509
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'508'}
Training> Name=main_level/agent, Worker=0, Episode=3951, Total reward=509.86, Steps=510993, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3952, Total reward=97.0, Steps=511035, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3953, Total reward=53.87, Steps=511078, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3954, Total reward=31.11, Steps=511091, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3955, Total reward=74.76, Steps=511131, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3956, Total reward=177.69, Steps=511254, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3957, Total reward=448.4, Steps=511558, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3958, Total reward=218.31, Steps=511669, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3959, Total reward=129.98, Steps=511767, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3960, Total reward=342.82, Steps=512005, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3961, Total reward=63.01, Steps=512052, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3962, Total reward=66.51, Steps=512112, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3963, Total reward=10.47, Steps=512132, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3964, Total reward=179.57, Steps=512316, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3965, Total reward=105.73, Steps=512396, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3966, Total reward=323.62, Steps=512605, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3967, Total reward=83.42, Steps=512664, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3968, Total reward=422.86, Steps=512897, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3969, Total reward=113.98, Steps=513008, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3970, Total reward=169.12, Steps=513117, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3971, Total reward=470.03, Steps=513418, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3972, Total reward=483.37, Steps=513711, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3973, Total reward=114.01, Steps=513774, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3974, Total reward=183.46, Steps=513894, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3975, Total reward=197.13, Steps=514033, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3976, Total reward=182.22, Steps=514167, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3977, Total reward=170.69, Steps=514325, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3978, Total reward=170.66, Steps=514443, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3979, Total reward=106.22, Steps=514540, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3980, Total reward=53.32, Steps=514580, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3981, Total reward=471.13, Steps=514865, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3982, Total reward=78.98, Steps=514964, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3983, Total reward=274.45, Steps=515200, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3984, Total reward=123.74, Steps=515375, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3985, Total reward=445.48, Steps=515670, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3986, Total reward=481.31, Steps=515952, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3987, Total reward=197.68, Steps=516063, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3988, Total reward=192.83, Steps=516174, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3989, Total reward=81.06, Steps=516276, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3990, Total reward=456.99, Steps=516564, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3991, Total reward=411.67, Steps=516876, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3992, Total reward=341.82, Steps=517074, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3993, Total reward=114.42, Steps=517115, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3994, Total reward=420.04, Steps=517421, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3995, Total reward=431.32, Steps=517717, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3996, Total reward=407.0, Steps=517990, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3997, Total reward=178.27, Steps=518111, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3998, Total reward=456.6, Steps=518408, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=3999, Total reward=30.7, Steps=518446, Training iteration=79
Training> Name=main_level/agent, Worker=0, Episode=4000, Total reward=210.24, Steps=518585, Training iteration=79
Policy training> Surrogate loss=0.002016695449128747, KL divergence=0.0006321871187537909, Entropy=0.1831563264131546, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017787806689739227, KL divergence=0.006770672742277384, Entropy=0.18171216547489166, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.022881217300891876, KL divergence=0.009786783717572689, Entropy=0.18131521344184875, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.028437644243240356, KL divergence=0.01203963067382574, Entropy=0.18082325160503387, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02723671682178974, KL divergence=0.01373274251818657, Entropy=0.180958092212677, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0305409524589777, KL divergence=0.01472911424934864, Entropy=0.1815711408853531, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.030089497566223145, KL divergence=0.01570436730980873, Entropy=0.18039070069789886, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03160710632801056, KL divergence=0.01648612506687641, Entropy=0.18021081387996674, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.033937808126211166, KL divergence=0.017025049775838852, Entropy=0.17967566847801208, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03341282159090042, KL divergence=0.017672579735517502, Entropy=0.1794740855693817, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/512_Step-518585.ckpt']
Uploaded 3 files for checkpoint 512 in 0.50 seconds
saved intermediate frozen graph: current/model/model_512.pb
Best checkpoint number: 473, Last checkpoint number: 510
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'509'}
Training> Name=main_level/agent, Worker=0, Episode=4001, Total reward=79.78, Steps=518629, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4002, Total reward=474.03, Steps=518932, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4003, Total reward=3.03, Steps=518949, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4004, Total reward=433.19, Steps=519253, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4005, Total reward=162.35, Steps=519371, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4006, Total reward=3.54, Steps=519384, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4007, Total reward=471.7, Steps=519687, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4008, Total reward=432.48, Steps=519959, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4009, Total reward=154.2, Steps=520054, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4010, Total reward=91.19, Steps=520142, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4011, Total reward=253.63, Steps=520304, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4012, Total reward=479.81, Steps=520581, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4013, Total reward=84.04, Steps=520625, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4014, Total reward=425.87, Steps=520917, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4015, Total reward=519.39, Steps=521207, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4016, Total reward=32.84, Steps=521225, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4017, Total reward=282.01, Steps=521406, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4018, Total reward=179.75, Steps=521507, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4019, Total reward=41.79, Steps=521544, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4020, Total reward=461.73, Steps=521842, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4021, Total reward=341.44, Steps=522079, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4022, Total reward=236.47, Steps=522302, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4023, Total reward=37.85, Steps=522370, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4024, Total reward=133.22, Steps=522500, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4025, Total reward=407.72, Steps=522772, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4026, Total reward=239.78, Steps=522904, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4027, Total reward=94.91, Steps=522955, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4028, Total reward=440.18, Steps=523244, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4029, Total reward=31.03, Steps=523262, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4030, Total reward=116.6, Steps=523345, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4031, Total reward=84.22, Steps=523410, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4032, Total reward=12.63, Steps=523422, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4033, Total reward=328.71, Steps=523625, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4034, Total reward=79.84, Steps=523656, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4035, Total reward=356.46, Steps=523970, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4036, Total reward=200.31, Steps=524080, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4037, Total reward=417.32, Steps=524391, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4038, Total reward=82.87, Steps=524429, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4039, Total reward=225.05, Steps=524631, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4040, Total reward=271.4, Steps=524835, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4041, Total reward=82.51, Steps=524878, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4042, Total reward=138.88, Steps=525006, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4043, Total reward=449.32, Steps=525305, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4044, Total reward=382.46, Steps=525564, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4045, Total reward=404.76, Steps=525836, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4046, Total reward=101.81, Steps=525894, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4047, Total reward=199.52, Steps=526028, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4048, Total reward=457.35, Steps=526305, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4049, Total reward=346.57, Steps=526512, Training iteration=80
Training> Name=main_level/agent, Worker=0, Episode=4050, Total reward=138.61, Steps=526626, Training iteration=80
Policy training> Surrogate loss=-0.005021367687731981, KL divergence=0.0007371243555098772, Entropy=0.1861831396818161, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.011369959451258183, KL divergence=0.006493727210909128, Entropy=0.18544432520866394, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020260903984308243, KL divergence=0.010062533430755138, Entropy=0.184437096118927, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025323737412691116, KL divergence=0.011534692719578743, Entropy=0.18454916775226593, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02587464265525341, KL divergence=0.012371410615742207, Entropy=0.18297156691551208, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03055459074676037, KL divergence=0.013930868357419968, Entropy=0.18386229872703552, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029974792152643204, KL divergence=0.01460680179297924, Entropy=0.18301460146903992, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.028622915968298912, KL divergence=0.015343529172241688, Entropy=0.18288905918598175, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.029731960967183113, KL divergence=0.015919677913188934, Entropy=0.1833961457014084, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02949054166674614, KL divergence=0.01679643616080284, Entropy=0.18191637098789215, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/513_Step-526626.ckpt']
Uploaded 3 files for checkpoint 513 in 0.48 seconds
saved intermediate frozen graph: current/model/model_513.pb
Best checkpoint number: 473, Last checkpoint number: 511
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'510'}
Training> Name=main_level/agent, Worker=0, Episode=4051, Total reward=418.59, Steps=526922, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4052, Total reward=360.7, Steps=527137, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4053, Total reward=11.56, Steps=527154, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4054, Total reward=62.39, Steps=527199, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4055, Total reward=470.08, Steps=527481, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4056, Total reward=448.89, Steps=527795, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4057, Total reward=164.55, Steps=527969, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4058, Total reward=446.28, Steps=528231, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4059, Total reward=16.58, Steps=528254, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4060, Total reward=403.01, Steps=528550, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4061, Total reward=76.4, Steps=528592, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4062, Total reward=52.24, Steps=528620, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4063, Total reward=506.91, Steps=528900, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4064, Total reward=110.31, Steps=528999, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4065, Total reward=10.7, Steps=529042, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4066, Total reward=441.36, Steps=529315, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4067, Total reward=131.31, Steps=529411, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4068, Total reward=124.18, Steps=529481, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4069, Total reward=104.44, Steps=529553, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4070, Total reward=143.25, Steps=529665, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4071, Total reward=162.17, Steps=529758, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4072, Total reward=85.17, Steps=529815, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4073, Total reward=175.62, Steps=529903, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4074, Total reward=354.3, Steps=530132, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4075, Total reward=170.93, Steps=530218, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4076, Total reward=511.94, Steps=530496, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4077, Total reward=103.44, Steps=530549, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4078, Total reward=277.87, Steps=530785, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4079, Total reward=25.39, Steps=530822, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4080, Total reward=441.62, Steps=531118, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4081, Total reward=61.53, Steps=531143, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4082, Total reward=50.72, Steps=531186, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4083, Total reward=133.65, Steps=531366, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4084, Total reward=395.07, Steps=531654, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4085, Total reward=320.87, Steps=531883, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4086, Total reward=484.54, Steps=532175, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4087, Total reward=193.23, Steps=532309, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4088, Total reward=350.31, Steps=532509, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4089, Total reward=214.1, Steps=532681, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4090, Total reward=114.05, Steps=532761, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4091, Total reward=277.14, Steps=532968, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4092, Total reward=225.74, Steps=533102, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4093, Total reward=130.86, Steps=533165, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4094, Total reward=380.14, Steps=533450, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4095, Total reward=413.25, Steps=533719, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4096, Total reward=467.84, Steps=534019, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4097, Total reward=103.01, Steps=534084, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4098, Total reward=130.81, Steps=534171, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4099, Total reward=18.35, Steps=534190, Training iteration=81
Training> Name=main_level/agent, Worker=0, Episode=4100, Total reward=466.87, Steps=534481, Training iteration=81
Policy training> Surrogate loss=0.0004767075297422707, KL divergence=0.00030546949710696936, Entropy=0.1836940497159958, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.013437497429549694, KL divergence=0.005322623532265425, Entropy=0.1832832396030426, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.022256512194871902, KL divergence=0.010250916704535484, Entropy=0.1815597414970398, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02244197018444538, KL divergence=0.012052880600094795, Entropy=0.18115167319774628, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026974765583872795, KL divergence=0.013244714587926865, Entropy=0.1798049807548523, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0248552355915308, KL divergence=0.014212136156857014, Entropy=0.17939400672912598, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.027817916125059128, KL divergence=0.01531247328966856, Entropy=0.1794961839914322, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.031227240338921547, KL divergence=0.01581881195306778, Entropy=0.17778682708740234, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.031098371371626854, KL divergence=0.016600172966718674, Entropy=0.17816206812858582, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.029845599085092545, KL divergence=0.017598053440451622, Entropy=0.1787003129720688, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/514_Step-534481.ckpt']
Uploaded 3 files for checkpoint 514 in 0.51 seconds
saved intermediate frozen graph: current/model/model_514.pb
Best checkpoint number: 473, Last checkpoint number: 512
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'511'}
Training> Name=main_level/agent, Worker=0, Episode=4101, Total reward=95.77, Steps=534563, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4102, Total reward=408.54, Steps=534842, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4103, Total reward=22.58, Steps=534890, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4104, Total reward=253.01, Steps=535078, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4105, Total reward=199.84, Steps=535242, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4106, Total reward=350.03, Steps=535479, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4107, Total reward=3.03, Steps=535491, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4108, Total reward=167.95, Steps=535602, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4109, Total reward=477.96, Steps=535907, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4110, Total reward=405.6, Steps=536148, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4111, Total reward=90.08, Steps=536215, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4112, Total reward=125.55, Steps=536297, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4113, Total reward=308.67, Steps=536495, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4114, Total reward=60.01, Steps=536542, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4115, Total reward=440.91, Steps=536852, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4116, Total reward=93.94, Steps=536917, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4117, Total reward=448.23, Steps=537210, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4118, Total reward=321.73, Steps=537424, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4119, Total reward=145.35, Steps=537523, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4120, Total reward=102.18, Steps=537584, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4121, Total reward=256.46, Steps=537749, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4122, Total reward=52.39, Steps=537776, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4123, Total reward=389.67, Steps=538040, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4124, Total reward=453.26, Steps=538322, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4125, Total reward=126.97, Steps=538443, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4126, Total reward=121.53, Steps=538510, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4127, Total reward=135.0, Steps=538636, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4128, Total reward=443.67, Steps=538925, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4129, Total reward=106.81, Steps=538983, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4130, Total reward=474.06, Steps=539275, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4131, Total reward=430.18, Steps=539587, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4132, Total reward=502.47, Steps=539876, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4133, Total reward=309.92, Steps=540084, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4134, Total reward=67.15, Steps=540113, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4135, Total reward=392.31, Steps=540392, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4136, Total reward=176.5, Steps=540500, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4137, Total reward=194.55, Steps=540605, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4138, Total reward=481.69, Steps=540879, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4139, Total reward=214.39, Steps=541044, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4140, Total reward=451.45, Steps=541349, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4141, Total reward=441.56, Steps=541657, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4142, Total reward=260.36, Steps=541898, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4143, Total reward=182.19, Steps=542094, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4144, Total reward=173.78, Steps=542234, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4145, Total reward=171.05, Steps=542363, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4146, Total reward=432.73, Steps=542644, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4147, Total reward=149.74, Steps=542759, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4148, Total reward=513.35, Steps=543019, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4149, Total reward=442.16, Steps=543321, Training iteration=82
Training> Name=main_level/agent, Worker=0, Episode=4150, Total reward=137.37, Steps=543423, Training iteration=82
Policy training> Surrogate loss=0.0007924573146738112, KL divergence=0.0007254641386680305, Entropy=0.1796802282333374, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014071915298700333, KL divergence=0.005774201359599829, Entropy=0.17657437920570374, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.019864456728100777, KL divergence=0.00888829492032528, Entropy=0.1753002405166626, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.024754365906119347, KL divergence=0.01026900950819254, Entropy=0.17476429045200348, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.027207879349589348, KL divergence=0.011678172275424004, Entropy=0.17452989518642426, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.027595117688179016, KL divergence=0.012683803215622902, Entropy=0.1754549890756607, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.026675531640648842, KL divergence=0.013487668707966805, Entropy=0.17379428446292877, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.026788393035531044, KL divergence=0.014453024603426456, Entropy=0.1736535131931305, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.028432991355657578, KL divergence=0.014974762685596943, Entropy=0.17378976941108704, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.031232066452503204, KL divergence=0.015630939975380898, Entropy=0.17297817766666412, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/515_Step-543423.ckpt']
Uploaded 3 files for checkpoint 515 in 0.54 seconds
saved intermediate frozen graph: current/model/model_515.pb
Best checkpoint number: 473, Last checkpoint number: 513
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'512'}
Training> Name=main_level/agent, Worker=0, Episode=4151, Total reward=472.14, Steps=543731, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4152, Total reward=12.86, Steps=543743, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4153, Total reward=443.78, Steps=544014, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4154, Total reward=181.86, Steps=544137, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4155, Total reward=307.13, Steps=544393, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4156, Total reward=490.02, Steps=544684, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4157, Total reward=83.59, Steps=544736, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4158, Total reward=177.61, Steps=544855, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4159, Total reward=43.14, Steps=544882, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4160, Total reward=235.93, Steps=545068, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4161, Total reward=517.94, Steps=545354, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4162, Total reward=487.17, Steps=545640, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4163, Total reward=78.81, Steps=545738, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4164, Total reward=446.94, Steps=546027, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4165, Total reward=508.29, Steps=546314, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4166, Total reward=164.18, Steps=546426, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4167, Total reward=459.1, Steps=546716, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4168, Total reward=467.94, Steps=547014, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4169, Total reward=296.01, Steps=547195, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4170, Total reward=131.2, Steps=547298, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4171, Total reward=53.84, Steps=547346, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4172, Total reward=418.92, Steps=547647, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4173, Total reward=447.83, Steps=547949, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4174, Total reward=68.35, Steps=548002, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4175, Total reward=34.89, Steps=548018, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4176, Total reward=25.95, Steps=548035, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4177, Total reward=484.08, Steps=548323, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4178, Total reward=429.72, Steps=548580, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4179, Total reward=127.66, Steps=548693, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4180, Total reward=484.21, Steps=548977, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4181, Total reward=340.12, Steps=549190, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4182, Total reward=16.67, Steps=549209, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4183, Total reward=201.81, Steps=549378, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4184, Total reward=463.39, Steps=549654, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4185, Total reward=133.19, Steps=549826, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4186, Total reward=86.38, Steps=549889, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4187, Total reward=143.93, Steps=549981, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4188, Total reward=21.13, Steps=549996, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4189, Total reward=524.16, Steps=550277, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4190, Total reward=472.04, Steps=550566, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4191, Total reward=29.61, Steps=550593, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4192, Total reward=477.25, Steps=550886, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4193, Total reward=64.74, Steps=550913, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4194, Total reward=93.84, Steps=550974, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4195, Total reward=41.43, Steps=550991, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4196, Total reward=421.9, Steps=551280, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4197, Total reward=201.15, Steps=551422, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4198, Total reward=259.79, Steps=551587, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4199, Total reward=359.42, Steps=551835, Training iteration=83
Training> Name=main_level/agent, Worker=0, Episode=4200, Total reward=90.85, Steps=551887, Training iteration=83
Policy training> Surrogate loss=2.386048436164856e-06, KL divergence=0.0005982483853586018, Entropy=0.18278451263904572, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.012476861476898193, KL divergence=0.005579028278589249, Entropy=0.18303200602531433, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.017842810600996017, KL divergence=0.009688472375273705, Entropy=0.18089532852172852, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.01964157074689865, KL divergence=0.011564798653125763, Entropy=0.1809045821428299, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.025749431923031807, KL divergence=0.013052340596914291, Entropy=0.17970968782901764, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0234344694763422, KL divergence=0.014182578772306442, Entropy=0.18021774291992188, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.027580875903367996, KL divergence=0.01496010459959507, Entropy=0.17974688112735748, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.026320869103074074, KL divergence=0.015522322617471218, Entropy=0.1792435646057129, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03044428490102291, KL divergence=0.016029346734285355, Entropy=0.17828874289989471, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.031058160588145256, KL divergence=0.017332158982753754, Entropy=0.178932785987854, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/516_Step-551887.ckpt']
Uploaded 3 files for checkpoint 516 in 0.58 seconds
saved intermediate frozen graph: current/model/model_516.pb
Best checkpoint number: 473, Last checkpoint number: 514
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'513'}
Training> Name=main_level/agent, Worker=0, Episode=4201, Total reward=111.39, Steps=551968, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4202, Total reward=62.31, Steps=552044, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4203, Total reward=451.87, Steps=552335, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4204, Total reward=9.03, Steps=552378, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4205, Total reward=392.25, Steps=552608, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4206, Total reward=481.43, Steps=552893, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4207, Total reward=168.37, Steps=553018, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4208, Total reward=161.44, Steps=553150, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4209, Total reward=33.88, Steps=553181, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4210, Total reward=378.87, Steps=553419, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4211, Total reward=342.93, Steps=553586, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4212, Total reward=5.36, Steps=553597, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4213, Total reward=26.35, Steps=553608, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4214, Total reward=464.35, Steps=553885, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4215, Total reward=496.97, Steps=554201, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4216, Total reward=403.2, Steps=554492, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4217, Total reward=215.79, Steps=554600, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4218, Total reward=279.24, Steps=554775, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4219, Total reward=50.04, Steps=554817, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4220, Total reward=78.38, Steps=554912, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4221, Total reward=461.18, Steps=555204, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4222, Total reward=420.95, Steps=555510, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4223, Total reward=287.48, Steps=555729, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4224, Total reward=139.12, Steps=555865, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4225, Total reward=453.3, Steps=556173, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4226, Total reward=449.68, Steps=556471, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4227, Total reward=519.12, Steps=556697, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4228, Total reward=161.25, Steps=556770, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4229, Total reward=159.84, Steps=556882, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4230, Total reward=270.52, Steps=557055, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4231, Total reward=90.77, Steps=557111, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4232, Total reward=327.37, Steps=557352, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4233, Total reward=70.3, Steps=557394, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4234, Total reward=31.78, Steps=557407, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4235, Total reward=94.27, Steps=557467, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4236, Total reward=100.79, Steps=557553, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4237, Total reward=383.73, Steps=557833, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4238, Total reward=157.9, Steps=557951, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4239, Total reward=22.44, Steps=557970, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4240, Total reward=121.4, Steps=558022, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4241, Total reward=169.36, Steps=558185, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4242, Total reward=435.59, Steps=558467, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4243, Total reward=31.27, Steps=558555, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4244, Total reward=128.18, Steps=558708, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4245, Total reward=181.28, Steps=558824, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4246, Total reward=205.51, Steps=558961, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4247, Total reward=444.34, Steps=559248, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4248, Total reward=237.8, Steps=559391, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4249, Total reward=464.0, Steps=559699, Training iteration=84
Training> Name=main_level/agent, Worker=0, Episode=4250, Total reward=231.71, Steps=559841, Training iteration=84
Policy training> Surrogate loss=-0.0039203898049890995, KL divergence=0.0009708079742267728, Entropy=0.18142256140708923, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015818674117326736, KL divergence=0.005878536496311426, Entropy=0.17994171380996704, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.018218757584691048, KL divergence=0.009108570404350758, Entropy=0.179548978805542, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023688919842243195, KL divergence=0.01095482800155878, Entropy=0.1790747195482254, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028663568198680878, KL divergence=0.012138430960476398, Entropy=0.17929686605930328, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030076980590820312, KL divergence=0.013559500686824322, Entropy=0.1799839287996292, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029870767146348953, KL divergence=0.014964976347982883, Entropy=0.1791742742061615, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.033150892704725266, KL divergence=0.01569991186261177, Entropy=0.179779514670372, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03325384110212326, KL divergence=0.016433723270893097, Entropy=0.17859125137329102, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.030257979407906532, KL divergence=0.017726659774780273, Entropy=0.17934083938598633, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/517_Step-559841.ckpt']
Uploaded 3 files for checkpoint 517 in 0.53 seconds
saved intermediate frozen graph: current/model/model_517.pb
Best checkpoint number: 473, Last checkpoint number: 515
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'514'}
Training> Name=main_level/agent, Worker=0, Episode=4251, Total reward=107.89, Steps=559909, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4252, Total reward=94.06, Steps=559966, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4253, Total reward=458.89, Steps=560254, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4254, Total reward=66.08, Steps=560283, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4255, Total reward=488.27, Steps=560590, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4256, Total reward=323.39, Steps=560848, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4257, Total reward=154.58, Steps=560978, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4258, Total reward=233.37, Steps=561182, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4259, Total reward=126.66, Steps=561271, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4260, Total reward=248.06, Steps=561461, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4261, Total reward=123.17, Steps=561541, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4262, Total reward=253.94, Steps=561729, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4263, Total reward=19.49, Steps=561790, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4264, Total reward=481.68, Steps=562056, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4265, Total reward=446.88, Steps=562360, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4266, Total reward=454.68, Steps=562657, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4267, Total reward=117.17, Steps=562743, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4268, Total reward=113.4, Steps=562819, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4269, Total reward=156.91, Steps=562947, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4270, Total reward=328.38, Steps=563169, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4271, Total reward=258.31, Steps=563332, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4272, Total reward=17.64, Steps=563344, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4273, Total reward=33.65, Steps=563356, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4274, Total reward=70.18, Steps=563412, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4275, Total reward=470.25, Steps=563709, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4276, Total reward=212.69, Steps=563841, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4277, Total reward=431.49, Steps=564135, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4278, Total reward=358.5, Steps=564380, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4279, Total reward=93.71, Steps=564431, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4280, Total reward=137.48, Steps=564535, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4281, Total reward=345.42, Steps=564866, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4282, Total reward=482.15, Steps=565164, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4283, Total reward=507.48, Steps=565445, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4284, Total reward=277.32, Steps=565683, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4285, Total reward=499.33, Steps=565975, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4286, Total reward=434.84, Steps=566255, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4287, Total reward=463.74, Steps=566493, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4288, Total reward=214.03, Steps=566617, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4289, Total reward=20.91, Steps=566630, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4290, Total reward=349.21, Steps=566865, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4291, Total reward=0.01, Steps=566876, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4292, Total reward=455.77, Steps=567161, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4293, Total reward=94.1, Steps=567222, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4294, Total reward=87.09, Steps=567276, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4295, Total reward=95.02, Steps=567373, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4296, Total reward=412.98, Steps=567659, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4297, Total reward=187.8, Steps=567768, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4298, Total reward=399.82, Steps=568019, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4299, Total reward=355.76, Steps=568265, Training iteration=85
Training> Name=main_level/agent, Worker=0, Episode=4300, Total reward=284.58, Steps=568556, Training iteration=85
Policy training> Surrogate loss=0.0001940930960699916, KL divergence=0.0005976427346467972, Entropy=0.19062909483909607, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.016394566744565964, KL divergence=0.007033844478428364, Entropy=0.18885745108127594, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.022657714784145355, KL divergence=0.010840366594493389, Entropy=0.18758785724639893, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.0253000408411026, KL divergence=0.012628935277462006, Entropy=0.18695925176143646, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.027154233306646347, KL divergence=0.0137326680123806, Entropy=0.18668296933174133, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0288886446505785, KL divergence=0.014812683686614037, Entropy=0.18629589676856995, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03003019094467163, KL divergence=0.015744803473353386, Entropy=0.1859387457370758, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03026973083615303, KL divergence=0.0164174847304821, Entropy=0.18544675409793854, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03182268887758255, KL divergence=0.017140842974185944, Entropy=0.1853981614112854, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.033541999757289886, KL divergence=0.01774008385837078, Entropy=0.18499502539634705, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/518_Step-568556.ckpt']
Uploaded 3 files for checkpoint 518 in 0.47 seconds
saved intermediate frozen graph: current/model/model_518.pb
Best checkpoint number: 473, Last checkpoint number: 516
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'515'}
Training> Name=main_level/agent, Worker=0, Episode=4301, Total reward=166.19, Steps=568715, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4302, Total reward=31.45, Steps=568736, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4303, Total reward=281.66, Steps=568926, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4304, Total reward=16.94, Steps=568963, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4305, Total reward=206.76, Steps=569117, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4306, Total reward=549.7, Steps=569403, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4307, Total reward=366.72, Steps=569669, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4308, Total reward=169.23, Steps=569744, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4309, Total reward=123.03, Steps=569836, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4310, Total reward=188.1, Steps=569937, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4311, Total reward=118.43, Steps=570014, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4312, Total reward=366.99, Steps=570219, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4313, Total reward=116.09, Steps=570286, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4314, Total reward=87.61, Steps=570339, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4315, Total reward=77.93, Steps=570389, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4316, Total reward=352.92, Steps=570635, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4317, Total reward=38.28, Steps=570661, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4318, Total reward=90.57, Steps=570715, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4319, Total reward=426.07, Steps=570983, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4320, Total reward=116.04, Steps=571044, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4321, Total reward=216.67, Steps=571219, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4322, Total reward=13.26, Steps=571236, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4323, Total reward=448.78, Steps=571539, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4324, Total reward=455.38, Steps=571828, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4325, Total reward=209.72, Steps=571947, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4326, Total reward=153.64, Steps=572043, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4327, Total reward=150.67, Steps=572176, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4328, Total reward=407.01, Steps=572476, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4329, Total reward=172.67, Steps=572599, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4330, Total reward=148.11, Steps=572702, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4331, Total reward=85.56, Steps=572774, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4332, Total reward=223.34, Steps=572907, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4333, Total reward=493.42, Steps=573208, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4334, Total reward=79.88, Steps=573267, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4335, Total reward=31.49, Steps=573284, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4336, Total reward=520.88, Steps=573563, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4337, Total reward=274.85, Steps=573747, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4338, Total reward=415.15, Steps=574062, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4339, Total reward=203.51, Steps=574247, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4340, Total reward=209.02, Steps=574436, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4341, Total reward=284.86, Steps=574658, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4342, Total reward=382.99, Steps=574957, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4343, Total reward=224.67, Steps=575146, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4344, Total reward=446.4, Steps=575424, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4345, Total reward=367.01, Steps=575718, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4346, Total reward=126.97, Steps=575832, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4347, Total reward=146.01, Steps=575944, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4348, Total reward=301.39, Steps=576098, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4349, Total reward=203.85, Steps=576213, Training iteration=86
Training> Name=main_level/agent, Worker=0, Episode=4350, Total reward=495.14, Steps=576522, Training iteration=86
Policy training> Surrogate loss=0.0018747634021565318, KL divergence=0.0008313193684443831, Entropy=0.1768634170293808, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015405522659420967, KL divergence=0.006669016554951668, Entropy=0.17547078430652618, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.01978532038629055, KL divergence=0.009968659840524197, Entropy=0.1743336021900177, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.021752864122390747, KL divergence=0.012144829146564007, Entropy=0.17382924258708954, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02650308422744274, KL divergence=0.013997165486216545, Entropy=0.1737350970506668, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02879175916314125, KL divergence=0.01486152783036232, Entropy=0.17297270894050598, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029875317588448524, KL divergence=0.01634437032043934, Entropy=0.17350618541240692, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.027965424582362175, KL divergence=0.01755109801888466, Entropy=0.17258968949317932, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02698623202741146, KL divergence=0.01829800195991993, Entropy=0.17182093858718872, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.027610016986727715, KL divergence=0.01916121132671833, Entropy=0.1727999597787857, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/519_Step-576522.ckpt']
Uploaded 3 files for checkpoint 519 in 0.67 seconds
saved intermediate frozen graph: current/model/model_519.pb
Best checkpoint number: 473, Last checkpoint number: 517
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'516'}
Training> Name=main_level/agent, Worker=0, Episode=4351, Total reward=506.86, Steps=576830, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4352, Total reward=15.13, Steps=576843, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4353, Total reward=482.19, Steps=577123, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4354, Total reward=420.93, Steps=577374, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4355, Total reward=486.76, Steps=577676, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4356, Total reward=112.84, Steps=577752, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4357, Total reward=432.47, Steps=578035, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4358, Total reward=18.27, Steps=578052, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4359, Total reward=483.33, Steps=578351, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4360, Total reward=86.65, Steps=578407, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4361, Total reward=102.5, Steps=578492, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4362, Total reward=188.55, Steps=578668, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4363, Total reward=384.34, Steps=578928, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4364, Total reward=469.53, Steps=579229, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4365, Total reward=441.43, Steps=579505, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4366, Total reward=176.06, Steps=579602, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4367, Total reward=507.37, Steps=579876, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4368, Total reward=195.36, Steps=580016, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4369, Total reward=14.98, Steps=580029, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4370, Total reward=431.39, Steps=580320, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4371, Total reward=151.09, Steps=580398, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4372, Total reward=51.57, Steps=580457, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4373, Total reward=57.62, Steps=580501, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4374, Total reward=39.33, Steps=580514, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4375, Total reward=268.64, Steps=580671, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4376, Total reward=495.5, Steps=580960, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4377, Total reward=401.94, Steps=581248, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4378, Total reward=102.79, Steps=581294, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4379, Total reward=483.57, Steps=581609, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4380, Total reward=315.46, Steps=581784, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4381, Total reward=534.33, Steps=582069, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4382, Total reward=235.52, Steps=582241, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4383, Total reward=162.87, Steps=582445, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4384, Total reward=434.19, Steps=582758, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4385, Total reward=440.29, Steps=583048, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4386, Total reward=531.52, Steps=583304, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4387, Total reward=234.83, Steps=583458, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4388, Total reward=484.25, Steps=583742, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4389, Total reward=59.22, Steps=583799, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4390, Total reward=444.33, Steps=584094, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4391, Total reward=166.56, Steps=584193, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4392, Total reward=13.01, Steps=584205, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4393, Total reward=454.87, Steps=584497, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4394, Total reward=224.36, Steps=584616, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4395, Total reward=215.11, Steps=584742, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4396, Total reward=44.46, Steps=584765, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4397, Total reward=117.11, Steps=584828, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4398, Total reward=86.11, Steps=584874, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4399, Total reward=126.48, Steps=584981, Training iteration=87
Training> Name=main_level/agent, Worker=0, Episode=4400, Total reward=252.85, Steps=585186, Training iteration=87
Policy training> Surrogate loss=0.0027592508122324944, KL divergence=0.0004729299689643085, Entropy=0.1836380660533905, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01430855318903923, KL divergence=0.005662241019308567, Entropy=0.18007253110408783, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02164635993540287, KL divergence=0.008784624747931957, Entropy=0.18189136683940887, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.019317612051963806, KL divergence=0.010954748839139938, Entropy=0.18401077389717102, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026200439780950546, KL divergence=0.012304033152759075, Entropy=0.18230023980140686, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.024099377915263176, KL divergence=0.013578558340668678, Entropy=0.18161910772323608, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03028208576142788, KL divergence=0.014271593652665615, Entropy=0.1812201589345932, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03192030265927315, KL divergence=0.015176834538578987, Entropy=0.18043865263462067, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03295157104730606, KL divergence=0.01601565256714821, Entropy=0.18072377145290375, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02928060293197632, KL divergence=0.01663176901638508, Entropy=0.18096806108951569, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/520_Step-585186.ckpt']
Uploaded 3 files for checkpoint 520 in 0.56 seconds
saved intermediate frozen graph: current/model/model_520.pb
Best checkpoint number: 473, Last checkpoint number: 518
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'517'}
Training> Name=main_level/agent, Worker=0, Episode=4401, Total reward=378.95, Steps=585486, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4402, Total reward=79.08, Steps=585545, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4403, Total reward=475.2, Steps=585833, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4404, Total reward=174.35, Steps=586016, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4405, Total reward=106.57, Steps=586116, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4406, Total reward=242.88, Steps=586276, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4407, Total reward=366.31, Steps=586504, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4408, Total reward=342.02, Steps=586703, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4409, Total reward=245.85, Steps=586860, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4410, Total reward=76.88, Steps=586910, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4411, Total reward=34.44, Steps=586937, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4412, Total reward=109.16, Steps=586992, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4413, Total reward=33.49, Steps=587004, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4414, Total reward=130.09, Steps=587087, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4415, Total reward=266.35, Steps=587210, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4416, Total reward=415.95, Steps=587493, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4417, Total reward=452.12, Steps=587800, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4418, Total reward=177.96, Steps=587904, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4419, Total reward=419.96, Steps=588187, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4420, Total reward=294.93, Steps=588413, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4421, Total reward=78.25, Steps=588478, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4422, Total reward=497.31, Steps=588770, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4423, Total reward=84.57, Steps=588885, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4424, Total reward=489.67, Steps=589165, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4425, Total reward=183.76, Steps=589317, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4426, Total reward=110.45, Steps=589423, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4427, Total reward=417.57, Steps=589722, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4428, Total reward=393.95, Steps=589953, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4429, Total reward=421.67, Steps=590183, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4430, Total reward=44.27, Steps=590222, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4431, Total reward=38.12, Steps=590278, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4432, Total reward=219.51, Steps=590427, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4433, Total reward=66.39, Steps=590468, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4434, Total reward=154.12, Steps=590584, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4435, Total reward=177.95, Steps=590678, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4436, Total reward=512.24, Steps=590976, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4437, Total reward=63.01, Steps=591017, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4438, Total reward=275.65, Steps=591219, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4439, Total reward=383.7, Steps=591507, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4440, Total reward=445.92, Steps=591809, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4441, Total reward=465.66, Steps=592113, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4442, Total reward=426.77, Steps=592410, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4443, Total reward=18.8, Steps=592466, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4444, Total reward=13.95, Steps=592498, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4445, Total reward=236.91, Steps=592663, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4446, Total reward=3.47, Steps=592676, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4447, Total reward=523.17, Steps=592956, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4448, Total reward=258.68, Steps=593084, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4449, Total reward=71.69, Steps=593164, Training iteration=88
Training> Name=main_level/agent, Worker=0, Episode=4450, Total reward=266.66, Steps=593344, Training iteration=88
Policy training> Surrogate loss=0.0035432889126241207, KL divergence=0.0005924105644226074, Entropy=0.17631398141384125, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.012424217537045479, KL divergence=0.0066130091436207294, Entropy=0.17411379516124725, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.017590848729014397, KL divergence=0.00947616621851921, Entropy=0.1732289344072342, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023317119106650352, KL divergence=0.010994135402143002, Entropy=0.17209430038928986, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.021593013778328896, KL divergence=0.012562176212668419, Entropy=0.1720748245716095, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02527112141251564, KL divergence=0.013609793037176132, Entropy=0.17247076332569122, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0313633568584919, KL divergence=0.014591654762625694, Entropy=0.1720055192708969, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030151264742016792, KL divergence=0.015393834561109543, Entropy=0.1723441779613495, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.0291538517922163, KL divergence=0.01619727537035942, Entropy=0.172946035861969, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02729492075741291, KL divergence=0.01662081852555275, Entropy=0.1723800152540207, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/521_Step-593344.ckpt']
Uploaded 3 files for checkpoint 521 in 0.47 seconds
saved intermediate frozen graph: current/model/model_521.pb
Best checkpoint number: 473, Last checkpoint number: 519
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'518'}
Training> Name=main_level/agent, Worker=0, Episode=4451, Total reward=140.3, Steps=593438, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4452, Total reward=174.32, Steps=593556, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4453, Total reward=33.23, Steps=593568, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4454, Total reward=381.88, Steps=593879, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4455, Total reward=102.63, Steps=593931, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4456, Total reward=172.32, Steps=594038, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4457, Total reward=351.59, Steps=594308, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4458, Total reward=355.8, Steps=594535, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4459, Total reward=475.8, Steps=594806, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4460, Total reward=497.25, Steps=595096, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4461, Total reward=396.32, Steps=595391, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4462, Total reward=298.58, Steps=595610, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4463, Total reward=498.98, Steps=595899, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4464, Total reward=454.15, Steps=596200, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4465, Total reward=401.61, Steps=596504, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4466, Total reward=113.39, Steps=596564, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4467, Total reward=398.86, Steps=596865, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4468, Total reward=388.39, Steps=597051, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4469, Total reward=482.22, Steps=597330, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4470, Total reward=206.13, Steps=597512, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4471, Total reward=486.24, Steps=597802, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4472, Total reward=485.56, Steps=598092, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4473, Total reward=196.68, Steps=598241, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4474, Total reward=32.39, Steps=598254, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4475, Total reward=237.23, Steps=598421, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4476, Total reward=110.02, Steps=598482, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4477, Total reward=454.6, Steps=598769, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4478, Total reward=83.52, Steps=598817, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4479, Total reward=212.23, Steps=599025, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4480, Total reward=124.65, Steps=599112, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4481, Total reward=219.66, Steps=599285, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4482, Total reward=73.53, Steps=599350, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4483, Total reward=223.64, Steps=599543, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4484, Total reward=504.76, Steps=599829, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4485, Total reward=472.81, Steps=600110, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4486, Total reward=126.71, Steps=600195, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4487, Total reward=99.23, Steps=600295, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4488, Total reward=452.19, Steps=600571, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4489, Total reward=106.63, Steps=600647, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4490, Total reward=353.15, Steps=600974, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4491, Total reward=408.58, Steps=601264, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4492, Total reward=86.57, Steps=601305, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4493, Total reward=58.54, Steps=601350, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4494, Total reward=32.26, Steps=601362, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4495, Total reward=532.51, Steps=601645, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4496, Total reward=417.36, Steps=601932, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4497, Total reward=392.11, Steps=602241, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4498, Total reward=19.05, Steps=602258, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4499, Total reward=26.55, Steps=602277, Training iteration=89
Training> Name=main_level/agent, Worker=0, Episode=4500, Total reward=207.78, Steps=602483, Training iteration=89
Policy training> Surrogate loss=0.003238596487790346, KL divergence=0.0007156785577535629, Entropy=0.18957845866680145, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.016981270164251328, KL divergence=0.006387349218130112, Entropy=0.1869434416294098, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020170941948890686, KL divergence=0.009186547249555588, Entropy=0.18631954491138458, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02305326610803604, KL divergence=0.010882072150707245, Entropy=0.1860327273607254, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.027228355407714844, KL divergence=0.012746883556246758, Entropy=0.1862470954656601, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0256444551050663, KL divergence=0.013618361204862595, Entropy=0.18392081558704376, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.031513821333646774, KL divergence=0.014787665568292141, Entropy=0.1843695044517517, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03212110698223114, KL divergence=0.015811800956726074, Entropy=0.1841537356376648, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03726067766547203, KL divergence=0.016430720686912537, Entropy=0.18411506712436676, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03595349192619324, KL divergence=0.017260152846574783, Entropy=0.18371596932411194, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/522_Step-602483.ckpt']
Uploaded 3 files for checkpoint 522 in 0.68 seconds
saved intermediate frozen graph: current/model/model_522.pb
Best checkpoint number: 473, Last checkpoint number: 520
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'519'}
Training> Name=main_level/agent, Worker=0, Episode=4501, Total reward=293.68, Steps=602698, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4502, Total reward=47.26, Steps=602729, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4503, Total reward=41.42, Steps=602778, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4504, Total reward=2.47, Steps=602807, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4505, Total reward=199.33, Steps=602963, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4506, Total reward=142.8, Steps=603081, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4507, Total reward=465.44, Steps=603319, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4508, Total reward=449.12, Steps=603601, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4509, Total reward=457.96, Steps=603907, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4510, Total reward=467.3, Steps=604216, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4511, Total reward=502.07, Steps=604506, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4512, Total reward=383.96, Steps=604713, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4513, Total reward=262.56, Steps=604849, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4514, Total reward=442.1, Steps=605141, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4515, Total reward=427.65, Steps=605427, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4516, Total reward=454.05, Steps=605726, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4517, Total reward=315.48, Steps=605916, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4518, Total reward=151.6, Steps=605993, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4519, Total reward=36.75, Steps=606017, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4520, Total reward=437.53, Steps=606318, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4521, Total reward=241.89, Steps=606537, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4522, Total reward=30.99, Steps=606556, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4523, Total reward=448.22, Steps=606876, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4524, Total reward=98.71, Steps=606969, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4525, Total reward=447.77, Steps=607257, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4526, Total reward=441.76, Steps=607560, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4527, Total reward=337.39, Steps=607782, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4528, Total reward=210.58, Steps=607887, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4529, Total reward=448.99, Steps=608172, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4530, Total reward=72.47, Steps=608235, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4531, Total reward=67.93, Steps=608271, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4532, Total reward=471.39, Steps=608554, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4533, Total reward=322.86, Steps=608749, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4534, Total reward=218.07, Steps=608927, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4535, Total reward=253.16, Steps=609091, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4536, Total reward=119.26, Steps=609184, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4537, Total reward=12.09, Steps=609208, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4538, Total reward=390.26, Steps=609483, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4539, Total reward=328.01, Steps=609745, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4540, Total reward=503.11, Steps=610044, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4541, Total reward=93.34, Steps=610122, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4542, Total reward=252.0, Steps=610322, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4543, Total reward=185.9, Steps=610475, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4544, Total reward=234.36, Steps=610666, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4545, Total reward=0.03, Steps=610700, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4546, Total reward=367.15, Steps=610915, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4547, Total reward=149.52, Steps=611053, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4548, Total reward=452.37, Steps=611350, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4549, Total reward=31.5, Steps=611371, Training iteration=90
Training> Name=main_level/agent, Worker=0, Episode=4550, Total reward=112.3, Steps=611462, Training iteration=90
Policy training> Surrogate loss=-0.0025104060769081116, KL divergence=0.0011555654928088188, Entropy=0.1807290017604828, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015832455828785896, KL divergence=0.006313650868833065, Entropy=0.17770856618881226, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02032196894288063, KL divergence=0.009122239425778389, Entropy=0.1774512082338333, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023775428533554077, KL divergence=0.011137760244309902, Entropy=0.17692649364471436, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.025051767006516457, KL divergence=0.011853870004415512, Entropy=0.1773754358291626, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0298922099173069, KL divergence=0.012538202106952667, Entropy=0.17674864828586578, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.026800435036420822, KL divergence=0.013724930584430695, Entropy=0.17667925357818604, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.031262192875146866, KL divergence=0.014571833424270153, Entropy=0.17689210176467896, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03207206726074219, KL divergence=0.015608571469783783, Entropy=0.17855514585971832, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.031286392360925674, KL divergence=0.015878405421972275, Entropy=0.17771533131599426, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/523_Step-611462.ckpt']
Uploaded 3 files for checkpoint 523 in 0.56 seconds
saved intermediate frozen graph: current/model/model_523.pb
Best checkpoint number: 473, Last checkpoint number: 521
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'520'}
Training> Name=main_level/agent, Worker=0, Episode=4551, Total reward=333.86, Steps=611648, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4552, Total reward=480.8, Steps=611930, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4553, Total reward=212.9, Steps=612047, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4554, Total reward=28.2, Steps=612059, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4555, Total reward=432.37, Steps=612377, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4556, Total reward=124.42, Steps=612453, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4557, Total reward=467.78, Steps=612740, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4558, Total reward=346.97, Steps=613050, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4559, Total reward=274.96, Steps=613250, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4560, Total reward=471.15, Steps=613543, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4561, Total reward=253.67, Steps=613743, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4562, Total reward=276.9, Steps=613921, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4563, Total reward=187.18, Steps=614085, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4564, Total reward=483.17, Steps=614381, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4565, Total reward=197.85, Steps=614570, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4566, Total reward=114.52, Steps=614669, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4567, Total reward=90.46, Steps=614735, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4568, Total reward=192.41, Steps=614853, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4569, Total reward=29.89, Steps=614872, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4570, Total reward=84.05, Steps=614962, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4571, Total reward=87.93, Steps=615030, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4572, Total reward=338.98, Steps=615230, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4573, Total reward=97.81, Steps=615293, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4574, Total reward=420.9, Steps=615590, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4575, Total reward=388.3, Steps=615882, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4576, Total reward=119.88, Steps=615973, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4577, Total reward=157.51, Steps=616106, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4578, Total reward=130.77, Steps=616205, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4579, Total reward=466.3, Steps=616498, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4580, Total reward=407.66, Steps=616810, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4581, Total reward=91.67, Steps=616856, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4582, Total reward=213.47, Steps=617040, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4583, Total reward=17.28, Steps=617076, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4584, Total reward=484.59, Steps=617370, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4585, Total reward=133.71, Steps=617509, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4586, Total reward=199.79, Steps=617647, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4587, Total reward=255.61, Steps=617796, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4588, Total reward=414.93, Steps=618080, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4589, Total reward=490.12, Steps=618362, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4590, Total reward=124.43, Steps=618461, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4591, Total reward=445.18, Steps=618750, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4592, Total reward=422.07, Steps=619067, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4593, Total reward=91.87, Steps=619128, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4594, Total reward=245.87, Steps=619293, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4595, Total reward=48.11, Steps=619310, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4596, Total reward=496.71, Steps=619589, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4597, Total reward=52.12, Steps=619633, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4598, Total reward=409.19, Steps=619881, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4599, Total reward=475.82, Steps=620177, Training iteration=91
Training> Name=main_level/agent, Worker=0, Episode=4600, Total reward=315.68, Steps=620351, Training iteration=91
Policy training> Surrogate loss=0.0035582040436565876, KL divergence=0.0003962733317166567, Entropy=0.17307572066783905, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.013033364899456501, KL divergence=0.004854164086282253, Entropy=0.1732998788356781, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.01782231032848358, KL divergence=0.008175794966518879, Entropy=0.17249678075313568, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02003982849419117, KL divergence=0.010037793777883053, Entropy=0.17082709074020386, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02449101209640503, KL divergence=0.011334709823131561, Entropy=0.1705532819032669, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02691301703453064, KL divergence=0.012423594482243061, Entropy=0.17000515758991241, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0286251287907362, KL divergence=0.013273511081933975, Entropy=0.1693568229675293, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.026975195854902267, KL divergence=0.013732156716287136, Entropy=0.16901811957359314, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.028200890868902206, KL divergence=0.014835592359304428, Entropy=0.1694738119840622, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.029055945575237274, KL divergence=0.015647700056433678, Entropy=0.16914653778076172, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/524_Step-620351.ckpt']
Uploaded 3 files for checkpoint 524 in 0.54 seconds
saved intermediate frozen graph: current/model/model_524.pb
Best checkpoint number: 473, Last checkpoint number: 522
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'521'}
Training> Name=main_level/agent, Worker=0, Episode=4601, Total reward=155.99, Steps=620459, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4602, Total reward=399.16, Steps=620745, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4603, Total reward=133.69, Steps=620940, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4604, Total reward=467.03, Steps=621232, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4605, Total reward=17.3, Steps=621270, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4606, Total reward=179.76, Steps=621406, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4607, Total reward=402.72, Steps=621707, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4608, Total reward=164.84, Steps=621782, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4609, Total reward=505.04, Steps=622090, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4610, Total reward=107.15, Steps=622174, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4611, Total reward=104.14, Steps=622245, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4612, Total reward=129.21, Steps=622320, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4613, Total reward=29.88, Steps=622330, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4614, Total reward=275.07, Steps=622498, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4615, Total reward=25.44, Steps=622513, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4616, Total reward=436.52, Steps=622797, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4617, Total reward=313.7, Steps=623041, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4618, Total reward=431.62, Steps=623342, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4619, Total reward=249.53, Steps=623534, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4620, Total reward=465.33, Steps=623837, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4621, Total reward=498.33, Steps=624131, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4622, Total reward=455.57, Steps=624403, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4623, Total reward=218.98, Steps=624591, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4624, Total reward=194.67, Steps=624754, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4625, Total reward=381.92, Steps=625062, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4626, Total reward=453.0, Steps=625381, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4627, Total reward=546.6, Steps=625670, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4628, Total reward=414.2, Steps=625949, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4629, Total reward=31.06, Steps=625989, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4630, Total reward=447.29, Steps=626282, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4631, Total reward=143.44, Steps=626372, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4632, Total reward=89.21, Steps=626429, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4633, Total reward=88.33, Steps=626469, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4634, Total reward=175.6, Steps=626560, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4635, Total reward=231.72, Steps=626732, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4636, Total reward=494.55, Steps=627005, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4637, Total reward=393.35, Steps=627278, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4638, Total reward=81.27, Steps=627317, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4639, Total reward=102.72, Steps=627373, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4640, Total reward=221.69, Steps=627558, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4641, Total reward=92.25, Steps=627633, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4642, Total reward=416.22, Steps=627909, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4643, Total reward=463.1, Steps=628209, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4644, Total reward=190.61, Steps=628391, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4645, Total reward=384.17, Steps=628670, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4646, Total reward=480.22, Steps=628946, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4647, Total reward=224.24, Steps=629089, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4648, Total reward=307.58, Steps=629227, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4649, Total reward=510.71, Steps=629504, Training iteration=92
Training> Name=main_level/agent, Worker=0, Episode=4650, Total reward=107.27, Steps=629590, Training iteration=92
Policy training> Surrogate loss=0.00044634027290157974, KL divergence=0.000907945039216429, Entropy=0.18182693421840668, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015472509898245335, KL divergence=0.006973213516175747, Entropy=0.17941398918628693, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02021528035402298, KL divergence=0.010351134464144707, Entropy=0.1785789579153061, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02335726097226143, KL divergence=0.012105264700949192, Entropy=0.17782606184482574, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.025398097932338715, KL divergence=0.0131084518507123, Entropy=0.17794503271579742, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026671655476093292, KL divergence=0.013893908821046352, Entropy=0.1773044317960739, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02783511020243168, KL divergence=0.01460355706512928, Entropy=0.1767064332962036, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03000936657190323, KL divergence=0.015713071450591087, Entropy=0.17683346569538116, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.0305764302611351, KL divergence=0.01646312326192856, Entropy=0.17651833593845367, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0318460650742054, KL divergence=0.017148664221167564, Entropy=0.17648819088935852, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/525_Step-629590.ckpt']
Uploaded 3 files for checkpoint 525 in 0.54 seconds
saved intermediate frozen graph: current/model/model_525.pb
Best checkpoint number: 473, Last checkpoint number: 523
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'522'}
Training> Name=main_level/agent, Worker=0, Episode=4651, Total reward=379.83, Steps=629896, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4652, Total reward=447.45, Steps=630191, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4653, Total reward=79.06, Steps=630232, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4654, Total reward=402.55, Steps=630540, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4655, Total reward=432.8, Steps=630829, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4656, Total reward=141.89, Steps=630918, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4657, Total reward=501.4, Steps=631215, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4658, Total reward=469.48, Steps=631516, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4659, Total reward=283.97, Steps=631691, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4660, Total reward=252.1, Steps=631877, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4661, Total reward=327.34, Steps=632085, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4662, Total reward=513.58, Steps=632383, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4663, Total reward=0.03, Steps=632415, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4664, Total reward=371.48, Steps=632711, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4665, Total reward=245.66, Steps=632865, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4666, Total reward=318.46, Steps=633052, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4667, Total reward=124.09, Steps=633117, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4668, Total reward=217.45, Steps=633257, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4669, Total reward=14.31, Steps=633271, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4670, Total reward=129.7, Steps=633368, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4671, Total reward=478.86, Steps=633654, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4672, Total reward=447.29, Steps=633947, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4673, Total reward=88.58, Steps=633991, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4674, Total reward=480.31, Steps=634278, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4675, Total reward=143.67, Steps=634387, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4676, Total reward=499.3, Steps=634683, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4677, Total reward=7.3, Steps=634696, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4678, Total reward=307.01, Steps=634970, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4679, Total reward=523.25, Steps=635273, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4680, Total reward=93.58, Steps=635329, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4681, Total reward=473.0, Steps=635637, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4682, Total reward=472.09, Steps=635935, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4683, Total reward=188.5, Steps=636146, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4684, Total reward=396.53, Steps=636393, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4685, Total reward=6.62, Steps=636420, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4686, Total reward=77.67, Steps=636473, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4687, Total reward=335.32, Steps=636685, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4688, Total reward=466.98, Steps=636922, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4689, Total reward=164.8, Steps=637021, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4690, Total reward=48.95, Steps=637090, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4691, Total reward=159.73, Steps=637169, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4692, Total reward=321.53, Steps=637331, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4693, Total reward=94.06, Steps=637394, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4694, Total reward=437.03, Steps=637714, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4695, Total reward=400.4, Steps=638026, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4696, Total reward=207.41, Steps=638156, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4697, Total reward=105.93, Steps=638213, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4698, Total reward=426.27, Steps=638489, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4699, Total reward=484.1, Steps=638789, Training iteration=93
Training> Name=main_level/agent, Worker=0, Episode=4700, Total reward=442.08, Steps=639083, Training iteration=93
Policy training> Surrogate loss=0.0009566570515744388, KL divergence=0.0007349621737375855, Entropy=0.17684417963027954, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.011677119880914688, KL divergence=0.006975405849516392, Entropy=0.17626579105854034, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.01896210014820099, KL divergence=0.009808199480175972, Entropy=0.17386287450790405, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.019503219053149223, KL divergence=0.011379769071936607, Entropy=0.17477965354919434, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029990259557962418, KL divergence=0.012542813085019588, Entropy=0.17410553991794586, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026617279276251793, KL divergence=0.01338029932230711, Entropy=0.17378534376621246, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.028365744277834892, KL divergence=0.014481757767498493, Entropy=0.17385804653167725, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02575496770441532, KL divergence=0.0152349341660738, Entropy=0.17443089187145233, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02950623258948326, KL divergence=0.015484473668038845, Entropy=0.17351134121418, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.028485767543315887, KL divergence=0.016296565532684326, Entropy=0.1730497032403946, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/526_Step-639083.ckpt']
Uploaded 3 files for checkpoint 526 in 0.54 seconds
saved intermediate frozen graph: current/model/model_526.pb
Best checkpoint number: 473, Last checkpoint number: 524
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'523'}
Training> Name=main_level/agent, Worker=0, Episode=4701, Total reward=421.29, Steps=639401, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4702, Total reward=160.2, Steps=639519, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4703, Total reward=133.42, Steps=639647, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4704, Total reward=479.05, Steps=639945, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4705, Total reward=463.06, Steps=640213, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4706, Total reward=494.91, Steps=640495, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4707, Total reward=498.6, Steps=640786, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4708, Total reward=155.89, Steps=640862, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4709, Total reward=147.07, Steps=640991, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4710, Total reward=77.52, Steps=641090, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4711, Total reward=443.92, Steps=641369, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4712, Total reward=529.39, Steps=641636, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4713, Total reward=219.59, Steps=641766, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4714, Total reward=161.39, Steps=641889, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4715, Total reward=72.57, Steps=641945, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4716, Total reward=419.27, Steps=642210, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4717, Total reward=354.37, Steps=642466, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4718, Total reward=352.05, Steps=642713, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4719, Total reward=106.04, Steps=642784, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4720, Total reward=240.94, Steps=642984, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4721, Total reward=283.47, Steps=643205, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4722, Total reward=64.4, Steps=643243, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4723, Total reward=77.44, Steps=643342, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4724, Total reward=154.22, Steps=643486, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4725, Total reward=3.53, Steps=643524, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4726, Total reward=433.39, Steps=643827, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4727, Total reward=467.28, Steps=644113, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4728, Total reward=261.81, Steps=644270, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4729, Total reward=353.53, Steps=644491, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4730, Total reward=70.28, Steps=644545, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4731, Total reward=108.36, Steps=644607, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4732, Total reward=440.04, Steps=644877, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4733, Total reward=11.5, Steps=644894, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4734, Total reward=34.79, Steps=644907, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4735, Total reward=511.68, Steps=645193, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4736, Total reward=221.14, Steps=645361, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4737, Total reward=384.19, Steps=645619, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4738, Total reward=389.57, Steps=645868, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4739, Total reward=34.37, Steps=645898, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4740, Total reward=116.0, Steps=645961, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4741, Total reward=268.84, Steps=646189, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4742, Total reward=163.02, Steps=646324, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4743, Total reward=20.5, Steps=646390, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4744, Total reward=9.22, Steps=646413, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4745, Total reward=493.65, Steps=646698, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4746, Total reward=459.13, Steps=646987, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4747, Total reward=490.3, Steps=647268, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4748, Total reward=210.37, Steps=647387, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4749, Total reward=39.23, Steps=647431, Training iteration=94
Training> Name=main_level/agent, Worker=0, Episode=4750, Total reward=452.99, Steps=647744, Training iteration=94
Policy training> Surrogate loss=0.003281512763351202, KL divergence=0.0006974398856982589, Entropy=0.1900019496679306, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.017876356840133667, KL divergence=0.005243922583758831, Entropy=0.18818584084510803, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.022846847772598267, KL divergence=0.009293198585510254, Entropy=0.18722926080226898, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025875724852085114, KL divergence=0.01142423041164875, Entropy=0.18758073449134827, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.025953056290745735, KL divergence=0.01277032122015953, Entropy=0.18738368153572083, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.030310586094856262, KL divergence=0.013763193041086197, Entropy=0.18699929118156433, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.03300796449184418, KL divergence=0.015146154910326004, Entropy=0.18642199039459229, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03315714746713638, KL divergence=0.015912551432847977, Entropy=0.18530569970607758, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03070586547255516, KL divergence=0.016494812443852425, Entropy=0.18580064177513123, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.033867187798023224, KL divergence=0.017263079062104225, Entropy=0.18423832952976227, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/527_Step-647744.ckpt']
Uploaded 3 files for checkpoint 527 in 0.51 seconds
saved intermediate frozen graph: current/model/model_527.pb
Best checkpoint number: 473, Last checkpoint number: 525
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'524'}
Training> Name=main_level/agent, Worker=0, Episode=4751, Total reward=454.78, Steps=648028, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4752, Total reward=7.93, Steps=648040, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4753, Total reward=449.08, Steps=648325, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4754, Total reward=499.16, Steps=648617, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4755, Total reward=149.86, Steps=648721, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4756, Total reward=399.12, Steps=649008, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4757, Total reward=200.13, Steps=649106, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4758, Total reward=404.96, Steps=649371, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4759, Total reward=235.18, Steps=649595, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4760, Total reward=432.42, Steps=649895, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4761, Total reward=369.82, Steps=650117, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4762, Total reward=9.85, Steps=650132, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4763, Total reward=358.4, Steps=650430, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4764, Total reward=105.88, Steps=650561, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4765, Total reward=124.88, Steps=650682, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4766, Total reward=471.14, Steps=650976, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4767, Total reward=194.77, Steps=651112, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4768, Total reward=420.69, Steps=651421, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4769, Total reward=15.45, Steps=651433, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4770, Total reward=430.39, Steps=651728, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4771, Total reward=475.5, Steps=652016, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4772, Total reward=114.77, Steps=652075, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4773, Total reward=241.27, Steps=652185, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4774, Total reward=439.94, Steps=652477, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4775, Total reward=296.92, Steps=652621, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4776, Total reward=422.24, Steps=652910, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4777, Total reward=513.09, Steps=653191, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4778, Total reward=192.11, Steps=653302, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4779, Total reward=87.06, Steps=653369, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4780, Total reward=311.54, Steps=653555, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4781, Total reward=46.21, Steps=653578, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4782, Total reward=476.11, Steps=653863, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4783, Total reward=17.59, Steps=653913, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4784, Total reward=448.69, Steps=654212, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4785, Total reward=84.14, Steps=654332, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4786, Total reward=409.96, Steps=654617, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4787, Total reward=159.51, Steps=654715, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4788, Total reward=217.39, Steps=654848, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4789, Total reward=85.78, Steps=654921, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4790, Total reward=0.01, Steps=654933, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4791, Total reward=372.46, Steps=655124, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4792, Total reward=303.19, Steps=655333, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4793, Total reward=472.76, Steps=655615, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4794, Total reward=387.04, Steps=655854, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4795, Total reward=207.79, Steps=655990, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4796, Total reward=506.79, Steps=656290, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4797, Total reward=160.09, Steps=656420, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4798, Total reward=393.48, Steps=656689, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4799, Total reward=468.86, Steps=656971, Training iteration=95
Training> Name=main_level/agent, Worker=0, Episode=4800, Total reward=105.65, Steps=657046, Training iteration=95
Policy training> Surrogate loss=0.0011168962810188532, KL divergence=0.0008995009120553732, Entropy=0.18198201060295105, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015787817537784576, KL divergence=0.006135362666100264, Entropy=0.1798562854528427, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.017468927428126335, KL divergence=0.00888002384454012, Entropy=0.1799524426460266, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.023773713037371635, KL divergence=0.010068901814520359, Entropy=0.1795739233493805, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.024761619046330452, KL divergence=0.01112950686365366, Entropy=0.17890508472919464, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02778669446706772, KL divergence=0.012037559412419796, Entropy=0.1784450113773346, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.028114838525652885, KL divergence=0.012934667989611626, Entropy=0.17763149738311768, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.028912659734487534, KL divergence=0.013656304217875004, Entropy=0.17732912302017212, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.029372496530413628, KL divergence=0.014230933040380478, Entropy=0.17706958949565887, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.02837783470749855, KL divergence=0.014923825860023499, Entropy=0.17641529440879822, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/528_Step-657046.ckpt']
Uploaded 3 files for checkpoint 528 in 0.61 seconds
saved intermediate frozen graph: current/model/model_528.pb
Best checkpoint number: 473, Last checkpoint number: 526
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'525'}
Training> Name=main_level/agent, Worker=0, Episode=4801, Total reward=435.09, Steps=657337, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4802, Total reward=282.44, Steps=657565, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4803, Total reward=252.77, Steps=657770, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4804, Total reward=6.63, Steps=657782, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4805, Total reward=82.11, Steps=657866, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4806, Total reward=267.29, Steps=658022, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4807, Total reward=434.46, Steps=658326, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4808, Total reward=410.94, Steps=658565, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4809, Total reward=313.05, Steps=658725, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4810, Total reward=314.52, Steps=658934, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4811, Total reward=472.72, Steps=659227, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4812, Total reward=510.38, Steps=659511, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4813, Total reward=336.97, Steps=659772, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4814, Total reward=29.72, Steps=659784, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4815, Total reward=523.6, Steps=660073, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4816, Total reward=480.65, Steps=660378, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4817, Total reward=102.68, Steps=660446, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4818, Total reward=50.12, Steps=660484, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4819, Total reward=38.17, Steps=660510, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4820, Total reward=480.24, Steps=660805, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4821, Total reward=157.55, Steps=660949, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4822, Total reward=53.54, Steps=660974, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4823, Total reward=165.54, Steps=661139, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4824, Total reward=450.06, Steps=661432, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4825, Total reward=517.46, Steps=661704, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4826, Total reward=462.09, Steps=661988, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4827, Total reward=301.81, Steps=662152, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4828, Total reward=510.94, Steps=662435, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4829, Total reward=37.62, Steps=662464, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4830, Total reward=359.37, Steps=662695, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4831, Total reward=132.25, Steps=662761, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4832, Total reward=112.2, Steps=662819, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4833, Total reward=495.22, Steps=663106, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4834, Total reward=484.18, Steps=663417, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4835, Total reward=224.23, Steps=663544, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4836, Total reward=188.48, Steps=663698, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4837, Total reward=491.21, Steps=663998, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4838, Total reward=514.81, Steps=664296, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4839, Total reward=485.09, Steps=664575, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4840, Total reward=439.69, Steps=664849, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4841, Total reward=450.62, Steps=665142, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4842, Total reward=59.42, Steps=665178, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4843, Total reward=24.68, Steps=665215, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4844, Total reward=18.04, Steps=665266, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4845, Total reward=200.57, Steps=665410, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4846, Total reward=166.05, Steps=665560, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4847, Total reward=195.88, Steps=665711, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4848, Total reward=455.73, Steps=666003, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4849, Total reward=175.53, Steps=666113, Training iteration=96
Training> Name=main_level/agent, Worker=0, Episode=4850, Total reward=449.8, Steps=666405, Training iteration=96
Policy training> Surrogate loss=-0.0007800045423209667, KL divergence=0.000581184052862227, Entropy=0.18087691068649292, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.013012085109949112, KL divergence=0.006315415725111961, Entropy=0.17871099710464478, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.018329625949263573, KL divergence=0.009121911600232124, Entropy=0.17775043845176697, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02129724808037281, KL divergence=0.010294703766703606, Entropy=0.17738483846187592, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.024111982434988022, KL divergence=0.011417089961469173, Entropy=0.17767809331417084, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.024027548730373383, KL divergence=0.01243297103792429, Entropy=0.17751449346542358, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02790098451077938, KL divergence=0.0133793530985713, Entropy=0.17746976017951965, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.026411699131131172, KL divergence=0.014134464785456657, Entropy=0.17640531063079834, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03104592300951481, KL divergence=0.015050588175654411, Entropy=0.1761983186006546, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.028526587411761284, KL divergence=0.0158027783036232, Entropy=0.17619475722312927, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/529_Step-666405.ckpt']
Uploaded 3 files for checkpoint 529 in 0.62 seconds
saved intermediate frozen graph: current/model/model_529.pb
Best checkpoint number: 473, Last checkpoint number: 527
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'526'}
Training> Name=main_level/agent, Worker=0, Episode=4851, Total reward=138.72, Steps=666501, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4852, Total reward=89.93, Steps=666555, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4853, Total reward=307.79, Steps=666737, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4854, Total reward=239.16, Steps=666865, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4855, Total reward=358.91, Steps=667147, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4856, Total reward=491.05, Steps=667434, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4857, Total reward=196.04, Steps=667538, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4858, Total reward=306.64, Steps=667775, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4859, Total reward=24.87, Steps=667803, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4860, Total reward=454.65, Steps=668099, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4861, Total reward=248.58, Steps=668322, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4862, Total reward=439.13, Steps=668607, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4863, Total reward=492.77, Steps=668891, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4864, Total reward=373.39, Steps=669119, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4865, Total reward=446.26, Steps=669415, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4866, Total reward=7.03, Steps=669429, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4867, Total reward=494.08, Steps=669713, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4868, Total reward=159.21, Steps=669781, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4869, Total reward=26.68, Steps=669801, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4870, Total reward=40.35, Steps=669889, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4871, Total reward=69.28, Steps=669937, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4872, Total reward=19.28, Steps=669950, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4873, Total reward=146.97, Steps=670016, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4874, Total reward=252.82, Steps=670142, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4875, Total reward=239.52, Steps=670270, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4876, Total reward=187.36, Steps=670409, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4877, Total reward=485.94, Steps=670707, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4878, Total reward=288.41, Steps=670923, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4879, Total reward=89.67, Steps=671002, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4880, Total reward=109.5, Steps=671106, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4881, Total reward=89.17, Steps=671171, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4882, Total reward=485.64, Steps=671466, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4883, Total reward=20.82, Steps=671527, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4884, Total reward=14.56, Steps=671569, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4885, Total reward=101.67, Steps=671669, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4886, Total reward=126.83, Steps=671816, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4887, Total reward=370.66, Steps=672021, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4888, Total reward=194.21, Steps=672139, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4889, Total reward=187.72, Steps=672253, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4890, Total reward=151.34, Steps=672362, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4891, Total reward=205.76, Steps=672457, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4892, Total reward=16.81, Steps=672469, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4893, Total reward=97.8, Steps=672538, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4894, Total reward=57.94, Steps=672565, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4895, Total reward=352.16, Steps=672795, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4896, Total reward=230.18, Steps=672940, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4897, Total reward=43.72, Steps=672981, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4898, Total reward=78.46, Steps=673016, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4899, Total reward=115.37, Steps=673128, Training iteration=97
Training> Name=main_level/agent, Worker=0, Episode=4900, Total reward=209.45, Steps=673285, Training iteration=97
Policy training> Surrogate loss=0.002983691403642297, KL divergence=0.0004279922286514193, Entropy=0.1898721158504486, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.02002173662185669, KL divergence=0.004839418455958366, Entropy=0.18946562707424164, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02230888418853283, KL divergence=0.009463424794375896, Entropy=0.18764139711856842, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02514692023396492, KL divergence=0.011814185418188572, Entropy=0.18599510192871094, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02809971384704113, KL divergence=0.01347091980278492, Entropy=0.18539579212665558, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026580482721328735, KL divergence=0.014629422686994076, Entropy=0.18424518406391144, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.030062062665820122, KL divergence=0.015795256942510605, Entropy=0.18511918187141418, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.029875269159674644, KL divergence=0.016631407663226128, Entropy=0.18408097326755524, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.033870209008455276, KL divergence=0.01760241389274597, Entropy=0.18301145732402802, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03616391867399216, KL divergence=0.018575327470898628, Entropy=0.1835462749004364, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/530_Step-673285.ckpt']
Uploaded 3 files for checkpoint 530 in 0.57 seconds
saved intermediate frozen graph: current/model/model_530.pb
Best checkpoint number: 473, Last checkpoint number: 528
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'527'}
Training> Name=main_level/agent, Worker=0, Episode=4901, Total reward=189.9, Steps=673436, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4902, Total reward=111.77, Steps=673561, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4903, Total reward=159.01, Steps=673728, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4904, Total reward=13.8, Steps=673779, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4905, Total reward=460.23, Steps=674072, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4906, Total reward=493.43, Steps=674353, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4907, Total reward=216.86, Steps=674478, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4908, Total reward=234.35, Steps=674601, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4909, Total reward=450.29, Steps=674906, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4910, Total reward=475.93, Steps=675198, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4911, Total reward=179.79, Steps=675314, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4912, Total reward=55.7, Steps=675341, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4913, Total reward=90.36, Steps=675382, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4914, Total reward=274.94, Steps=675539, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4915, Total reward=408.27, Steps=675808, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4916, Total reward=453.81, Steps=676100, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4917, Total reward=162.31, Steps=676217, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4918, Total reward=394.47, Steps=676473, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4919, Total reward=114.62, Steps=676562, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4920, Total reward=475.97, Steps=676870, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4921, Total reward=106.79, Steps=676943, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4922, Total reward=429.21, Steps=677245, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4923, Total reward=254.08, Steps=677459, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4924, Total reward=147.21, Steps=677598, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4925, Total reward=3.48, Steps=677624, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4926, Total reward=410.98, Steps=677854, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4927, Total reward=421.74, Steps=678136, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4928, Total reward=357.86, Steps=678297, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4929, Total reward=450.33, Steps=678584, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4930, Total reward=90.71, Steps=678674, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4931, Total reward=75.77, Steps=678711, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4932, Total reward=438.54, Steps=679004, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4933, Total reward=87.13, Steps=679044, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4934, Total reward=502.96, Steps=679330, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4935, Total reward=152.53, Steps=679429, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4936, Total reward=491.88, Steps=679718, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4937, Total reward=205.14, Steps=679877, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4938, Total reward=293.1, Steps=680089, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4939, Total reward=285.18, Steps=680334, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4940, Total reward=485.36, Steps=680635, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4941, Total reward=204.66, Steps=680797, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4942, Total reward=444.92, Steps=681076, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4943, Total reward=132.87, Steps=681221, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4944, Total reward=421.67, Steps=681510, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4945, Total reward=145.05, Steps=681633, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4946, Total reward=88.99, Steps=681703, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4947, Total reward=209.72, Steps=681863, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4948, Total reward=102.31, Steps=681929, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4949, Total reward=76.47, Steps=681999, Training iteration=98
Training> Name=main_level/agent, Worker=0, Episode=4950, Total reward=487.15, Steps=682289, Training iteration=98
Policy training> Surrogate loss=0.0039380514062941074, KL divergence=0.0008091179188340902, Entropy=0.18436813354492188, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.018097082152962685, KL divergence=0.005456906743347645, Entropy=0.1818815916776657, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.018742775544524193, KL divergence=0.009201238863170147, Entropy=0.18179365992546082, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.026315229013562202, KL divergence=0.011294187046587467, Entropy=0.18101011216640472, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.03185843303799629, KL divergence=0.012796909548342228, Entropy=0.18052929639816284, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03398742526769638, KL divergence=0.01402535755187273, Entropy=0.18068945407867432, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0344393290579319, KL divergence=0.015301159583032131, Entropy=0.1809145212173462, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.031863752752542496, KL divergence=0.01652674935758114, Entropy=0.18053826689720154, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03383832052350044, KL divergence=0.017437055706977844, Entropy=0.18060113489627838, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.032885968685150146, KL divergence=0.018226787447929382, Entropy=0.18026115000247955, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/531_Step-682289.ckpt']
Uploaded 3 files for checkpoint 531 in 0.53 seconds
saved intermediate frozen graph: current/model/model_531.pb
Best checkpoint number: 473, Last checkpoint number: 529
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'528'}
Training> Name=main_level/agent, Worker=0, Episode=4951, Total reward=150.24, Steps=682385, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4952, Total reward=124.82, Steps=682463, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4953, Total reward=29.9, Steps=682474, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4954, Total reward=273.03, Steps=682656, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4955, Total reward=38.22, Steps=682674, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4956, Total reward=155.54, Steps=682765, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4957, Total reward=188.17, Steps=682905, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4958, Total reward=310.79, Steps=683160, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4959, Total reward=445.32, Steps=683463, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4960, Total reward=308.78, Steps=683684, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4961, Total reward=89.79, Steps=683771, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4962, Total reward=339.8, Steps=684009, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4963, Total reward=173.68, Steps=684206, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4964, Total reward=14.56, Steps=684244, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4965, Total reward=210.76, Steps=684408, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4966, Total reward=215.29, Steps=684532, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4967, Total reward=523.62, Steps=684812, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4968, Total reward=344.61, Steps=684987, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4969, Total reward=152.18, Steps=685116, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4970, Total reward=62.51, Steps=685174, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4971, Total reward=87.38, Steps=685241, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4972, Total reward=9.73, Steps=685254, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4973, Total reward=507.61, Steps=685542, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4974, Total reward=417.27, Steps=685833, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4975, Total reward=149.98, Steps=685969, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4976, Total reward=212.45, Steps=686133, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4977, Total reward=450.1, Steps=686431, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4978, Total reward=481.29, Steps=686714, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4979, Total reward=488.89, Steps=687005, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4980, Total reward=98.32, Steps=687069, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4981, Total reward=543.64, Steps=687348, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4982, Total reward=444.78, Steps=687650, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4983, Total reward=21.61, Steps=687704, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4984, Total reward=298.77, Steps=687903, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4985, Total reward=109.51, Steps=687987, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4986, Total reward=197.56, Steps=688139, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4987, Total reward=143.69, Steps=688234, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4988, Total reward=476.15, Steps=688507, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4989, Total reward=512.19, Steps=688798, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4990, Total reward=124.68, Steps=688906, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4991, Total reward=67.95, Steps=688944, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4992, Total reward=65.63, Steps=688990, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4993, Total reward=224.9, Steps=689129, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4994, Total reward=441.32, Steps=689407, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4995, Total reward=458.33, Steps=689695, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4996, Total reward=490.96, Steps=689987, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4997, Total reward=276.39, Steps=690181, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4998, Total reward=158.38, Steps=690268, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=4999, Total reward=464.22, Steps=690557, Training iteration=99
Training> Name=main_level/agent, Worker=0, Episode=5000, Total reward=430.21, Steps=690856, Training iteration=99
Policy training> Surrogate loss=0.0006299747619777918, KL divergence=0.0005435951752588153, Entropy=0.18699558079242706, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015290601179003716, KL divergence=0.00662653986364603, Entropy=0.18628107011318207, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.023888492956757545, KL divergence=0.009507318958640099, Entropy=0.18279054760932922, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02302142046391964, KL divergence=0.011615512892603874, Entropy=0.18287968635559082, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02506323903799057, KL divergence=0.013686610385775566, Entropy=0.18115302920341492, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.03232564777135849, KL divergence=0.014983117580413818, Entropy=0.18147407472133636, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02762036770582199, KL divergence=0.015929166227579117, Entropy=0.18114450573921204, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.03209075704216957, KL divergence=0.017405714839696884, Entropy=0.1808091402053833, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.033300552517175674, KL divergence=0.018120678141713142, Entropy=0.18113356828689575, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0308587197214365, KL divergence=0.018939506262540817, Entropy=0.17935660481452942, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/532_Step-690856.ckpt']
Uploaded 3 files for checkpoint 532 in 0.56 seconds
saved intermediate frozen graph: current/model/model_532.pb
Best checkpoint number: 473, Last checkpoint number: 530
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'529'}
Training> Name=main_level/agent, Worker=0, Episode=5001, Total reward=199.13, Steps=690994, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5002, Total reward=318.05, Steps=691200, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5003, Total reward=24.17, Steps=691249, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5004, Total reward=517.01, Steps=691532, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5005, Total reward=474.18, Steps=691816, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5006, Total reward=450.04, Steps=692067, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5007, Total reward=88.08, Steps=692119, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5008, Total reward=98.88, Steps=692157, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5009, Total reward=189.41, Steps=692281, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5010, Total reward=162.39, Steps=692359, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5011, Total reward=476.58, Steps=692644, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5012, Total reward=431.7, Steps=692925, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5013, Total reward=448.18, Steps=693214, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5014, Total reward=285.22, Steps=693396, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5015, Total reward=261.7, Steps=693582, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5016, Total reward=431.04, Steps=693827, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5017, Total reward=145.85, Steps=693924, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5018, Total reward=488.78, Steps=694222, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5019, Total reward=115.33, Steps=694316, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5020, Total reward=213.03, Steps=694519, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5021, Total reward=61.07, Steps=694560, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5022, Total reward=393.5, Steps=694832, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5023, Total reward=19.73, Steps=694850, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5024, Total reward=470.4, Steps=695136, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5025, Total reward=297.03, Steps=695348, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5026, Total reward=390.98, Steps=695651, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5027, Total reward=91.58, Steps=695705, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5028, Total reward=384.41, Steps=695994, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5029, Total reward=23.8, Steps=696010, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5030, Total reward=384.26, Steps=696212, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5031, Total reward=363.2, Steps=696416, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5032, Total reward=445.77, Steps=696707, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5033, Total reward=64.73, Steps=696743, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5034, Total reward=473.51, Steps=697021, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5035, Total reward=238.27, Steps=697174, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5036, Total reward=426.71, Steps=697466, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5037, Total reward=443.1, Steps=697752, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5038, Total reward=76.19, Steps=697788, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5039, Total reward=24.19, Steps=697814, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5040, Total reward=464.14, Steps=698095, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5041, Total reward=275.68, Steps=698264, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5042, Total reward=52.27, Steps=698289, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5043, Total reward=285.56, Steps=698497, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5044, Total reward=326.63, Steps=698716, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5045, Total reward=2.91, Steps=698732, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5046, Total reward=495.93, Steps=698995, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5047, Total reward=430.71, Steps=699290, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5048, Total reward=411.68, Steps=699468, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5049, Total reward=21.24, Steps=699515, Training iteration=100
Training> Name=main_level/agent, Worker=0, Episode=5050, Total reward=432.51, Steps=699829, Training iteration=100
Policy training> Surrogate loss=-0.0010741922305896878, KL divergence=0.0008933048811741173, Entropy=0.17853401601314545, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.013732198625802994, KL divergence=0.0061614178121089935, Entropy=0.17736339569091797, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.02276156097650528, KL divergence=0.008990928530693054, Entropy=0.17897243797779083, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.025838613510131836, KL divergence=0.011536233127117157, Entropy=0.17915983498096466, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.028856106102466583, KL divergence=0.012980443425476551, Entropy=0.17883248627185822, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.0261517483741045, KL divergence=0.014067469164729118, Entropy=0.17816677689552307, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.032782722264528275, KL divergence=0.014932195656001568, Entropy=0.17776475846767426, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02985512837767601, KL divergence=0.01583443395793438, Entropy=0.17879091203212738, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03549954295158386, KL divergence=0.01661294884979725, Entropy=0.1769191324710846, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.033467959612607956, KL divergence=0.017266109585762024, Entropy=0.1767919659614563, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/533_Step-699829.ckpt']
Uploaded 3 files for checkpoint 533 in 0.61 seconds
saved intermediate frozen graph: current/model/model_533.pb
Best checkpoint number: 473, Last checkpoint number: 531
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'530'}
Training> Name=main_level/agent, Worker=0, Episode=5051, Total reward=420.19, Steps=700120, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5052, Total reward=313.57, Steps=700304, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5053, Total reward=79.91, Steps=700345, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5054, Total reward=35.03, Steps=700357, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5055, Total reward=355.35, Steps=700588, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5056, Total reward=327.48, Steps=700853, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5057, Total reward=399.34, Steps=701124, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5058, Total reward=506.53, Steps=701415, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5059, Total reward=377.19, Steps=701738, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5060, Total reward=242.84, Steps=701949, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5061, Total reward=495.74, Steps=702235, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5062, Total reward=290.55, Steps=702430, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5063, Total reward=433.72, Steps=702716, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5064, Total reward=253.19, Steps=702916, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5065, Total reward=465.96, Steps=703195, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5066, Total reward=112.59, Steps=703262, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5067, Total reward=435.54, Steps=703529, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5068, Total reward=383.71, Steps=703738, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5069, Total reward=88.08, Steps=703804, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5070, Total reward=70.03, Steps=703858, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5071, Total reward=73.94, Steps=703910, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5072, Total reward=81.9, Steps=703964, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5073, Total reward=91.47, Steps=704005, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5074, Total reward=487.91, Steps=704310, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5075, Total reward=412.89, Steps=704621, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5076, Total reward=18.96, Steps=704637, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5077, Total reward=427.59, Steps=704954, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5078, Total reward=244.25, Steps=705131, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5079, Total reward=211.97, Steps=705308, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5080, Total reward=292.91, Steps=705532, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5081, Total reward=197.25, Steps=705711, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5082, Total reward=495.14, Steps=706020, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5083, Total reward=3.05, Steps=706047, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5084, Total reward=30.11, Steps=706118, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5085, Total reward=413.14, Steps=706413, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5086, Total reward=233.83, Steps=706552, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5087, Total reward=414.89, Steps=706808, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5088, Total reward=209.26, Steps=706922, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5089, Total reward=244.51, Steps=707090, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5090, Total reward=64.94, Steps=707139, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5091, Total reward=144.1, Steps=707205, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5092, Total reward=467.52, Steps=707504, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5093, Total reward=264.95, Steps=707726, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5094, Total reward=454.07, Steps=708019, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5095, Total reward=117.37, Steps=708102, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5096, Total reward=455.9, Steps=708400, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5097, Total reward=76.76, Steps=708455, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5098, Total reward=342.19, Steps=708683, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5099, Total reward=320.14, Steps=708947, Training iteration=101
Training> Name=main_level/agent, Worker=0, Episode=5100, Total reward=114.14, Steps=709023, Training iteration=101
Policy training> Surrogate loss=0.0025446838699281216, KL divergence=0.0010506974067538977, Entropy=0.17543455958366394, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01366224605590105, KL divergence=0.006746888160705566, Entropy=0.17245090007781982, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.018544882535934448, KL divergence=0.010234293527901173, Entropy=0.17251716554164886, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.022422628477215767, KL divergence=0.011456101201474667, Entropy=0.17257152497768402, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.025096358731389046, KL divergence=0.012488218024373055, Entropy=0.17098170518875122, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02500598132610321, KL divergence=0.013540764339268208, Entropy=0.17145338654518127, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.0251289252191782, KL divergence=0.014441268518567085, Entropy=0.1712479591369629, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.032681070268154144, KL divergence=0.014840923249721527, Entropy=0.17080236971378326, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.023003019392490387, KL divergence=0.015861298888921738, Entropy=0.17039722204208374, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.0319553017616272, KL divergence=0.016382671892642975, Entropy=0.16847288608551025, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/534_Step-709023.ckpt']
Uploaded 3 files for checkpoint 534 in 0.62 seconds
saved intermediate frozen graph: current/model/model_534.pb
Best checkpoint number: 473, Last checkpoint number: 532
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'531'}
Training> Name=main_level/agent, Worker=0, Episode=5101, Total reward=295.47, Steps=709228, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5102, Total reward=428.77, Steps=709518, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5103, Total reward=6.61, Steps=709540, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5104, Total reward=504.78, Steps=709814, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5105, Total reward=192.52, Steps=709955, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5106, Total reward=381.9, Steps=710261, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5107, Total reward=195.53, Steps=710390, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5108, Total reward=189.41, Steps=710499, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5109, Total reward=32.62, Steps=710539, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5110, Total reward=422.03, Steps=710827, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5111, Total reward=392.79, Steps=711049, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5112, Total reward=456.92, Steps=711322, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5113, Total reward=123.15, Steps=711391, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5114, Total reward=419.82, Steps=711689, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5115, Total reward=400.9, Steps=711991, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5116, Total reward=218.62, Steps=712109, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5117, Total reward=75.95, Steps=712142, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5118, Total reward=172.55, Steps=712241, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5119, Total reward=95.86, Steps=712295, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5120, Total reward=391.67, Steps=712580, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5121, Total reward=111.0, Steps=712636, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5122, Total reward=445.95, Steps=712910, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5123, Total reward=169.33, Steps=713077, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5124, Total reward=148.76, Steps=713224, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5125, Total reward=459.97, Steps=713510, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5126, Total reward=7.05, Steps=713522, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5127, Total reward=215.07, Steps=713646, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5128, Total reward=234.06, Steps=713778, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5129, Total reward=467.01, Steps=714080, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5130, Total reward=362.09, Steps=714267, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5131, Total reward=155.92, Steps=714356, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5132, Total reward=482.35, Steps=714673, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5133, Total reward=383.81, Steps=714935, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5134, Total reward=197.91, Steps=715035, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5135, Total reward=416.29, Steps=715310, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5136, Total reward=348.75, Steps=715581, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5137, Total reward=171.2, Steps=715672, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5138, Total reward=367.74, Steps=715901, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5139, Total reward=496.4, Steps=716206, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5140, Total reward=310.05, Steps=716477, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5141, Total reward=325.29, Steps=716701, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5142, Total reward=496.93, Steps=716980, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5143, Total reward=484.4, Steps=717284, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5144, Total reward=371.61, Steps=717596, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5145, Total reward=418.82, Steps=717902, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5146, Total reward=98.67, Steps=717965, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5147, Total reward=4.85, Steps=717981, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5148, Total reward=261.02, Steps=718162, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5149, Total reward=112.87, Steps=718218, Training iteration=102
Training> Name=main_level/agent, Worker=0, Episode=5150, Total reward=43.09, Steps=718258, Training iteration=102
Policy training> Surrogate loss=-0.0004123348044231534, KL divergence=0.001036362024024129, Entropy=0.18456429243087769, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01701205037534237, KL divergence=0.007777134422212839, Entropy=0.18116244673728943, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.022066878154873848, KL divergence=0.010504435747861862, Entropy=0.18068645894527435, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.024675754830241203, KL divergence=0.01229853555560112, Entropy=0.18052920699119568, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026173017919063568, KL divergence=0.013451638631522655, Entropy=0.18011757731437683, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.02826084941625595, KL divergence=0.014573297463357449, Entropy=0.18007928133010864, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02891366370022297, KL divergence=0.015567655675113201, Entropy=0.17989584803581238, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.030545979738235474, KL divergence=0.016673816367983818, Entropy=0.179836705327034, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03112529031932354, KL divergence=0.01725587248802185, Entropy=0.1795908808708191, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.031924568116664886, KL divergence=0.017813032492995262, Entropy=0.1794481873512268, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/535_Step-718258.ckpt']
Uploaded 3 files for checkpoint 535 in 0.51 seconds
saved intermediate frozen graph: current/model/model_535.pb
Best checkpoint number: 473, Last checkpoint number: 533
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'532'}
Training> Name=main_level/agent, Worker=0, Episode=5151, Total reward=438.75, Steps=718564, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5152, Total reward=351.47, Steps=718783, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5153, Total reward=482.25, Steps=719076, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5154, Total reward=270.73, Steps=719248, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5155, Total reward=450.81, Steps=719537, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5156, Total reward=480.74, Steps=719816, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5157, Total reward=335.06, Steps=720117, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5158, Total reward=523.82, Steps=720377, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5159, Total reward=41.48, Steps=720406, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5160, Total reward=103.45, Steps=720455, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5161, Total reward=214.26, Steps=720626, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5162, Total reward=75.77, Steps=720722, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5163, Total reward=418.02, Steps=721030, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5164, Total reward=181.62, Steps=721158, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5165, Total reward=405.07, Steps=721470, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5166, Total reward=441.05, Steps=721741, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5167, Total reward=223.17, Steps=721873, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5168, Total reward=178.05, Steps=721981, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5169, Total reward=39.56, Steps=722017, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5170, Total reward=153.19, Steps=722118, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5171, Total reward=42.55, Steps=722144, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5172, Total reward=111.14, Steps=722226, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5173, Total reward=527.52, Steps=722499, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5174, Total reward=103.43, Steps=722573, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5175, Total reward=402.36, Steps=722852, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5176, Total reward=225.59, Steps=723008, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5177, Total reward=410.48, Steps=723322, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5178, Total reward=321.11, Steps=723555, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5179, Total reward=325.5, Steps=723786, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5180, Total reward=3.83, Steps=723809, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5181, Total reward=419.98, Steps=724111, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5182, Total reward=315.02, Steps=724322, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5183, Total reward=17.1, Steps=724334, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5184, Total reward=134.95, Steps=724482, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5185, Total reward=169.27, Steps=724609, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5186, Total reward=472.05, Steps=724910, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5187, Total reward=384.97, Steps=725211, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5188, Total reward=156.92, Steps=725293, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5189, Total reward=148.84, Steps=725398, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5190, Total reward=136.69, Steps=725491, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5191, Total reward=477.2, Steps=725778, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5192, Total reward=14.77, Steps=725804, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5193, Total reward=495.93, Steps=726098, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5194, Total reward=429.91, Steps=726401, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5195, Total reward=232.3, Steps=726500, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5196, Total reward=175.23, Steps=726661, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5197, Total reward=13.42, Steps=726676, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5198, Total reward=439.94, Steps=726973, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5199, Total reward=118.92, Steps=727091, Training iteration=103
Training> Name=main_level/agent, Worker=0, Episode=5200, Total reward=453.96, Steps=727379, Training iteration=103
Policy training> Surrogate loss=0.0010654836660251021, KL divergence=0.000436168338637799, Entropy=0.18139411509037018, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.014877267181873322, KL divergence=0.0056654103100299835, Entropy=0.18110813200473785, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.018357113003730774, KL divergence=0.008566862903535366, Entropy=0.1801874190568924, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.022155145183205605, KL divergence=0.010018664412200451, Entropy=0.1805092692375183, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.029685283079743385, KL divergence=0.011553197167813778, Entropy=0.17913848161697388, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026107707992196083, KL divergence=0.012651948258280754, Entropy=0.18050143122673035, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.029733041301369667, KL divergence=0.013688643462955952, Entropy=0.1792321801185608, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02740796096622944, KL divergence=0.014670424163341522, Entropy=0.18028831481933594, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.02819853276014328, KL divergence=0.015309179201722145, Entropy=0.17844124138355255, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03186296671628952, KL divergence=0.01586635783314705, Entropy=0.177824005484581, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/536_Step-727379.ckpt']
Uploaded 3 files for checkpoint 536 in 0.53 seconds
saved intermediate frozen graph: current/model/model_536.pb
Best checkpoint number: 473, Last checkpoint number: 534
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'533'}
Training> Name=main_level/agent, Worker=0, Episode=5201, Total reward=71.23, Steps=727405, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5202, Total reward=479.18, Steps=727697, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5203, Total reward=239.82, Steps=727879, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5204, Total reward=472.96, Steps=728155, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5205, Total reward=101.92, Steps=728247, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5206, Total reward=519.11, Steps=728522, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5207, Total reward=111.41, Steps=728608, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5208, Total reward=505.27, Steps=728900, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5209, Total reward=38.6, Steps=728919, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5210, Total reward=26.25, Steps=728955, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5211, Total reward=478.57, Steps=729247, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5212, Total reward=121.05, Steps=729332, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5213, Total reward=485.34, Steps=729630, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5214, Total reward=55.94, Steps=729659, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5215, Total reward=140.29, Steps=729731, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5216, Total reward=458.34, Steps=730009, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5217, Total reward=86.93, Steps=730063, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5218, Total reward=478.88, Steps=730360, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5219, Total reward=200.14, Steps=730510, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5220, Total reward=260.35, Steps=730719, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5221, Total reward=103.08, Steps=730765, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5222, Total reward=441.58, Steps=731033, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5223, Total reward=420.89, Steps=731331, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5224, Total reward=398.78, Steps=731584, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5225, Total reward=368.31, Steps=731853, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5226, Total reward=460.02, Steps=732117, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5227, Total reward=204.57, Steps=732229, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5228, Total reward=174.03, Steps=732336, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5229, Total reward=502.52, Steps=732626, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5230, Total reward=498.64, Steps=732907, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5231, Total reward=475.24, Steps=733182, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5232, Total reward=345.47, Steps=733369, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5233, Total reward=93.78, Steps=733411, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5234, Total reward=60.64, Steps=733471, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5235, Total reward=27.52, Steps=733488, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5236, Total reward=127.82, Steps=733559, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5237, Total reward=150.58, Steps=733640, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5238, Total reward=457.86, Steps=733941, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5239, Total reward=498.02, Steps=734230, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5240, Total reward=80.36, Steps=734289, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5241, Total reward=314.4, Steps=734506, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5242, Total reward=444.79, Steps=734803, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5243, Total reward=11.17, Steps=734822, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5244, Total reward=413.36, Steps=735116, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5245, Total reward=439.46, Steps=735418, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5246, Total reward=420.65, Steps=735711, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5247, Total reward=511.83, Steps=735992, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5248, Total reward=238.71, Steps=736128, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5249, Total reward=465.0, Steps=736410, Training iteration=104
Training> Name=main_level/agent, Worker=0, Episode=5250, Total reward=512.67, Steps=736706, Training iteration=104
Policy training> Surrogate loss=-0.00020875449990853667, KL divergence=0.0007836743025109172, Entropy=0.1784430295228958, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.015215075574815273, KL divergence=0.007602771744132042, Entropy=0.17714136838912964, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.021474987268447876, KL divergence=0.010454089380800724, Entropy=0.17669542133808136, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.02681785821914673, KL divergence=0.011882499791681767, Entropy=0.17599885165691376, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.026950333267450333, KL divergence=0.012940529733896255, Entropy=0.17606814205646515, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.029021378606557846, KL divergence=0.013869875110685825, Entropy=0.17559725046157837, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.030513204634189606, KL divergence=0.014649320393800735, Entropy=0.17549943923950195, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.031748101115226746, KL divergence=0.01547098346054554, Entropy=0.1753135770559311, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.03179351985454559, KL divergence=0.0162022914737463, Entropy=0.17480312287807465, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.03404359892010689, KL divergence=0.016925323754549026, Entropy=0.1748620867729187, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/537_Step-736706.ckpt']
Uploaded 3 files for checkpoint 537 in 0.49 seconds
saved intermediate frozen graph: current/model/model_537.pb
Best checkpoint number: 473, Last checkpoint number: 535
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'534'}
Training> Name=main_level/agent, Worker=0, Episode=5251, Total reward=379.37, Steps=736915, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5252, Total reward=9.22, Steps=736927, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5253, Total reward=59.84, Steps=736955, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5254, Total reward=154.91, Steps=737052, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5255, Total reward=104.9, Steps=737136, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5256, Total reward=503.07, Steps=737414, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5257, Total reward=281.46, Steps=737625, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5258, Total reward=142.52, Steps=737713, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5259, Total reward=450.24, Steps=738008, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5260, Total reward=129.42, Steps=738114, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5261, Total reward=344.28, Steps=738340, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5262, Total reward=54.59, Steps=738400, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5263, Total reward=430.51, Steps=738680, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5264, Total reward=441.0, Steps=738969, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5265, Total reward=159.91, Steps=739092, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5266, Total reward=322.38, Steps=739302, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5267, Total reward=239.59, Steps=739446, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5268, Total reward=205.62, Steps=739586, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5269, Total reward=20.61, Steps=739603, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5270, Total reward=430.94, Steps=739897, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5271, Total reward=150.65, Steps=739983, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5272, Total reward=442.99, Steps=740281, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5273, Total reward=60.36, Steps=740323, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5274, Total reward=379.73, Steps=740620, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5275, Total reward=468.89, Steps=740902, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5276, Total reward=16.11, Steps=740923, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5277, Total reward=202.69, Steps=741058, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5278, Total reward=74.81, Steps=741097, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5279, Total reward=273.78, Steps=741299, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5280, Total reward=474.33, Steps=741588, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5281, Total reward=459.36, Steps=741870, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5282, Total reward=281.11, Steps=742067, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5283, Total reward=451.16, Steps=742333, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5284, Total reward=237.71, Steps=742480, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5285, Total reward=496.22, Steps=742775, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5286, Total reward=96.68, Steps=742856, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5287, Total reward=467.31, Steps=743142, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5288, Total reward=396.61, Steps=743442, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5289, Total reward=92.15, Steps=743508, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5290, Total reward=99.54, Steps=743585, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5291, Total reward=126.16, Steps=743654, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5292, Total reward=74.93, Steps=743697, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5293, Total reward=68.48, Steps=743726, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5294, Total reward=34.31, Steps=743738, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5295, Total reward=168.48, Steps=743831, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5296, Total reward=19.64, Steps=743850, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5297, Total reward=297.94, Steps=744094, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5298, Total reward=334.7, Steps=744339, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5299, Total reward=329.9, Steps=744590, Training iteration=105
Training> Name=main_level/agent, Worker=0, Episode=5300, Total reward=190.87, Steps=744792, Training iteration=105
Policy training> Surrogate loss=-0.0025184601545333862, KL divergence=0.0007846608059480786, Entropy=0.1819155365228653, training epoch=0, learning_rate=1e-05
Policy training> Surrogate loss=-0.01270783320069313, KL divergence=0.0057807122357189655, Entropy=0.17956988513469696, training epoch=1, learning_rate=1e-05
Policy training> Surrogate loss=-0.020305730402469635, KL divergence=0.00829088594764471, Entropy=0.1794273406267166, training epoch=2, learning_rate=1e-05
Policy training> Surrogate loss=-0.021034646779298782, KL divergence=0.010640053078532219, Entropy=0.17860285937786102, training epoch=3, learning_rate=1e-05
Policy training> Surrogate loss=-0.02568643167614937, KL divergence=0.01240520179271698, Entropy=0.17943240702152252, training epoch=4, learning_rate=1e-05
Policy training> Surrogate loss=-0.026843391358852386, KL divergence=0.013701844029128551, Entropy=0.17851977050304413, training epoch=5, learning_rate=1e-05
Policy training> Surrogate loss=-0.02912127412855625, KL divergence=0.014630424790084362, Entropy=0.17703361809253693, training epoch=6, learning_rate=1e-05
Policy training> Surrogate loss=-0.02937830425798893, KL divergence=0.015484257601201534, Entropy=0.1766822785139084, training epoch=7, learning_rate=1e-05
Policy training> Surrogate loss=-0.029483312740921974, KL divergence=0.01620384119451046, Entropy=0.17655307054519653, training epoch=8, learning_rate=1e-05
Policy training> Surrogate loss=-0.032505083829164505, KL divergence=0.016521945595741272, Entropy=0.17471042275428772, training epoch=9, learning_rate=1e-05
Checkpoint> Saving in path=['./checkpoint/538_Step-744792.ckpt']
Uploaded 3 files for checkpoint 538 in 0.57 seconds
saved intermediate frozen graph: current/model/model_538.pb
Best checkpoint number: 473, Last checkpoint number: 536
Copying the frozen checkpoint from ./frozen_models/agent/model_473.pb to /opt/ml/model/agent/model.pb.
Deleting the frozen models in s3 for the iterations: {'535'}
Training> Name=main_level/agent, Worker=0, Episode=5301, Total reward=523.61, Steps=745064, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5302, Total reward=257.97, Steps=745257, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5303, Total reward=144.15, Steps=745392, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5304, Total reward=201.27, Steps=745563, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5305, Total reward=426.58, Steps=745861, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5306, Total reward=229.23, Steps=746009, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5307, Total reward=406.07, Steps=746258, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5308, Total reward=98.2, Steps=746294, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5309, Total reward=154.14, Steps=746420, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5310, Total reward=425.35, Steps=746724, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5311, Total reward=384.33, Steps=747021, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5312, Total reward=104.96, Steps=747063, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5313, Total reward=7.69, Steps=747079, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5314, Total reward=401.21, Steps=747373, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5315, Total reward=511.15, Steps=747661, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5316, Total reward=24.66, Steps=747680, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5317, Total reward=484.69, Steps=747964, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5318, Total reward=454.58, Steps=748253, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5319, Total reward=99.78, Steps=748317, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5320, Total reward=517.83, Steps=748601, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5321, Total reward=299.87, Steps=748827, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5322, Total reward=224.03, Steps=749041, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5323, Total reward=198.29, Steps=749197, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5324, Total reward=123.37, Steps=749324, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5325, Total reward=480.07, Steps=749599, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5326, Total reward=464.55, Steps=749890, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5327, Total reward=408.73, Steps=750186, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5328, Total reward=322.91, Steps=750347, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5329, Total reward=168.48, Steps=750461, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5330, Total reward=18.51, Steps=750506, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5331, Total reward=462.79, Steps=750808, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5332, Total reward=81.85, Steps=750866, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5333, Total reward=438.15, Steps=751147, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5334, Total reward=409.95, Steps=751468, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5335, Total reward=175.47, Steps=751557, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5336, Total reward=439.41, Steps=751859, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5337, Total reward=295.83, Steps=752124, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5338, Total reward=208.53, Steps=752243, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5339, Total reward=124.82, Steps=752328, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5340, Total reward=304.27, Steps=752567, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5341, Total reward=86.49, Steps=752665, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5342, Total reward=180.48, Steps=752793, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5343, Total reward=234.4, Steps=752999, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5344, Total reward=172.68, Steps=753188, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5345, Total reward=453.33, Steps=753471, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5346, Total reward=415.3, Steps=753788, Training iteration=106
Training> Name=main_level/agent, Worker=0, Episode=5347, Total reward=259.38, Steps=753906, Training iteration=106
